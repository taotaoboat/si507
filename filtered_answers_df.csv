Id,OwnerUserId,CreationDate,ParentId,Score,Body
39586425,2901002.0,2016-09-20T05:37:14Z,39586398,2,"<p>You need cast column to <code>float</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow""><code>astype</code></a> and then compare columns, because <code>type</code> of values in columns is <code>string</code>. Then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mask.html"" rel=""nofollow""><code>mask</code></a> and as condition use boolean column <code>Result</code>:</p>

<pre><code>print (type(df1.ix[0,'AA']))
&lt;class 'str'&gt;

print (type(df1.ix[0,'BB']))
&lt;class 'str'&gt;

df1['Result'] = df1.AA.astype(float) == df1.BB.astype(float)
df1.BB = df1.BB.mask(df1.Result,df1.AA)
print (df1)
       AA      BB Result
0  -14.35  -14.35   True
1   632.0   632.0   True
2   619.5   619.5   True
3  352.35  352.35   True
4   347.7   347.7   True
5     100     200  False
</code></pre>

<p>Another solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ix.html"" rel=""nofollow""><code>ix</code></a>:</p>

<pre><code>df1['Result'] = df1.AA.astype(float) == df1.BB.astype(float)
df1.ix[df1.Result, 'BB'] = df1.AA
print (df1)
       AA      BB Result
0  -14.35  -14.35   True
1   632.0   632.0   True
2   619.5   619.5   True
3  352.35  352.35   True
4   347.7   347.7   True
5     100     200  False
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>#len(df) = 6k
df1 = pd.concat([df1]*1000).reset_index(drop=True)

In [31]: %timeit df1.ix[df1.Result, 'BB'] = df1.AA
The slowest run took 4.88 times longer than the fastest. This could mean that an intermediate result is being cached.
1000 loops, best of 3: 1.19 ms per loop

In [33]: %timeit df1.BB = df1.BB.mask(df1.Result,df1.AA)
1000 loops, best of 3: 900 Âµs per loop
</code></pre>
"
39587314,613246.0,2016-09-20T06:40:42Z,39585328,0,"<p>The <a href=""https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP"" rel=""nofollow"">numpy_indexed</a> package (disclaimer: I am its author) has efficient functionality for doing things like this (and related functionality):</p>

<pre><code>import numpy_indexed as npi
uniques = npi.intersection(x, y)
</code></pre>

<p>Note that this solution does not use hashing, but bitwise equality of the elements of the sequence; so no risk of hash collisions, and likely a lot faster in practice.</p>
"
39590286,1433392.0,2016-09-20T09:21:53Z,39590187,5,"<p>It means it will select the lastest version of the package, greater or equal to 0.6.10, but still in the 0.6.* version, so it won't download the 0.7.0 for example. It ensures you will get security fixes but keep backward-compatibility, if the package maintener respects the semantic versionning (which state that breaking changes should occurs only in major versions).</p>

<p>Or, as said by the PEP 440:</p>

<blockquote>
  <p>For a given release identifier V.N , the compatible release clause is approximately equivalent to the pair of comparison clauses:</p>
  
  <p><code>&gt;= V.N, == V.*</code></p>
</blockquote>

<ul>
<li><a href=""https://www.python.org/dev/peps/pep-0440/#compatible-release"">Definition in the PEP 440</a></li>
<li><a href=""https://pip.pypa.io/en/stable/reference/pip_install/#example-requirements-file"">Complete example here in the documentation</a></li>
</ul>
"
39590317,1584386.0,2016-09-20T09:23:10Z,39590187,1,"<blockquote>
  <p>A compatible release clause consists of the compatible release operator ~= and a version identifier. It matches any candidate version that is expected to be compatible with the specified version.</p>
</blockquote>

<p>You can read more here: <a href=""https://www.python.org/dev/peps/pep-0440/#compatible-release"" rel=""nofollow"">https://www.python.org/dev/peps/pep-0440/#compatible-release</a></p>
"
39590393,,2016-09-20T09:26:57Z,39590187,2,"<p>That's the 'compatible release' <a href=""https://www.python.org/dev/peps/pep-0440/#version-specifiers"" rel=""nofollow"">version specifier</a>.</p>

<p>It's equivalent to: <code>mock-django &gt;= 0.6.10, == 0.6.*</code>, and is a tidy way of matching a version which is expected to be compatible. In plain English, it's a bit like saying: ""I need a version of mock-django which is at least as new as 0.6.10, but not so new that it isn't compatible with it.""</p>

<p>If you're not sure about all this version number stuff, a quick look at the PEP440 <a href=""https://www.python.org/dev/peps/pep-0440/#version-scheme"" rel=""nofollow"">version scheme</a> should sort you out!</p>
"
39590412,671150.0,2016-09-20T09:27:48Z,39590187,1,"<p>~= means a compatible version. Not less than 0.6.10 and higher (0.6.*).</p>
"
39592089,5688175.0,2016-09-20T10:46:21Z,39591831,2,"<p>The solution was, and I do not know why, to cast inputs to numpy.float:</p>

<pre><code>m_1 = np.float32(m_1)
m_2 = np.float32(m_2)
s_1 = np.float32(s_1)
s_2 = np.float32(s_2)
</code></pre>
"
39592711,682355.0,2016-09-20T11:19:40Z,39585328,0,"<p>Use np.unique's return_index property to return flags for the unique values given by in1d</p>

<p>code:</p>

<pre><code>import numpy as np
import hashlib
x=np.array([[1, 2, 3],[1, 2, 3], [4, 5, 6], [7, 8, 9]])
y=np.array([[1, 2, 3], [7, 8, 9]])
xhash=[hashlib.sha1(row).digest() for row in x]
yhash=[hashlib.sha1(row).digest() for row in y]
z=np.in1d(xhash,yhash)

##Use unique to get unique indices to ind1 results
_,unique=np.unique(np.array(xhash)[z],return_index=True)

##Compute indices by indexing an array of indices
idx=np.array(range(len(xhash)))
unique_idx=(np.array(idx)[z])[unique]

print('x=',x)
print('unique_idx=',unique_idx)
print('x[unique_idx]=',x[unique_idx])
</code></pre>

<p>Output:</p>

<pre><code>x= [[1 2 3]
 [1 2 3]
 [4 5 6]
 [7 8 9]]
unique_idx= [3 0]
x[unique_idx]= [[7 8 9]
 [1 2 3]]
</code></pre>
"
39594165,6207849.0,2016-09-20T12:30:29Z,39593275,1,"<p>Just slice the <code>ListVector</code>:</p>

<pre><code>%Rpull x
pd.DataFrame(data=[i[0] for i in x], columns=['X'])
</code></pre>

<p><a href=""http://i.stack.imgur.com/qNYVx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qNYVx.png"" alt=""Image""></a></p>

<p>If you want a dictionary instead:</p>

<pre><code>dict([[i,j[0]] for i,j in enumerate(x)])
{0: 'first', 1: 'first'}
</code></pre>
"
39594479,294017.0,2016-09-20T12:44:54Z,39593275,0,"<p>Since you created an R list (rather than, say, a data frame),
the Python object returned is a <code>ListVector</code>.</p>

<p>R lists can have duplicated names, making the conversion to a <code>dict</code> something that cannot be guaranteed to be safe. </p>

<p>Should you feel lucky, and wanting a dict, it is rather straightforward.
For example:</p>

<pre><code> from rpy2.robjects.vectors import ListVector
 l = ListVector({'a':1, 'b':2})
 d = dict(l.items())
</code></pre>
"
39597509,2380830.0,2016-09-20T15:01:07Z,39597405,1,"<p>At the time of writing, most chipsets evaluate <code>log</code> using a Taylor series coupled with a table of specific pre-computed values.</p>

<p>With the Taylor series, a number closer to 1 is slower to <em>converge</em> than a number further away from 1. That could go some way to explaining the difference in execution time observed here.</p>

<p><code>0.99</code> may also be closer to one of the tabulated values, which would also help.</p>

<p><strong>Or your observations may not even be statistically significant.</strong></p>
"
39598657,3510736.0,2016-09-20T15:52:30Z,39590942,2,"<p>If you use numpy properly, then no - it is not a consideration.</p>

<p>If you look at the <a href=""https://docs.scipy.org/doc/numpy/reference/internals.html"" rel=""nofollow"">numpy internals documentation</a>, you can see that</p>

<blockquote>
  <p>Numpy arrays consist of two major components, the raw array data (from now on, referred to as the data buffer), and the information about the raw array data. The data buffer is typically what people think of as arrays in C or Fortran, a contiguous (and fixed) block of memory containing fixed sized data items. Numpy also contains a significant set of data that describes how to interpret the data in the data buffer.</p>
</blockquote>

<p>So, irrespective of the dimensions of the array, all data is stored in a continuous buffer. Now consider</p>

<pre><code>a = np.array([1, 2, 3, 4])
</code></pre>

<p>and</p>

<pre><code>b = np.array([[1, 2], [3, 4]])
</code></pre>

<p>It is true that accessing <code>a[1]</code> requires (slightly) less operations than <code>b[1, 1]</code> (as the translation of <code>1, 1</code> to the flat index requires some calculations), but, for high performance, <a href=""https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/ch04.html"" rel=""nofollow"">vectorized operations</a> are required anyway. </p>

<p>If you want to sum all elements in the arrays, then, in both case you would use the same thing: <code>a.sum()</code>, and <code>b.sum()</code>, and the sum would be over elements in contiguous memory anyway. Conversely, if the data is inherently 2d, then you could do things like <code>b.sum(axis=1)</code> to sum over rows. Doing this yourself in a 1d array would be error prone, and not more efficient.</p>

<p>So, basically a 2d array, if it is natural for the problem just gives greater functionality, with zero or negligible overhead.</p>
"
39598726,704848.0,2016-09-20T15:56:15Z,39598618,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.is_quarter_end.html#pandas.Series.dt.is_quarter_end"" rel=""nofollow""><code>is_quarter_end</code></a> to filter the row labels:</p>

<pre><code>In [151]:
df = pd.DataFrame(np.random.randn(400,1), index= pd.date_range(start=dt.datetime(2016,1,1), periods=400))
df.loc[df.index.is_quarter_end]

Out[151]:
                   0
2016-03-31 -0.474125
2016-06-30  0.931780
2016-09-30 -0.281271
2016-12-31  0.325521
</code></pre>
"
39599390,2336654.0,2016-09-20T16:30:45Z,39599192,3,"<p>you need to use the <code>loffset</code> argument</p>

<pre><code>a.resample('15S', loffset='5S')
</code></pre>

<p><a href=""http://i.stack.imgur.com/rQK52.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rQK52.png"" alt=""enter image description here""></a></p>
"
39599454,5276797.0,2016-09-20T16:34:29Z,39599192,3,"<p>For the sake of completeness, the <code>base</code> argument works too:</p>

<pre><code>a.resample('15S', base=5).mean()
Out[4]: 
2016-05-25 00:00:35    1.0
2016-05-25 00:00:50    3.0
2016-05-25 00:01:05    4.0
2016-05-25 00:01:20    NaN
2016-05-25 00:01:35    3.0
2016-05-25 00:01:50    NaN
2016-05-25 00:02:05    5.0
Freq: 15S, dtype: float64
</code></pre>
"
39599529,509824.0,2016-09-20T16:39:11Z,39599192,4,"<p>Both @IanS and @piRSquared address the shifting of the base. As for filling <code>NaN</code>s: pandas has methods for forward-filling (<code>.ffill()</code>/<code>.pad()</code>) and backward-filling (<code>.bfill()</code>/<code>.backfill()</code>), but not for taking the mean. A quick way of doing it is by taking the mean manually:</p>

<pre><code>b = a.resample('15S', base=5)
(b.ffill() + b.bfill()) / 2
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>2016-05-25 00:00:35    1.0
2016-05-25 00:00:50    3.0
2016-05-25 00:01:05    4.0
2016-05-25 00:01:20    3.5
2016-05-25 00:01:35    3.0
2016-05-25 00:01:50    4.0
2016-05-25 00:02:05    5.0
Freq: 15S, dtype: float64
</code></pre>

<p>EDIT: I stand corrected: there is a built-in method: <code>.interpolate()</code>.</p>

<pre><code>a.resample('15S', base=5).interpolate()
</code></pre>
"
39600219,3832970.0,2016-09-20T17:21:32Z,39600161,2,"<p>You need to use a negative lookahead to restrict a more generic pattern, and a <code>re.findall</code> to find all matches.</p>

<p>Use</p>

<pre><code>res = re.findall(r'-(?!(?:aa|bb)-)(\w+)(?=-)', s)
</code></pre>

<p>or  - if your values in between hyphens can be any but a hyphen, use a negated character class <code>[^-]</code>:</p>

<pre><code>res = re.findall(r'-(?!(?:aa|bb)-)([^-]+)(?=-)', s)
</code></pre>

<p>Here is the <a href=""https://regex101.com/r/eJ4uG7/2"" rel=""nofollow"">regex demo</a>.</p>

<p><strong>Details</strong>:</p>

<ul>
<li><code>-</code> - a hyphen</li>
<li><code>(?!(?:aa|bb)-)</code> - if there is a<code>aa-</code> or <code>bb-</code> after the first hyphen, no match should be returned</li>
<li><code>(\w+)</code> - Group 1 (this value will be returned by the <code>re.findall</code> call) capturing 1 or more word chars <strong>OR</strong> <code>[^-]+</code> - 1 or more characters other than <code>-</code></li>
<li><code>(?=-)</code> - there must be a <code>-</code> after the word chars. The lookahead is required here to ensure overlapping matches (as this hyphen will be a starting point for the next match).</li>
</ul>

<p><a href=""http://ideone.com/qPZOny"" rel=""nofollow"">Python demo</a>:</p>

<pre><code>import re
p = re.compile(r'-(?!(?:aa|bb)-)([^-]+)(?=-)')
s = ""-a-bc-aa-def-bb-ghij-""
print(p.findall(s)) # =&gt; ['a', 'bc', 'def', 'ghij']
</code></pre>
"
39600224,3150943.0,2016-09-20T17:21:46Z,39600161,6,"<p>You can make use of negative look aheads.</p>

<p>For example,</p>

<pre><code>&gt;&gt;&gt; re.findall(r'-(?!aa|bb)([^-]+)', string)
['a', 'bc', 'def', 'ghij']
</code></pre>

<hr>

<ul>
<li><p><code>-</code> Matches <code>-</code></p></li>
<li><p><code>(?!aa|bb)</code> Negative lookahead, checks if <code>-</code> is not followed by <code>aa</code> or <code>bb</code></p></li>
<li><p><code>([^-]+)</code> Matches ony or more character other than <code>-</code></p></li>
</ul>

<hr>

<p><strong>Edit</strong></p>

<p>The above regex will not match those which start with <code>aa</code> or <code>bb</code>, for example like <code>-aabc-</code>. To take care of that we can add <code>-</code> to the lookaheads like, </p>

<pre><code>&gt;&gt;&gt; re.findall(r'-(?!aa-|bb-)([^-]+)', string)
</code></pre>
"
39600891,2357112.0,2016-09-20T18:05:22Z,39600833,2,"<p>That's valid syntax, so you didn't get a SyntaxError. It's just not a meaningful or supported operation on Python lists. Similarly, <code>""5"" + fish</code> isn't a SyntaxError, <code>1/0</code> isn't a SyntaxError, and <code>I.am.a.monkey</code> isn't a SyntaxError.</p>

<p>You can't just expect all syntactically valid expressions to be meaningful.</p>
"
39600904,2988730.0,2016-09-20T18:06:38Z,39600833,2,"<p>A <code>slice_list</code> should contain as many ""dimensions"" as the object being indexed. The multi-dimensional capability is not used by any Python library object that I am aware of, but you can test it easily with <code>numpy</code>:</p>

<pre><code>import numpy as np
a = np.array([[1, 2], [3, 4]])
a[0:1, 0]
</code></pre>

<p>There are a number of such features in the Python language that are not used directly in the main library. The <code>__matmul__</code> magic method (<code>@</code> operator) is another example.</p>
"
39600980,932593.0,2016-09-20T18:10:58Z,39600161,0,"<p>Although a regex solution was asked for, I would argue that this problem can be solved easier with simpler python functions, namely string splitting and filtering:</p>

<pre><code>input_list = ""-a-bc-aa-def-bb-ghij-""
exclude = set([""aa"", ""bb""])
result = [s for s in input_list.split('-')[1:-1] if s not in exclude]
</code></pre>

<p>This solution has the additional advantage that <code>result</code> could also be turned into a generator and the result list does not need to be constructed explicitly. </p>
"
39601204,4952130.0,2016-09-20T18:23:43Z,39600833,1,"<p>This isn't a syntax issue hence no <code>SyntaxError</code>, <em>this syntax is totally supported</em>. <code>list</code>'s just don't know what to do with your slices. Take for example a dummy class that does <em>nothing</em> but define <code>__getitem__</code> that receives the contents of subscriptions <code>[]</code>:</p>

<pre><code>class DummySub:
    def __getitem__(self, arg):
        print(arg)

f = DummySub()
</code></pre>

<p>It just prints its <code>arg</code>. We can supply slices, as permitted by the grammar, but, <em>it's up to the implementing object to decide if these are an operation</em> that's supported and act on them (like <code>nparray</code>s do) or not (and raise a <code>TypeError</code>):</p>

<pre><code>f[1:2:3, 4:4:4]
(slice(1, 2, 3), slice(4, 4, 4))
</code></pre>

<p>Heck:</p>

<pre><code>f[1:2:3, 4:5:6, 7:8:9, ...]  # totally valid
(slice(1, 2, 3), slice(4, 5, 6), slice(7, 8, 9), Ellipsis)
</code></pre>

<p>By reading further on in the <a href=""https://docs.python.org/3/reference/expressions.html#slicings"" rel=""nofollow""><em>reference for slicings</em></a> you should see that:</p>

<blockquote>
  <p>The semantics for a slicing are as follows. The primary is indexed (using the same <code>__getitem__()</code> method as normal subscription) with a key that is constructed from the slice list, as follows. <em>If the slice list contains at least one comma, the key is a tuple containing the conversion of the slice items</em>; otherwise, the conversion of the lone slice item is the key. </p>
</blockquote>

<p><sup><em>(emphasis mine)</em></sup></p>
"
39601454,1126841.0,2016-09-20T18:38:42Z,39600833,2,"<p>Note that the grammar is structured this way to allow two things:</p>

<ol>
<li><p>A family of <code>slice</code> literals:</p>

<ol>
<li><code>x:y:z == slice(x, y, z)</code></li>
<li><code>x:y == slice(x, y, None)</code></li>
<li><code>x: == slice(x, None, None)</code></li>
<li><code>x::z == slice(x, None, z)</code></li>
<li><code>::z == slice(None, None, z)</code></li>
<li><code>:y:z == slice(None, y, z)</code></li>
<li><code>:: == slice(None, None, None)</code></li>
<li><code>:y: == slice(None, y, None)</code></li>
</ol>

<p>There are a few other patterns possible (<code>x:y:</code>, <code>:y</code>, etc), but each
is a variation on one of the above.</p></li>
<li><p>Slice literals may <em>only</em> be used inside <code>[...]</code>, not in any arbitrary expression.</p></li>
</ol>

<p>Otherwise, the comma-separate list is treated like any other tuple. When you write an expression like <code>f[1, 2:3, 5::7]</code>, then <code>f.__getitem__</code> receives a tuple <code>(1, slice(2, 3, None), slice(5, None, 7)</code> as its argument. What <code>f.__getitem__</code> <em>does</em> with that argument is entirely up to <code>type(f)</code>'s implementation of <code>__getitem__</code>. For instance, lists and strings only accept <code>int</code> and <code>slice</code> values as arguments, and dicts only accept hashable values.</p>
"
39602108,2285236.0,2016-09-20T19:14:33Z,39602004,9,"<p>You can do a similar thing with <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html"" rel=""nofollow"">numpy's isclose</a>:</p>

<pre><code>df[np.isclose(df['A'].values[:, None], [3, 6], atol=.5).any(axis=1)]
Out: 
     A    B
1  6.0  2.0
2  3.3  3.2
</code></pre>

<hr>

<p>np.isclose returns this:</p>

<pre><code>np.isclose(df['A'].values[:, None], [3, 6], atol=.5)
Out: 
array([[False, False],
       [False,  True],
       [ True, False],
       [False, False]], dtype=bool)
</code></pre>

<p>It is a pairwise comparison of <code>df['A']</code>'s elements and <code>[3, 6]</code> (that's why we needed <code>df['A'].values[: None]</code> - for broadcasting). Since you are looking for whether it is close to any one of them in the list, we call <code>.any(axis=1)</code> at the end.</p>

<hr>

<p>For multiple columns, change the slice a little bit:</p>

<pre><code>mask = np.isclose(df[['A', 'B']].values[:, :, None], [3, 6], atol=0.5).any(axis=(1, 2))
mask
Out: array([False,  True,  True, False], dtype=bool)
</code></pre>

<p>You can use this mask to slice the DataFrame (i.e. <code>df[mask]</code>)</p>

<hr>

<p>If you want to compare <code>df['A']</code> and <code>df['B']</code> (and possible other columns) with different vectors, you can create two different masks:</p>

<pre><code>mask1 = np.isclose(df['A'].values[:, None], [1, 2, 3], atol=.5).any(axis=1)
mask2 = np.isclose(df['B'].values[:, None], [4, 5], atol=.5).any(axis=1)
mask3 = ...
</code></pre>

<p>Then slice:</p>

<pre><code>df[mask1 &amp; mask2]  # or df[mask1 &amp; mask2 &amp; mask3 &amp; ...]
</code></pre>
"
39602862,704848.0,2016-09-20T20:03:06Z,39602824,2,"<p>You don't need to pass <code>regex=True</code> here, as this will look for partial matches, as you''re after exact matches just pass the params as separate args:</p>

<pre><code>In [7]:
df['prod_type'] = df['prod_type'].replace('respon' ,'responsvie')
df['prod_type'] = df['prod_type'].replace('r', 'responsive')
df

Out[7]:
    prod_type
0  responsive
1  responsive
2  responsvie
3  responsive
4  responsvie
5  responsive
6  responsive
</code></pre>
"
39602902,2901002.0,2016-09-20T20:06:18Z,39602824,3,"<p>Solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html"" rel=""nofollow""><code>replace</code></a> by <code>dictionary</code>:</p>

<pre><code>df['prod_type'] = df['prod_type'].replace({'respon':'responsive', 'r':'responsive'})
print (df)
    prod_type
0  responsive
1  responsive
2  responsive
3  responsive
4  responsive
5  responsive
6  responsive
</code></pre>

<p>If need set all values in column to some <code>string</code>:</p>

<pre><code>df['prod_type'] = 'responsive' 
</code></pre>
"
39603224,6005062.0,2016-09-20T20:30:55Z,39602824,1,"<p>Other solution in case all items from <code>df['prod_type']</code> will be the same:</p>

<pre><code>df['prod_type'] = ['responsive' for item in df['prod_type']]
In[0]: df
Out[0]:
prod_type
0  responsive
1  responsive
2  responsive
3  responsive
4  responsive
5  responsive
6  responsive
</code></pre>
"
39603437,2901002.0,2016-09-20T20:46:07Z,39603399,2,"<p>You need convert <code>string</code> columns to <code>float</code> first:</p>

<pre><code>#add parameter parse_dates for convert to datetime first column
df=pd.read_csv('data.csv', index_col=0, parse_dates=[0])

df['Temp1'] = df.Temp1.astype(float)
df['Temp2'] = df.Temp2.astype(float)

df_avg = df.resample('D', how = 'mean')
</code></pre>

<hr>

<p>If <code>astype</code> return <code>error</code>, problem is there are some non numeric values. So you need use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow""><code>to_numeric</code></a> with <code>errors='coerce'</code> - then all 'problematic' values are converted to <code>NaN</code>:</p>

<pre><code>df['Temp1'] = pd.to_numeric(df.Temp1, errors='coerce')
df['Temp2'] = pd.to_numeric(df.Temp2, errors='coerce')
</code></pre>

<p>You can also check all rows with problematic values with <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>print df[pd.to_numeric(df.Temp1, errors='coerce').isnull()]
print df[pd.to_numeric(df.Temp2, errors='coerce').isnull()]
</code></pre>
"
39603504,674039.0,2016-09-20T20:51:07Z,39603391,27,"<p>Just use a plain old for loop:</p>

<pre><code>results = {}
for function in [check_a, check_b, ...]:
    results[function.__name__] = result = function()
    if not result:
        break
</code></pre>

<p>The results will be a mapping of the function name to their return values, and you can do what you want with the values after the loop breaks.  </p>

<p>Use an <code>else</code> clause on the for loop if you want special handling for the case where all of the functions have returned truthy results.  </p>
"
39603506,400617.0,2016-09-20T20:51:16Z,39603391,9,"<p>Write a function that takes an iterable of functions to run.  Call each one and append the result to a list, or return <code>None</code> if the result is <code>False</code>.  Either the function will stop calling further checks after one fails, or it will return the results of all the checks.</p>

<pre><code>def all_or_none(checks, *args, **kwargs):
    out = []

    for check in checks:
        rv = check(*args, **kwargs)

        if not rv:
            return None

        out.append(rv)

    return out
</code></pre>



<pre><code>rv = all_or_none((check_a, check_b, check_c))

# rv is a list if all checks passed, otherwise None
if rv is not None:
    return rv
</code></pre>



<pre><code>def check_a(obj):
    ...

def check_b(obj):
    ...

# pass arguments to each check, useful for writing reusable checks
rv = all_or_none((check_a, check_b), obj=my_object)
</code></pre>
"
39603512,3159288.0,2016-09-20T20:51:32Z,39603391,-2,"<p>Try this:</p>

<pre><code>mapping = {'a': assign_a(), 'b': assign_b()}
if None not in mapping.values():
    return mapping
</code></pre>

<p>Where <code>assign_a</code> and <code>assign_b</code> are of the form:</p>

<pre><code>def assign_&lt;variable&gt;():
    if condition:
        return value
    else:
        return None
</code></pre>
"
39603516,3809375.0,2016-09-20T20:51:46Z,39603391,3,"<p>You could use either a list or an OrderedDict, using a for loop would serve the purpose of emulating short circuiting.</p>

<pre><code>from collections import OrderedDict


def check_a():
    return ""A""


def check_b():
    return ""B""


def check_c():
    return ""C""


def check_d():
    return False


def method1(*args):
    results = []
    for i, f in enumerate(args):
        value = f()
        results.append(value)
        if not value:
            return None

    return results


def method2(*args):
    results = OrderedDict()

    for f in args:
        results[f.__name__] = result = f()
        if not result:
            return None

    return results

# Case 1, it should return check_a, check_b, check_c
for m in [method1, method2]:
    print(m(check_a, check_b, check_c))

# Case 1, it should return None
for m in [method1, method2]:
    print(m(check_a, check_b, check_d, check_c))
</code></pre>
"
39603737,3661518.0,2016-09-20T21:07:49Z,39603391,-3,"<p>If i understand correctly you don't need skip the last functions because if the the first condition fails, the others will not be evaluated. </p>

<p>For me your code:</p>

<pre><code>a = check_a()
b = check_b()
c = check_c()
....
if a and b and c and ...
    return (a, b, c, ...)
</code></pre>

<p>is right. If a fails, b and c will not be evaluated (redundant but necessary).</p>
"
39603747,1634191.0,2016-09-20T21:08:32Z,39603571,1,"<p>In your <code>for</code> loop, try adding something akin to</p>

<pre><code>for uid, row in staticData.iterrows():
    if uid not in stockData.columns:
        stockData[uid + ""_FX""] = np.nan
        stockData[uid + ""_LAST_ADJ""] = np.nan

    # continue with what you have:
    # no longer needed
    #stockData[uid+""_LAST_ADJ_EUR""] = np.nan

    stockData[uid+""_LAST_ADJ_EUR""] = stockData[uid+""_FX""]*stockData[uid+""_LAST_ADJ""]
</code></pre>

<p>While doing it inside the <code>for</code> loop is probably most efficient, you could also do it all at once like:</p>

<pre><code>stockData = pd.concat([stockData, pd.DataFrame(columns=staticData.index)])
</code></pre>

<p>For example:</p>

<pre><code>df = pd.DataFrame(np.random.rand(10, 3), columns=list('abc'))
          a         b         c
0  0.627303  0.183463  0.714470
1  0.458124  0.135907  0.515340
2  0.629373  0.725247  0.306275
3  0.113927  0.259965  0.996407
4  0.321131  0.734002  0.766044
5  0.740858  0.238741  0.531810
6  0.063990  0.974056  0.178260
7  0.977651  0.047287  0.435681
8  0.972060  0.606288  0.600896
9  0.250377  0.807237  0.153419

pd.concat([df, pd.DataFrame(columns=list('abcde'))])
          a         b         c    d    e
0  0.627303  0.183463  0.714470  NaN  NaN
1  0.458124  0.135907  0.515340  NaN  NaN
2  0.629373  0.725247  0.306275  NaN  NaN
3  0.113927  0.259965  0.996407  NaN  NaN
4  0.321131  0.734002  0.766044  NaN  NaN
5  0.740858  0.238741  0.531810  NaN  NaN
6  0.063990  0.974056  0.178260  NaN  NaN
7  0.977651  0.047287  0.435681  NaN  NaN
8  0.972060  0.606288  0.600896  NaN  NaN
9  0.250377  0.807237  0.153419  NaN  NaN
</code></pre>
"
39603963,2336654.0,2016-09-20T21:25:04Z,39603571,2,"<p>I'd start by parsing your columns into a multiindex</p>

<pre><code>tups = df.columns.to_series() \
         .str.extract(r'(.*)_(LAST_ADJ|FX)', expand=False) \
         .apply(tuple, 1).tolist()

df.columns = pd.MultiIndex.from_tuples(tups).swaplevel(0, 1)

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/1dSlS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1dSlS.png"" alt=""enter image description here""></a></p>

<p>Then multiplication becomes simple</p>

<pre><code>df.LAST_ADJ * df.FX
</code></pre>

<p><a href=""http://i.stack.imgur.com/OMDls.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OMDls.png"" alt=""enter image description here""></a></p>

<p>Tricky part for me is inserting it back with <code>'EUR'</code>.  I did this</p>

<pre><code>pd.concat([df, pd.concat([df.LAST_ADJ.mul(df.FX)], axis=1, keys=['EUR'])], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/5SiHG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5SiHG.png"" alt=""enter image description here""></a></p>
"
39604756,2336654.0,2016-09-20T22:33:56Z,39604586,2,"<p>have <code>function</code> return a <code>pd.Series</code> instead.  Returning a list is making apply try to fit the list into the existing row.  Returning a <code>pd.Series</code> convinces pandas of something different.</p>

<pre><code>def function(row):
    return pd.Series([row.A, row.A/2])


df2 = pd.DataFrame({'A' : np.random.randn(8),
                    'B' : pd.date_range('1/1/2011', periods=8, freq='H'),
                    'C' : np.random.randn(8)})
df2[['D','E']] = df2.apply(function, axis=1)

df2
</code></pre>

<p><a href=""http://i.stack.imgur.com/oreC4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oreC4.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Attempt to explain</em></strong>  </p>

<pre><code>s = pd.Series([1, 2, 3])
s

0    1
1    2
2    3
dtype: int64
</code></pre>

<hr>

<pre><code>s.loc[:] = [4, 5, 6]
s

0    4
1    5
2    6
dtype: int64
</code></pre>

<hr>

<pre><code>s.loc[:] = [7, 8]
</code></pre>

<blockquote>
  <p>ValueError: cannot set using a slice indexer with a different length than the value</p>
</blockquote>
"
39605384,1011859.0,2016-09-20T23:44:30Z,39604903,2,"<p>I chose to rewrite a bit this answer, because I had failed to properly look at the problem's context. As the other answers and comments tell, you code lacks thread-safety. </p>

<p>The best way to fix this is to try to really think ""in threads"", to restrict yourself to only use objects living in the same thread, or functions that are known as ""threadsafe"".</p>

<p>Throwing in some signals and slots will help, but maybe you want to think back a bit to your original problem. In your current code, each time a button is pressed, <strong>a new thread in launched</strong>, that will, every 10 seconds, do 2 things :
 - Read some text from <code>self.text</code>
 - Append it to <code>self.bilgiler</code></p>

<p>Both of these operations are non-threadsafe, and <strong>must</strong> be called from the thread that owns these objects (the main thread). You want to make the worker threads ""schedule &amp; wait"" the read &amp; append oeprations, instead of simply ""executing"" them.</p>

<p>I recommend using the other answer (the thread halting problem is automatically fixed by using proper QThreads that integrate well with Qt's event loop), which would make you use a cleaner approach, more integrated with Qt.</p>

<p>You may also want to rethink your problem, because maybe there is a simpler approach to your problem, for example : not spawning threads each time <code>bilgi_cek</code> is clicked, or using <code>Queue</code> objects so that your worker is completely agnostic of your GUI, and only interact with it using threadsafe objects.</p>

<p>Good luck, sorry if I caused any confusion. My original answer is still available <a href=""http://stackoverflow.com/revisions/39605384/2"">here</a>. I think it would be wise to mark the other answer as the valid answer for this question.</p>
"
39605495,1048572.0,2016-09-20T23:57:59Z,39603391,6,"<p>In other languages that did have <a href=""http://stackoverflow.com/q/4869770/1048572"">assignments as expressions</a> you would be able to use</p>

<pre><code>if (a = check_a()) and (b = check_b()) and (c = check_c()):
</code></pre>

<p>but Python is no such language. Still, we can circumvent the restriction and emulate that behaviour:</p>

<pre><code>result = []
def put(value):
    result.append(value)
    return value

if put(check_a()) and put(check_b()) and put(check_c()):
    # if you need them as variables, you could do
    # (a, b, c) = result
    # but you just want
    return tuple(result)
</code></pre>

<p>This might loosen the connection between the variables and function calls a bit too much, so if you want to do lots of separate things with the variables, instead of using the <code>result</code> elements in the order they were put in the list, I would rather avoid this approach. Still, it might be quicker and shorter than some loop.</p>
"
39605651,2063361.0,2016-09-21T00:21:43Z,39605640,6,"<p>Your JSON is the <code>list</code> of <code>dict</code> objects. By doing <code>j[1]</code>, you are accessing the item in the list at index <code>1</code>. In order to get all the records, you need to iterate all the elements of the list as:</p>

<pre><code>for item in j:
    print item['name']
</code></pre>

<p>where <code>j</code> is result of <code>j = json.loads(the_page)</code> as is mentioned in your answer</p>
"
39605815,364696.0,2016-09-21T00:41:57Z,39605640,2,"<p>Slightly nicer for mass-conversions than repeated <code>dict</code> lookup is using <a href=""https://docs.python.org/3/library/operator.html#operator.itemgetter"" rel=""nofollow""><code>operator.itemgetter</code></a>:</p>

<pre><code>from future_builtins import map  # Only on Py2, to get lazy, generator based map
from operator import itemgetter

for name, phone_number in map(itemgetter('name', 'phone_number'), j):
    print name, phone_number
</code></pre>

<p>If you needed to look up individual things as needed (so you didn't always need <code>name</code> or <code>phone_number</code>), then regular <code>dict</code> lookups would make sense, this just optimizes the case where you're always retrieving the same set of items by pushing work to builtin functions (which, on the CPython reference interpreter, are implemented in C, so they run a bit faster than hand-rolled code). Using a generator based <code>map</code> isn't strictly necessary, but it avoids making (potentially large) temporary <code>list</code>s when you're just going to iterate the result anyway.</p>

<p>It's basically just a faster version of:</p>

<pre><code>for emp in j:
    name, phone_number = emp['name'], emp['phone_number']
    print name, phone_number
</code></pre>
"
39605897,4795263.0,2016-09-21T00:51:34Z,39605839,3,"<p>The OP approach is <code>O(n*m)</code>, where <code>n</code> is the number of genes and <code>m</code> is the number of TEs. Rather than test each gene against each TE as in the OP, this approach leverages the ordered nature of the genes and TEs, and the specified rules of matching, to look at each gene and TE only once, except for the gene look-ahead described in <code>3.</code> below. This approach is <code>O(n + m)</code> provided that the average gene look-ahead is small relative to <code>n</code>. The sequence in which each gene and TE is visited is described by:   </p>

<ol>
<li>After we finish testing the current TE against the current gene, we
get the next TE.</li>
<li>When the current TE's start position is past the current gene's end
position, we get the next gene until it's not.</li>
<li>If we find a matching TE/gene pair, we test each successive gene
against the current TE until there is no match, leaving the current
gene unchanged.</li>
</ol>

<hr>

<pre><code>def get_TE_in_TSS(genes, TEs):
    TE_in_TSS = []
    gene_pos, TE_pos = 0, 0
    gene_count, TE_count = len(genes), len(TEs)
    while gene_pos &lt; gene_count:
        while (TE_pos &lt; TE_count) and (TEs[TE_pos][1] &lt;= genes[gene_pos][2]):
            match_gene_pos = gene_pos
            while (match_gene_pos &lt; gene_count) and (TEs[TE_pos][2] &gt;= genes[match_gene_pos][1]):
                TE_in_TSS.append((genes[match_gene_pos][0], TEs[TE_pos][0],
                                  TEs[TE_pos][1], TEs[TE_pos][2]))
                match_gene_pos += 1 # look ahead to see if this TE matches the next gene
            TE_pos += 1
        gene_pos += 1
    return TE_in_TSS
</code></pre>

<hr>

<p><em>performance</em>, as reported by OP:</p>

<pre><code>1 second (compared to 2 days + for OP code) for 801,948 TEs, 6,007 genes
</code></pre>

<hr>

<p><em>test data:</em></p>

<pre><code>genes = (('HTR3A', 7, 9), ('ADAMTSL4', 10,100), ('THSD4',2000, 2800), ('PAPLN', 2850, 3000))
TEs = (('a', 10, 11), ('b', 13, 17), ('c', 50, 2500), ('d', 2550, 2700),
       ('e', 2800, 2900), ('f', 9999, 9999)) 
TE_in_TSS = get_TE_in_TSS(genes, TEs)
print(TE_in_TSS)
</code></pre>

<p><em>Output:</em></p>

<pre><code>[('ADAMTSL4', 'a', 10, 11), ('ADAMTSL4', 'b', 13, 17), ('ADAMTSL4', 'c', 50, 2500), 
 ('THSD4', 'c', 50, 2500), ('THSD4', 'd', 2550, 2700), ('THSD4', 'e', 2800, 2900), 
 ('PAPLN', 'e', 2800, 2900)]
</code></pre>

<hr>

<p>Note that the first 9 comments on this post refer to a more efficient <code>O(n * m)</code> approach that became outdated by clarified specs. </p>
"
39606699,6005451.0,2016-09-21T02:43:43Z,39605839,1,"<p>Here is a solution using multi-threading, comparing code used for nested loop methods.</p>

<p>I created two csv's, one with 8k rows and one 800 rows of (int, float1,float2) random generated numbers and import as below:</p>

<pre><code>import time
import itertools 

start = time.time()

def f((TE_id, TEstart, TEend)):
    a=[]
    for gene, gstart, gend in gfftree['chr1']:
        if (gstart &lt;= TEstart &lt;=gend) or (gstart&lt;=TEend &lt;=gend):
            a.append((gene,TE_id,TEstart,TEend))
    return a

'''
#slow
TEinTSS = []
for TE_id, TEstart, TEend in TElocation['chr1']:
    for gene, gstart, gend in gfftree['chr1']:
        if (gstart &lt;= TEstart &lt;=gend) or (gstart&lt;=TEend &lt;=gend):
            TEinTSS.append((gene,TE_id,TEstart,TEend))
print len(TEinTSS)
print time.time()-start

#faster
TEinTSS = []
for things in TElocation['chr1']:
    TEinTSS.extend(f(things))
print len(TEinTSS)
print time.time()-start
'''

#fastest (especially with multi-core, multithreading)
from multiprocessing import Pool

if __name__ == '__main__':
    p=Pool()
    TEinTSS = list(itertools.chain.from_iterable(p.imap_unordered(f, b)))   
    print len(TEinTSS)
    print time.time() - start
</code></pre>
"
39607916,1518924.0,2016-09-21T05:02:10Z,39607540,3,"<p>You can use the <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html"" rel=""nofollow""><code>groupby</code></a> method.</p>

<p>Example - </p>

<pre><code>In [36]: df.groupby([""country""]).count().sort_values([""accident""], ascending=False).rename(columns={""accident"" : ""Sum of accidents""}).reset_index()
Out[36]:
    country  Sum of accidents
0   England                 3
1       USA                 3
2   Germany                 1
3  Thailand                 1
</code></pre>

<p>Explanation - </p>

<pre><code>df.groupby([""country""]).                               # Group by country
    count().                                           # Aggregation function which counts the number of occurences of country
    sort_values(                                       # Sorting it 
        [""accident""],                                  
        ascending=False).        
    rename(columns={""accident"" : ""Sum of accidents""}). # Renaming the columns
    reset_index()                                      # Resetting the index, it takes the country as the index if you don't do this.
</code></pre>
"
39608197,2336654.0,2016-09-21T05:28:59Z,39607540,5,"<p><strong><em>Option 1</em></strong><br>
Use <code>value_counts</code></p>

<pre><code>df.Country.value_counts().reset_index(name='Sum of Accidents')
</code></pre>

<p><a href=""http://i.stack.imgur.com/G0Gii.png""><img src=""http://i.stack.imgur.com/G0Gii.png"" alt=""enter image description here""></a></p>

<p><strong><em>Option 2</em></strong><br>
Use <code>groupby</code> then <code>size</code></p>

<pre><code>df.groupby('Country').size().sort_values(ascending=False) \
  .reset_index(name='Sum of Accidents')
</code></pre>

<p><a href=""http://i.stack.imgur.com/K7RDt.png""><img src=""http://i.stack.imgur.com/K7RDt.png"" alt=""enter image description here""></a></p>
"
39608394,2901002.0,2016-09-21T05:45:34Z,39608282,2,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow""><code>to_numeric</code></a>:</p>

<pre><code>df = pd.DataFrame({'AltID':['123456','ABC12345','123456'],
                   'B':[4,5,6]})

print (df)
      AltID  B
0    123456  4
1  ABC12345  5
2    123456  6

df.ix[df.AltID.str.isdigit(), 'AltID']  = pd.to_numeric(df.AltID, errors='coerce')

print (df)
      AltID  B
0    123456  4
1  ABC12345  5
2    123456  6

print (df['AltID'].apply(type))
0    &lt;class 'float'&gt;
1      &lt;class 'str'&gt;
2    &lt;class 'float'&gt;
Name: AltID, dtype: object
</code></pre>
"
39608396,2336654.0,2016-09-21T05:45:41Z,39608282,2,"<p>Use <code>apply</code> and <code>pd.to_numeric</code> with parameter <code>errors='ignore'</code></p>

<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>s = pd.Series(['12345', 'abc12', '456', '65hg', 54, '12-31-2001'])

s.apply(pd.to_numeric, errors='ignore')

0         12345
1         abc12
2           456
3          65hg
4            54
5    12-31-2001
dtype: object
</code></pre>

<hr>

<p>Notice the types</p>

<pre><code>s.apply(pd.to_numeric, errors='ignore').apply(type)

0    &lt;type 'numpy.int64'&gt;
1            &lt;type 'str'&gt;
2    &lt;type 'numpy.int64'&gt;
3            &lt;type 'str'&gt;
4            &lt;type 'int'&gt;
5            &lt;type 'str'&gt;
dtype: object
</code></pre>
"
39609439,2901002.0,2016-09-21T06:56:35Z,39609391,3,"<p>You need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a> by difference of first value of column <code>dt</code> converted to <code>hour</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow""><code>astype</code></a>:</p>

<pre><code>S = pd.to_datetime(df.dt)
for i, g in df.groupby([(S - S[0]).astype('timedelta64[h]')]):
        print (g.reset_index(drop=True))

               dt text
0  20160811 11:05    a
1  20160811 11:35    b
2  20160811 12:03    c
               dt text
0  20160811 12:36    d
1  20160811 12:52    e
               dt text
0  20160811 14:32    f
</code></pre>

<p><code>List comprehension</code> solution:</p>

<pre><code>S = pd.to_datetime(df.dt)

print ((S - S[0]).astype('timedelta64[h]'))
0    0.0
1    0.0
2    0.0
3    1.0
4    1.0
5    3.0
Name: dt, dtype: float64

L = [g.reset_index(drop=True) for i, g in df.groupby([(S - S[0]).astype('timedelta64[h]')])]

print (L[0])
               dt text
0  20160811 11:05    a
1  20160811 11:35    b
2  20160811 12:03    c

print (L[1])
               dt text
0  20160811 12:36    d
1  20160811 12:52    e

print (L[2])
               dt text
0  20160811 14:32    f
</code></pre>

<hr>

<p>Old solution, which split by <code>hour</code>:</p>

<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.hour.html"" rel=""nofollow""><code>dt.hour</code></a>, but first need convert <code>dt</code> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html"" rel=""nofollow""><code>to_datetime</code></a>:</p>

<pre><code>for i, g in df.groupby([pd.to_datetime(df.dt).dt.hour]):
    print (g.reset_index(drop=True))

               dt text
0  20160811 11:05    a
1  20160811 11:35    b
               dt text
0  20160811 12:03    c
1  20160811 12:36    d
2  20160811 12:52    e
               dt text
0  20160811 14:32    f
</code></pre>

<p><code>List comprehension</code> solution:</p>

<pre><code>L = [g.reset_index(drop=True) for i, g in df.groupby([pd.to_datetime(df.dt).dt.hour])]

print (L[0])
               dt text
0  20160811 11:05    a
1  20160811 11:35    b

print (L[1])
               dt text
0  20160811 12:03    c
1  20160811 12:36    d
2  20160811 12:52    e

print (L[2])
               dt text
0  20160811 14:32    f
</code></pre>

<hr>

<p>Or use <code>list comprehension</code> with converting column <code>dt</code> to <code>datetime</code>:</p>

<pre><code>df.dt = pd.to_datetime(df.dt)
L =[g.reset_index(drop=True) for i, g in df.groupby([df['dt'].dt.hour])]

print (L[1])
                   dt text
0 2016-08-11 12:03:00    c
1 2016-08-11 12:36:00    d
2 2016-08-11 12:52:00    e

print (L[2])
                   dt text
0 2016-08-11 14:32:00    f
</code></pre>

<hr>

<p>If need split by <code>date</code>s and <code>hour</code>s:</p>

<pre><code>#changed dataframe for testing
print (df)
               dt text
0  20160811 11:05    a
1  20160812 11:35    b
2  20160813 12:03    c
3  20160811 12:36    d
4  20160811 12:52    e
5  20160811 14:32    f

serie = pd.to_datetime(df.dt)
for i, g in df.groupby([serie.dt.date, serie.dt.hour]):
    print (g.reset_index(drop=True))
               dt text
0  20160811 11:05    a
               dt text
0  20160811 12:36    d
1  20160811 12:52    e
               dt text
0  20160811 14:32    f
               dt text
0  20160812 11:35    b
               dt text
0  20160813 12:03    c    
</code></pre>
"
39609639,1518924.0,2016-09-21T07:07:57Z,39609426,7,"<p>You can use the <code>urllib</code> library and apply it using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html""><code>map</code></a> method of a series.
Example - </p>

<pre><code>In [23]: import urllib

In [24]: dfSearch[""str_value""].map(lambda x:urllib.unquote(x).decode('utf8'))
Out[24]:
0    Mock the Week
1              law
2        euro 2016
</code></pre>
"
39609677,1547004.0,2016-09-21T07:09:37Z,39604903,3,"<p>A few things:</p>

<ol>
<li><p>You shouldn't be calling GUI code from outside the main thread.  GUI elements are not thread-safe. <code>self.kontenjan_ara</code> updates and reads from GUI elements, it shouldn't be the target of your <code>thread</code>.</p></li>
<li><p>In almost all cases, you should use <code>QThreads</code> instead of python threads. They integrate nicely with the event and signal system in Qt.</p></li>
</ol>

<p>If you just want to run something every few seconds, you can use a <code>QTimer</code></p>

<pre><code>def __init__(self, parent=None):
    ...
    self.timer = QTimer(self)
    self.timer.timeout.connect(self.kontenjan_ara)
    self.timer.start(10000)

def kontenjan_ara(self):
    self.bilgiler.append(self.text.text())
</code></pre>

<p>If your thread operations are more computationally complex you can create a worker thread and pass data between the worker thread and the main GUI thread using signals. </p>

<pre><code>class Worker(QObject):

    work_finished = QtCore.pyqtSignal(object)

    @QtCore.pyqtSlot()
    def do_work(self):
        data = 'Text'
        while True:
            # Do something with data and pass back to main thread
            data = data + 'text'
            self.work_finished.emit(data)
            time.sleep(10)


class MyWidget(QtGui.QWidget):

    def __init__(self, ...)
        ...
        self.worker = Worker()
        self.thread = QtCore.QThread(self)
        self.worker.work_finished.connect(self.on_finished)
        self.worker.moveToThread(self.thread)
        self.thread.started.connect(self.worker.do_work)
        self.thread.start()

    @QtCore.pyqtSlot(object)
    def on_finished(self, data):
        self.bilgiler.append(data)
        ...
</code></pre>

<p>Qt will automatically kill all the subthreads when the main thread exits the event loop.</p>
"
39609723,2336654.0,2016-09-21T07:11:50Z,39609391,2,"<p>take the difference of dates with first date and group by total_seconds</p>

<pre><code>df.groupby((df.dt - df.dt[0]).dt.total_seconds() // 3600,
           as_index=False).apply(pd.DataFrame.reset_index, drop=True)
</code></pre>

<p><a href=""http://i.stack.imgur.com/9MxQ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9MxQ6.png"" alt=""enter image description here""></a></p>
"
39609961,3276974.0,2016-09-21T07:23:25Z,39603391,1,"<p>main logic:</p>

<pre><code>results = list(takewhile(lambda x: x, map(lambda x: x(), function_list)))
if len(results) == len(function_list):
  return results
</code></pre>

<p>you can learn a lot about collection transformations if you look at all methods of an api like <a href=""http://www.scala-lang.org/api/2.11.7/#scala.collection.immutable.List"" rel=""nofollow"">http://www.scala-lang.org/api/2.11.7/#scala.collection.immutable.List</a> and search/implement python equivalents</p>

<p>logic with setup and alternatives:</p>

<pre><code>import sys
if sys.version_info.major == 2:
  from collections import imap
  map = imap

def test(bool):
  def inner():
    print(bool)
    return bool
  return inner

def function_for_return():
  function_list = [test(True),test(True),test(False),test(True)]

  from itertools import takewhile

  print(""results:"")

  results = list(takewhile(lambda x:x,map(lambda x:x(),function_list)))
  if len(results) == len(function_list):
    return results

  print(results)
  #personally i prefer another syntax:
  class Iterator(object):
    def __init__(self,iterable):
      self.iterator = iter(iterable)

    def __next__(self):
      return next(self.iterator)

    def __iter__(self):
      return self

    def map(self,f):
      return Iterator(map(f,self.iterator))

    def takewhile(self,f):
      return Iterator(takewhile(f,self.iterator))

  print(""results2:"")
  results2 = list(
    Iterator(function_list)
      .map(lambda x:x())
      .takewhile(lambda x:x)    
  )

  print(results2)

  print(""with additional information"")
  function_list2 = [(test(True),""a""),(test(True),""b""),(test(False),""c""),(test(True),""d"")]
  results3 = list(
    Iterator(function_list2)
      .map(lambda x:(x[0](),x[1]))
      .takewhile(lambda x:x[0])    
  )
  print(results3)

function_for_return()
</code></pre>
"
39610678,6857180.0,2016-09-21T07:58:42Z,39603391,0,"<p>Since I can not comment ""wim"":s answer as guest, I'll just add an extra answer.
Since you want a tuple, you should collect the results in a list and then cast to tuple.</p>

<pre><code>def short_eval(*checks):
    result = []
    for check in checks:
        checked = check()
        if not checked:
            break
        result.append(checked)
    return tuple(result)

# Example
wished = short_eval(check_a, check_b, check_c)
</code></pre>
"
39611845,58635.0,2016-09-21T08:55:32Z,39603391,2,"<p>There are lots of ways to do this!  Here's another.</p>

<p>You can use a generator expression to defer the execution of the functions.  Then you can use <code>itertools.takewhile</code> to implement the short-circuiting logic by consuming items from the generator until one of them is false.</p>

<pre><code>from itertools import takewhile
functions = (check_a, check_b, check_c)
generator = (f() for f in functions)
results = tuple(takewhile(bool, generator))
if len(results) == len(functions):
    return results
</code></pre>
"
39612376,2336654.0,2016-09-21T09:18:52Z,39612300,3,"<p>You are chaining you're selectors, leading to the warning.  Consolidate the selection into one.<br>
Use <code>loc</code> instead</p>

<pre><code>DF.loc[DF['A'] == 1, 'B'] = 'X'
DF
</code></pre>

<p><a href=""http://i.stack.imgur.com/HX26a.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HX26a.png"" alt=""enter image description here""></a></p>
"
39612385,2901002.0,2016-09-21T09:19:05Z,39612300,3,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ix.html"" rel=""nofollow""><code>ix</code></a>:</p>

<pre><code>import pandas as pd
DF = pd.DataFrame({'A':[1,1,2,1,2,2,1,2,1],'B':['a','a','b','c','x','t','i','x','b']})


DF.ix[DF['A'] == 1, 'B'] = 'X'
print (DF)

0  1  X
1  1  X
2  2  b
3  1  X
4  2  x
5  2  t
6  1  X
7  2  x
8  1  X
</code></pre>

<p>Another solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mask.html"" rel=""nofollow""><code>mask</code></a>:</p>

<pre><code>DF.B = DF.B.mask(DF['A'] == 1, 'X')
print (DF)
   A  B
0  1  X
1  1  X
2  2  b
3  1  X
4  2  x
5  2  t
6  1  X
7  2  x
8  1  X
</code></pre>

<p>Nice article about <a href=""http://tomaugspurger.github.io/modern-1.html"" rel=""nofollow""><code>SettingWithCopy</code> by Tom Augspurger</a>.</p>
"
39612548,5276797.0,2016-09-21T09:26:03Z,39602785,0,"<p>If I understand correctly: after a <code>diff</code> the first row is <code>NaT</code>. In order to preserve the first row you could replace <code>NaT</code> values with something that would not be filtered out by your condition, for instance <code>0</code>.</p>

<p>Here I simply add <code>.fillna(0)</code> at the end of your first line:</p>

<pre><code>df['time_diff'] = df.groupby('Send_Customer')['Send_Time'].apply(
        lambda x : x.diff().abs()
    ).fillna(0)

df[df['time_diff'] &lt;= dt.timedelta(seconds=36000)]
</code></pre>
"
39612853,4059524.0,2016-09-21T09:39:03Z,39603391,0,"<p>You can try use <code>@lazy_function</code> decorator from <code>lazy_python</code> 
<a href=""https://pypi.python.org/pypi/lazy_python/0.2.1"" rel=""nofollow"">package</a>. Example of usage:</p>

<pre><code>from lazy import lazy_function, strict

@lazy_function
def check(a, b):
    strict(print('Call: {} {}'.format(a, b)))
    if a + b &gt; a * b:
        return '{}, {}'.format(a, b)

a = check(-1, -2)
b = check(1, 2)
c = check(-1, 2)

print('First condition')
if c and a and b: print('Ok: {}'.format((a, b)))

print('Second condition')
if c and b: print('Ok: {}'.format((c, b)))
# Output:
# First condition
# Call: -1 2
# Call: -1 -2
# Second condition
# Call: 1 2
# Ok: ('-1, 2', '1, 2')
</code></pre>
"
39613007,833402.0,2016-09-21T09:46:09Z,39603391,1,"<p>Flexible short circuiting is really best done with Exceptions. For a very simple prototype you could even just assert each check result:</p>

<pre><code>try:
    a = check_a()
    assert a
    b = check_b()
    assert b
    c = check_c()
    assert c
    return  a, b, c
except AssertionException as e:
    return None
</code></pre>

<p>You should probably raise a custom Exception instead. You could change your check_X functions to raise Exceptions themself, in an arbitrary nested way. Or you could wrap or decorate your check_X functions to raise errors on falsy return values.</p>

<p>In short, exception handling is very flexible and exactly what you are looking for, don't be afraid to use it. If you learned somewhere that exception handling is not to be used for your own flow control, this does not apply to python. Liberal use of exception handling is considered pythonic, as in <a href=""https://docs.python.org/2/glossary.html#term-eafp"" rel=""nofollow"" title=""EAFP"">EAFP</a>.</p>
"
39615548,124319.0,2016-09-21T11:38:48Z,39603391,1,"<p>If you don't need to take an arbitrary number of expressions at runtime (possibly wrapped in lambdas), you can expand your code directly into this pattern:</p>

<pre><code>def f ():
    try:
        return (&lt;a&gt; or jump(),
                &lt;b&gt; or jump(),
                &lt;c&gt; or jump())
    except NonLocalExit:
        return None
</code></pre>

<p>Where those definitions apply:</p>

<pre><code>class NonLocalExit(Exception):
    pass

def jump():
    raise NonLocalExit()
</code></pre>
"
39619388,5987.0,2016-09-21T14:26:14Z,39618943,73,"<p><code>repr</code> (and <code>str</code> in Python 3) will put out as many digits as required to make the value unambiguous. In this case the result of the multiplication <code>3*0.1</code> isn't the closest value to 0.3 (0x1.3333333333333p-2 in hex), it's actually one LSB higher (0x1.3333333333334p-2) so it needs more digits to distinguish it from 0.3.</p>

<p>On the other hand, the multiplication <code>4*0.1</code> <em>does</em> get the closest value to 0.4 (0x1.999999999999ap-2 in hex), so it doesn't need any additional digits.</p>

<p>You can verify this quite easily:</p>

<pre><code>&gt;&gt;&gt; 3*0.1 == 0.3
False
&gt;&gt;&gt; 4*0.1 == 0.4
True
</code></pre>

<p>I used hex notation above because it's nice and compact and shows the bit difference between the two values. You can do this yourself using e.g. <code>(3*0.1).hex()</code>. If you'd rather see them in all their decimal glory, here you go:</p>

<pre><code>&gt;&gt;&gt; Decimal(3*0.1)
Decimal('0.3000000000000000444089209850062616169452667236328125')
&gt;&gt;&gt; Decimal(0.3)
Decimal('0.299999999999999988897769753748434595763683319091796875')
&gt;&gt;&gt; Decimal(4*0.1)
Decimal('0.40000000000000002220446049250313080847263336181640625')
&gt;&gt;&gt; Decimal(0.4)
Decimal('0.40000000000000002220446049250313080847263336181640625')
</code></pre>
"
39619467,1204143.0,2016-09-21T14:30:11Z,39618943,287,"<p>The simple answer is because <code>3*0.1 != 0.3</code> due to quantization (roundoff) error (whereas <code>4*0.1 == 0.4</code> because multiplying by a power of two is usually an ""exact"" operation).</p>

<p>You can use the <code>.hex</code> method in Python to view the internal representation of a number (basically, the <em>exact</em> binary floating point value, rather than the base-10 approximation). This can help to explain what's going on under the hood.</p>

<pre><code>&gt;&gt;&gt; (0.1).hex()
'0x1.999999999999ap-4'
&gt;&gt;&gt; (0.3).hex()
'0x1.3333333333333p-2'
&gt;&gt;&gt; (0.1*3).hex()
'0x1.3333333333334p-2'
&gt;&gt;&gt; (0.4).hex()
'0x1.999999999999ap-2'
&gt;&gt;&gt; (0.1*4).hex()
'0x1.999999999999ap-2'
</code></pre>

<p>0.1 is 0x1.999999999999a times 2^-4. The ""a"" at the end means the digit 10 - in other words, 0.1 in binary floating point is <em>very slightly</em> larger than the ""exact"" value of 0.1 (because the final 0x0.99 is rounded up to 0x0.a). When you multiply this by 4, a power of two, the exponent shifts up (from 2^-4 to 2^-2) but the number is otherwise unchanged, so <code>4*0.1 == 0.4</code>.</p>

<p>However, when you multiply by 3, the little tiny difference between 0x0.99 and 0x0.a0 (0x0.07) magnifies into a 0x0.15 error, which shows up as a one-digit error in the last position. This causes 0.1*3 to be <em>very slightly</em> larger than the rounded value of 0.3.</p>

<p>Python 3's float <code>repr</code> is designed to be <em>round-trippable</em>, that is, the value shown should be exactly convertible into the original value. Therefore, it cannot display <code>0.3</code> and <code>0.1*3</code> exactly the same way, or the two <em>different</em> numbers would end up the same after round-tripping. Consequently, Python 3's <code>repr</code> engine chooses to display one with a slight apparent error.</p>
"
39619506,5521894.0,2016-09-21T14:32:14Z,39603391,0,"<p>This is similar to Bergi's answer but I think that answer misses the point of wanting separate functions (check_a, check_b, check_c):</p>

<pre><code>list1 = []

def check_a():
    condition = True
    a = 1
    if (condition):
        list1.append(a)
        print (""checking a"")
        return True
    else:
        return False

def check_b():
    condition = False
    b = 2
    if (condition):
        list1.append(b)
        print (""checking b"")
        return True
    else:
        return False

def check_c():
    condition = True
    c = 3
    if (condition):
        list1.append(c)
        print (""checking c"")
        return True
    else:
        return False


if check_a() and check_b() and check_c():
    # won't get here

tuple1 = tuple(list1)    
print (tuple1)    

# output is:
# checking a
# (1,)
</code></pre>

<p>Or, if you don't want to use the global list, pass a reference of a local list to each of the functions.</p>
"
39620977,4952130.0,2016-09-21T15:38:30Z,39619676,2,"<p>I'm guessing you could wrap this in a <code>genexp</code> and feed decoded lines to it:</p>

<pre><code>from contextlib import closing

with closing(requests.get(url, stream=True)) as r:
    f = (line.decode('utf-8') for line in r.iter_lines())
    reader = csv.reader(f, delimiter=',', quotechar='""')
    for row in reader:
        print(row)
</code></pre>

<p>Using some sample data in <code>3.5</code> this shuts up <code>csv.reader</code>, every line fed to it is first <code>decoded</code> in the genexp. Also, I'm using <a href=""https://docs.python.org/3/library/contextlib.html#contextlib.closing"" rel=""nofollow""><code>closing</code></a> from <a href=""https://docs.python.org/3/library/contextlib.html"" rel=""nofollow""><code>contextlib</code></a> as is <a href=""http://docs.python-requests.org/en/master/user/advanced/#body-content-workflow"" rel=""nofollow""><em>generally suggested</em></a> to automatically <code>close</code> the responce.</p>
"
39621672,6855437.0,2016-09-21T16:16:03Z,39620351,0,"<p>Look into using selenium Library! I have scraped multiple websites with this library. People state that it is slow, but for my purposes it works great.</p>

<p>Also if your kinda new to web scraping, look into what Xpaths are for scraping elements that would otherwise be difficult to get to. With Xpath, all you need to do in a chrome browser is right clickt he page, insspect element, unfold all the tags, and then right click the tag you want to scrape and click copy Xpath, then you can paste the path into modules in the selenium library. Really simple, heres a link for selenium information. </p>

<p><a href=""http://selenium-python.readthedocs.io/"" rel=""nofollow"">http://selenium-python.readthedocs.io/</a></p>
"
39622066,2761575.0,2016-09-21T16:38:12Z,39618648,5,"<p>This is how I would approach it, with an implementation in R provided: </p>

<p>There is not a unique solution for imputing the missing data points, such that the pairwise correlation of the complete (imputed) data is equal to the pairwise correlation of the incomplete data. So to find a 'good' solution rather than just 'any' solution, we can introduce an additional criteria that the complete imputed data should also share the same linear regression with the original data.  This leads us to a fairly simple approach. </p>

<ol>
<li>calculate a linear regression model for the original data. </li>
<li>find the imputed values for missing values that would lie exactly on this regression line. </li>
<li>generate a random scatter of residuals for the imputed values around this regression line</li>
<li>scale the imputed residuals to force the correlation of the complete imputed data to be equal to that of the original data</li>
</ol>

<p>A solution like this in R:</p>

<pre><code>library(data.table)
set.seed(123)

rho = cor(dt$m,dt$r,'pairwise')

# calculate linear regression of original data
fit1 = lm(r ~ m, data=dt)
fit2 = lm(m ~ r, data=dt)
# extract the standard errors of regression intercept (in each m &amp; r direction)
# and multiply s.e. by sqrt(n) to get standard deviation 
sd1 = summary(fit1)$coefficients[1,2] * sqrt(dt[!is.na(r), .N])
sd2 = summary(fit2)$coefficients[1,2] * sqrt(dt[!is.na(m), .N])

# find where data points with missing values lie on the regression line
dt[is.na(r), r.imp := coefficients(fit1)[1] + coefficients(fit1)[2] * m] 
dt[is.na(m), m.imp := coefficients(fit2)[1] + coefficients(fit2)[2] * r]

# generate randomised residuals for the missing data, using the s.d. calculated above
dt[is.na(r), r.ran := rnorm(.N, sd=sd1)] 
dt[is.na(m), m.ran := rnorm(.N, sd=sd2)] 

# function that scales the residuals by a factor x, then calculates how close correlation of imputed data is to that of original data
obj = function(x, dt, rho) {
  dt[, r.comp := r][, m.comp := m]
  dt[is.na(r), r.comp := r.imp + r.ran*x] 
  dt[is.na(m), m.comp := m.imp + m.ran*x] 
  rho2 = cor(dt$m.comp, dt$r.comp,'pairwise')
  (rho-rho2)^2
}

# find the value of x that minimises the discrepencay of imputed versus original correlation
fit = optimize(obj, c(-5,5), dt, rho)

x=fit$minimum
dt[, r.comp := r][, m.comp := m]
dt[is.na(r), r.comp := r.imp + r.ran*x] 
dt[is.na(m), m.comp := m.imp + m.ran*x] 
rho2 = cor(dt$m.comp, dt$r.comp,'pairwise')
(rho-rho2)^2  # check that rho2 is approximately equal to rho
</code></pre>

<p>As a final check, calculate linear regression of the complete imputed data
and plot to show that regression line is same as for original data.  Note that the plot below was for the large data set shown below, to demonstrate use of this method on large data.</p>

<pre><code>fit.comp = lm(r.comp ~ m.comp, data=dt)
plot(dt$m.comp, dt$r.comp)
points(dt$m, dt$r, col=""red"")
abline(fit1, col=""green"")
abline(fit.comp, col=""blue"")
mtext(paste("" Rho ="", round(rho,5)), at=-1)
mtext(paste("" Rho2 ="", round(rho2, 5)), at=6)
</code></pre>

<p><a href=""http://i.stack.imgur.com/nH8fm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nH8fm.png"" alt=""enter image description here""></a></p>

<p><strong>DATA</strong></p>

<p>original toy data from OP example:</p>

<pre><code>dt=structure(list(m = c(2, 0.8, NA, 1.3, 2.1, NA, NA, 2.5, 1.2, 2), 
                  r = c(3.3, NA, 4, NA, 5.2, 2.3, 1.9, NA, 3, 2.6)), 
             .Names = c(""m"", ""r""), row.names = c(NA, -10L), 
             class = c(""data.table"", ""data.frame""))
</code></pre>

<p>A larger data set to demonstrate on big data</p>

<pre><code>dt = data.table(m=rnorm(1e5, 3, 2))[, r:=1.5 + 1.1*m + rnorm(1e5,0,2)]
dt[sample(.N, 3e4), m:=NA]
dt[sample(which(!is.na(m)), 3e4), r := NA]
</code></pre>
"
39622265,2452553.0,2016-09-21T16:49:20Z,39605839,1,"<p>If the aim of the process is purely to find the gene IDs falling inside a specific start range and <em>you're not too worried about how you achieve this but are simply looking for the fastest solution</em>, then you may want to consider dropping the concept of a loop altogether and looking at a pre-existing solution mechanism.</p>

<p>Assuming your data is in CSV format, the following would suit your requirements, returning a dataframe containing the IDs, gene names and associated chromasones, grouped by chromasone.</p>

<p><strong>File: genometest.py</strong></p>

<pre><code>import pandas as pd
columns = ['id', 'chromasone', 'start', 'end', 'gene_name']

te_locations = pd.read_csv('Sequences/te.bed', delimiter='\t', header=None, names=columns)
gene_locations = pd.read_csv('Sequences/gene.bed', delimiter='\t', header=None, names=columns)

dataframe = pd.merge(te_locations, gene_locations, on=['gene_name', 'chromasone'], how='outer', suffixes=('_te', '_ge'))
dataset = dataframe.query('start_te &gt;= start_ge &amp; start_te &lt;= end_ge')[['peak_id_te', 'gene_name', 'chromasone']]
dataset.groupby('chromasone')
</code></pre>

<p><strong>Input sizes</strong></p>

<ul>
<li>TE_Locations dataset size = 337848</li>
<li>Gene_Locations dataset size = 50307</li>
</ul>

<p><strong>Output size</strong></p>

<ul>
<li>dataset size = 7085</li>
</ul>

<p><strong>Performance</strong></p>

<pre><code>$ python3 -m timeit 'import genometest'
10 loops, best of 3: 0.391 usec per loop
</code></pre>
"
39622269,1888503.0,2016-09-21T16:49:37Z,39622081,1,"<p>You can recast all of the other columns after each addition using:</p>

<pre><code>data['y'] = data['y'].astype(int)
</code></pre>

<p>Not the most efficient solution if you need to add a lot of columns on the fly. Alternatively you could create the entire data frame using Series in advance and type the whole thing a creation time instead if that's an option.</p>
"
39622498,2357112.0,2016-09-21T17:03:09Z,39622288,2,"<blockquote>
  <p>Perhaps there's an important use case for calling exhausted generators multiple times and getting <code>StopIteration</code>?</p>
</blockquote>

<p>There is, specifically, when you want to perform multiple loops on the same iterator. Here's an example from the <a href=""https://docs.python.org/3/library/itertools.html"" rel=""nofollow""><code>itertools</code></a> docs that relies on this behavior:</p>

<pre><code>def grouper(iterable, n, fillvalue=None):
    ""Collect data into fixed-length chunks or blocks""
    # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx""
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)
</code></pre>
"
39622585,5378816.0,2016-09-21T17:07:34Z,39622288,3,"<p>It is a part of the iteration protocol:</p>

<blockquote>
  <p>Once an iteratorâs <code>__next__()</code> method raises StopIteration, it must
  continue to do so on subsequent calls. Implementations that do not
  obey this property are deemed broken.</p>
</blockquote>

<p>Source: <a href=""https://docs.python.org/3/library/stdtypes.html#iterator-types"" rel=""nofollow"">https://docs.python.org/3/library/stdtypes.html#iterator-types</a></p>
"
39622690,901925.0,2016-09-21T17:13:30Z,39622059,2,"<p>This is not a 2d array.  It is a 1d array, whose elements are objects, in this case some 4 element lists and one 5 element one.  And this lists contain strings.</p>

<pre><code>In [577]: np.array([['34.5500000', '36.9000000', '37.3200000', '37.6700000'],
     ...:        ['41.7900000', '44.8000000', '48.2600000', '46.1800000'],
     ...:        ['36.1200000', '37.1500000', '39.3100000', '38.1000000'],
     ...:        ['82.1000000', '82.0900000', '76.0200000', '77.7000000'],
     ...:        ['48.0100000', '51.2500000', '51.1700000', '52.5000000', '55.25
     ...: 00000'],
     ...:        ['39.7500000', '39.5000000', '36.8100000', '37.2500000']], dtyp
     ...: e=object)
Out[577]: 
array([['34.5500000', '36.9000000', '37.3200000', '37.6700000'],
       ['41.7900000', '44.8000000', '48.2600000', '46.1800000'],
       ['36.1200000', '37.1500000', '39.3100000', '38.1000000'],
       ['82.1000000', '82.0900000', '76.0200000', '77.7000000'],
       ['48.0100000', '51.2500000', '51.1700000', '52.5000000', '55.2500000'],
       ['39.7500000', '39.5000000', '36.8100000', '37.2500000']], dtype=object)
In [578]: MyArray=_
In [579]: MyArray.shape
Out[579]: (6,)
In [580]: MyArray[0]
Out[580]: ['34.5500000', '36.9000000', '37.3200000', '37.6700000']
In [581]: MyArray[5]
Out[581]: ['39.7500000', '39.5000000', '36.8100000', '37.2500000']
In [582]: MyArray[4]
Out[582]: ['48.0100000', '51.2500000', '51.1700000', '52.5000000', '55.2500000']
In [583]: 
</code></pre>

<p>To <code>slice</code> this you need to iterate on the elements of the array</p>

<pre><code>In [584]: [d[:4] for d in MyArray]
Out[584]: 
[['34.5500000', '36.9000000', '37.3200000', '37.6700000'],
 ['41.7900000', '44.8000000', '48.2600000', '46.1800000'],
 ['36.1200000', '37.1500000', '39.3100000', '38.1000000'],
 ['82.1000000', '82.0900000', '76.0200000', '77.7000000'],
 ['48.0100000', '51.2500000', '51.1700000', '52.5000000'],
 ['39.7500000', '39.5000000', '36.8100000', '37.2500000']]
</code></pre>

<p>Now with all the sublists the same length, <code>np.array</code> will create a 2d array:</p>

<pre><code>In [585]: np.array(_)
Out[585]: 
array([['34.5500000', '36.9000000', '37.3200000', '37.6700000'],
       ['41.7900000', '44.8000000', '48.2600000', '46.1800000'],
       ['36.1200000', '37.1500000', '39.3100000', '38.1000000'],
       ['82.1000000', '82.0900000', '76.0200000', '77.7000000'],
       ['48.0100000', '51.2500000', '51.1700000', '52.5000000'],
       ['39.7500000', '39.5000000', '36.8100000', '37.2500000']], 
      dtype='&lt;U10')
</code></pre>

<p>Still strings, though</p>

<pre><code>In [586]: np.array(__,dtype=float)
Out[586]: 
array([[ 34.55,  36.9 ,  37.32,  37.67],
       [ 41.79,  44.8 ,  48.26,  46.18],
       [ 36.12,  37.15,  39.31,  38.1 ],
       [ 82.1 ,  82.09,  76.02,  77.7 ],
       [ 48.01,  51.25,  51.17,  52.5 ],
       [ 39.75,  39.5 ,  36.81,  37.25]])
</code></pre>
"
39622939,3293881.0,2016-09-21T17:25:59Z,39622059,1,"<p>Here's an almost* vectorized approach -</p>

<pre><code>def slice_2Dobject_arr(arr,out_shape):
    lens = np.array(map(len,arr))
    id_arr = np.ones(lens.sum(),dtype=int)
    id_arr[lens[:-1].cumsum()] = -lens[:-1]+1
    mask = id_arr.cumsum()&lt;=out_shape[1]
    vals = np.concatenate(arr)
    return vals[mask].reshape(-1,out_shape[1])[:out_shape[0]]
</code></pre>

<p>*: Almost because of the use of <code>map</code> at the start to get the lengths of the lists in the input array, which seems isn't a vectorized operation. But, computationally that should be comparatively negligible.</p>

<p>Sample runs -</p>

<pre><code>In [92]: arr
Out[92]: array([[3, 4, 5, 3], [3, 7, 8], [4, 9, 6, 4, 2], [3, 9, 4]], dtype=object)

In [93]: slice_2Dobject_arr(arr,(4,3))
Out[93]: 
array([[3, 4, 5],
       [3, 7, 8],
       [4, 9, 6],
       [3, 9, 4]])

In [94]: slice_2Dobject_arr(arr,(3,3))
Out[94]: 
array([[3, 4, 5],
       [3, 7, 8],
       [4, 9, 6]])

In [95]: slice_2Dobject_arr(arr,(3,2))
Out[95]: 
array([[3, 4],
       [3, 7],
       [4, 9]])
</code></pre>
"
39623207,261181.0,2016-09-21T17:42:10Z,39618943,19,"<p>Here's a simplified conclusion from other answers.</p>

<blockquote>
  <p>If you check a float on Python's command line or print it, it goes through function <code>repr</code> which creates its string representation.</p>
  
  <p>Starting with version 3.2, Python's <code>str</code> and <code>repr</code> use a complex rounding scheme, which prefers
  nice-looking decimals if possible, but uses more digits where
  necessary to guarantee bijective (one-to-one) mapping between floats
  and their string representations.</p>
  
  <p>This scheme guarantees that value of <code>repr(float(s))</code> looks nice for simple
  decimals, even if they can't be
  represented precisely as floats (eg. when <code>s = ""0.1"")</code>.</p>
  
  <p>At the same time it guarantees that <code>float(repr(x)) == x</code> holds for every float <code>x</code></p>
</blockquote>
"
39625571,2026276.0,2016-09-21T20:01:53Z,39621900,1,"<p>Well tbh. there's no real elegant way other than wrapping a pointer with a length or wrapping to C arrays and then to D.</p>

<p>However you can make a somewhat elegant purpose with the first way using a struct that has a pointer, a length and a property that converts it to a D array.</p>

<p>Then the function you export takes your struct, all that function should do is call an internal function that takes an actual D array and you'd simply pass the array to it and the conversion would happen at that moment through alias this and the conversion property.</p>

<p>An example usage is here:
    module main;</p>

<pre><code>import core.stdc.stdlib : malloc;

import std.stdio;

struct DArray(T) {
    T* data;
    size_t length;
    /// This field can be removed, only used for testing purpose
    size_t offset;

    @property T[] array() {
        T[] arr;

        foreach(i; 0 .. length) {
            arr ~= data[i];
        }

        return arr;
    }

    alias array this;

    /// This function can be removed, only used for testing purpose
    void init(size_t size) {
        data = cast(T*)malloc(size * T.sizeof);
        length = size;
    }

    /// This function can be removed, only used for testing purpose
    void append(T value) {
        data[offset] = value;

        offset++;
    }
}

// This function is the one exported
void externalFoo(DArray!int intArray) {
    writeln(""Calling extern foo"");

    internalFoo(intArray);
}

// This function is the one you use
private void internalFoo(int[] intArray) {
    writeln(""Calling internal foo"");

    writeln(intArray);
}


void main() {
    // Constructing our test array
    DArray!int arrayTest;
    arrayTest.init(10);

    foreach (int i; 0 .. 10) {
        arrayTest.append(i);
    }

    // Testing the exported function 
    externalFoo(arrayTest);
}
</code></pre>

<p>Here is an absolute minimum version of how to do it</p>

<pre><code>struct DArray(T) {
    T* data;
    size_t length;

    @property T[] array() {
        T[] arr;

        foreach(i; 0 .. length) {
            arr ~= data[i];
        }

        return arr;
    }

    alias array this;

}

// This function is the one exported
void externalFoo(DArray!int intArray) {
    writeln(""Calling extern foo"");

    internalFoo(intArray);
}

// This function is the one you use
private void internalFoo(int[] intArray) {
    writeln(""Calling internal foo"");

    writeln(intArray);
}
</code></pre>
"
39626641,901925.0,2016-09-21T21:09:18Z,39626233,3,"<p>To really get into broadcasting details you need to understand array shape and strides.  But a lot of the work is now implemented in <code>c</code> code using <code>nditer</code>.  You can read about it at <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html"" rel=""nofollow"">http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html</a>.  <code>np.nditer</code> gives you access to the tool at the Python level, but its real value comes when used with <code>cython</code> or your own <code>c</code> code.</p>

<p><code>np.lib.stride_tricks</code> has functions that let you play with strides.  One of its functions helps visualize how arrays are broadcasted together.  In practice the work is done with <code>nditer</code>, but this function may help understand the action:</p>

<pre><code>In [629]: np.lib.stride_tricks.broadcast_arrays(np.arange(6).reshape(2,3), 
                                  np.array([[1],[2]]))
Out[629]: 
[array([[0, 1, 2],
        [3, 4, 5]]), 
 array([[1, 1, 1],
        [2, 2, 2]])]
</code></pre>

<p>Note that, effectively the 2nd array has been replicated to match the 1st's shape.  But the replication is done with stride tricks, not with actual copies.</p>

<pre><code>In [631]: A,B=np.lib.stride_tricks.broadcast_arrays(np.arange(6).reshape(2,3), 
                                      np.array([[1],[2]]))
In [632]: A.shape
Out[632]: (2, 3)
In [633]: A.strides
Out[633]: (12, 4)
In [634]: B.shape
Out[634]: (2, 3)
In [635]: B.strides
Out[635]: (4, 0)         
</code></pre>

<p>It's this <code>(4,0)</code> strides that does the replication without copy.</p>

<p>=================</p>

<p>Using the python level <code>nditer</code>, here's what it does during broadcasting.</p>

<pre><code>In [1]: A=np.arange(6).reshape(2,3)
In [2]: B=np.array([[1],[2]])
</code></pre>

<p>The plain nditer feeds elements one set at a time
<a href=""http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html#using-an-external-loop"" rel=""nofollow"">http://docs.scipy.org/doc/numpy/reference/arrays.nditer.html#using-an-external-loop</a></p>

<pre><code>In [5]: it =np.nditer((A,B))
In [6]: for a,b in it:
   ...:     print(a,b)

0 1
1 1
2 1
3 2
4 2
5 2
</code></pre>

<p>But when I turn <code>extenal_loop</code> on, it iterates in chunks, here respective rows of the broadcasted arrays:</p>

<pre><code>In [7]: it =np.nditer((A,B), flags=['external_loop'])
In [8]: for a,b in it:
   ...:     print(a,b)

[0 1 2] [1 1 1]
[3 4 5] [2 2 2]
</code></pre>

<p>With a more complex broadcasting the <code>external_loop</code> still produces 1d arrays that allow simple <code>c</code> style iteration:</p>

<pre><code>In [13]: A1=np.arange(24).reshape(3,2,4)
In [18]: it =np.nditer((A1,np.arange(3)[:,None,None]), flags=['external_loop'])
In [19]: while not it.finished:
    ...:     print(it[:])
    ...:     it.iternext()
    ...:     
(array([0, 1, 2, 3, 4, 5, 6, 7]), array([0, 0, 0, 0, 0, 0, 0, 0]))
(array([ 8,  9, 10, 11, 12, 13, 14, 15]), array([1, 1, 1, 1, 1, 1, 1, 1]))
(array([16, 17, 18, 19, 20, 21, 22, 23]), array([2, 2, 2, 2, 2, 2, 2, 2]))
</code></pre>

<p>Note that while <code>A1</code> is (3,2,4), the nditer loop yields 3 steps (1st axis) with 2*4 length elements.  </p>

<p>I found in another <code>cython/nditer</code> SO question that the first approach did not produce much of a speed improvement, but the 2nd helped a lot.  In <code>c</code> or <code>cython</code> the <code>external_loop</code> case would do simple low level iteration.</p>

<p>===============</p>

<p>If I broadcast on the 1 and 3rd axis, the iterator takes 2*3 steps (effectively flattening the 1st 2 axes, and feeding the 3rd):</p>

<pre><code>In [20]: it =np.nditer((A1,np.arange(2)[None,:,None]), flags=['external_loop'])
In [21]: while not it.finished:
    ...:     print(it[:])
    ...:     it.iternext()
    ...:     
(array([0, 1, 2, 3]), array([0, 0, 0, 0]))
(array([4, 5, 6, 7]), array([1, 1, 1, 1]))
(array([ 8,  9, 10, 11]), array([0, 0, 0, 0]))
(array([12, 13, 14, 15]), array([1, 1, 1, 1]))
(array([16, 17, 18, 19]), array([0, 0, 0, 0]))
(array([20, 21, 22, 23]), array([1, 1, 1, 1]))
</code></pre>

<p>But with <code>buffered</code>, it iterates once, feeding me 2 1d arrays:</p>

<pre><code>In [22]: it =np.nditer((A1,np.arange(2)[None,:,None]), flags=['external_loop','buffered'])
In [23]: while not it.finished:
    ...:     print(it[:])
    ...:     it.iternext()
    ...:     
(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]), 
 array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1]))
</code></pre>

<p><a href=""http://stackoverflow.com/questions/39058641/does-cython-offer-any-reasonably-easy-and-efficient-way-to-iterate-numpy-arrays"">Does Cython offer any reasonably easy and efficient way to iterate Numpy arrays as if they were flat?</a>
has some speed tests, showing that buffered external loop is fastest</p>

<p><code>cython</code> translates this into fast simple <code>c</code> iteration:</p>

<pre><code>for xarr in it:
   x = xarr
   size = x.shape[0]
   for i in range(size):
       x[i] = x[i]+1.0
</code></pre>
"
39626721,5606318.0,2016-09-21T21:16:33Z,39608282,0,"<p>Finally it worked by using 'converters' option in pandas read_excel format as</p>

<pre><code>df_w02 = pd.read_excel(excel_name, names = df_header,converters = {'AltID':str,'RatingReason' : str}).fillna("""")
</code></pre>

<p>converters can 'cast' a type as defined by my function/value and keeps intefer stored as string without adding decimal point.</p>
"
39627242,3322400.0,2016-09-21T22:01:21Z,39627188,0,"<p>I'd simply use <code>threading.Thread(target=process, args=(fname,))</code> and start a new thread for processing.</p>

<p>But before that, end last processing thread :</p>

<pre><code>t = None
for fname in download(urls):
    if t is not None: # wait for last processing thread to end
        t.join()
    t = threading.Thread(target=process, args=(fname,))
    t.start()
    print('[i] thread started for %s' % fname)
</code></pre>

<p>See <a href=""https://docs.python.org/3/library/threading.html"" rel=""nofollow"">https://docs.python.org/3/library/threading.html</a></p>
"
39628280,5285918.0,2016-09-22T00:06:15Z,39627490,3,"<p>I think the best thing to do here is to fake it following <a href=""http://stackoverflow.com/a/11558629/5285918"">this answer</a> since you don't have a ""ScalarMappable"" to work with.</p>

<p>For a discrete colormap</p>

<pre><code>from matplotlib.colors import ListedColormap
sm = plt.cm.ScalarMappable(cmap=ListedColormap(color_palette),
                           norm=plt.Normalize(vmin=0, vmax=3))
sm._A = []
plt.colorbar(sm)
</code></pre>

<p>If you want a linear (continuous) colormap and to only show integer ticks</p>

<pre><code>sm = plt.cm.ScalarMappable(cmap=sns.cubehelix_palette(3, as_cmap=True),
                           norm=plt.Normalize(vmin=0, vmax=3))
sm._A = []
plt.colorbar(sm, ticks=range(4))
</code></pre>

<p><a href=""http://i.stack.imgur.com/pswBu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pswBu.png"" alt=""enter image description here""></a></p>
"
39628556,4665213.0,2016-09-22T00:44:10Z,39628456,0,"<p>In my experience, the only reason that you would ever have two lists that happen to have the same length is because they were both constructed from the same source, e.g. they are <code>map</code>s of the same underlying source, they are constructed inside the same loop, etc. In these cases, rather than creating them separately and then zipping them, I usually just create a single pre-zipped list of tuples. Most of the times that I actually use zip, one of the iterables is infinite, and in these cases I'm glad that it lets me.</p>
"
39628603,2271269.0,2016-09-22T00:51:29Z,39628456,9,"<h2>Reason 1: Historical Reason</h2>

<p><code>zip</code> allows unequal-length arguments because it was meant to improve upon <code>map</code> by <em>allowing</em> unequal-length arguments. This behavior is the reason <code>zip</code> exists at all.</p>

<p>Here's how you did <code>zip</code> before it existed:</p>

<pre><code>&gt;&gt;&gt; a = (1, 2, 3)
&gt;&gt;&gt; b = (4, 5, 6)
&gt;&gt;&gt; for i in map(None, a, b): print i
...
(1, 4)
(2, 5)
(3, 6)
&gt;&gt;&gt; map(None, a, b)
[(1, 4), (2, 5), (3, 6)]
</code></pre>

<p>This is terrible behaviour, and <em>does not</em> support unequal-length lists. This was a major design concern, which you can see plain-as-day in <a href=""https://www.python.org/dev/peps/pep-0201/#lockstep-for-loops"">the official RFC proposing <code>zip</code> for the first time</a>:</p>

<blockquote>
  <p>While the map() idiom is a common one in Python, it has several
  disadvantages:</p>
  
  <ul>
  <li><p>It is non-obvious to programmers without a functional programming
  background.</p></li>
  <li><p>The use of the magic <code>None</code> first argument is non-obvious.</p></li>
  <li><p>It has arbitrary, often unintended, and inflexible semantics when the
  lists are not of the same length - the shorter sequences are padded
  with <code>None</code> :</p>
  
  <p><code>&gt;&gt;&gt; c = (4, 5, 6, 7)</code></p>
  
  <p><code>&gt;&gt;&gt; map(None, a, c)</code></p>
  
  <p><code>[(1, 4), (2, 5), (3, 6), (None, 7)]</code></p></li>
  </ul>
</blockquote>

<p>So, no, this behaviour would not be treated as an error - it is <em>why it was designed in the first place</em>.</p>

<hr>

<h2>Reason 2: Practical Reason</h2>

<p>Because it is pretty useful, is clearly specified and doesn't have to be thought of as an error at all. </p>

<p>By allowing unequal lengths, <code>zip</code> only requires that its arguments conform to the <a href=""http://stackoverflow.com/questions/16301253/what-exactly-is-pythons-iterator-protocol"">iterator protocol</a>. This allows <code>zip</code> to be extended to generators, tuples, dictionary keys and literally anything in the world that implements <code>__next__()</code> and <code>__iter__()</code>, precisely because it doesn't inquire about length. </p>

<p>This is significant, because generators <em>do not</em> support <code>len()</code> and thus there is no way to check the length beforehand. Add a check for length, and you break <code>zip</code>s ability to work on generators, <em>when it should</em>. That's a fairly serious disadvantage, wouldn't you agree?</p>

<hr>

<h2>Reason 3: By Fiat</h2>

<p>Guido van Rossum wanted it this way:</p>

<blockquote>
  <p><em>Optional padding.</em> An earlier version of this PEP proposed an optional pad keyword argument, which would be used when the argument sequences were not the same length. This is similar behavior to the map(None, ...) semantics except that the user would be able to specify pad object. <strong>This has been rejected by the BDFL in favor of always truncating to the shortest sequence, because of the KISS principle.</strong> If there's a true need, it is easier to add later. If it is not needed, it would still be impossible to delete it in the future.</p>
</blockquote>

<p>KISS trumps everything.</p>
"
39630463,267540.0,2016-09-22T04:47:09Z,39629823,3,"<p>This is not entirely suprising. In the first instance you are inserting the python predefined constant <a href=""https://docs.python.org/2/library/constants.html"" rel=""nofollow"">None</a></p>

<blockquote>
  <p>The sole value of types.NoneType. None is frequently used to represent
  the absence of a value, as when default arguments are not passed to a
  function.</p>
</blockquote>

<p>That equates to SQL <code>NULL</code>. In the second instance you are inserting a String called 'None' into the table. These two are very different. If you insert a string into a double or float field you will see all kinds of errors, most often, the exact one you have seen.</p>

<p>in the first instance it works because you have declared :</p>

<pre><code> `float8value` double DEFAULT NULL,
</code></pre>

<p>This accepts NULL and None is in the 8th place in your values list. When lots of different parameters are being used, it's always a good idea to use named parameters so that it's obvious at a glance what's being bound to each column.</p>

<p>Updates:</p>

<p>After running your code, the only conclusion that can be reached is that you have found a bug, by using <code>print(cursor.statement)</code> it's possible to discover that the executed query is. </p>

<pre><code>INSERT INTO runsettings (apcrunid,equipment,runnumber,wafer,settingname,intvalue,floatvalue,float8value) 
VALUES (471285,'CT19',7,'271042','Etch Time Min',NULL,NULL,NULL),
       (471285,'CT19',7,'00000','Etch Time Min',NULL,NULL,'None')
</code></pre>

<p>This does not produce an error, but if you erase the first set of values the error is indeed produced. My recommendation is to file a bug report</p>
"
39630862,3627387.0,2016-09-22T05:25:11Z,39630773,3,"<p>This can be easily explained.</p>

<pre><code>auth_user_groups
auth_group_permissions
auth_user_user_permissions
</code></pre>

<p>Are relational tables. Which are used to store info about how model tables are related.</p>

<p>For example <code>auth_user_groups</code> is storing how users and groups are related.
If you do</p>

<pre><code>SELECT * FROM auth_user_groups;
|id|user_id|group_id|
....
</code></pre>

<p>And here we can see that which groups are related to which users.</p>

<p>So basically django will automatically create such tables for when you use <code>ManyToManyField</code> on your models</p>

<p><strong>Answering comment</strong>
Django migration table is created automatically when you call <code>./manage.py migrate</code> for the first time. This table stores history of applying your migrations. Migration model can be found in <code>django/db/migrations/recorder.py</code>. This model is used when running new migrations to see which are applied and which should be applied on this command run. And this model is part of core django's functionality, that's why you don't need to add it in <code>INSTALLED_APPS</code> because <code>INSTALLED_APPS</code> contain only pluggable apps. Which you can include/exclude from your project if you want.</p>
"
39631629,6796665.0,2016-09-22T06:19:57Z,39630676,0,"<p>You are not creating ThreadPoolExecutor every time , rather using the pre initialized pool for every iteration. I really not able to track which print statement is hindering you?</p>
"
39632209,2476444.0,2016-09-22T06:55:08Z,39631386,3,"<p>Not sure what you mean by 'dynamic' in this case, but have you considered using CSS selectors?</p>

<p>With Beautifulsoup you could get it e.g like this:</p>

<pre><code>soup.select('div#quote-header-info section span')[0]  
</code></pre>

<p>And there are some variations you could use on the pattern, such as using the '>' filter.</p>

<p>You could get the same with just <code>lxml</code>, no need for BeautifulSoup:</p>

<pre><code>import lxml.html as html
page = html.parse(url).getroot()
content = page.cssselect('div#quote-header-info section &gt; span:first-child')[0].text
</code></pre>

<p>Which immediately illustrates a more specific selector. </p>

<p>If you're interested in more efficient DOM-traversal, research xpaths.</p>
"
39633884,5343587.0,2016-09-22T08:25:23Z,39618943,5,"<p>Not really specific to Python's implementation but should apply to any float to decimal string functions.</p>

<p>A floating point number is essentially a binary number, but in scientific notation with a fixed limit of significant figures.</p>

<p>The inverse of any number that has a prime number factor that is not shared with the base will always result in a recurring dot point representation. For example 1/7 has a prime factor, 7, that is not shared with 10, and therefore has a recurring decimal representation, and the same is true for 1/10 with prime factors 2 and 5, the latter not being shared with 2; this means that 0.1 cannot be exactly represented by a finite number of bits after the dot point.</p>

<p>Since 0.1 has no exact representation, a function that converts the approximation to a decimal point string will usually try to approximate certain values so that they don't get unintuitive results like 0.1000000000004121.</p>

<p>Since the floating point is in scientific notation, any multiplication by a power of the base only affects the exponent part of the number. For example 1.231e+2 * 100 = 1.231e+4 for decimal notation, and likewise, 1.00101010e11 * 100 = 1.00101010e101 in binary notation. If I multiply by a non-power of the base, the significant digits will also be affected. For example 1.2e1 * 3 = 3.6e1</p>

<p>Depending on the algorithm used, it may try to guess common decimals based on the significant figures only. Both 0.1 and 0.4 have the same significant figures in binary, because their floats are essentially truncations of (8/5)<em>(2^-4) and (8/5)</em>(2^-6) respectively. If the algorithm identifies the 8/5 sigfig pattern as the decimal 1.6, then it will work on 0.1, 0.2, 0.4, 0.8, etc. It may also have magic sigfig patterns for other combinations, such as the float 3 divided by float 10 and other magic patterns statistically likely to be formed by division by 10.</p>

<p>In the case of 3*0.1, the last few significant figures will likely be different from dividing a float 3 by float 10, causing the algorithm to fail to recognize the magic number for the 0.3 constant depending on its tolerance for precision loss.</p>

<p>Edit:
<a href=""https://docs.python.org/3.1/tutorial/floatingpoint.html"" rel=""nofollow"">https://docs.python.org/3.1/tutorial/floatingpoint.html</a></p>

<blockquote>
  <p>Interestingly, there are many different decimal numbers that share the same nearest approximate binary fraction. For example, the numbers 0.1 and 0.10000000000000001 and 0.1000000000000000055511151231257827021181583404541015625 are all approximated by 3602879701896397 / 2 ** 55. Since all of these decimal values share the same approximation, any one of them could be displayed while still preserving the invariant eval(repr(x)) == x.</p>
</blockquote>

<p>There is no tolerance for precision loss, if float x (0.3) is not exactly equal to float y (0.1*3), then repr(x) is not exactly equal to repr(y).</p>
"
39635322,2141635.0,2016-09-22T09:32:47Z,39631386,2,"<p>The  data is obviously populated using <em>reactjs</em> so you won't be able to parse it reliably using class names etc.. You can get all the data in <em>json</em> format from the page source from the <code>root.App.main</code> script :</p>

<pre><code>import  requests
from bs4 import BeautifulSoup
import re
from json import loads

soup = BeautifulSoup(requests.get(""http://finance.yahoo.com/quote/AAPL/profile?p=AAPL"").content)
script = soup.find(""script"",text=re.compile(""root.App.main"")).text
data = loads(re.search(""root.App.main\s+=\s+(\{.*\})"", script).group(1))
print(data)
</code></pre>

<p>Which gives you a whole load of json, you can go through the data and pick what you need like below :</p>

<pre><code>stores = data[""context""][""dispatcher""][""stores""]
from  pprint import pprint as pp

pp(stores[u'QuoteSummaryStore']) 
</code></pre>

<p>Which gives you:</p>

<pre><code>{u'price': {u'averageDailyVolume10Day': {u'fmt': u'63.06M',
                                         u'longFmt': u'63,056,525',
                                         u'raw': 63056525},
            u'averageDailyVolume3Month': {u'fmt': u'36.53M',
                                          u'longFmt': u'36,527,196',
                                          u'raw': 36527196},
            u'currency': u'USD',
            u'currencySymbol': u'$',
            u'exchange': u'NMS',
            u'exchangeName': u'NasdaqGS',
            u'longName': u'Apple Inc.',
            u'marketState': u'PRE',
            u'maxAge': 1,
            u'openInterest': {},
            u'postMarketChange': {u'fmt': u'0.11', u'raw': 0.11000061},
            u'postMarketChangePercent': {u'fmt': u'0.10%',
                                         u'raw': 0.0009687416},
            u'postMarketPrice': {u'fmt': u'113.66', u'raw': 113.66},
            u'postMarketSource': u'DELAYED',
            u'postMarketTime': 1474502277,
            u'preMarketChange': {u'fmt': u'0.42', u'raw': 0.41999817},
            u'preMarketChangePercent': {u'fmt': u'0.37%',
                                        u'raw': 0.0036987949},
            u'preMarketPrice': {u'fmt': u'113.97', u'raw': 113.97},
            u'preMarketSource': u'FREE_REALTIME',
            u'preMarketTime': 1474536411,
            u'quoteType': u'EQUITY',
            u'regularMarketChange': {u'fmt': u'-0.02', u'raw': -0.019996643},
            u'regularMarketChangePercent': {u'fmt': u'-0.02%',
                                            u'raw': -0.00017607327},
            u'regularMarketDayHigh': {u'fmt': u'113.99', u'raw': 113.989},
            u'regularMarketDayLow': {u'fmt': u'112.44', u'raw': 112.441},
            u'regularMarketOpen': {u'fmt': u'113.82', u'raw': 113.82},
            u'regularMarketPreviousClose': {u'fmt': u'113.57',
                                            u'raw': 113.57},
            u'regularMarketPrice': {u'fmt': u'113.55', u'raw': 113.55},
            u'regularMarketSource': u'FREE_REALTIME',
            u'regularMarketTime': 1474488000,
            u'regularMarketVolume': {u'fmt': u'31.57M',
                                     u'longFmt': u'31,574,028.00',
                                     u'raw': 31574028},
            u'shortName': u'Apple Inc.',
            u'strikePrice': {},
            u'symbol': u'AAPL',
            u'underlyingSymbol': None},
 u'price,summaryDetail': {},
 u'quoteType': {u'exchange': u'NMS',
                u'headSymbol': None,
                u'longName': u'Apple Inc.',
                u'market': u'us_market',
                u'messageBoardId': u'finmb_24937',
                u'quoteType': u'EQUITY',
                u'shortName': u'Apple Inc.',
                u'symbol': u'AAPL',
                u'underlyingExchangeSymbol': None,
                u'underlyingSymbol': None,
                u'uuid': u'8b10e4ae-9eeb-3684-921a-9ab27e4d87aa'},
 u'summaryDetail': {u'ask': {u'fmt': u'114.00', u'raw': 114},
                    u'askSize': {u'fmt': u'100',
                                 u'longFmt': u'100',
                                 u'raw': 100},
                    u'averageDailyVolume10Day': {u'fmt': u'63.06M',
                                                 u'longFmt': u'63,056,525',
                                                 u'raw': 63056525},
                    u'averageVolume': {u'fmt': u'36.53M',
                                       u'longFmt': u'36,527,196',
                                       u'raw': 36527196},
                    u'averageVolume10days': {u'fmt': u'63.06M',
                                             u'longFmt': u'63,056,525',
                                             u'raw': 63056525},
                    u'beta': {u'fmt': u'1.52', u'raw': 1.51744},
                    u'bid': {u'fmt': u'113.68', u'raw': 113.68},
                    u'bidSize': {u'fmt': u'400',
                                 u'longFmt': u'400',
                                 u'raw': 400},
                    u'dayHigh': {u'fmt': u'113.99', u'raw': 113.989},
                    u'dayLow': {u'fmt': u'112.44', u'raw': 112.441},
                    u'dividendRate': {u'fmt': u'2.28', u'raw': 2.28},
                    u'dividendYield': {u'fmt': u'2.01%', u'raw': 0.0201},
                    u'exDividendDate': {u'fmt': u'2016-08-04',
                                        u'raw': 1470268800},
                    u'expireDate': {},
                    u'fiftyDayAverage': {u'fmt': u'108.61',
                                         u'raw': 108.608284},
                    u'fiftyTwoWeekHigh': {u'fmt': u'123.82', u'raw': 123.82},
                    u'fiftyTwoWeekLow': {u'fmt': u'89.47', u'raw': 89.47},
                    u'fiveYearAvgDividendYield': {},
                    u'forwardPE': {u'fmt': u'12.70', u'raw': 12.701344},
                    u'marketCap': {u'fmt': u'611.86B',
                                   u'longFmt': u'611,857,399,808',
                                   u'raw': 611857399808},
                    u'maxAge': 1,
                    u'navPrice': {},
                    u'open': {u'fmt': u'113.82', u'raw': 113.82},
                    u'openInterest': {},
                    u'payoutRatio': {u'fmt': u'24.80%', u'raw': 0.248},
                    u'previousClose': {u'fmt': u'113.57', u'raw': 113.57},
                    u'priceToSalesTrailing12Months': {u'fmt': u'2.78',
                                                      u'raw': 2.777534},
                    u'regularMarketDayHigh': {u'fmt': u'113.99',
                                              u'raw': 113.989},
                    u'regularMarketDayLow': {u'fmt': u'112.44',
                                             u'raw': 112.441},
                    u'regularMarketOpen': {u'fmt': u'113.82', u'raw': 113.82},
                    u'regularMarketPreviousClose': {u'fmt': u'113.57',
                                                    u'raw': 113.57},
                    u'regularMarketVolume': {u'fmt': u'31.57M',
                                             u'longFmt': u'31,574,028',
                                             u'raw': 31574028},
                    u'strikePrice': {},
                    u'totalAssets': {},
                    u'trailingAnnualDividendRate': {u'fmt': u'2.13',
                                                    u'raw': 2.13},
                    u'trailingAnnualDividendYield': {u'fmt': u'1.88%',
                                                     u'raw': 0.018754954},
                    u'trailingPE': {u'fmt': u'13.24', u'raw': 13.240438},
                    u'twoHundredDayAverage': {u'fmt': u'102.39',
                                              u'raw': 102.39367},
                    u'volume': {u'fmt': u'31.57M',
                                u'longFmt': u'31,574,028',
                                u'raw': 31574028},
                    u'yield': {},
                    u'ytdReturn': {}},
 u'symbol': u'AAPL'}
</code></pre>
"
39635598,5349916.0,2016-09-22T09:43:14Z,39634577,1,"<p>Which approach is best depends entirely on your problem. If you have only few operations, functions are simpler. If you have many operations which depend on the type/features of data, classes are efficient.</p>

<p>Personally, I prefer having classes for the same type of data to improve abstraction and modularity. Basically, using classes requires you to think about what your data is like, what is allowed on it and what is appropriate. It enforces that you separate, compartmentalize and <em>understand</em> what you are doing. Once you've done that, you can treat them like black boxes that just work.</p>

<p>I've seen many data-analysis programs fail because they just had functions working on arbitrary data. At first, it was simple computations. Then state needed to be preserved/cached, so data got appended or modified directly. Then someone realized that if you did x before you shouldn't do y later, so all sorts of flags, fields and other things get tacked on, which only functions a, b and d understood. Then someone added function f which extended on that, while someone added function k which extended it differently. That creates a cluster-foo that's impossible to understand, maintain, or <em>trust</em> in creating results.</p>

<p>So if you are unsure, do classes. You'll be happier in the end.</p>

<hr>

<p>Concerning your second question, I can only answer that for python. However, <em>many</em> languages do it similarly.</p>

<p>Regular methods in python are defined on the class and created with it. That means the actual function represented by a method is shared by all instances, without memory overhead. Basically, a bare instance is just a wrapped reference to the class, from which methods are fetched. Only things <em>exclusive</em> to an instance, like data, add to memory notably.</p>

<p><em>Calling</em> a method does add some overhead, because the method gets bound to the instance - basically, the function is fetched from the class and the first parameter <code>self</code> gets bound. This technically incurs some overhead.</p>

<pre><code># Method Call
$ python -m timeit -s 'class Foo():' -s ' def p(self):' -s '  pass' -s 'foo = Foo()' 'foo.p()'
10000000 loops, best of 3: 0.158 usec per loop
# Method Call of cached method
$ python -m timeit -s 'class Foo():' -s ' def p(self):' -s '  pass' -s 'foo = Foo()' -s 'p=foo.p' 'p()'
10000000 loops, best of 3: 0.0984 usec per loop
# Function Call
$ python -m timeit -s 'def p():' -s ' pass' 'p()'
10000000 loops, best of 3: 0.0846 usec per loop
</code></pre>

<p>However, practically <em>any</em> operation does this; you'll only notice the added overhead if your applications does nothing else but call your method, and the method also does nothing.</p>

<p>I've also seen people write data analysis applications with so many levels of abstraction that in fact they mostly just called methods/functions. This is a smell of writing code in general, not whether to use methods <em>or</em> functions.</p>

<p>So if you are unsure, do classes. You'll be happier in the end.</p>
"
39639399,242457.0,2016-09-22T12:42:57Z,39639342,3,"<p>It's a deliberate feature. If the python code you run evaluates to exactly <code>None</code> then it is not displayed.</p>

<p>This is useful a lot of the time. For example, calling a function with a side effect may be useful, and such functions actually return <code>None</code> but you don't usually want to see the result.</p>

<p>For example, calling <code>print()</code> returns <code>None</code>, but you don't usually want to see it:</p>

<pre><code>&gt;&gt;&gt; print(""hello"")
hello
&gt;&gt;&gt; y = print(""hello"")
hello
&gt;&gt;&gt; y
&gt;&gt;&gt; print(y)
None
</code></pre>
"
39639575,3545273.0,2016-09-22T12:50:18Z,39639342,1,"<p>In Python, a function that does not return anything but is called only for its side effects actually returns None. As such functions are common enough, Python interactive interpreter does not print anything in that case. By extension, it does not print anything when the interactive expression evaluates to None, even if it is not a function call.</p>

<p>If can be misleading for beginners because you have </p>

<pre><code>&gt;&gt;&gt; a = 1
&gt;&gt;&gt; a
1
&gt;&gt;&gt;
</code></pre>

<p>but </p>

<pre><code>&gt;&gt;&gt; a = None
&gt;&gt;&gt; a
&gt;&gt;&gt;
</code></pre>

<p>but is is indeed <em>by design</em></p>
"
39639637,3125566.0,2016-09-22T12:52:56Z,39639342,1,"<p><a href=""https://docs.python.org/3/library/constants.html#None"" rel=""nofollow""><code>None</code></a> represents the absence of a value, but that absence can be observed. Because it represents <em>something</em> in Python, its <code>__repr__</code> cannot possibly return <em>nothing</em>; <code>None</code> is not nothing.</p>

<p>The outcome is deliberate. If for example a function returns <code>None</code> (similar to having no return statement), the return value of a call to such function does not get shown in the console, so for example <code>print(None)</code> does not <em>print</em> <code>None</code> twice, as the function <code>print</code> equally returns <code>None</code>.</p>

<p>On a side note, <code>print(repr())</code> will raise a <code>TypeError</code> in Python.</p>
"
39641065,4014959.0,2016-09-22T13:54:52Z,39639342,2,"<p>Yes, this behaviour is intentional.</p>

<p>From the <a href=""https://docs.python.org/3/reference/simple_stmts.html#expression-statements"" rel=""nofollow"">Python docs</a></p>

<blockquote>
  <p>7.1. Expression statements</p>
  
  <p>Expression statements are used (mostly interactively) to compute and
  write a value, or (usually) to call a procedure (a function that
  returns no meaningful result; in Python, procedures return the value
  <code>None</code>). Other uses of expression statements are allowed and
  occasionally useful. The syntax for an expression statement is:</p>

<pre><code>expression_stmt ::=  starred_expression
</code></pre>
  
  <p>An expression statement evaluates the expression list (which may be a
  single expression).</p>
  
  <p>In interactive mode, if the value is not <code>None</code>, it is converted to a
  string using the built-in <code>repr()</code> function and the resulting string
  is written to standard output on a line by itself (except if the
  result is <code>None</code>, so that procedure calls do not cause any output.)</p>
</blockquote>
"
39641355,3405554.0,2016-09-22T14:07:01Z,39641171,0,"<p>You can do like this, just use list of tuple where first is the function name, and rest are arguments. </p>

<pre><code>def a(*args,**kwargs):
    print 1

list_f = [(a,1,2,3)]

for f in list_f:
    f[0](f[1:])
</code></pre>
"
39641425,6582873.0,2016-09-22T14:10:44Z,39641171,3,"<p>You can use the <a href=""https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists"" rel=""nofollow"">*-operator to unpack the arguments out of the tuple</a>.</p>

<p>For example, suppose the format of items in your invocation list is <code>(function,arg1,arg2,...)</code>. In other words, <code>item[0]</code> is your function, while <code>item[1:]</code> are the arguments the should be given to the function.</p>

<pre><code>def f1(x1):
    print(""f1 {0}"".format(x1))

def f2(x1,x2):
    print(""f2 {0} {1}"".format(x1,x2))

for data in ((f1,5),(f2,3,4)):
    data[0](*data[1:])
# output:
#   f1 5
#   f2 3 4
</code></pre>
"
39641684,3960021.0,2016-09-22T14:21:58Z,39641171,0,"<p>Assuming your function and arguments are separate in the tuple, </p>

<pre><code>def squarer(*args):
    return map(lambda x: x*2, args)

def subtracter(*args):
    return map(lambda x: x-1, args)

funcAndArgs = [(squarer, [1,2,3,4]), (subtracter, [1,2,3,4])]

for func, args in funcAndArgs:
    print func(*args)
</code></pre>

<p>Output:</p>

<pre><code>[2, 4, 6, 8]
[0, 1, 2, 3]
</code></pre>
"
39642669,833402.0,2016-09-22T15:06:54Z,39603391,2,"<p>Another way to tackle this is using a generator, since generators use lazy evaluation. First put all checks into a generator:</p>

<pre><code>def checks():
    yield check_a()
    yield check_b()
    yield check_c()
</code></pre>

<p>Now you could force evaluation of everything by converting it to a list:</p>

<pre><code>list(checks())
</code></pre>

<p>But the standard all function does proper short cut evaluation on the iterator returned from checks(), and returns whether all elements are truthy:</p>

<pre><code>all(checks())
</code></pre>

<p>Last, if you want the results of succeeding checks up to the failure you can use itertools.takewhile to take the first run of truthy values only. Since the result of takewhile is lazy itself you'll need to convert it to a list to see the result in a REPL:</p>

<pre><code>from itertools import takewhile
takewhile(lambda x: x, checks())
list(takewhile(lambda x: x, checks()))
</code></pre>
"
39643014,224671.0,2016-09-22T15:23:28Z,39642721,2,"<p>You could use one of the stack functions (<a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.stack.html"" rel=""nofollow"">stack</a>/<a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.hstack.html"" rel=""nofollow"">hstack</a>/<a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.vstack.html"" rel=""nofollow"">vstack</a>/<a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.dstack.html"" rel=""nofollow"">dstack</a>/<a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.concatenate.html"" rel=""nofollow"">concatenate</a>) to join multiple arrays together.</p>

<pre><code>numpy.dstack( ( your_input_array, numpy.zeros((25, 54)) ) )
</code></pre>
"
39643047,3551700.0,2016-09-22T15:25:12Z,39642721,0,"<p>If you have your current image as rgb variable then just use:</p>

<pre><code>rgba = numpy.concatenate((rgb, numpy.zeros((205, 54, 1))), axis=2)
</code></pre>

<p>Concatenate function merge rgb and zeros array together. Zeros function creates array of zeros. We set axis to 2 what means we merge in the thirde dimensions. Note: axis are counted from 0.</p>
"
39644347,416467.0,2016-09-22T16:30:51Z,39644202,3,"<p>This is related to Python's multiple assignment (sequence unpacking):</p>

<pre><code>a, b, c = 1, 2, 3
</code></pre>

<p>works the same as:</p>

<pre><code>[a, b, c] = 1, 2, 3
</code></pre>

<p>Since strings are sequences of characters, you can also do:</p>

<pre><code>a, b, c = ""abc""    # assign each character to a variable
</code></pre>

<p>What you've discovered is the degenerative case: empty sequences on both sides. Syntactically valid because it's a list on the left rather than a tuple. Nice find; never thought to try that before!</p>

<p>Interestingly, if you try that with an empty tuple on the left, Python complains:</p>

<pre><code>() = ()            # SyntaxError: can't assign to ()
</code></pre>

<p>Looks like the Python developers forgot to close a little loophole!</p>
"
39644456,4600339.0,2016-09-22T16:36:18Z,39644202,1,"<p>Do some search on packing/unpacking on python and you will find your answer.
This is basically for assigning multiple variables in a single go.</p>

<pre><code>&gt;&gt;&gt; [a,v] = [2,4]
&gt;&gt;&gt; print a
2
&gt;&gt;&gt; print v
4
</code></pre>
"
39644552,771848.0,2016-09-22T16:41:36Z,39644517,7,"<p>You can <em>slice</em> <code>A</code> and its sublists:</p>

<pre><code>In [1]: A = [[1,  2,  3,  4 ],
   ...:      [11, 12, 13, 14],
   ...:      [21, 22, 23, 24],
   ...:      [31, 32, 33, 34]]

In [2]: B = [l[1:3] for l in A[1:3]]

In [3]: B
Out[3]: [[12, 13], [22, 23]]
</code></pre>
"
39644792,5420829.0,2016-09-22T16:55:51Z,39644748,4,"<p>It's impossible to ""fix"" syntactically invalid python by making a custom class.</p>

<p>I think the closest you can get to the mathematical interval notation in python is</p>

<pre><code>Interval('[a, b)')
</code></pre>

<p>This way becomes even more lightweight if you are passing intervals as arguments to a function and the function converts it's arguments to an appropriate type before using them. Example:</p>

<pre><code>def do_foo(interval, bar, baz):
    interval = Interval(interval)
    # do stuff

do_foo('[3,4)', 42, true)
</code></pre>

<p><a href=""http://ideone.com/BXBeXi"" rel=""nofollow"">Possible implementation</a>:</p>

<pre><code>import re

class Interval:
    def __init__(self, interval):
        """"""Initialize an Interval object from a string representation of an interval
           e.g: Interval('(3,4]')""""""
        if isinstance(interval, Interval):
            self.begin, self.end = interval.begin, interval.end
            self.begin_included = interval.begin_included
            self.end_included = interval.end_included
            return
        number_re = '-?[0-9]+(?:.[0-9]+)?'
        interval_re = ('^\s*'
                       +'(\[|\()'  # opeing brecket
                       + '\s*'
                       + '(' + number_re + ')'  # beginning of the interval
                       + '\s*,\s*'
                       + '(' + number_re + ')'  # end of the interval
                       + '\s*'
                       + '(\]|\))'  # closing brecket
                       + '\s*$'
                      )
        match = re.search(interval_re, interval)
        if match is None:
            raise ValueError('Got an incorrect string representation of an interval: {!r}'. format(interval))
        opening_brecket, begin, end, closing_brecket = match.groups()
        self.begin, self.end = float(begin), float(end)
        if self.begin &gt;= self.end:
            raise ValueError(""Interval's begin shoud be smaller than it's end"")
        self.begin_included = opening_brecket == '['
        self.end_included = closing_brecket == ']'
        # It might have been batter to use number_re = '.*' and catch exeptions float() raises instead

    def __repr__(self):
        return 'Interval({!r})'.format(str(self))

    def __str__(self):
        opening_breacket = '[' if self.begin_included else '('
        closing_breacket = ']' if self.end_included else ')'
        return '{}{}, {}{}'.format(opening_breacket, self.begin, self.end, closing_breacket)

    def __contains__(self, number):
        if self.begin &lt; number &lt; self.end:
            return True
        if number == self.begin:
            return self.begin_included
        if number == self.end:
            return self.end_included
</code></pre>
"
39645000,2063361.0,2016-09-22T17:07:46Z,39644517,0,"<p>You may also perform nested list slicing using <a href=""https://docs.python.org/2/library/functions.html#map"" rel=""nofollow""><code>map()</code></a> function as:</p>

<pre><code>B = map(lambda x: x[1:3], A[1:3])
# Value of B: [[12, 13], [22, 23]]
</code></pre>

<p>where <code>A</code> is the list mentioned in the question.</p>
"
39645057,2402008.0,2016-09-22T17:11:20Z,39644885,0,"<p>You <em>can</em> do magical things with <code>__getattr__</code>, but if you can avoid doing so then I would - it starts to get complicated! In this situation you'd likely need to overwrite <code>__getattribute__</code>, but please don't because you will bite yourself in the seat of your own pants if you start messing around with <code>__getattribute__</code>.</p>

<p>You can achieve this in a very simple way, by simply defining the first one, and then doing <code>__and__ = __add__</code> in the other functions.</p>

<pre><code>class MyClass(object):
    def comparison_1(self, thing):
        return self is not thing

    comparison_2 = comparison_1


A = MyClass()

print A.comparison_1(None)
print A.comparison_2(None)
print A.comparison_1(A)
print A.comparison_2(A)
</code></pre>

<p>gives</p>

<pre><code>$ python tmp_x.py 
True
True
False
False
</code></pre>

<p>However, I'm not a fan of this kind of hackery. I would just do</p>

<pre><code>class MyClass(object):
    def comparison_1(self, thing):
        ""Compares this thing and another thing""
        return self is not thing

    def comparison_2(self, thing):
        ""compares this thing and another thing as well""
        return self.comparison_1(thing)
</code></pre>

<p>Better to write the extra couple of lines for clarity.</p>

<h1>EDIT:</h1>

<p>So I tried it with <code>__getattribute__</code>, doesn't work :/. I admit I don't know why.</p>

<pre><code>class MyClass(object):
    def add(self, other):
        print self, other
        return None

    def __getattribute__(self, attr):
        if attr == '__add__':
            attr = 'add'

        return object.__getattribute__(self, attr)


X = MyClass()
print X.__add__
X + X
</code></pre>

<p>Doesn't work :/</p>

<pre><code>andy@batman[18:15:12]:~$ p tmp_x.py 
&lt;bound method MyClass.add of &lt;__main__.MyClass object at 0x7f52932ea450&gt;&gt;
Traceback (most recent call last):
  File ""tmp_x.py"", line 15, in &lt;module&gt;
    X + X
TypeError: unsupported operand type(s) for +: 'MyClass' and 'MyClass'
</code></pre>
"
39645098,4811678.0,2016-09-22T17:14:25Z,39644478,1,"<p>The reason <code>{% spaceless %}</code> didn't give remove all the space for you is because it only works <strong>between</strong> HTML tags. You whitespace is showing up <strong>within</strong> the <code>&lt;li&gt;</code> tag.</p>

<p>I can't seem to find a good solution for Django's standard templating system, but it does look like <a href=""http://jinja.pocoo.org/docs/dev/templates/"" rel=""nofollow"">Jinja</a> offers what you're looking for. It uses a dash to strip trailing or leading whitespace:</p>

<pre><code>{% for item in seq -%}
    {{ item }}
{%- endfor %}
</code></pre>

<p>In order to use Jinja instead of Django's default templating system, you'll have to change your <code>settings.py</code> file as described by <a href=""https://docs.djangoproject.com/en/1.10/topics/templates/#s-configuration"" rel=""nofollow"">Django's docs</a>:</p>

<pre><code>TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.jinja2.Jinja2.',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            # ... some options here ...
        },
    },
]
</code></pre>
"
39645129,162768.0,2016-09-22T17:16:35Z,39644885,1,"<p>You can do this with the <code>operator</code> module, which gives you functional versions of the operators. For example, <code>operator.and_(a, b)</code> is the same as <code>a &amp; b</code>.</p>

<p>So <code>return Vector(a + x for a in self)</code> becomes <code>return Vector(op(a, x) for a in self)</code> and you can parameterize <code>op</code>. You still need to define all of the magic methods, but they can be simple pass-throughs.</p>
"
39645243,557022.0,2016-09-22T17:23:31Z,39645153,3,"<p>Is this what you are looking for? (the polygon that results from the intersection)</p>

<pre><code>x = p1.intersection(p2)
x.area
</code></pre>

<p>Find more information in the documentation <a href=""http://toblerity.org/shapely/manual.html"" rel=""nofollow"">here</a></p>
"
39645305,464744.0,2016-09-22T17:27:15Z,39644885,2,"<p>You could use a class decorator to mutate your class and add them all in with the help of a factory function:</p>

<pre><code>import operator

def natural_binary_operators(cls):
    for name, op in {
        '__add__': operator.add,
        '__sub__': operator.sub,
        '__mul__': operator.mul,
        '__truediv__': operator.truediv,
        '__floordiv__': operator.floordiv,
        '__and__': operator.and_,
        '__or__': operator.or_,
        '__xor__': operator.xor
    }.items():
        setattr(cls, name, cls._make_binop(op))

    return cls


@natural_binary_operators
class Vector(tuple):
    @classmethod
    def _make_binop(cls, operator):
        def binop(self, other):
            try:
                return cls([operator(a, x) for a, x in zip(self, other)])
            except:
                return cls([operator(a, other) for a in self])

        return binop
</code></pre>

<p>There are a few other ways to do this, but the general idea is still the same.</p>
"
39645463,2247550.0,2016-09-22T17:36:47Z,39644885,1,"<h2>Update:</h2>

<p>This might be super-slow, but you can create an abstract class with all of the binary methods and inherit from it.</p>

<pre><code>import operator

def binary_methods(cls):
    operator_list = (
        '__add__', '__sub__', '__mul__', '__truediv__', 
        '__floordiv__', '__and__', '__or__', '__xor__'
    )

    for name in operator_list:
        bop = getattr(operator, name)
        method = cls.__create_binary_method__(bop)
        setattr(cls, name, method)

    return cls

@binary_methods
class AbstractBinary:
    @classmethod
    def __create_binary_method__(cls, bop):
        def binary_method(self, xs):
            try:
                return self.__class__(bop(a, x) for a, x in zip(self, xs))
            except:
                return self.__class__(bop(a, x) for a in self)
        return binary_method

class Vector(AbstractBinary, tuple):
    def __new__(self, x):
        return super(self, Vector).__new__(Vector, x)
</code></pre>

<hr>

<h2>Original:</h2>

<p>Okay, I think I've got a working solution (only tested in Python 2.X) that uses a class decorator to dynamically create the binary methods.</p>

<pre><code>import operator

def add_methods(cls):
    operator_list = ('__add__', '__and__', '__mul__')

    for name in operator_list:
        func = getattr(operator, name)
        # func needs to be a default argument to avoid the late-binding closure issue
        method = lambda self, xs, func=func: cls.__bop__(self, func, xs)
        setattr(cls, name, method)

    return cls

@add_methods
class Vector(tuple):
    def __new__(self, x):
        return super(self, Vector).__new__(Vector, x)

    def __bop__(self, bop, xs):
        try:
            return Vector(bop(a, x) for a, x in zip(self, xs))
        except:
            return Vector(bop(a, x) for a in self)
</code></pre>

<p>Here's some example usage:</p>

<pre><code>v1 = Vector((1,2,3))
v2 = Vector((3,4,5))

print v1 * v2 
# (3, 8, 15)
</code></pre>
"
39645859,1340389.0,2016-09-22T17:59:04Z,39644748,0,"<p>You cannot make this exact syntax work.  But you <em>could</em> do something like this by overriding the relevant comparison methods:</p>

<pre><code>a &lt;= Interval() &lt; b
</code></pre>

<p>This whole expression could then return a new <code>Interval</code> object that includes everything greater than or equal to a and strictly less than b.  <code>Interval()</code> by itself could be interpreted as the fully open interval from negative to positive infinity (i.e. the unbounded interval of all real numbers), and <code>Interval() &lt; b</code> by itself could refer to an interval bounded from above but not from below.</p>

<p>NumPy uses a similar technique for array comparison operations (where A &lt; B means ""return an array of ones and zeros that correspond to whether or not each element of A is less than the respective element of B"").</p>
"
39645923,4182185.0,2016-09-22T18:02:50Z,39645804,0,"<p>I think you could do something like this (from the gzip module <a href=""https://docs.python.org/3/library/gzip.html#examples-of-usage"" rel=""nofollow"">examples</a>)</p>

<pre><code>import gzip
with gzip.open('/home/joe/file.txt.gz', 'rb') as f:
    header = f.readline()
    # Read lines any way you want now. 
</code></pre>
"
39645994,212858.0,2016-09-22T18:06:50Z,39645804,1,"<p>The first answer you linked suggests using <a href=""https://docs.python.org/3/library/gzip.html#gzip.GzipFile"" rel=""nofollow""><code>gzip.GzipFile</code></a> - this gives you a file-like object that decompresses for you on the fly.</p>

<p>Now you just need some way to parse csv data out of a file-like object ... like <a href=""https://docs.python.org/3/library/csv.html#csv.reader"" rel=""nofollow"">csv.reader</a>.</p>

<p>The <code>csv.reader</code> object will give you a list of fieldnames, so you know the columns, their names, and how many there are.</p>

<p>Then you need to get the first 100 csv row objects, which will work exactly like in the second question you linked, and each of those 100 objects will be a list of fields.</p>

<p>So far this is all covered in your linked questions, apart from knowing about the existence of the csv module, which is listed in the <a href=""https://docs.python.org/3/library/index.html"" rel=""nofollow"">library index</a>.</p>
"
39646218,355230.0,2016-09-22T18:19:32Z,39644748,1,"<p>You can't change Python's existing syntax rules (without changing the whole language), but you can get usably close to what you want:</p>

<pre><code>class Interval(object):
    def __init__(self, left_bracket, a, b, right_bracket):
        if len(left_bracket) !=1 or left_bracket not in '[(':
            raise ValueError(
                'Unknown left bracket character: {!r}'.format(left_bracket))
        if len(right_bracket) !=1 or right_bracket not in '])':
            raise ValueError(
                'Unknown right bracket character: {!r}'.format(right_bracket))

        if a &lt; b:
            self.lower, self.upper = a, b
        else:
            self.lower, self.upper = b, a

        self.left_bracket, self.right_bracket = left_bracket, right_bracket

        if left_bracket == '[':
            if right_bracket == ']':
                self._contains = (
                    lambda self, val: self.lower &lt;= val &lt;= self.upper)
            else:
                self._contains = (
                    lambda self, val: self.lower &lt;= val &lt;  self.upper)
        else:
            if right_bracket == ']':
                self._contains = (
                    lambda self, val:  self.lower &lt; val &lt;= self.upper)
            else:
                self._contains = (
                    lambda self, val:  self.lower &lt; val &lt;  self.upper)

    __contains__ = lambda self, val: self._contains(self, val)

    def __str__(self):
        return '{}{}, {}{}'.format(self.left_bracket, self.lower, self.upper,
                                   self.right_bracket)

    def __repr__(self):
        return '{}({!r}, {}, {}, {!r})'.format(self.__class__.__name__,
                self.left_bracket, self.lower, self.upper, self.right_bracket)

if __name__ == '__main__':
    interval1 = Interval('[', 1, 3, ']')  # closed interval
    interval2 = Interval('[', 1, 3, ')')  # half-open interval

    print('{} in {}? {}'.format(3, interval1,  3 in interval1))
    print('{} in {}? {}'.format(3, interval2,  3 in interval2))
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>3 in [1, 3]? True
3 in [1, 3)? False
</code></pre>

<p>Note: The <code>a</code> and <code>b</code> arguments can be any type that can be compared.</p>
"
39646258,2800918.0,2016-09-22T18:21:59Z,39645804,1,"<p>Your code is OK;</p>

<p>pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow"">read_csv</a></p>

<blockquote>
  <p><strong>warn_bad_lines</strong> : boolean, default True </p>

<pre><code>If error_bad_lines is False, and warn_bad_lines is True, 
a warning for each âbad lineâ will be output. (Only valid with C parser).
</code></pre>
</blockquote>
"
39646318,295246.0,2016-09-22T18:25:14Z,39645804,1,"<p>Pretty much what you've already done, except <code>read_csv</code> also has <code>nrows</code> where you can specify the number of columns you want from the data set.</p>

<p>Additionally, to prevent the errors you were getting, you can set <code>error_bad_lines</code> to <code>False</code>. You'll still get warnings (if that bothers you, set <code>warn_bad_lines</code> to <code>False</code> as well). These are there to indicate inconsistency in how your dataset is filled out.</p>

<pre><code>import pandas as pd
data = pd.read_csv('google-us-data.csv.gz', nrows=100, compression='gzip',
                   error_bad_lines=False)
print(data)
</code></pre>

<p>You can easily do something similar with the <code>csv</code> built-in library, but it'll require a <code>for</code> loop to iterate over the data, has shown in other examples.</p>
"
39646555,2867928.0,2016-09-22T18:38:54Z,39646401,4,"<p>Use <code>zip</code> within a list comprehension:</p>

<pre><code>In [13]: ["""".join(seq) for seq in zip(lst, lst[1:])]
Out[13]: ['ab', 'bc', 'cd']
</code></pre>

<p>Or since you just want to concatenate two character you can also use <code>add</code> operator, by using <a href=""https://docs.python.org/3/library/itertools.html#itertools.starmap"" rel=""nofollow""><code>itertools.starmap</code></a> in order to apply the add function on character pairs:</p>

<pre><code>In [14]: from itertools import starmap

In [15]: list(starmap(add, zip(lst, lst[1:])))
Out[15]: ['ab', 'bc', 'cd']
</code></pre>
"
39646594,5801215.0,2016-09-22T18:40:49Z,39646401,0,"<p>Just one line of code is enough :</p>

<pre><code>a = ['a','b','c','d']
output = [a[i] + a[i+1] for i in xrange(len(a)) if i &lt; len(a)-1]
print output
</code></pre>
"
39646839,104349.0,2016-09-22T18:53:11Z,39646760,1,"<p><code>foo</code> has been imported into main.py; its scope is restricted to that file (and to the file where it was originally defined, of course). It does not exist within outside_code.py.</p>

<p>The real <code>eval</code> function accepts locals and globals dicts to allow you to add elements to the namespace of the evaluted code. But you can't do anything if your <code>eval_string</code> doesn't already pass those on.</p>
"
39646859,6779307.0,2016-09-22T18:54:06Z,39646760,1,"<p>The relevant documentation: <a href=""https://docs.python.org/3.5/library/functions.html#eval"" rel=""nofollow"">https://docs.python.org/3.5/library/functions.html#eval</a> </p>

<p><code>eval</code> takes an optional dictionary mapping global names to values</p>

<pre><code>eval('foo(4)', {'foo': foo})
</code></pre>

<p>Will do what you need.  It is mapping the string 'foo' to the function object foo.</p>

<p>EDIT</p>

<p>Rereading your question, it looks like this won't work for you.  My only other thought is to try </p>

<pre><code>eval_str('eval(""foo(4)"", {""foo"": lambda x: print(""Your number is {}!"".format(x))})')
</code></pre>

<p>But that's a very hackish solution and doesn't scale well to functions that don't fit in lambdas.</p>
"
39646997,100297.0,2016-09-22T19:03:18Z,39646760,1,"<p>You'd have to add those names to the scope of <code>outside_code</code>. If <code>outside_code</code> is a regular Python module, you can do so directly:</p>

<pre><code>import outside_code
import functions

for name in getattr(functions, '__all__', (n for n in vars(functions) if not n[0] == '_')):
    setattr(outside_code, name, getattr(functions, name))
</code></pre>

<p>This takes all names <code>functions</code> exports (which you'd import with <code>from functions import *</code>) and adds a reference to the corresponding object to <code>outside_code</code> so that <code>eval()</code> inside <code>outside_code.eval_string()</code> can find them.</p>

<p>You could use the <a href=""https://docs.python.org/2/library/ast.html#ast.parse"" rel=""nofollow""><code>ast.parse()</code> function</a> to produce a parse tree from the expression before passing it to <code>eval_function()</code> and then extract all global names from the expression and only add <em>those</em> names to <code>outside_code</code> to limit the damage, so to speak, but you'd still be clobbering the other module namespace to make this work.</p>

<p>Mind you, this is almost as evil as using <code>eval()</code> in the first place, but it's your only choice if you can't tell <code>eval()</code> in that other module to take a namespace parameter. That's because <em>by default</em>, <code>eval()</code> uses the global namespace of the module it is run in as the namespace.</p>

<p>If, however, your <code>eval_string()</code> function actually accepts more parameters, look for a namespace or <code>globals</code> option. If that exists, the function probably looks more like this:</p>

<pre><code>def eval_string(x, namespace=None):
    return eval(x, globals=namespace)
</code></pre>

<p>after which you could just do:</p>

<pre><code>outside_code.eval_string('foo(4)', vars(functions))
</code></pre>

<p>where <code>vars(functions)</code> gives you the namespace of the <code>functions</code> module.</p>
"
39647647,100297.0,2016-09-22T19:44:01Z,39647566,10,"<p>From the <a href=""https://docs.python.org/3/library/functions.html#exec"" rel=""nofollow""><code>exec()</code> documentation</a>:</p>

<blockquote>
  <p>Remember that at module level, globals and locals are the same dictionary. If <code>exec</code> gets two separate objects as <em>globals</em> and <em>locals</em>, the code will be executed as if it were embedded in a class definition.</p>
</blockquote>

<p>You passed in two separate dictionaries, but tried to execute code that requires module-scope globals to be available. <code>import math</code> in a class would produce a <em>local scope attribute</em>, and the function you create won't be able to access that as class scope names are not considered for function closures.</p>

<p>See <a href=""https://docs.python.org/3/reference/executionmodel.html#naming-and-binding"" rel=""nofollow""><em>Naming and binding</em></a> in the Python execution model reference:</p>

<blockquote>
  <p>Class definition blocks and arguments to <code>exec()</code> and <code>eval()</code> are special in the context of name resolution. A class definition is an executable statement that may use and define names. These references follow the normal rules for name resolution with an exception that unbound local variables are looked up in the global namespace. The namespace of the class definition becomes the attribute dictionary of the class. The scope of names defined in a class block is limited to the class block; it does not extend to the code blocks of methods[.]</p>
</blockquote>

<p>You can reproduce the error by trying to execute the code in a class definition:</p>

<pre><code>&gt;&gt;&gt; class Demo:
...     import math
...     def func(x):
...         return math.sin(x)
...     func(10)
...
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 5, in Demo
  File ""&lt;stdin&gt;"", line 4, in func
NameError: name 'math' is not defined
</code></pre>

<p>Just pass in <em>one</em> dictionary.</p>
"
39647665,2901002.0,2016-09-22T19:45:38Z,39647538,1,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.aggregate.html"" rel=""nofollow""><code>aggregate</code></a> by <code>mean</code> and <code>size</code> and then add missing values by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html"" rel=""nofollow""><code>unstack</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"" rel=""nofollow""><code>stack</code></a>.</p>

<p>Also if need change order of level <code>Sub_Type</code>, use <a href=""http://pandas.pydata.org/pandas-docs/stable/categorical.html#sorting-and-order"" rel=""nofollow"">ordered categorical</a>.</p>

<pre><code>#generating all months ('1 Month','2 Month'...'12 Month')
cat = [str(x) + ' Month' for x in range(1,13)]
df.Sub_Type = df.Sub_Type.astype('category', categories=cat, ordered=True)

df1 = df.Price.groupby([df.Sub_Date.values.astype('datetime64[D]'), df.Sub_Type])
        .agg(['mean', 'size'])
        .rename(columns={'size':'Quantity','mean':'Price'})
        .unstack(fill_value=0)
        .stack()

print (df1)
                      Price  Quantity
           Sub_Type                  
2011-02-09 4 Month     0.00         0
           6 Month     0.00         0
           12 Month  333.25         1
2011-02-14 4 Month     0.00         0
           6 Month     0.00         0
           12 Month  333.25         1
2011-03-31 4 Month     0.00         0
           6 Month     0.00         0
           12 Month  331.00         1
</code></pre>
"
39648014,1584386.0,2016-09-22T20:07:32Z,39647824,1,"<p>You can use the <a href=""http://docs.ansible.com/ansible/pip_module.html"" rel=""nofollow"">Pip module</a> in Ansible to ensure that certain packages are installed.</p>

<p>For your conditionals I would refer to: <a href=""http://stackoverflow.com/questions/21892603/how-to-make-ansible-execute-a-shell-script-if-a-package-is-not-installed"">How to make Ansible execute a shell script if a package is not installed</a> and <a href=""http://docs.ansible.com/ansible/playbooks_conditionals.html#register-variables"" rel=""nofollow"">http://docs.ansible.com/ansible/playbooks_conditionals.html#register-variables</a> - these should get you on the right track.</p>

<p>So your playbook would look a little like:</p>

<pre><code>- pip: name=pew

- name: Check if pew env exists
  command: pew command
  register: pew_env_check

- name: Execute script if pew environment doesn't exist
  command: somescript
  when: pew_env_check.stdout.find('no packages found') != -1
</code></pre>
"
39648039,1461210.0,2016-09-22T20:09:16Z,39647774,2,"<p>The first argument to <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar"" rel=""nofollow""><code>plt.bar</code></a> specifies the positions of the <em>left-hand edges</em> for each bar. To make the <em>centre</em> of each bar to align with your plot of <code>z(J2)</code> you need to offset the positions of the edges by minus half the bar width:</p>

<pre><code>plt.bar(J2 - 0.5 * width, z(J2), width=width)
</code></pre>
"
39648079,3790449.0,2016-09-22T20:12:23Z,39647977,0,"<p>The reason you aren't seeing the input is that the form field ""direccion"" is not a member of the class InformationForm.  When you load data from the request with <code>form = InformationForm(request.POST or None)</code> the direccion field is not captured.</p>

<p>I would recommend adding a new member to the <code>InformationForm</code> form (direccion), and set the widget to <code>HiddenInput</code> (read more about Widgets here: <a href=""https://docs.djangoproject.com/en/1.10/ref/forms/widgets/"" rel=""nofollow"">https://docs.djangoproject.com/en/1.10/ref/forms/widgets/</a>)</p>

<p>This keep the input hidden on the form but will pass the information back to the View. You can then remove the hard coded hidden input HTML from your template. </p>
"
39648179,4400277.0,2016-09-22T20:19:21Z,39647459,1,"<p><a href=""https://www.crummy.com/software/BeautifulSoup/"" rel=""nofollow"">BeautifulSoup</a> is a very useful module for parsing HTML and XML.    </p>

<pre><code>from bs4 import BeautifulSoup
import os

# read the file into a BeautifulSoup object
soup = BeautifulSoup(open(os.getcwd() + ""\\input.txt""))

results = {}

# parse the data, and put it into a dict, where the values are dicts
for item in soup.findAll('item'):
    # assemble dicts on the fly using a dict comprehension:
    # http://stackoverflow.com/a/14507637/4400277
    results[item.code.text] = {data.id.text:data.value.text for data in item.findAll('data')}

&gt;&gt;&gt; results
{u'A786C': {u'mountain': u'5000', u'UTEM': u''}, 
 u'A456B': {u'mountain': u'12000', u'UTEM': u'53.2'}
</code></pre>
"
39648261,5936628.0,2016-09-22T20:24:35Z,39647440,1,"<p>You can use <a href=""https://github.com/jiaaro/pydub"" rel=""nofollow"">pydub</a> for audio manipulation , including playing repetedly.</p>

<p>Here is an example. You can develop this further using examples from <a href=""http://pydub.com/"" rel=""nofollow"">pydub site.</a></p>

<pre><code>from pydub import AudioSegment
from pydub.playback import play

n = 2

audio = AudioSegment.from_file(""sound.wav"") #your audio file
play(audio * n)  #Play audio 2 times
</code></pre>

<p>Change <code>n</code> above to the number that you need.</p>
"
39648264,6866505.0,2016-09-22T20:24:41Z,39647459,0,"<p>This might be what you want:</p>

<pre><code>import xml.etree.cElementTree as ET

name = 'test.xml'
tree = ET.parse(name)
root = tree.getroot()
codes={}

for item in root.iter('Item'):
    code = item.find('Code').text
    codes[code] = {}

    for datum in item.iter('Data'):
        if datum.find('Value') is not None:
            value = datum.find('Value').text
        else:
            value = None
        if datum.find('Id') is not None:
            id = datum.find('Id').text
            codes[code][id] = value

print codes
</code></pre>

<p>This produces:
<code>{'A456B' : {'mountain' : '12000', 'UTEM' : '53.2'}, 'A786C' : {'mountain' : '5000', 'UTEM' : None}}</code></p>

<p>This iterates over all Item tags, and for each one, creates a dict key pointing to a dict of id/value pairs. An id/data pair is only created if the Id tag is not empty.</p>
"
39648475,1584386.0,2016-09-22T20:38:52Z,39648189,1,"<p>You could try creating adding something like this to your <code>.bashrc</code></p>

<pre><code>pip() {
    if [ -n ""$VIRTUAL_ENV"" ]; then
        # Run pip install
    else
        echo ""You're not in a virtualenv""
    fi
}
</code></pre>

<p>My knowledge of bash isn't the greatest but this should put you on the right path I think.</p>
"
39649124,1330293.0,2016-09-22T21:25:38Z,39645125,3,"<p>Yes, you can skip the use of <code>LabelEncoder</code> if you only want to encode string features. On the other hand if you have a categorical column of integers (instead of strings) then <code>pd.get_dummies</code> will leave as it is (see your A or C column for example). In that case you should use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow""><code>OneHotEncoder</code></a>. Ideally <code>OneHotEncoder</code> would support both integer and strings but this is being <a href=""https://github.com/scikit-learn/scikit-learn/pull/7327"" rel=""nofollow"">worked on at the moment</a>.</p>
"
39649787,416467.0,2016-09-22T22:23:36Z,39649709,0,"<p>Use <code>int()</code> to convert your hex value to an integer and then <code>chr()</code> to convert that number to a character:</p>

<pre><code>import itertools

hexdigits = ""123456789abcdef""

for dig1, dig2 in itertools.product(hexdigits, hexdigits):
    char = chr(int(dig1 + dig2, 16))
    temp = char + '\x7e\x15\x16\x28\xae\xd2\xa6\xab\xf7\x15\x88\x09\xcf\x4f\x3c'
</code></pre>
"
39649879,364696.0,2016-09-22T22:32:52Z,39649709,1,"<p>A somewhat faster solution that repeated <code>int</code>/<code>chr</code> calls (assuming you're using more than just the first byte produced) is to create a complete hex string and parse it all at once:</p>

<pre><code>import itertools
import binascii

hexdigits = ""123456789abcdef""
completehex = ''.join(map(''.join, itertools.product(hexdigits, repeat=2)))
completebytes = binascii.unhexlify(completehex)
</code></pre>

<p>This will bulk decode all the hexpairs into the raw byte values (the ""escapes"" you want), so <code>completebytes</code> would be <code>'\x00\x01\x02\x03...\xfd\xfe\xff'</code>.</p>

<p>Of course, for this specific case (if your real problem isn't just generating all possible bytes values in order), you could simplify it even further, because what you're doing is just generating all possible byte values:</p>

<pre><code># Py3
completebytes = bytes(range(256))

# On Py2, bytes is alias for str, so must use bytearray first to accept iterable of int
completebytes = bytes(bytearray(range(256)))
</code></pre>

<p>Or, just for fun, the fastest possible way abusing <code>maketrans</code>:</p>

<pre><code># Py3:
completebytes = bytes.maketrans(b'', b'')  # Add .decode('latin-1') if you really want str

# Py2:
import string
completebytes = string.maketrans('', '')
</code></pre>
"
39650142,1658617.0,2016-09-22T23:01:26Z,39650110,2,"<p><code>itertools.groupby()</code> requires the data to be consistent or sorted.</p>

<p><code>[(148, Decimal('3.0')), (148, Decimal('2.0')), (325, Decimal('3.0'))]</code> will work but <code>[(148, Decimal('3.0')), (325, Decimal('3.0')), (148, Decimal('2.0'))]</code> will not as the id is <code>148, 325, 148</code> instead of <code>148, 148, 325</code>.</p>
"
39650170,2141635.0,2016-09-22T23:07:04Z,39650110,4,"<p>You would first need to <em>sort</em> the data for the <em>groupby</em> to work, it groups <em>consecutive elements</em> based on the key you provide:</p>

<pre><code>import operator, itertools
from decimal import *
test=[(148, Decimal('3.0')), (325, Decimal('3.0')), (148, Decimal('2.0')), (183, Decimal('1.0')), (308, Decimal('1.0')), (530, Decimal('1.0')), (594, Decimal('1.0')), (686, Decimal('1.0')), (756, Decimal('1.0')), (806, Decimal('1.0'))]

for _k, data in itertools.groupby(sorted(test), operator.itemgetter(0)):
    print list(data)
</code></pre>

<p>But you would be better using a dict to group to avoid an unnecessary O(n log n) sort:</p>

<pre><code>from collections import defaultdict

d = defaultdict(list)

for t in test:
    d[t[0]].append(t)

for v in d.values():
    print(v)
</code></pre>

<p>Both would give you the same groupings, just not necessarily in the same order.</p>
"
39650183,5966448.0,2016-09-22T23:08:42Z,39649709,0,"<p>To answer the OP question, here is a way to convert,  <strong>using the escape notation</strong>, a string containing two hexadecimal digits to a character</p>

<pre><code>h = '11'
temp = eval( r""'\x"" + h + ""'"" )
</code></pre>

<p>It is not, however, the best way to do the conversion (see other answers). I would suggest <code>chr(int(h,16))</code>.  Another way would be to use integers instead of strings <code>h=0x11</code> and <code>temp = chr(h)</code>.</p>
"
39650771,1832539.0,2016-09-23T00:23:36Z,39650735,5,"<p>Your third one is actually correct. In Python 3 <a href=""https://docs.python.org/3/library/functions.html#map"">map</a> returns a map object, so you just have to call <code>list</code> on it to get a <em>list</em>. </p>

<pre><code>DATA =  [['5', '1'], ['5', '5'], ['3', '1'], ['6', '1'], ['4', '3']]

d = [list(map(int, x)) for x in DATA]

# Output:
# [[5, 1], [5, 5], [3, 1], [6, 1], [4, 3]]

# type of one of the items in the sublist
# print(type(d[0][0])
# &lt;class 'int'&gt;
</code></pre>
"
39651600,6779307.0,2016-09-23T02:23:59Z,39651540,4,"<p>My solution uses <code>Counter</code> form the <code>collections</code> module.</p>

<pre><code>from collections import Counter
def first_unique(s):
    c = Counter(s)
    for i in range(len(s)):
        if c[s[i]] == 1:
            return i
    return -1
</code></pre>
"
39651637,674039.0,2016-09-23T02:27:47Z,39651540,4,"<p>Suave version:</p>

<pre><code>from collections import Counter, OrderedDict

class OrderedCounter(Counter, OrderedDict):
    pass

def first_unique(s):
    counter = OrderedCounter(s)
    try:
        return counter.values().index(1)
    except ValueError:
        return -1
</code></pre>

<hr>

<p>Weird version:</p>

<pre><code>from collections import OrderedDict

def first_unique(s):
    nah = {0}-{0}  # funny pair of eyes 
    yeah = OrderedDict()
    for i,c in enumerate(s):
        if c not in nah:
            try:
                del yeah[c]
            except KeyError:
                yeah[c] = i
            else:
                nah.add(c)
    return next(yeah.itervalues(), -1)
</code></pre>
"
39651658,1422451.0,2016-09-23T02:29:51Z,39647269,0,"<p>Consider the following adjustment that runs a list comprehension to build all combinations of both dataframe column names that is then iterated on to  <code>&gt; 90%</code> threshold matches.</p>

<pre><code># LIST COMPREHENSION (TUPLE PAIRS) LEAVES OUT CHARACTERISTIC (FIRST COL) AND SAME NAMED COLS
columnpairs = [(i,j) for i in new.columns[1:] for j in master.columns[1:] if i != j]

# DICTIONARY COMPREHENSION TO INITIALIZE DICT OBJ
summary_dict = {col:[] for col in new.columns[1:]}

for p in columnpairs:
    i, j = p

    is_same = (new['Characteristic'] == master['Characteristic']) &amp; \
              (new[i] == master[j]) &amp; (new[i] != '--') &amp; (master[j] != '--')
    pct_same = sum(is_same) * 100 / len(master)

    if pct_same &gt; 90:        
        summary_dict[i].append({'match' : j, 'pct': pct_same})

print(summary_dict)
# {'S4': [], 'S5': [{'match': 'S2', 'pct': 100.0}]}
</code></pre>
"
39651698,3809375.0,2016-09-23T02:35:22Z,39651540,1,"<p>Your version isn't bad for few cases with ""nice"" strings... but using count is quite expensive for long ""bad"" strings, I'd suggest you cache items, for instance:</p>

<pre><code>def f1(s):
    if s == '':
        return -1
    for item in s:
        if s.count(item) == 1:
            return s.index(item)
            break
    return -1


def f2(s):
    cache = set()

    if s == '':
        return -1
    for item in s:
        if item not in cache:
            if s.count(item) == 1:
                return s.index(item)
            else:
                cache.add(item)

    return -1

import timeit
import random
import string

random.seed(1)

K, N = 500, 100000
data = ''.join(random.choice(string.ascii_uppercase + string.digits)
               for _ in range(K))

print(
    timeit.timeit('f1(data)', setup='from __main__ import f1, data', number=N))
print(
    timeit.timeit('f2(data)', setup='from __main__ import f2, data', number=N))
</code></pre>

<p>The results on my laptop are:</p>

<pre><code>32.05926330029437
4.267771588590406
</code></pre>

<p>The version using cache gives you 8x speed up factor vs yours wich is using count function all the time. So, my general advice would be... cache as much as possible whether it's possible</p>

<p>EDIT:</p>

<p>I've added Patrick Haugh version to the benchmark and it gave <code>10.92784585620725</code></p>

<p>EDIT2:</p>

<p>I've added Mehmet Furkan Demirel version to the benchmark and it gave <code>10.325440507549331</code></p>

<p>EDIT3:</p>

<p>I've added wim version to the benchmark and it gave <code>12.47985351744839</code></p>

<p>CONCLUSION:</p>

<p>I'd use the version i've proposed initially using a simple cache without relying on Python counter modules, it's not necessary (in terms of performance)</p>
"
39651714,6298477.0,2016-09-23T02:37:06Z,39651540,0,"<p>I would use a for loop to iterate the <code>String</code> from the beginning and at each index, I would check if the rest of the <code>String</code> has the character at the current index by using Substring.</p>

<p>Try this:</p>

<pre><code>def firstUniqChar(word):
for i in range(0,len(word)):  ## iterate through the string
    if not word[i] in word[i+1:len(word)]: ## check if the rest contains that char
        index=i
        break
return index
</code></pre>

<p>Hope it helps!</p>
"
39652228,3437504.0,2016-09-23T03:43:03Z,39647269,1,"<p>Another option</p>

<pre><code>import numpy as np
import pandas as pd
from sklearn.utils.extmath import cartesian
</code></pre>

<p>leveraging sklearn's cartesian function</p>

<pre><code>col_combos = cartesian([ new.columns[1:], master.columns[1:]])
print (col_combos)

[['S4' 'S1']
 ['S4' 'S2']
 ['S4' 'S3']
 ['S5' 'S1']
 ['S5' 'S2']
 ['S5' 'S3']]
</code></pre>

<p>Creating a dict with a key for every column in new except Characteristic. 
Note, this seems like a waste of space.  Maybe just save the ones with matches?</p>

<pre><code>summary_dict = {c:[] for c in new.columns[1:]} #copied from @Parfait's answer
</code></pre>

<p>Pandas/Numpy makes it easy to compare two Series.<br>
Example;</p>

<pre><code>print (new_master['S4'] == new_master['S1'])

0     True
1     True
2    False
dtype: bool
</code></pre>

<p>And now we iterate thru the Series combos and count the Trues with the help of numpy's count_nonzero().  The rest is similar to what you have</p>

<pre><code>for combo in col_combos:
    match_count = np.count_nonzero(new_master[combo[0]] == new_master[combo[1]])
    pct_same = match_count * 100 / len(new_master)
    if pct_same &gt; 90:
        summary_dict[combo[0]].append({'match' : combo[1], 'pct': match_count / len(new_master)})

print (summary_dict)

{'S4': [], 'S5': [{'pct': 1.0, 'match': 'S2'}]}
</code></pre>

<p>I'd be interested to know how it performs.  Good luck!</p>
"
39652337,2411802.0,2016-09-23T03:56:06Z,39651540,0,"<p>The idea in this solution is to use a pair of defaultdicts. The first one contains an integer count of each character and the second contains the index location of the latest character read.</p>

<p>After reading all of the characters, a list comprehension is used to find all of those that only occurred once (<code>result</code>).  The minimum index locations of these characters (found in our other defaultdict <code>order</code>) will give us the first index location of non-repeating characters.</p>

<pre><code>from collections import defaultdict
# To Create random string:
from string import ascii_lowercase
from random import choice, randint, seed

# Create a random sentence of 1000 words (1-8 characters each).
seed(0)
gibberish = ' '.join(''.join(choice(ascii_lowercase) 
                             for _ in range(randint(1, 8))) 
                     for _ in range(1000))
print(len(gibberish))
# Output: 5614

# Solution.
def first_unique(s):
    dd = defaultdict(int)
    order = defaultdict(int)
    for n, c in enumerate(s):
        dd[c] += 1
        order[c] = n
    result = [order[c] for c in dd if dd[c] == 1]
    return min(result) if result else -1
</code></pre>

<h1>Timings</h1>

<pre><code>%timeit first_unique(gibberish)
100 loops, best of 3: 2.13 ms per loop

@wim solution:
%timeit first_unique(gibberish)
100 loops, best of 3: 5.06 ms per loop

@PatrickHaugh solution (which is much easier to understand than mine):
%timeit first_unique(gibberish)
100 loops, best of 3: 4.2 ms per loop

@BPL solution:
%timeit f1(gibberish)
10 loops, best of 3: 39.2 ms per loop
%timeit f2(gibberish)
1000 loops, best of 3: 890 Âµs per loop
</code></pre>

<p>Using a much shorter sentence of twenty words (133 characters):</p>

<pre><code>%timeit first_unique(gibberish)
10000 loops, best of 3: 62.8 Âµs per loop

@wim solution:
%timeit first_unique(gibberish)
10000 loops, best of 3: 169 Âµs per loop

@PatrickHaugh solution:
%timeit first_unique(gibberish)
10000 loops, best of 3: 101 Âµs per loop

@BPL solution:
%timeit f1(gibberish)
10000 loops, best of 3: 55.1 Âµs per loop
%timeit f2(gibberish)
10000 loops, best of 3: 31 Âµs per loop
</code></pre>

<h1>Test cases.</h1>

<pre><code>s1 = 'leetcode'
s2 = 'loveleetcode'

&gt;&gt;&gt; first_unique(s1)
0

&gt;&gt;&gt; first_unique(s2)
2
</code></pre>
"
39652442,2012396.0,2016-09-23T04:05:24Z,39650397,3,"<p>I'm not sure what to tell you.  I just tried to duplicate your findings, but for me, all 3 versions take roughly the same amount of time</p>

<pre><code>wget   8.035s  
go     8.174s
python 8.242s
</code></pre>

<p>Maybe try the same experiment inside a clean VM or docker container?</p>
"
39652970,1231450.0,2016-09-23T05:06:06Z,39652922,2,"<p>You were not totally off base:</p>

<pre><code>/\*                 # match /*
(?:(?!\*/)[\s\S])+? # match anything lazily, do not overrun */
special             # match special
[\s\S]+?            # match anything lazily afterwards
\*/                 # match the closing */
</code></pre>

<p>The technique is called a tempered greedy token, see <a href=""https://regex101.com/r/mD1nJ2/4"" rel=""nofollow""><strong>a demo on regex101.com</strong></a> (mind the modifiers, e.g. <code>x</code> for verbose mode !).<br>
<hr>
You might want to try another approach tough: analyze your document, grep the comments (using eg <code>BeautifulSoup</code>) and run string functions over them (<code>if ""special"" in comment...</code>). </p>
"
39653112,6867496.0,2016-09-23T05:20:50Z,39630676,1,"<p>Try using</p>

<pre><code>from multiprocessing import set_start_method

... rest of your code here ....

if __name__ == '__main__':
    set_start_method('spawn')
    main()
</code></pre>

<p>If you search Stackoverflow for python multiprocessing and multithreading you will find a a fair few questions mentioning similar hanging issues. (esp. for python version 2.7 and 3.2)</p>

<p>Mixing multithreading and multiprocessing ist still a bit of an issue and even the python docs for <a href=""https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods"" rel=""nofollow"">multiprocessing.set_start_method</a> mention that. In your case <em>'spawn'</em> and <em>'forkserver'</em> should work without any issues.</p>

<p>Another option might be to use MultiProcessingPool directly, but this may not be possible for you in a more complex use case.</p>

<p>Btw. <em>'Not Finished'</em> may still appear in your output, as you are not waiting for your sub processes to finish, but the whole code should not hang anymore and always finish cleanly.</p>
"
39653849,2901002.0,2016-09-23T06:19:54Z,39653812,2,"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow""><code>DataFrame.sort_values</code></a> if <code>class</code> is <code>Series</code>:</p>

<pre><code>print (type(df['class']))
&lt;class 'pandas.core.series.Series'&gt;


print (df.sort_values(by='class'))
   class  col2  col3  col4  col5
0      1   4.0   5.0   5.0   5.0
2      1   3.5   5.0   6.0   4.5
4      2   3.0   3.5   3.8   6.1
3      3   3.0   4.0   4.0   4.0
1      4   4.0   4.5   5.5   6.0
</code></pre>

<p>Also if need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a>, use parameter <code>by</code>:</p>

<pre><code>print (df.groupby(by='class').sum())
       col2  col3  col4  col5
class                        
1       7.5  10.0  11.0   9.5
2       3.0   3.5   3.8   6.1
3       3.0   4.0   4.0   4.0
4       4.0   4.5   5.5   6.0
</code></pre>

<hr>

<p>And if <code>class</code> is <code>index</code>, use <a href=""http://stackoverflow.com/a/39653884/2901002""><code>Kartik solution</code></a>:</p>

<pre><code>print (df.index)
Int64Index([1, 4, 1, 3, 2], dtype='int64', name='class')

print (df.sort_index())
       col2  col3  col4  col5
class                        
1       4.0   5.0   5.0   5.0
1       3.5   5.0   6.0   4.5
2       3.0   3.5   3.8   6.1
3       3.0   4.0   4.0   4.0
4       4.0   4.5   5.5   6.0
</code></pre>

<p>Also if need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a>, use parameter <code>level</code>:</p>

<pre><code>print (df.groupby(level='class').sum())
       col2  col3  col4  col5
class                        
1       7.5  10.0  11.0   9.5
2       3.0   3.5   3.8   6.1
3       3.0   4.0   4.0   4.0
4       4.0   4.5   5.5   6.0
</code></pre>

<p>or <code>index</code>, but first solution is better, because is more general:</p>

<pre><code>print (df.groupby(df.index).sum())
       col2  col3  col4  col5
class                        
1       7.5  10.0  11.0   9.5
2       3.0   3.5   3.8   6.1
3       3.0   4.0   4.0   4.0
4       4.0   4.5   5.5   6.0
</code></pre>
"
39653884,3765319.0,2016-09-23T06:22:07Z,39653812,0,"<p>If you are starting with the data in your question:</p>

<blockquote>
<pre><code>class col2 col3 col4 col5
1     4    5    5    5
4     4    4.5  5.5  6
1     3.5  5    6    4.5
3     3    4    4     4
2     3   3.5   3.8   6.1
</code></pre>
</blockquote>

<p>And want to sort that, then it depends on whether <code>'class'</code> is an index or column. If index:</p>

<pre><code>df.sort_index()
</code></pre>

<p>should give you the answer. If column, follow <a href=""http://stackoverflow.com/a/39653849/3765319"">answer by @jezarael</a></p>
"
39654354,6285124.0,2016-09-23T06:50:04Z,39632486,3,"<p>I figured out a way to bind signals for particular models, here's the code how I did it:</p>

<pre class=""lang-py prettyprint-override""><code>from mongoengine import *
from mongoengine import signals
from datetime import datetime


class User(Document):
    uid = StringField(max_length=60, required=True)
    platform = StringField(max_length=20, required=True)
    index = StringField(max_length=80)
    last_updated = DateTimeField(required=True, default=datetime.now())

    meta = {
        'collection': 'social_users'
    }

    @classmethod
    def pre_save(cls, sender, document, **kwargs):
        if document.platform and document.uid:
            document.index = document.platform+'/'+document.uid

signals.pre_save.connect(User.pre_save, sender=User)
</code></pre>

<p>Hope this helps the people who face the same problem.</p>
"
39655184,6633975.0,2016-09-23T07:34:15Z,39654646,0,"<p>You can use <a href=""https://docs.python.org/2/library/sets.html"" rel=""nofollow"">Set</a> for pattern matching:</p>

<pre><code>from sets import Set
full_log = ['AB21','BG54','HG89','NS72','Error','CF54','SD62','KK02','FE34']

tc1 = ['HG89','NS72']
tc2 = ['AB21','BG54']
tc3 = ['KK02','FE34']
tc4 = ['CF54','SD62']

set(full_log) &amp; set(tc1)
</code></pre>

<p>output: <code>{'HG89', 'NS72'}</code></p>

<pre><code>#Finding index of set elements:

result=set(full_log) &amp; set(tc1)



def all_indices(value, qlist):
    indices = []
    idx = -1
    while True:
        try:
            idx = qlist.index(value, idx+1)
            indices.append(idx)
        except ValueError:
            break
    return indices


r=[]
for i in range(len(result)):
   s=all_indices(list(result)[i], full_log)
   r.append(s)

r
Output: [[2], [3]]
</code></pre>
"
39655284,3190054.0,2016-09-23T07:39:03Z,39654646,3,"<p>You need to create a map (dictionary) of the elements of your small list:</p>

<pre><code>m = {k: v for k, v in zip(map(tuple, [tc1, tc2, tc3, tc4])), [""tc1"", ""tc2"", ""tc3"", ""tc4""])}
&gt;&gt;&gt; {('KK02', 'FE34'): 'tc3', ('AB21', 'BG54'): 'tc2', ('CF54', 'SD62'): 'tc4', ('HG89', 'NS72'): 'tc1'}
</code></pre>

<p>You could then use an iterator to loop over the list:</p>

<pre class=""lang-python prettyprint-override""><code>itr = iter(full_log)

for i in itr:
    if i != ""Error"":
        n = next(itr)
        if n != ""Error"":
            if (i, n) in m:
                print m[(i, n)]
        else:
            print ""Er""
    else:
        print ""Er""



&gt;&gt;&gt; tc2
    tc1
    Er
    tc4
    tc3
</code></pre>

<p>If you dont mind expanding your ""Error"" entries in the first list:</p>

<pre><code>full_log2 = [item for sublist in [[i] if i != ""Error"" else [""Error"", ""Error""] for i in full_log] for item in sublist]
&gt;&gt;&gt; ['AB21', 'BG54', 'HG89', 'NS72', 'Error', 'Error', 'CF54', 'SD62', 'KK02', 'FE34']
</code></pre>

<p>Then you could use a list comprehension:</p>

<pre><code>print [m[(full_log2[i], full_log2[i+1])] if (full_log2[i], full_log2[i+1]) in m else ""Er"" for i in range(0, len(full_log2)-1, 2)]
&gt;&gt;&gt; ['tc2', 'tc1', 'Er', 'tc4', 'tc3']
</code></pre>
"
39655476,5043793.0,2016-09-23T07:50:00Z,39654646,2,"<p>In case all your short lists are equal length you could just create a <code>dict</code> where key is <code>tuple</code> of strings and value is one of the labels. The you could go through <code>full_log</code>, take a block with suitable length and see if that can be found from <code>dict</code>.</p>

<p>In case the short lists are different lengths the above approach won't work since the block length to take from <code>full_log</code> is not constant. In that case one possible solution is to add items from short lists to a tree structure where the leaf node is a label. Then for every index in <code>full_log</code> see if you can find a path from tree. If path is found jump it's length forward, otherwise try from next index:</p>

<pre><code>from collections import defaultdict
from itertools import islice

full_log = ['AB21','BG54','HG89','NS72','Error','CF54','SD62','KK02','FE34']

# Construct a tree
dd = lambda: defaultdict(dd)
labels = defaultdict(dd)
labels['HG89']['NS72'] = 'tc1'
labels['AB21']['BG54'] = 'tc2'
labels['KK02']['FE34'] = 'tc3'
labels['CF54']['SD62'] = 'tc4'

# Find label, return tuple (label, length) or (None, 1)
def find_label(it):
    length = 0
    node = labels
    while node and isinstance(node, dict):
        node = node.get(next(it, None))
        length += 1

    return node, (length if node else 1)

i = 0
result = []
while i &lt; len(full_log):
    label, length = find_label(islice(full_log, i, None))
    result.append(label if label else full_log[i])
    i += length

print result # ['tc2', 'tc1', 'Error', 'tc4', 'tc3']
</code></pre>

<p>The tree used above is kind of <a href=""https://en.wikipedia.org/wiki/Trie"" rel=""nofollow"">trie</a> with an exception that nodes can either contain children or a value (label).</p>
"
39655847,2754533.0,2016-09-23T08:10:50Z,39654060,0,"<p>On the serverside, you're trying to use the same connection for your two threads t and u.</p>

<p>I think it might work if you listened for another connection in your <code>while True:</code> loop on the server, after you started your first thread.</p>

<p>I always use the more high-level <code>socketserver</code> module (<a href=""https://docs.python.org/3.6/library/socketserver.html#socketserver-tcpserver-example"" rel=""nofollow"">Python Doc on socketserver</a>), which also natively supports Threading. I recommend checking it out!</p>

<p>By the way, since you do a lot of <code>if (x == 'r' or x == 'R')</code>: you could just do <code>if x.lower() == 'r'</code> </p>
"
39657514,3510736.0,2016-09-23T09:38:53Z,39657330,2,"<p>IIUC the explanation by @IanS (thanks again!), you can do</p>

<pre><code>In [75]: np.array([df.DATA.rolling(4).max().shift(-i) == df.DATA for i in range(4)]).T.sum(axis=1)
Out[75]: array([0, 0, 3, 0, 0, 0, 3, 0, 0])
</code></pre>

<p>To update the column:</p>

<pre><code>In [78]: df = pd.DataFrame({'DATA':s, 'POINTS':0})

In [79]: df.POINTS += np.array([df.DATA.rolling(4).max().shift(-i) == df.DATA for i in range(4)]).T.sum(axis=1)

In [80]: df
Out[80]: 
   DATA  POINTS
0     1       0
1     2       0
2     3       3
3     2       0
4     1       0
5     2       0
6     3       3
7     2       0
8     1       0
</code></pre>
"
39658662,704848.0,2016-09-23T10:35:00Z,39658574,3,"<p>What we can do is <code>apply</code> <code>nunique</code> to calc the number of unique values in the df and drop the columns which only have a single unique value:</p>

<pre><code>In [285]:
cols = list(df)
nunique = df.apply(pd.Series.nunique)
cols_to_drop = nunique[nunique == 1].index
df.drop(cols_to_drop, axis=1)

Out[285]:
   index   id   name  data1
0      0  345  name1      3
1      1   12  name2      2
2      5    2  name6      7
</code></pre>

<p>Another way is to just <code>diff</code> the numeric columns and <code>sums</code> them:</p>

<pre><code>In [298]:
cols = df.select_dtypes([np.number]).columns
diff = df[cols].diff().sum()
df.drop(diff[diff== 0].index, axis=1)
â
Out[298]:
   index   id   name  data1
0      0  345  name1      3
1      1   12  name2      2
2      5    2  name6      7
</code></pre>

<p>Another approach is to use the property that the standard deviation will be zero for a column with the same value:</p>

<pre><code>In [300]:
cols = df.select_dtypes([np.number]).columns
std = df[cols].std()
cols_to_drop = std[std==0].index
df.drop(cols_to_drop, axis=1)

Out[300]:
   index   id   name  data1
0      0  345  name1      3
1      1   12  name2      2
2      5    2  name6      7
</code></pre>

<p>Actually the above can be done in a one-liner:</p>

<pre><code>In [306]:
df.drop(df.std()[(df.std() == 0)].index, axis=1)

Out[306]:
   index   id   name  data1
0      0  345  name1      3
1      1   12  name2      2
2      5    2  name6      7
</code></pre>
"
39658853,2901002.0,2016-09-23T10:45:33Z,39658574,2,"<p>Another solution is <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow""><code>set_index</code></a> from column which are not compared and then compare first row selected by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html"" rel=""nofollow""><code>iloc</code></a> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.eq.html"" rel=""nofollow""><code>eq</code></a> with all <code>DataFrame</code> and last use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>df1 = df.set_index(['index','id','name',])
print (~df1.eq(df1.iloc[0]).all())
value     False
value2    False
value3    False
data1      True
val5      False
dtype: bool

print (df1.ix[:, (~df1.eq(df1.iloc[0]).all())].reset_index())
   index   id   name  data1
0      0  345  name1      3
1      1   12  name2      2
2      5    2  name6      7
</code></pre>
"
39659297,2035262.0,2016-09-23T11:09:22Z,39658759,1,"<p>In python code you are sending a JSON and adding a header. I bet it makes sense to do that in ruby as well. The code below is untested, since I canât test it, but it should lead you into the right direction:</p>

<pre><code>#!/usr/bin/ruby
require 'httparty'
require 'json'

response = HTTParty.post(
  ""http://192.168.1.2/info/handlers/handler.php"",
  headers: {'Content-Type', 'application/json'},
  query: { data: { 'region' =&gt; ""Europe"" } } 
  # or maybe query: { 'region' =&gt; ""Europe"" }
) 

puts response.inspect
</code></pre>
"
39659948,4771130.0,2016-09-23T11:43:08Z,39657330,1,"<pre><code>import pandas as pd

s = pd.Series([1,2,3,2,1,2,3,2,1])    
df = pd.DataFrame({'DATA':s, 'POINTS':0})

df.POINTS=df.DATA.rolling(4).max().shift(-1)
df.POINTS=(df.POINTS*(df.POINTS==df.DATA)).fillna(0)
</code></pre>
"
39660949,6870368.0,2016-09-23T12:37:11Z,39658719,1,"<p>I can reproduce your problem: after calling the QtWidget, the </p>

<p><code>print(datetime.datetime.strptime('Tue', '%a'))</code></p>

<p>results in an error.</p>

<p>If I execute after QtWidget</p>

<p><code>print(datetime.datetime.strptime('Die', '%a'))</code>
this works.</p>

<p>I am located in Switzerland, so <em>Die</em> in German is equivalent to <em>Tue</em>.</p>

<p>It seems that Qt somehow has an influence on the region settings as %A and %a evaluates the local weekday's name(<a href=""https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"" rel=""nofollow"">Datetime</a>). Maybe a Qt expert can explain more in detail, what is ongoing.</p>
"
39661116,3642398.0,2016-09-23T12:44:59Z,39660934,3,"<p>Try this:</p>

<pre><code>from importlib import util
util.find_spec(""nmap"")
</code></pre>

<p>I intend to investigate, but honestly I don't know why one works and the other doesn't. Also, observe the following interactive session:</p>

<pre><code>&gt;&gt;&gt; import importlib
&gt;&gt;&gt; importlib.util
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'importlib' has no attribute 'util'
&gt;&gt;&gt; from importlib import util
&gt;&gt;&gt; util
&lt;module 'importlib.util' from '/usr/lib/python3.5/importlib/util.py'&gt;
&gt;&gt;&gt; importlib.util
&lt;module 'importlib.util' from '/usr/lib/python3.5/importlib/util.py'&gt;
</code></pre>

<p>So...yeah. I am sure this makes perfect sense to someone, but not to me. I will update once I figure it out.</p>

<h1>Update:</h1>

<p>Comparing this to something like:</p>

<pre><code>&gt;&gt;&gt; import datetime
&gt;&gt;&gt; datetime
&lt;module 'datetime' from '/usr/lib/python3.5/datetime.py'&gt;
&gt;&gt;&gt; datetime.datetime
&lt;class 'datetime.datetime'&gt;
</code></pre>

<p>I think the difference is that in this case the first <code>datetime</code> is a module and the second is a class, while in the <code>importlib.util</code> case both are modules. So perhaps <code>module.module</code> is not OK unless the code from both modules has been loaded, while <code>module.class</code> is OK, because the class code is loaded when the module is imported.</p>

<h1>Update #2</h1>

<p>Nope, it seems like in many cases <code>module.module</code> is fine. For example:</p>

<pre><code>&gt;&gt;&gt; import urllib
&gt;&gt;&gt; urllib
&lt;module 'urllib' from '/usr/lib/python3.5/urllib/__init__.py'&gt;
&gt;&gt;&gt; urllib.error
&lt;module 'urllib.error' from '/usr/lib/python3.5/urllib/error.py'&gt;
</code></pre>

<p>So perhaps it is something specific to <code>importlib</code>.</p>

<h1>Update #3</h1>

<p>As <strong>@kfb</strong> pointed out in the comments, it does seem to be related to <code>importlib</code> specifically. See the following comment from the <a href=""https://hg.python.org/cpython/file/tip/Lib/importlib/__init__.py"" rel=""nofollow""><code>__init__.py</code> for <code>importlib</code></a>:</p>

<pre><code># Until bootstrapping is complete, DO NOT import any modules that attempt
# to import importlib._bootstrap (directly or indirectly). Since this
# partially initialised package would be present in sys.modules, those
# modules would get an uninitialised copy of the source version, instead
# of a fully initialised version (either the frozen one or the one
# initialised below if the frozen one is not available).
</code></pre>

<p><code>importlib/util.py</code> <em>does</em> import <code>importlib._bootstrap</code> so I would assume that this is realted. If my understanding is correct, when you do <code>import importlib</code> the submodules will be initialized, but are not initialized for the <code>importlib</code> module object that you have imported. At this point, if you do <code>dir(importlib)</code> you will not see <code>util</code>. Interestingly, <em>after</em> you have tried to access <code>importlib.util</code> and gotten an <code>AttributeError</code>, <code>util</code> (along with the other submodules) gets loaded/initialized, and now you <em>can</em> access <code>importlib.util</code>!</p>

<pre><code>&gt;&gt;&gt; import importlib
&gt;&gt;&gt; dir(importlib)
['_RELOADING', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__import__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_bootstrap', '_bootstrap_external', '_imp', '_r_long', '_w_long', 'find_loader', 'import_module', 'invalidate_caches', 'reload', 'sys', 'types', 'warnings']
&gt;&gt;&gt; importlib.util
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'importlib' has no attribute 'util'
&gt;&gt;&gt; importlib.util
&lt;module 'importlib.util' from '/usr/lib/python3.5/importlib/util.py'&gt;
&gt;&gt;&gt; dir(importlib)
['_RELOADING', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__import__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_bootstrap', '_bootstrap_external', '_imp', '_r_long', '_w_long', 'abc', 'find_loader', 'import_module', 'invalidate_caches', 'machinery', 'reload', 'sys', 'types', 'util', 'warnings']
</code></pre>
"
39661311,3119756.0,2016-09-23T12:54:16Z,39598666,1,"<p>Depending on the size of the data and the application I'd tackle this with either of the following methods:</p>

<ol>
<li>Database pinning:</li>
</ol>

<p>Extend your database router to allow pinning functions to specific databases. For example:</p>

<pre><code>from customrouter.pinning import use_master

@use_master
def save_and_fetch_foo():
    ...
</code></pre>

<p>A good example of that can be seen in <a href=""https://github.com/jbalogh/django-multidb-router/blob/master/multidb/pinning.py"" rel=""nofollow"">django-multidb-router</a>.
Of cource you could just use this package as well.</p>

<ol start=""2"">
<li><p>Use a <a href=""https://docs.djangoproject.com/en/1.10/topics/db/multi-db/#using-managers-with-multiple-databases"" rel=""nofollow"">model manager to route queries to specific databases</a>. </p>

<pre><code>class MyManager(models.Manager):
    def get_queryset(self):
        qs = CustomQuerySet(self.model)
        if self._db is not None:
            qs = qs.using(self._db)
        return qs
</code></pre></li>
<li><p>Write a middleware that'd route your requests to master/slave automatically.
Basically same as the pinning method but you wouldn't specify when to run <code>GET</code> requests against master.</p></li>
</ol>
"
39661572,3906713.0,2016-09-23T13:06:58Z,39658719,0,"<p>To elaborate on the nice answer by Patrick, I have now found a way to undo the localization enforced by QT</p>

<pre><code>import sys
import datetime as datetime
import locale

from PyQt5 import QtWidgets

## Start the QT window
print(datetime.datetime.strptime('Tue', '%a'))
app = QtWidgets.QApplication(sys.argv)
locale.setlocale(locale.LC_TIME, ""en_GB.utf8"")
print(datetime.datetime.strptime('Tue', '%a'))
#sys.exit(app.exec_())
</code></pre>
"
39662166,2144390.0,2016-09-23T13:34:37Z,39661582,5,"<p>You can use a <a href=""https://msdn.microsoft.com/en-us/library/bb177895(v=office.12).aspx"" rel=""nofollow"">CREATE VIEW</a> statement to create a saved Select Query in Access. The pyodbc equivalent to your VBA example would be</p>

<pre class=""lang-python prettyprint-override""><code>crsr = conn.cursor()
sql = """"""\
CREATE VIEW TestQuery AS
SELECT * FROM TestTable
""""""
crsr.execute(sql)
</code></pre>

<p>To delete that saved query you could simply execute a <a href=""https://msdn.microsoft.com/en-us/library/bb177897(v=office.12).aspx"" rel=""nofollow"">DROP VIEW</a> statement.</p>

<p>For more information on DDL in Access see</p>

<p><a href=""https://msdn.microsoft.com/en-us/library/bb267262(v=office.12).aspx"" rel=""nofollow"">Data Definition Language</a></p>
"
39662343,772649.0,2016-09-23T13:42:51Z,39648038,1,"<p>Here is a faster method to calculate the enveloop by local max:</p>

<pre><code>def calc_envelope(x, ind):
    x_abs = np.abs(x)
    loc = np.where(np.diff(np.sign(np.diff(x_abs))) &lt; 0)[0] + 1
    peak = x_abs[loc]
    envelope = np.interp(ind, loc, peak)
    return envelope
</code></pre>

<p>Here is an example output:</p>

<p><a href=""http://i.stack.imgur.com/tRfDN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tRfDN.png"" alt=""enter image description here""></a></p>

<p>It's about 6x faster than hilbert. To speedup even more, you can write a cython function that find next local max point and does normalization up to the local max point iteratively.</p>
"
39663974,1422451.0,2016-09-23T15:03:39Z,39661582,1,"<p>Consider the Python equivalent of the VBA running exactly what VBA uses: a COM interface to the Access Object library. With Python's <code>win32com</code> third-party module, you can call the <a href=""https://msdn.microsoft.com/en-us/library/office/ff195966.aspx"" rel=""nofollow"">CreateQueryDef</a> method. Do note: this COM interfacing can be applied in other languages such as PHP and R!</p>

<p>Below uses a <code>try/except/finally</code> block to ensure the Access application process closes regardless of error or success of code (similar to VBA's <code>On Error</code> handling):</p>

<pre><code>import win32com.client

# OPEN ACCESS APP AND DATABASE
dbases = [""..\Test #1.accdb"", ""..\Test #2.accdb"", ""..\Test #3.accdb"", ""..\Test #4.accdb""]

try:
    oApp = win32com.client.Dispatch(""Access.Application"")

    # CREATE QUERYDEF
    for db in dbases:
        oApp.OpenCurrentDatabase(db)
        currentdb = oApp.CurrentDb()
        currentdb.CreateQueryDef(""TestQuery"", ""SELECT * FROM TestTable"")
        currentdb = None
        oApp.DoCmd.CloseDatabase

except Exception as e:
    print(e)

finally:
    currentdb = None
    oApp.Quit
    oApp = None
</code></pre>

<hr>

<p>Also, if you need to run DML statements via pyodbc and not a COM interface, consider distributed queries as Access can query other databases directly in SQL. Below should work in Python (be sure to escape the backslash):</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT t.* FROM [C:\Path\To\Other\Database.accdb].TestTable t
</code></pre>
"
39664218,14122.0,2016-09-23T15:16:24Z,39664058,4,"<p>The proposed replacement to the Python <code>re</code> module, <a href=""https://pypi.python.org/pypi/regex"" rel=""nofollow"">available from <code>pypi</code> under the name <code>regex</code></a>, has this feature. Its canonical source repository and bug tracker are <a href=""https://bitbucket.org/mrabarnett/mrab-regex"" rel=""nofollow"">in bitbucket</a>.</p>

<p>This was added in late 2015, in <a href=""https://bitbucket.org/mrabarnett/mrab-regex/issues/151/request-k"" rel=""nofollow"">ticket 151</a>; taking an example of its use from that ticket:</p>

<blockquote>
<pre><code>import regex as mrab
&gt;&gt;&gt; bsk = mrab.compile(r'start=&gt;\K.*')
&gt;&gt;&gt; print(bsk.search('boring stuff start=&gt;interesting stuff'))
&lt;regex.Match object; span=(20, 37), match='interesting stuff'&gt;
</code></pre>
</blockquote>
"
39664564,104349.0,2016-09-23T15:32:33Z,39664509,3,"<p>You use <code>super</code> to call the original implementation.</p>

<pre><code>class New_Object(Object):
    def __init__(self):
        super(NewObject, self).__init__()
        self.info = 'whatever'
</code></pre>
"
39664566,674039.0,2016-09-23T15:32:36Z,39664509,2,"<p>That's what <a href=""https://docs.python.org/2/library/functions.html#super"" rel=""nofollow""><code>super</code></a> is for:</p>

<pre><code>class NewObject(Object):

    def __init__(self):
        super(NewObject, self).__init__()
        # self.data exists now, and you can modify it if necessary
</code></pre>
"
39664645,5951320.0,2016-09-23T15:36:40Z,39664509,1,"<p>You can use <code>super().__init__()</code> to call <code>Object.__init__()</code> from <code>New_Object.__init__()</code>.</p>

<p>What you would do:</p>

<pre><code>class Object:
    def __init__(self):
        print(""Object init"")
        self.data = ""1234""

class New_Object(Object):
    def __init__(self):
        print(""calling super"")
        super().__init__()
        print(""data is now"", self.data)
        self.data = self.data.split(""3"")

o = New_Object()

# calling super
# Object init
# data is now 1234
</code></pre>

<p>Note that you do not have to give any arguments to <code>super()</code>, as long as you are using Python 3.</p>
"
39664748,3019689.0,2016-09-23T15:42:25Z,39664509,1,"<p>The answer is that you call the superclass's <code>__init__</code> explicitly during the subclass's <code>__init__</code>.   This can be done either of two ways:</p>

<pre><code>Object.__init__(self)   # requires you to name the superclass explicitly
</code></pre>

<p>or</p>

<pre><code>super(NewObject, self).__init__()   # requires you to name the subclass explicitly
</code></pre>

<p>The latter also requires you to ensure that you're using ""new-style"" classes: in Python 3 that's always the case, but in Python 2 you must be sure to inherit from the builtin <code>object</code> class.  In Python 3 it can actually be expressed even more simply:</p>

<pre><code>super().__init__()
</code></pre>

<p>Personally, in most of my code the ""disadvantage"" of having to name the superclass explicitly is no disadvantage at all, and <code>Object.__init__()</code> lends transparency since it makes it absolutely clear what is being called. This is because most of my code is single-inheritance only. The <code>super</code> route comes into its own when you have multiple inheritance. See <a href=""http://stackoverflow.com/questions/222877/how-to-use-super-in-python"">How to use &#39;super&#39; in Python?</a></p>

<p>Python 2 example:</p>

<pre><code>class Object(object):
    def __init__(self):
        self.data = ""1234""

class NewObject:
    def __init__(self):
        # subclass-specific stuff
        super(NewObject, self).__init__()
        # more subclass-specific stuff
</code></pre>
"
39664936,10238.0,2016-09-23T15:54:09Z,39662555,2,"<p>There isn't any reason that the XlsxWriter output would work in Excel 2007 and not in Excel 2013. The file format is the default 2007 file format and Excel is very good at backward compatibility.</p>

<p>Also, I don't see the issue you describe. I modified your example to add some sample input data:</p>

<pre><code>import xlsxwriter

workbook = xlsxwriter.Workbook('a.xlsx', 
                               {'default_date_format': 'dd/mm/yyyy'})

sheet_name = 'Data'

sheet = workbook.add_worksheet(sheet_name)
sheet.set_column(0, 1, 25)
rowCount = 1

data = [
    ['Foo', 4],
    ['Bar', 5],
    ['Baz', 6],
]

# Write the data
for text, total in data:
    sheet.write_row(rowCount, 0, (text, total))
    rowCount += 1

column_chart = workbook.add_chart({'type': 'column'})
column_chart.set_size({'width': 850, 'height': 600})

column_chart.set_x_axis({'text_axis': True})
column_chart.add_series({
    'name': sheet_name,
    'categories': [sheet_name, 1, 0, rowCount, 0], # row, col row, col
    'values': [sheet_name, 1, 1, rowCount, 1],
    'data_labels': {'value': True}
})
sheet.insert_chart('D10', column_chart)
workbook.close()
</code></pre>

<p>And the output looks correct in Excel 2013:</p>

<p><a href=""http://i.stack.imgur.com/r4JI8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/r4JI8.png"" alt=""enter image description here""></a></p>
"
39665338,100297.0,2016-09-23T16:17:33Z,39665286,16,"<p>The second version does more work.</p>

<p>The <code>%s</code> operator calls <code>str()</code> on the value it interpolates, but it also has to parse the template string first to find the placeholder in the first place.</p>

<p>Unless your template string contains <em>more text</em>, there is no point in asking Python to spend more cycles on the <code>""%s"" % obj</code> expression.</p>

<p>However, paradoxically, the <code>str()</code> conversion is, in practice, slower as looking up the name <code>str()</code> and pushing the stack to call the function takes more time than the string parsing:</p>

<pre><code>&gt;&gt;&gt; from timeit import timeit
&gt;&gt;&gt; timeit('str(obj)', 'obj = 4.524')
0.32349491119384766
&gt;&gt;&gt; timeit('""%s"" % obj', 'obj = 4.524')
0.27424097061157227
</code></pre>

<p>You can recover most of that difference by binding <code>str</code> to a local name first:</p>

<pre><code>&gt;&gt;&gt; timeit('_str(obj)', 'obj = 4.524; _str = str')
0.28351712226867676
</code></pre>

<p>To most Python developers, using the string templating option is going to be confusing as <code>str()</code> is far more straightforward. Stick to the function unless you have a critical section that does a lot of string conversions.</p>
"
39665988,984421.0,2016-09-23T16:59:03Z,39657924,2,"<p>The <code>cursorPositionChanged</code> signal can be used to detect whenever the mouse or keyboard moves the caret to a position within a number. A regexp can then be used to find where the number starts and ends on the current line. Once you have that, you can use some low-level functions to calculate the global position of the number.</p>

<p>The method below uses that approach to show a tooltip below the relevant text:</p>

<pre><code>def on_cursor_position_changed(self, line, index):
    text = self.text(line)
    for match in re.finditer('(?:^|(?&lt;=\W))\d+(?:\.\d+)?(?=$|\W)', text):
        start, end = match.span()
        if start &lt;= index &lt;= end:
            pos = self.positionFromLineIndex(line, start)
            x = self.SendScintilla(
                QsciScintilla.SCI_POINTXFROMPOSITION, 0, pos)
            y = self.SendScintilla(
                QsciScintilla.SCI_POINTYFROMPOSITION, 0, pos)
            point = self.mapToGlobal(QtCore.QPoint(x, y))
            num = float(match.group())
            message = 'number: %s' % num
            break
    else:
        point = QtCore.QPoint(0, 0)
        message = ''
    QtWidgets.QToolTip.showText(point, message)
</code></pre>
"
39666157,3293881.0,2016-09-23T17:10:07Z,39666136,5,"<p>You can use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html""><code>np.where</code></a> for making such selection based on a mask -</p>

<pre><code>np.where(A,B,'')
</code></pre>

<p>Sample run -</p>

<pre><code>In [4]: A
Out[4]: array([ True, False,  True], dtype=bool)

In [5]: B
Out[5]: 
array(['eggs', 'milk', 'cheese'], 
      dtype='|S6')

In [6]: np.where(A,B,'')
Out[6]: 
array(['eggs', '', 'cheese'], 
      dtype='|S6')
</code></pre>
"
39666179,2912340.0,2016-09-23T17:11:35Z,39666136,1,"<p>Since strings may be multiplied by integers, and booleans are integers:</p>

<pre><code>A = [True, False, True]
B = ['eggs', 'milk', 'cheese']
C = [a*b for a, b in zip(A, B)]
# C = ['eggs', '', 'cheese']
</code></pre>

<p>I still uses some kind of loop (same as numpy solution), but it's hidden in concise list comprehension.</p>

<p>Alternatively:</p>

<pre><code>C = [a if b else '' for a, b in zip(A, B)]  # explicit loop may be clearer than multiply-sequence trick
</code></pre>
"
39667197,2141635.0,2016-09-23T18:17:56Z,39666890,2,"<p>The text is in the <em>td</em> not the <em>tr</em> so get the <em>td</em> using the text and then call <em>.parent</em> to get the <em>tr</em>:</p>

<pre><code>In [12]: table = soup.find('table',{'id' :'acr'})

In [13]: tr = table.find('td', text='EPS').parent

In [14]: print(tr)
&lt;tr&gt;&lt;td class=""TTRow_left"" style=""padding-left: 30px;""&gt;EPS&lt;/td&gt;&lt;td class=""TTRow_right""&gt;48.80&lt;/td&gt;
&lt;td class=""TTRow_right""&gt;42.10&lt;/td&gt;
&lt;td class=""TTRow_right""&gt;35.50&lt;/td&gt;
&lt;td class=""TTRow_right""&gt;28.50&lt;/td&gt;
&lt;td class=""TTRow_right""&gt;22.10&lt;/td&gt;
&lt;/tr&gt;
In [15]: [td.text for td in tr.select(""td + td"")]
Out[15]: [u'48.80', u'42.10', u'35.50', u'28.50', u'22.10']
</code></pre>

<p>Which you will see exactly matches what is on the page.</p>

<p>Another approach would be to call <em>find_next_siblings</em>:</p>

<pre><code>In [17]: tds = table.find('td', text='EPS').find_next_siblings(""td"")

In [18]: tds
Out[19]: 
[&lt;td class=""TTRow_right""&gt;48.80&lt;/td&gt;,
 &lt;td class=""TTRow_right""&gt;42.10&lt;/td&gt;,
 &lt;td class=""TTRow_right""&gt;35.50&lt;/td&gt;,
 &lt;td class=""TTRow_right""&gt;28.50&lt;/td&gt;,
 &lt;td class=""TTRow_right""&gt;22.10&lt;/td&gt;]
In [20]: [td.text for td in tds]
Out[20]: [u'48.80', u'42.10', u'35.50', u'28.50', u'22.10']
</code></pre>
"
39667248,3510736.0,2016-09-23T18:21:10Z,39667089,5,"<p>Say you first build an <code>xyzy</code> array:</p>

<pre><code>import itertools

xyz = [np.array(p) for p in itertools.product(range(volume.shape[0]), range(volume.shape[1]), range(volume.shape[2]))]
</code></pre>

<p>Now, using <a href=""http://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.linalg.norm.html""><code>numpy.linalg.norm</code></a>, </p>

<pre><code>np.linalg.norm(xyz - roi, axis=1) &lt; radius
</code></pre>

<p>checks whether the distance for each tuple from <code>roi</code> is smaller than radius.</p>

<p>Finally, just <code>reshape</code> the result to the dimensions you need.</p>
"
39667342,3293881.0,2016-09-23T18:26:59Z,39667089,14,"<p><strong>Approach #1</strong></p>

<p>Here's a vectorized approach -</p>

<pre><code>m,n,r = volume.shape
x,y,z = np.mgrid[0:m,0:n,0:r]
X = x - roi[0]
Y = y - roi[1]
Z = z - roi[2]
mask = X**2 + Y**2 + Z**2 &lt; radius**2
</code></pre>

<p>Possible improvement : We can probably speedup the last step with <code>numexpr</code> module -</p>

<pre><code>import numexpr as ne

mask = ne.evaluate('X**2 + Y**2 + Z**2 &lt; radius**2')
</code></pre>

<p><strong>Approach #2</strong></p>

<p>We can also gradually build the three ranges corresponding to the shape parameters and perform the subtraction against the three elements of <code>roi</code> on the fly without actually creating the meshes as done earlier with <code>np.mgrid</code>. This would be benefited by the use of <a href=""http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html""><code>broadcasting</code></a> for efficiency purposes. The implementation would look like this -</p>

<pre><code>m,n,r = volume.shape
vals = ((np.arange(m)-roi[0])**2)[:,None,None] + \
       ((np.arange(n)-roi[1])**2)[:,None] + ((np.arange(r)-roi[2])**2)
mask = vals &lt; radius**2
</code></pre>

<p>Simplified version : Thanks to @Bi Rico for suggesting an improvement here as we can use <code>np.ogrid</code> to perform those operations in a bit more concise manner, like so -</p>

<pre><code>m,n,r = volume.shape    
x,y,z = np.ogrid[0:m,0:n,0:r]-roi
mask = (x**2+y**2+z**2) &lt; radius**2
</code></pre>

<hr>

<p><strong>Runtime test</strong></p>

<p>Function definitions -</p>

<pre><code>def vectorized_app1(volume, roi, radius):
    m,n,r = volume.shape
    x,y,z = np.mgrid[0:m,0:n,0:r]
    X = x - roi[0]
    Y = y - roi[1]
    Z = z - roi[2]
    return X**2 + Y**2 + Z**2 &lt; radius**2

def vectorized_app1_improved(volume, roi, radius):
    m,n,r = volume.shape
    x,y,z = np.mgrid[0:m,0:n,0:r]
    X = x - roi[0]
    Y = y - roi[1]
    Z = z - roi[2]
    return ne.evaluate('X**2 + Y**2 + Z**2 &lt; radius**2')

def vectorized_app2(volume, roi, radius):
    m,n,r = volume.shape
    vals = ((np.arange(m)-roi[0])**2)[:,None,None] + \
           ((np.arange(n)-roi[1])**2)[:,None] + ((np.arange(r)-roi[2])**2)
    return vals &lt; radius**2

def vectorized_app2_simplified(volume, roi, radius):
    m,n,r = volume.shape    
    x,y,z = np.ogrid[0:m,0:n,0:r]-roi
    return (x**2+y**2+z**2) &lt; radius**2
</code></pre>

<p>Timings -</p>

<pre><code>In [106]: # Setup input arrays  
     ...: volume = np.random.rand(90,110,100) # Half of original input sizes 
     ...: roi = np.random.rand(3)
     ...: radius = 3.4
     ...: 

In [107]: %timeit _make_mask(volume, roi, radius)
1 loops, best of 3: 41.4 s per loop

In [108]: %timeit vectorized_app1(volume, roi, radius)
10 loops, best of 3: 62.3 ms per loop

In [109]: %timeit vectorized_app1_improved(volume, roi, radius)
10 loops, best of 3: 47 ms per loop

In [110]: %timeit vectorized_app2(volume, roi, radius)
100 loops, best of 3: 4.26 ms per loop

In [139]: %timeit vectorized_app2_simplified(volume, roi, radius)
100 loops, best of 3: 4.36 ms per loop
</code></pre>

<p>So, as always <code>broadcasting</code> showing its magic for a crazy almost <strong><code>10,000x</code></strong> speedup over the original code and more than <strong><code>10x</code></strong> better than creating meshes by using on-the-fly broadcasted operations!</p>
"
39667515,901925.0,2016-09-23T18:38:32Z,39666136,1,"<p><code>np.char</code> applies string methods to elements of an array:</p>

<pre><code>In [301]: np.char.multiply(B, A.astype(int))
Out[301]: 
array(['eggs', '', 'cheese'], 
      dtype='&lt;U6')
</code></pre>

<p>I had to convert the boolean to integer, and place it second.</p>

<p>Timing in other questions indicates that <code>np.char</code> iterates and applies the Python methods.  Speed's about the same as for list comprehension.</p>

<p>For in-place change, use masked assignment instead of <code>where</code></p>

<pre><code>In [306]: B[~A]=''
In [307]: B
Out[307]: 
array(['eggs', '', 'cheese'], 
      dtype='&lt;U6')
</code></pre>
"
39667609,2308683.0,2016-09-23T18:45:19Z,39667572,8,"<p><code>result</code> is a <code>datetime</code> object</p>

<p><code>datetime.utcnow()</code> is a class method of all <code>datetime</code> objects. </p>

<p><code>result</code> is not changing at all. <code>utcnow()</code> is </p>
"
39668837,5190654.0,2016-09-23T20:12:26Z,39665207,3,"<p>Your problem appears to be a limitation in the current inequality solver: the algorithm that transforms a system of inequalities to a union of sets apparently needs to sort the boundary points determined by those inequalities (even if there's only one such point). Reduction of inequalities with symbolic limits has not been implemented yet.</p>

<p>I suggest a <strong>dirty trick</strong> to get around this limitation. Define:</p>

<pre><code>class SymbolTrick(NumberSymbol):
    def __new__(self, name):
        obj = NumberSymbol.__new__(self)
        obj._name = name
        return obj

    _as_mpf_val = pi._as_mpf_val
    approximation_interval = pi.approximation_interval
    __str__ = lambda self: str(self._name)
</code></pre>

<p>This defines a subclass of <em>NumberSymbol</em> having the same numeric value of <em>pi</em> (it is necessary to specify one, as the inequality reduction algorithm needs to sort the list boundaries otherwise it will fail).</p>

<p>At this point:</p>

<pre><code>In [7]: Y = SymbolTrick(""Y"")

In [8]: E(X, X &gt; Y, evaluate=False)
Out[8]: 
â                              
â                               
â®                    2         
â®            -(X - Î¼)          
â®            ââââââââââ        
â®                  2           
â®               2âÏ            
â®      â2âXââ¯                  
â® ââââââââââââââââââââââââââ dX
â®        â                     
â®        â                      
â®        â®             2       
â®        â®     -(X - Î¼)        
â®        â®     ââââââââââ      
â®        â®           2         
â®        â®        2âÏ          
â®        â® â2ââ¯                
â® 2ââÏâÏââ® ââââââââââââââ dX   
â®        â®     2ââÏâÏ          
â®        â¡                     
â®        Y                     
â¡      
Y
</code></pre>
"
39669293,3832970.0,2016-09-23T20:47:06Z,39669147,1,"<p><code>\A</code> is unambiguous string start anchor. <code>^</code> can change its behavior depending on whether the <code>re.M</code> modifier is used or not.</p>

<p><em>When to use <code>\A</code> and when to use <code>^</code>?</em></p>

<p>When you want to match either start of a string <em>OR</em> a line, you use <code>^</code>, when you only need to match the start of a string <em>regardless of any modifiers</em>, use <code>\A</code>. You may re-use one and the same string pattern but compile it with different flags.</p>

<p>This also means that if you are using <code>re.M</code> flag, and you want to match the string start and then line start positions, you will mix the <code>\A</code> and <code>^</code> inside that pattern.</p>
"
39669301,4500584.0,2016-09-23T20:47:31Z,39669147,2,"<p>Both of these match:</p>

<pre><code>re.search('^abc', 'abc')
re.search('\Aabc', 'abc')
</code></pre>

<p>This also matches:</p>

<pre><code>re.search('^abc', 'firstline\nabc', re.M)
</code></pre>

<p>This does not:</p>

<pre><code>re.search('\Aabc', 'firstline\nabc', re.M)
</code></pre>
"
39669649,674039.0,2016-09-23T21:13:39Z,39669538,3,"<p>A nicer way might be to build up the expected calls your self then use a direct assertion:</p>

<pre><code>&gt;&gt;&gt; from mock import call, Mock
&gt;&gt;&gt; f = Mock()
&gt;&gt;&gt; f('first call')
&lt;Mock name='mock()' id='31270416'&gt;
&gt;&gt;&gt; f('second call')
&lt;Mock name='mock()' id='31270416'&gt;
&gt;&gt;&gt; expected_calls = [call(s + ' call') for s in ('first', 'second')]
&gt;&gt;&gt; f.assert_has_calls(expected_calls)
</code></pre>

<p>Note that the calls must be sequential, if you don't want that then override the <code>any_order</code> kwarg to the assertion.  </p>

<p>Also note that it's permitted for there to be extra calls before or after the
specified calls.  If you don't want that, you'll need to add another assertion:</p>

<pre><code>&gt;&gt;&gt; assert f.call_count == len(expected_calls)
</code></pre>

<hr>

<p>Addressing the comment of mgilson, here's an example of creating a dummy object that you can use for wildcard equality comparisons:</p>

<pre><code>&gt;&gt;&gt; class AnySuffix(object):
...     def __eq__(self, other):
...         try:
...             return other.startswith('PASS')
...         except Exception:
...             return False
...        
&gt;&gt;&gt; f = Mock()
&gt;&gt;&gt; f('PASS and some other stuff')
&lt;Mock name='mock()' id='28717456'&gt;
&gt;&gt;&gt; f('PASS more stuff')
&lt;Mock name='mock()' id='28717456'&gt;
&gt;&gt;&gt; f(""PASS blah blah don't care"")
&lt;Mock name='mock()' id='28717456'&gt;
&gt;&gt;&gt; expected_calls = [call(AnySuffix())]*3
&gt;&gt;&gt; f.assert_has_calls(expected_calls)
</code></pre>

<p>And an example of the failure mode:</p>

<pre><code>&gt;&gt;&gt; Mock().assert_has_calls(expected_calls)
AssertionError: Calls not found.
Expected: [call(&lt;__main__.AnySuffix object at 0x1f6d750&gt;),
 call(&lt;__main__.AnySuffix object at 0x1f6d750&gt;),
 call(&lt;__main__.AnySuffix object at 0x1f6d750&gt;)]
Actual: []
</code></pre>
"
39669722,748858.0,2016-09-23T21:20:48Z,39669538,2,"<p>I think that many of the difficulties here are wrapped up in the treatment of the ""call"" object.  It can be thought of as a tuple with 2 members <code>(args, kwargs)</code> and so it's frequently nice to unpack it:</p>

<pre><code>args, kwargs = call
</code></pre>

<p>Once it's unpacked, then you can make your assertions separately for args and kwargs (since one is a tuple and the other a dict)</p>

<pre><code>def test_foo(self):
    def foo(fn):
        fn('PASS and some other stuff')

    f = Mock()
    foo(f)
    foo(f)
    foo(f)

    for call in f.call_args_list:
        args, kwargs = call
        self.assertTrue(all(a.startswith('PASS') for a in args))
</code></pre>

<p>Note that sometimes the terseness isn't helpful (e.g. if there is an error):</p>

<pre><code>for call in f.call_args_list:
    args, kwargs = call
    for a in args:
        self.assertTrue(a.startswith('PASS'), msg=""%s doesn't start with PASS"" % a)
</code></pre>
"
39670509,2172464.0,2016-09-23T22:38:10Z,39669718,0,"<p>In general <code>reactor.callLater()</code> is the function you want. So if the function needs to be called 5 seconds later, your code would look like this:</p>

<pre><code>from twisted.internet import reactor
reactor.callLater(5, cancelTest)
</code></pre>

<p>One thing that is strange is that your <code>task.deferLater</code> implementation should also work. However without seeing more of your code I don't think I can help you more other than stating that it's strange :)</p>

<h1>References</h1>

<ul>
<li><a href=""https://twistedmatrix.com/documents/current/core/howto/defer.html#callbacks"" rel=""nofollow"">https://twistedmatrix.com/documents/current/core/howto/defer.html#callbacks</a>
<a href=""http://twistedmatrix.com/documents/current/api/twisted.internet.base.ReactorBase.html#callLater"" rel=""nofollow"">http://twistedmatrix.com/documents/current/api/twisted.internet.base.ReactorBase.html#callLater</a></li>
</ul>
"
39671531,3543300.0,2016-09-24T01:07:57Z,39670940,1,"<p>You've likely installed opencv 3, which doesn't have the <code>cv2.cv</code> module. It's all in <code>cv2</code> now. </p>

<p>To verify run this in a python interpreter </p>

<pre><code>import cv2
print cv2.__version__ 
</code></pre>

<p>Anything like <code>3.0.0</code> or <code>3.1.0</code> means that the <code>cv2.cv</code> module doesn't exist.</p>
"
39671543,6866505.0,2016-09-24T01:09:51Z,39671504,4,"<p>You can capture stdout while <code>help(myclass)</code> runs:</p>

<pre><code>from cStringIO import StringIO
import sys

stdout = sys.stdout
buffer = StringIO()
sys.stdout = buffer

help(myclass)

sys.stdout = stdout

myhelp = buffer.getvalue()
</code></pre>
"
39671870,353839.0,2016-09-24T02:18:32Z,39671786,0,"<p>I think the answer to <a href=""http://stackoverflow.com/questions/33539606/excluding-basic-authentication-in-a-single-view-django-rest-framework"">this question</a> applies here as well.</p>

<p>If you don't want to check tokens for one view, you can add <code>@authentication_classes([])</code> to the view. That should keep the default in place for other views while treating this one differently.</p>
"
39671872,674039.0,2016-09-24T02:18:41Z,39671786,1,"<p>You seem to be mixing up <strong>authentication</strong> and <strong>authorization</strong>.</p>

<p>By using the <code>@permission_classes</code> decorator on your view, you have overridden the default authorization from settings.  But you still have the default authentication classes from settings.</p>

<p>Try adding also to your view another decorator, to bypass the <code>TokenAuthentication</code>:</p>

<pre><code>@authentication_classes([])
</code></pre>

<p>Note that if you put this on a <code>POST</code> endpoint, your app is now vulnerable to nasty stuff like Cross-Site Request Forgery.  </p>
"
39673192,3599803.0,2016-09-24T06:17:13Z,39662555,-1,"<p>Problem actually solved by upgrading my version of the library. My version was 0.5.1 and now is 0.9. I wonder if it was a bug in the earlier version. Works great now, too bad it took me some time to figure out</p>
"
39673729,6765112.0,2016-09-24T07:25:59Z,39673377,1,"<p>You would use boolean indexing to do this. To obtain the three examples you give (in the same order as you posted them, where <code>x</code> is your original 2d array), you could write:</p>

<pre><code>numpy.atleast_2d( x[ x[:,1]==21 ] )
numpy.atleast_2d( x[ x[:,2]==0 ] )
numpy.atleast_2d( x[ x[:,2]==1 ] )
</code></pre>

<p>The first should be interpreted as saying 'extract the rows of <code>x</code> where the element in the second column equals 21' and so on. There is a page in the scipy docs that explains how to use indexing in numpy <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"" rel=""nofollow"" title=""here"">here</a>. Since you required that the returned arrays all be 2D, I have used the <code>atleast_2d</code> function.</p>
"
39674145,3293881.0,2016-09-24T08:18:45Z,39673377,1,"<p>Here's an approach to handle many such groupings -</p>

<pre><code># Sort array based on second column
sorted_a = a[np.argsort(a[:,1])]

# Get shifting indices for first col. Split along axis=0 using those.
shift_idx = np.unique(sorted_a[:,1],return_index=True)[1][1:]
out = np.split(sorted_a,shift_idx)
</code></pre>

<p>Alternatively, for performance efficiency purposes, we can get <code>shift_idx</code>, like so -</p>

<pre><code>shift_idx = np.flatnonzero(sorted_a[1:,1] &gt; sorted_a[:-1,1])+1
</code></pre>

<p>Sample run -</p>

<pre><code>In [27]: a
Out[27]: 
array([[33, 21,  1],
       [33, 21,  2],
       [32, 22,  0],
       [33, 21,  3],
       [34, 34,  1]])
In [28]: sorted_a = a[np.argsort(a[:,1])]

In [29]: np.split(sorted_a,np.unique(sorted_a[:,1],return_index=True)[1][1:])
Out[29]: 
[array([[33, 21,  1],
        [33, 21,  2],
        [33, 21,  3]]), array([[32, 22,  0]]), array([[34, 34,  1]])]
</code></pre>
"
39674434,455262.0,2016-09-24T08:52:03Z,39590741,2,"<p>Try building it like this instead:</p>

<pre><code>cmake -DWITH_QUICKTIME=OFF -DWITH_GSTREAMER=OFF -DWITH_FFMPEG=OFF -DCMAKE_C_COMPILER=/usr/bin/clang -DCMAKE_CXX_COMPILER=/usr/bin/clang++ -DCMAKE_BUILD_TYPE=Release .. ; make -j4
</code></pre>
"
39675635,105043.0,2016-09-24T11:06:15Z,39603391,0,"<p>If the main objection is</p>

<blockquote>
  <p>This is messy if there are many checks:</p>
</blockquote>

<pre><code>if a:
   b = check_b()

   if b:
      c = check_c()

      if c:
          return a, b, c
</code></pre>

<p>A fairly nice pattern is to reverse the condition and return early</p>

<pre><code>if not a:
    return  # None, or some value, or however you want to handle this
b = check_b()
if not b:
    return
c = check_c()
if not c:
    return

# ok, they were all truthy
return a, b, c
</code></pre>
"
39676766,3964322.0,2016-09-24T13:19:13Z,39676437,2,"<p>You can use this relation: <a href=""https://wikimedia.org/api/rest_v1/media/math/render/svg/f6404a766d86e9d78a5c4f82e05de37469a5f8e9"" rel=""nofollow"">https://wikimedia.org/api/rest_v1/media/math/render/svg/f6404a766d86e9d78a5c4f82e05de37469a5f8e9</a></p>

<p>from <a href=""https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant"" rel=""nofollow"">https://en.wikipedia.org/wiki/Determinant#Properties_of_the_determinant</a></p>

<p>So divide your matrix by the mean and then compute the determinant to avoid overflow. Later you can multiply with the mean to the power of n (length of one axis)</p>

<p>edit: I'm not sure if the mean is the ideal choice though. This is more a math question</p>
"
39677259,2151446.0,2016-09-24T14:09:36Z,39670940,0,"<p>Presumable you did:</p>

<pre><code>git clone git@github.com:opencv/opencv.git
mkdir build; cd build
cmake ../opencv &amp;&amp; make &amp;&amp; sudo make install
</code></pre>

<p>But if you do this:</p>

<pre><code>cd opencv
git describe
</code></pre>

<p>You will get something like</p>

<pre><code>3.1.0-1374-g7f14a27
</code></pre>

<p>That is, the default branch is OpenCV 3, which doesn't have the cv2.cv module.
Either change your code to work with the OpenCV 3 cv2 module.
Or downgrade to OpenCV2. You can do this by:</p>

<pre><code>cd opencv
git checkout 2.4.13.1
cd ../build &amp;&amp; so on ...
</code></pre>
"
39677393,2074832.0,2016-09-24T14:23:05Z,39677070,1,"<p>Pseudo-code algorithm:</p>

<pre><code>declare two-dimensional array workers
for each shift in shifts_yesterday
    for each element x in shift
        add x to workers[x]
        for each element y != x in shift
            add y to workers[x]

for each list xs in workers
    print xs[0] + "": ""
    for each element w in xs except the first
        print xs[w] + "", ""
</code></pre>

<p>The time complexity is <code>O(n*m^2 + w*m)</code> where <code>n</code> is the number of shifts, <code>m</code> is the maximum number of workers in any shift and <code>w</code> is the total number of workers. If you could settle for seeing each worker once (don't display both <code>a: b</code> and <code>b: a</code>) you could shave off one <code>m</code>. That's a quadratic algorithm, I believe that's the best you can do.</p>
"
39677443,2141635.0,2016-09-24T14:28:36Z,39677070,2,"<p>A simplified and more efficient version of your own code using sets to store values and <em>itertools.combinations</em> to pair up the workers:</p>

<pre><code>shifts = [['a', 'b', 'c', 'd'], ['b', 'c', 'e', 'f']]


from itertools import combinations
import collections

d = collections.defaultdict(set)
for sub in shifts:
    for a, b in combinations(sub, 2):
        d[a].add(b)
        d[b].add(a)

for k, v in sorted(d.items()):
print(k, v)
</code></pre>

<p>Which would give you:</p>

<pre><code>('a', set(['c', 'b', 'd']))
('b', set(['a', 'c', 'e', 'd', 'f']))
('c', set(['a', 'b', 'e', 'd', 'f']))
('d', set(['a', 'c', 'b']))
('e', set(['c', 'b', 'f']))
('f', set(['c', 'b', 'e']))
</code></pre>

<p>On your small sample input:</p>

<pre><code>In [1]: import collections

In [2]: %%timeit
   ...: shifts = [['a', 'b', 'c', 'd'], ['b', 'c', 'e', 'f']]
   ...: workers = [i for s in shifts for i in s]
   ...: d = collections.defaultdict(list)
   ...: for w in workers:
   ...:     for s in shifts:
   ...:         for i in s:
   ...:             if i != w and w in s:
   ...:                 if w in d.keys():
   ...:                     if i not in d[w]:
   ...:                         d[w].append(i)
   ...:                 else:
   ...:                     d[w].append(i)
   ...: 
10000 loops, best of 3: 21.6 Âµs per loop

In [3]: from itertools import combinations

In [4]: %%timeit
   ...: shifts = [['a', 'b', 'c', 'd'], ['b', 'c', 'e', 'f']]
   ...: d = collections.defaultdict(set)
   ...: for sub in shifts:
   ...:     for a, b in combinations(sub, 2):
   ...:         d[a].add(b)
   ...:         d[b].add(a)
   ...: 
100000 loops, best of 3: 4.55 Âµs per loop
</code></pre>
"
39677461,5827613.0,2016-09-24T14:30:02Z,39677070,1,"<p>There is should be more conditions specified. For instance, if you have a total ""shifts_yesterday"" array size limited to 64 then you can use long type to store shift-bit for worker. then you can answer on the question via single operation :  </p>

<pre><code>a = 00000001  
b = 00000011  
d = 00000010  
f = 00000010
</code></pre>

<p>Does b work with d ?   </p>

<pre><code>((b &amp; d) != 0) : true
</code></pre>

<p>Does a work with f ?   </p>

<pre><code>((a &amp; f) != 0) : false
</code></pre>
"
39677544,389289.0,2016-09-24T14:37:57Z,39677070,3,"<pre><code>result = defaultdict(set)

for shift in shifts:
    for worker in shift:
        result[worker].update(shift)

# now, result[a] contains: a, b, c, d - so remove the a

for k, v in result.iteritems():
    v.remove(k)
</code></pre>
"
39677637,2867928.0,2016-09-24T14:46:54Z,39677462,2,"<p>You can use following function in order to remove the holes, by replacing the color of each pixel with a maximum colors of it's environment pixels:  </p>

<pre><code>import numpy as np
import cv2

def remove_noise(gray, num):
    Y, X = gray.shape
    nearest_neigbours = [[
        np.argmax(
            np.bincount(
                gray[max(i - num, 0):min(i + num, Y), max(j - num, 0):min(j + num, X)].ravel()))
        for j in range(X)] for i in range(Y)]
    result = np.array(nearest_neigbours, dtype=np.uint8)
    cv2.imwrite('result2.jpg', result)
    return result
</code></pre>

<p>Demo:</p>

<pre><code>img = cv2.imread('mCOFl.png')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

remove_noise(gray, 10)
</code></pre>

<p>Input image:</p>

<p><a href=""http://i.stack.imgur.com/AAypM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AAypM.png"" alt=""enter image description here""></a></p>

<p>Out put:</p>

<p><a href=""http://i.stack.imgur.com/PYttM.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PYttM.jpg"" alt=""enter image description here""></a></p>

<p>Note: Since this function replace the color of corner pixels too, you can sue <code>cv2.goodFeaturesToTrack</code> function to find the corners and restrict the denoising for that pixels </p>

<pre><code>corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 30)
corners = np.squeeze(np.int0(corners))
</code></pre>
"
39677680,347567.0,2016-09-24T14:52:06Z,39677070,1,"<p>I think you're looking for a set membership relationship. Let's call it <code>coworkers</code>:</p>

<pre><code>shifts_yesterday = [['a', 'b', 'c', 'd'], ['b', 'c', 'e', 'f']]

def coworkers(worker, shifts):
    coworkers = set()
    coworkers.update( *[shift for shift in shifts if worker in shift] )
    return coworkers
</code></pre>

<p>For each worker, you create a set of all the shifts that include the worker.</p>

<pre><code>everybody = set()
everybody.update( *shifts_yesterday )

for worker in everybody:
     print(""{}: {}"".format(worker, coworkers(worker, shifts_yesterday)))
</code></pre>

<p>The output is</p>

<pre><code>a: set(['a', 'c', 'b', 'd'])
c: set(['a', 'c', 'b', 'e', 'd', 'f'])
b: set(['a', 'c', 'b', 'e', 'd', 'f'])
e: set(['c', 'b', 'e', 'f'])
d: set(['a', 'c', 'b', 'd'])
f: set(['c', 'b', 'e', 'f'])
</code></pre>
"
39678093,3510736.0,2016-09-24T15:39:35Z,39677967,3,"<p>Say you create an array of your generators:</p>

<pre><code>generators = [
    np.random.multivariate_normal([1, 1], [[5, 1], [1, 5]]),             
    np.random.multivariate_normal([0, 0], [[5, 1], [1, 5]]), 
    np.random.multivariate_normal([-1, -1], [[5, 1], [1, 5]])]
</code></pre>

<p>Now you can create a weighted random of generator indices, since <a href=""http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html"" rel=""nofollow""><code>np.random.choice</code></a> supports weighted sampling:</p>

<pre><code>draw = np.random.choice([0, 1, 2], 100, p=[0.7, 0.2, 0.1])
</code></pre>

<p>(<code>draw</code> is a length-100 array of entries, each from <em>{0, 1, 2}</em> with probability <em>0.7, 0.2, 0.1</em>, respectively.)</p>

<p>Now just generate the samples:</p>

<pre><code>[generators[i] for i in draw]
</code></pre>
"
39680089,6486738.0,2016-09-24T19:21:58Z,39675898,3,"<h2>Is it bad practice?</h2>

<p>It's reasonable to assume that it isn't bad practice for this example because:</p>

<ul>
<li>The author doesn't give any reason. Maybe it's just disliked by him/her.</li>
<li>Python documentation doesn't mention it's bad practice (from what I've seen).</li>
<li><code>foo += 'ooo'</code> is just as readable (according to me) and is approximately 100 times faster than <code>foo = ''.join([foo, 'ooo'])</code>.</li>
</ul>

<h2>When should one be used over the other?</h2>

<p>Concatenation of strings have the disadvantage of needing to create a new string and allocate new memory <em>for every concatenation</em>! This is time consuming, but isn't that big of a deal with few and small strings. When you know the number of strings to concatenate and don't need more than maybe 2-4 concatenations I'd go for it.</p>

<hr>

<p>When joining strings Python only has to allocate new memory for the final string, which is much more efficient, but could take longer to compute. Also, because strings are immutable it's often more practical to use a list of strings to dynamically mutate, and only convert it to a string when needed.</p>

<p>It's often convenient to create strings with str.join() since it takes an iterable. For example:</p>

<pre><code>letters = "", "".join(""abcdefghij"")
</code></pre>

<h2>To conclude</h2>

<p>In most cases it makes more sense to use <code>str.join()</code> but there are times when concatenation is just as viable. Using any form of string concatenation for huge or many strings would be bad practice just as using <code>str.join()</code> would be bad practice for short and few strings, in my own opinion.</p>

<p>I believe that the author was just trying to create a rule of thumb to easier identify when to use what without going in too much detail or make it complicated.</p>
"
39680380,2411320.0,2016-09-24T19:54:47Z,39660968,2,"<p>It would be interesting to try <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html"" rel=""nofollow"">sklearn.neighbors.NearestNeighbors</a>, which offers <code>n_jobs</code> parameter:</p>

<blockquote>
  <p>The number of <strong>parallel jobs</strong> to run for neighbors search.</p>
</blockquote>

<p>This package also provides the Ball Tree algorithm, which you can test versus the kd-tree one, however my hunch is that the kd-tree will be better (but that again does depend on your data, so research that!).</p>

<hr>

<p>You might also want to use <em>dimensionality reduction</em>, which is easy. The idea is that you reduce your dimensions, thus your data contain less info, so that tackling the Nearest Neighbour Problem can be done much faster. Of course, there is a trade off here, accuracy!</p>

<p>You might/will get less accuracy with dimensionality reduction, but it might worth the try. However, this usually applies in a high dimensional space, and <em>you are just in 3D</em>. So I don't know if <em>for your specific case</em> it would make sense to use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow"">sklearn.decomposition.PCA</a>.</p>

<hr>

<p>A remark:</p>

<p>If you really want high performance though, you won't get it with <a href=""/questions/tagged/python"" class=""post-tag"" title=""show questions tagged &#39;python&#39;"" rel=""tag"">python</a>, you could switch to <a href=""/questions/tagged/c%2b%2b"" class=""post-tag"" title=""show questions tagged &#39;c++&#39;"" rel=""tag"">c++</a>, and use <a href=""http://doc.cgal.org/latest/Spatial_searching/index.html"" rel=""nofollow"">CGAL</a> for example.</p>
"
39682387,6876382.0,2016-09-25T00:50:39Z,39672514,3,"<p><strong>Update:</strong> <a href=""https://github.com/tensorflow/models/pull/448"" rel=""nofollow"">fix applied</a></p>

<p>Sorry about this! The signature of the function <code>resize_images(...)</code> in TensorFlow was changed last week, which caused this breakage.</p>

<p>I will get a fix in for this shortly. If you would like to use the fix before then, you need to modify the file im2txt/im2txt/ops/image_processing.py.</p>

<p>Simply change this line:</p>

<pre><code>image = tf.image.resize_images(image,
                               new_height=resize_height,
                               new_width=resize_width,
                               method=tf.image.ResizeMethod.BILINEAR)
</code></pre>

<p>to this:</p>

<pre><code>image = tf.image.resize_images(image,
                               size=[resize_height, resize_width],
                               method=tf.image.ResizeMethod.BILINEAR)
</code></pre>
"
39682723,809198.0,2016-09-25T02:05:10Z,39682688,0,"<p>Append your paths so there is only one PYTHONPATH.  </p>

<pre><code>PYTHONPATH=""/Applications/python/common:/Applications/sitecustomize:$PYTHONPATH""
export PYTHONPATH
</code></pre>

<p>Then <code>source ~/.bash_profile</code></p>

<p>OR import them into your Python script (this would only work for the script added to):</p>

<pre><code>import sys
sys.path.append(""/Applications/python/common"")
sys.path.append(""/Applications/sitecustomize"")
</code></pre>
"
39683095,6642340.0,2016-09-25T03:30:00Z,39680733,1,"<p>Your <code>self.video.append((plt.pcolormesh(Frame),))</code> line is fine. You just changed the order of statements.</p>

<pre><code>import matplotlib
matplotlib.use(""Agg"")
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np


class Dummy():
    def __init__(self, fname):
        self.fname = fname
        self.video = []
        self.fig, self.ax = plt.subplots()  # Create new figure here

    def addFrame(self):
        Frame = np.random.rand(10, 10)
        self.video.append((self.ax.pcolormesh(Frame), ))

    def saveVideo(self):
        Writer = animation.writers['ffmpeg']
        writer = Writer(fps=15, metadata=dict(artist='XY'), bitrate=3600)
        im_ani = animation.ArtistAnimation(self.fig, self.video, interval=500,
                                           repeat_delay=3000, blit=True)
        im_ani.save(self.fname, writer=writer)


foo1 = Dummy('foo1.mp4')
for i in range(20):
   foo1.addFrame()
foo1.saveVideo()

foo2 = Dummy('foo2.mp4')
for i in range(40):
   foo2.addFrame()
foo2.saveVideo()
</code></pre>
"
39683581,6760995.0,2016-09-25T05:04:40Z,39683274,-1,"<p>Try this:</p>

<pre><code>import tkinter as tk

class windowclass():
    def __init__(self, master):
        self.master = master
        self.btn = tk.Button(master, text=""Button"", command=self.command)
        self.btn.pack()

    def command(self):
        self.master.withdraw()
        toplevel = tk.Toplevel(self.master)
        toplevel.geometry(""350x350"")
        app = Demo2(toplevel)

class Demo2:
    def __init__(self, master):
        self.master = master
        self.frame = tk.Frame(self.master)
        self.quitButton = tk.Button(self.frame, text = 'Quit', width = 25, command = self.close_windows)
        self.quitButton.pack()
        self.frame.pack()
    def close_windows(self):
        self.master.destroy()

root = tk.Tk()
root.title(""window"")
root.geometry(""350x350"")
cls = windowclass(root)
root.mainloop()
</code></pre>

<p>Or maybe sometimes,you want to do something else,you can hide it.</p>

<pre><code>def __init__(self, parent):
    self.root = parent
    self.root.title(""Main"")
    self.frame = Tk.Frame(parent)
    self.frame.pack()
    btn = Tk.Button(self.frame, text=""New"", command=self.openFrame)
    btn.pack()

def hide(self):
    self.root.withdraw()

def openFrame(self):
    self.hide()
    otherFrame = Tk.Toplevel()
    otherFrame.geometry(""400x300"")
    handler = lambda: self.CloseOtherFrame(otherFrame)
    btn = Tk.Button(otherFrame, text=""Close"", command=handler)
    btn.pack()


def CloseOtherFrame(self, otherFrame):
    otherFrame.destroy()
    self.show()
</code></pre>

<p><strong>Hope this helps.<br>
If this works for you,please accept this answer.</strong></p>
"
39683922,5741205.0,2016-09-25T06:02:49Z,39674876,1,"<p>If I understood the source code for <code>pivot_table(index, columns, values, aggfunc)</code> correctly it's tuned up equivalent for:</p>

<pre><code>df.groupby([index + columns]).agg(aggfunc).unstack(columns)
</code></pre>

<p><strong>plus:</strong></p>

<ul>
<li>margins (subtotals and grand totals as <a href=""http://stackoverflow.com/questions/39674876/is-there-complete-overlap-between-pd-pivot-table-and-pd-dataframe-groupby#comment66651719_39674876"">@ayhan has already said</a>)</li>
<li><code>pivot_table()</code> also removes extra multi-levels from columns axis (see example below)</li>
<li>convenient <code>dropna</code> parameter: Do not include columns whose entries are all NaN</li>
</ul>

<p>Demo: (I took this DF from the docstring [source code for <code>pivot_table()</code>])</p>

<pre><code>In [40]: df
Out[40]:
     A    B      C  D
0  foo  one  small  1
1  foo  one  large  2
2  foo  one  large  2
3  foo  two  small  3
4  foo  two  small  3
5  bar  one  large  4
6  bar  one  small  5
7  bar  two  small  6
8  bar  two  large  7

In [41]: df.pivot_table(index=['A','B'], columns='C', values='D', aggfunc=[np.sum,np.mean])
Out[41]:
          sum        mean
C       large small large small
A   B
bar one   4.0   5.0   4.0   5.0
    two   7.0   6.0   7.0   6.0
foo one   4.0   1.0   2.0   1.0
    two   NaN   6.0   NaN   3.0
</code></pre>

<p>pay attention at the top level column: <code>D</code></p>

<pre><code>In [42]: df.groupby(['A','B','C']).agg([np.sum, np.mean]).unstack('C')
Out[42]:
            D
          sum        mean
C       large small large small
A   B
bar one   4.0   5.0   4.0   5.0
    two   7.0   6.0   7.0   6.0
foo one   4.0   1.0   2.0   1.0
    two   NaN   6.0   NaN   3.0
</code></pre>

<blockquote>
  <p>why not deprecate pivot_tale? groupby seems much more general.</p>
</blockquote>

<p>IMO, because it's very easy to use and very convenient!
;)</p>
"
39683997,4495081.0,2016-09-25T06:14:19Z,39683228,0,"<p>In PyCharm edit your project's Run configuration (<code>Run</code> -> <code>Edit Configurations</code> then select your project) and in the <code>Additional options</code> field add <code>--log_level=debug</code>.</p>

<p>BTW - you don't need to set the <code>logger</code> options, the above should suffice.</p>
"
39684278,1628638.0,2016-09-25T06:59:58Z,39598724,0,"<p>Here the changes that appear to have made the <a href=""https://github.com/MicrocontrollersAndMore/OpenCV_3_KNN_Character_Recognition_Python/blob/master/TrainAndTest.py"" rel=""nofollow"">full code</a> work for me for OpenCV 2.4.13:</p>

<pre><code>60c60
&lt;     kNearest = cv2.ml.KNearest_create()                   # instantiate KNN object
---
&gt;     kNearest = cv2.KNearest()                   # instantiate KNN object
62c62
&lt;     kNearest.train(npaFlattenedImages, cv2.ml.ROW_SAMPLE, npaClassifications)
---
&gt;     kNearest.train(npaFlattenedImages, npaClassifications)
85c85
&lt;     imgContours, npaContours, npaHierarchy = cv2.findContours(imgThreshCopy,             # input image, make sure to use a copy since the function will modify this image in the course of finding contours
---
&gt;     npaContours, npaHierarchy = cv2.findContours(imgThreshCopy,             # input image, make sure to use a copy since the function will modify this image in the course of finding contours
125c125
&lt;         retval, npaResults, neigh_resp, dists = kNearest.findNearest(npaROIResized, k = 1)     # call KNN function find_nearest
---
&gt;         retval, npaResults, neigh_resp, dists = kNearest.find_nearest(npaROIResized, k = 1)     # call KNN function find_nearest
</code></pre>
"
39684564,648265.0,2016-09-25T07:49:41Z,39598724,0,"<ul>
<li>Unlike the generic <a href=""http://docs.opencv.org/2.4/modules/ml/doc/statistical_models.html#bool%20CvStatModel::train%28const%20Mat&amp;%20train_data,%20[int%20tflag,]%20...,%20const%20Mat&amp;%20responses,%20...,%20[const%20Mat&amp;%20var_idx,]%20...,%20[const%20Mat&amp;%20sample_idx,]%20...%20[const%20Mat&amp;%20var_type,]%20...,%20[const%20Mat&amp;%20missing_mask,]%20%3Cmisc_training_alg_params%3E%20...%29"" rel=""nofollow""><code>CvStatModel::train()</code></a>, <a href=""http://docs.opencv.org/2.4/modules/ml/doc/k_nearest_neighbors.html?highlight=cv_row_sample#cv2.KNearest.train"" rel=""nofollow""><code>cv2.KNearest.train()</code></a> doesn't have the 2nd optional argument <code>int tflag</code>, and the docs say: ""Only <code>CV_ROW_SAMPLE</code> data layout is supported"".

<ul>
<li>The error message (btw the cryptic mnemonics are <a href=""http://docs.opencv.org/2.4/modules/core/doc/basic_structures.html#datatype"" rel=""nofollow"">OpenCV data types</a>) was thus caused by the function trying to use <code>npaClassifications</code> as the next argument, <code>sampleIdx</code>.</li>
</ul></li>
</ul>

<p>Further errors after fixing this:</p>

<ul>
<li><p><a href=""http://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html?highlight=findcontours#cv2.findContours"" rel=""nofollow""><code>cv2.findCountours()</code></a> only returns 2 values: <code>â contours, hierarchy</code> (you don't need the 3rd one, <code>imgContours</code>, anyway).</p></li>
<li><p><code>KNearest.findNearest()</code> was <a href=""http://docs.opencv.org/2.4/modules/ml/doc/k_nearest_neighbors.html?highlight=find%20nearest#cv2.KNearest.find_nearest"" rel=""nofollow""><code>KNearest.find_nearest()</code></a>.</p></li>
</ul>

<p>And the result now:</p>

<p><a href=""http://i.stack.imgur.com/I2hsI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/I2hsI.png"" alt=""result""></a></p>

<p><a href=""http://stackoverflow.com/a/39684278/648265"">Ulrich Stern already did me a favor to provide a raw diff</a>.</p>
"
39684629,5741205.0,2016-09-25T07:59:01Z,39684548,0,"<p>assuming you have the following DF:</p>

<pre><code>In [30]: df
Out[30]:
         Date      Val
0  2016-09-23      100
1  2016-09-22    9.60M
2  2016-09-21   54.20K
3  2016-09-20  115.30K
4  2016-09-19   18.90K
5  2016-09-16  176.10K
6  2016-09-15   31.60K
7  2016-09-14   10.00K
8  2016-09-13    3.20M
</code></pre>

<p>you can do it this way:</p>

<pre><code>In [31]: df.Val = (df.Val.replace(r'[KM]+$', '', regex=True).astype(float) * \
   ....:           df.Val.str.extract(r'[\d\.]+([KM]+)', expand=False)
   ....:             .fillna(1)
   ....:             .replace(['K','M'], [10**3, 10**6]).astype(int))

In [32]: df
Out[32]:
         Date        Val
0  2016-09-23      100.0
1  2016-09-22  9600000.0
2  2016-09-21    54200.0
3  2016-09-20   115300.0
4  2016-09-19    18900.0
5  2016-09-16   176100.0
6  2016-09-15    31600.0
7  2016-09-14    10000.0
8  2016-09-13  3200000.0
</code></pre>

<p>Explanation:</p>

<pre><code>In [36]: df.Val.replace(r'[KM]+$', '', regex=True).astype(float)
Out[36]:
0    100.0
1      9.6
2     54.2
3    115.3
4     18.9
5    176.1
6     31.6
7     10.0
8      3.2
Name: Val, dtype: float64

In [37]: df.Val.str.extract(r'[\d\.]+([KM]+)', expand=False)
Out[37]:
0    NaN
1      M
2      K
3      K
4      K
5      K
6      K
7      K
8      M
Name: Val, dtype: object

In [38]: df.Val.str.extract(r'[\d\.]+([KM]+)', expand=False).fillna(1)
Out[38]:
0    1
1    M
2    K
3    K
4    K
5    K
6    K
7    K
8    M
Name: Val, dtype: object

In [39]: df.Val.str.extract(r'[\d\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6]).astype(int)
Out[39]:
0          1
1    1000000
2       1000
3       1000
4       1000
5       1000
6       1000
7       1000
8    1000000
Name: Val, dtype: int32
</code></pre>
"
39684687,21945.0,2016-09-25T08:04:47Z,39682512,3,"<p>Check out this module: <a href=""https://github.com/nlitsme/vimdecrypt"" rel=""nofollow"">https://github.com/nlitsme/vimdecrypt</a>. You can use it to decrypt your files, or study the code to learn how to implement it yourself. Example usage:</p>

<pre><code>from collections import namedtuple
from vimdecrypt import decryptfile

args = namedtuple('Args', ('verbose', 'test'))(False, False)
password = 'password'
with open('somefile', 'rb') as somefile:
    decrypted = decryptfile(somefile.read(), password, args)
</code></pre>
"
39685053,1209921.0,2016-09-25T08:54:28Z,39684942,5,"<p>What you need is a negative lookbehind.</p>

<pre><code>pattern = re.compile(r'(?&lt;!-)\bword\b')
result = pattern.sub(lambda x: ""match"", ""-word- word"")
</code></pre>

<p>To cite the <a href=""https://docs.python.org/3/library/re.html#regular-expression-syntax"" rel=""nofollow"">documentation</a>:</p>

<blockquote>
  <p><code>(?&lt;!...)</code>
      Matches if the current position in the string is not preceded by a match for ....</p>
</blockquote>

<p>So this will only match, if the word-break <code>\b</code> is not preceded with a minus sign <code>-</code>.</p>

<p>If you need this for the end of the string you'll have to use a negative lookahead which will look like this: <code>(?!-)</code>. The complete regular expression will then result in: <code>(?&lt;!-)\bword(?!-)\b</code> </p>
"
39685126,1020526.0,2016-09-25T09:02:50Z,39684942,1,"<p><code>\b</code> basically denotes a word boundary on characters other than <code>[a-zA-Z0-9_]</code> which includes spaces as well. Surround <code>word</code> with negative lookarounds to ensure there is no non-space character after and before it:</p>

<pre><code>re.compile(r'(?&lt;!\S)word(?!\S)')
</code></pre>
"
39685218,492162.0,2016-09-25T09:13:32Z,39684942,0,"<p>Instead of word boundaries, you could also match the character before and after the word with a <code>(\s|^)</code> and <code>(\s|$)</code> pattern. </p>

<p><strong>Breakdown</strong>: <code>\s</code> matches every whitespace character, which seems to be what you are trying to achieve, as you are excluding the dashes. The <code>^</code> and <code>$</code> ensure that if the word is either the first or last in the string(ie. no character before or after) those are matched too.</p>

<p>Your code would become something like this:</p>

<pre><code>pattern = re.compile(r'(\s|^)(word)(\s|$)')
result = pattern.sub(r""\1match\3"", ""-word- word"")
</code></pre>

<p>Because this solution uses character classes such as <code>\s</code>, it means that those could be easily replaced or extended. For example if  you wanted your words to be delimited by spaces or commas, your pattern would become something like this: <code>r'(,|\s|^)(word)(,|\s|$)'</code>.</p>
"
39685793,1004206.0,2016-09-25T10:23:54Z,39685168,2,"<p>This one is tricky. I think I have it. The basic idea is that we try to get a diagonal matrix with the means on the diagonal, and a matrix that is like M, but has ones at the nonzero data locations in M. Then we multiply those and subtract the product from M. Here goes...   </p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import scipy.sparse as sp
&gt;&gt;&gt; a = sp.csr_matrix([[1., 0., 2.], [1.,2.,3.]])
&gt;&gt;&gt; a.todense()
matrix([[ 1.,  0.,  2.],
        [ 1.,  2.,  3.]])
&gt;&gt;&gt; tot = np.array(a.sum(axis=1).squeeze())[0]
&gt;&gt;&gt; tot
array([ 3.,  6.])
&gt;&gt;&gt; cts = np.diff(a.indptr)
&gt;&gt;&gt; cts
array([2, 3], dtype=int32)
&gt;&gt;&gt; mu = tot/cts
&gt;&gt;&gt; mu
array([ 1.5,  2. ])
&gt;&gt;&gt; d = sp.diags(mu, 0)
&gt;&gt;&gt; d.todense()
matrix([[ 1.5,  0. ],
        [ 0. ,  2. ]])
&gt;&gt;&gt; b = a.copy()
&gt;&gt;&gt; b.data = np.ones_like(b.data)
&gt;&gt;&gt; b.todense()
matrix([[ 1.,  0.,  1.],
        [ 1.,  1.,  1.]])
&gt;&gt;&gt; (d * b).todense()
matrix([[ 1.5,  0. ,  1.5],
        [ 2. ,  2. ,  2. ]])
&gt;&gt;&gt; (a - d*b).todense()
matrix([[-0.5,  0. ,  0.5],
        [-1. ,  0. ,  1. ]])
</code></pre>

<p>Good Luck! Hope that helps.</p>
"
39686012,1262426.0,2016-09-25T10:49:40Z,39684300,0,"<p>You can install <a href=""https://software.intel.com/en-us/intel-distribution-for-python"" rel=""nofollow"">Intel Python Distribution</a>, this comes packaged with optimized NumPy, SciPy etc. You can install Tensorflow post this installation, Tensorflow uses Eigen, which is optimized to use SIMD vector lanes on CPU well. </p>
"
39686130,2704763.0,2016-09-25T11:05:06Z,39684415,2,"<p>After dabbling around for quite a while. I found two functions that could be useful.</p>

<p>One is <code>tf.gather_nd()</code> which might be useful if you can produce a tensor 
of the form <code>[[0, 0], [1, 1]]</code> and thereby you could do </p>

<p><code>index = tf.constant([[0, 0], [1, 1]])</code></p>

<p><code>tf.gather_nd(A, index)</code></p>

<p>If you are unable to produce a vector of the form <code>[[0, 0], [1, 1]]</code>(I couldn't produce this as the number of rows in my case was dependent on a placeholder) for some reason then the work around I found is to use the <code>tf.py_func()</code>. Here is an example code on how this can be done </p>

<pre><code>import tensorflow as tf 
import numpy as np 

def index_along_every_row(array, index):
    N, _ = array.shape 
    return array[np.arange(N), index]

a = tf.Variable([[1, 2], [3, 4]], dtype=tf.int32)
index = tf.Variable([0, 1], dtype=tf.int32)
a_slice_op = tf.py_func(index_along_every_row, [a, index], [tf.int32])[0]
session = tf.InteractiveSession()

a.initializer.run()
index.initializer.run()
a_slice = a_slice_op.eval() 
</code></pre>

<p><code>a_slice</code> will be a numpy array <code>[1, 4]</code></p>
"
39686244,5741205.0,2016-09-25T11:21:35Z,39686132,3,"<p>Those files are parts of a saved <a href=""https://turi.com/products/create/docs/generated/graphlab.SFrame.html"" rel=""nofollow"">SFrame</a>.</p>

<p>So you can load them this way:</p>

<pre><code>import sframe

sf = sframe.SFrame('/path/to/dir/')
</code></pre>

<p>Demo: I've downloaded all files from <a href=""https://github.com/smarthi/UnivOfWashington-Machine-Learning/tree/master/MLFoundations/week4/people_wiki.gl"" rel=""nofollow"">people_wiki.gl</a> and put them under: <code>D:/download/sframe/</code></p>

<pre><code>In [7]: import sframe

In [7]: sf = sframe.SFrame('D:/download/sframe/')

In [8]: sf
Out[8]:
Columns:
        URI     str
        name    str
        text    str

Rows: 59071

Data:
+-------------------------------+---------------------+
|              URI              |         name        |
+-------------------------------+---------------------+
| &lt;http://dbpedia.org/resour... |    Digby Morrell    |
| &lt;http://dbpedia.org/resour... |    Alfred J. Lewy   |
| &lt;http://dbpedia.org/resour... |    Harpdog Brown    |
| &lt;http://dbpedia.org/resour... | Franz Rottensteiner |
| &lt;http://dbpedia.org/resour... |        G-Enka       |
| &lt;http://dbpedia.org/resour... |    Sam Henderson    |
| &lt;http://dbpedia.org/resour... |    Aaron LaCrate    |
| &lt;http://dbpedia.org/resour... |   Trevor Ferguson   |
| &lt;http://dbpedia.org/resour... |     Grant Nelson    |
| &lt;http://dbpedia.org/resour... |     Cathy Caruth    |
+-------------------------------+---------------------+
+-------------------------------+
|              text             |
+-------------------------------+
| digby morrell born 10 octo... |
| alfred j lewy aka sandy le... |
| harpdog brown is a singer ... |
| franz rottensteiner born i... |
| henry krvits born 30 decem... |
| sam henderson born october... |
| aaron lacrate is an americ... |
| trevor ferguson aka john f... |
| grant nelson born 27 april... |
| cathy caruth born 1955 is ... |
+-------------------------------+
[59071 rows x 3 columns]
Note: Only the head of the SFrame is printed.
You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.
</code></pre>

<p>Now you can convert it to Pandas DF if you need:</p>

<pre><code>In [17]: df = sf.to_dataframe()

In [18]: pd.options.display.max_colwidth = 40

In [19]: df.head()
Out[19]:
                                       URI                 name                                     text
0  &lt;http://dbpedia.org/resource/Digby_M...        Digby Morrell  digby morrell born 10 october 1979 i...
1  &lt;http://dbpedia.org/resource/Alfred_...       Alfred J. Lewy  alfred j lewy aka sandy lewy graduat...
2  &lt;http://dbpedia.org/resource/Harpdog...        Harpdog Brown  harpdog brown is a singer and harmon...
3  &lt;http://dbpedia.org/resource/Franz_R...  Franz Rottensteiner  franz rottensteiner born in waidmann...
4     &lt;http://dbpedia.org/resource/G-Enka&gt;               G-Enka  henry krvits born 30 december 1974 i...

In [20]: df.shape
Out[20]: (59071, 3)
</code></pre>
"
39686415,6842947.0,2016-09-25T11:42:53Z,39686132,2,"<p>Just clarifying on the answer by <a href=""http://stackoverflow.com/a/39686244/6842947"">MaxU</a>, you are trying to read it the wrong way. It is a raw file and its formatting is contained in the other files which are there in the same folder in that <a href=""https://github.com/smarthi/UnivOfWashington-Machine-Learning/tree/master/MLFoundations/week4/people_wiki.gl"" rel=""nofollow"">link</a>. Pandas requires you to know the encoded format of the file beforehand (i.e delimiters, number of columns etc). It cannot be used as a magic wand to read any file without being aware of it.</p>

<p>The IPython notebook just outside the folder in your <a href=""https://github.com/smarthi/UnivOfWashington-Machine-Learning/tree/master/MLFoundations/week4/people_wiki.gl"" rel=""nofollow"">link</a>, shows exactly how to read that data. <a href=""http://stackoverflow.com/a/39686244/6842947"">MaxU</a> has correctly mentioned that the specific file in question is just a part of the SFrame which is a structure of GraphLab framework. Hence, you are trying to extract meaningful data just from a part of the whole and hence you can't do that meaningfully.</p>

<p>You can however read the graphlab file and convert it into a Pandas dataframe. For details see <a href=""http://stackoverflow.com/questions/33461953/opening-folder-with-gl-extension-in-python-or-pandas"">here</a>.</p>
"
39686629,100297.0,2016-09-25T12:08:58Z,39686553,2,"<p>Leap seconds are occasionally <em>manually</em> scheduled. Currently, computer clocks have no facility to honour leap seconds; there is no standard to tell them up-front to insert one. Instead, computer clocks periodically re-synch their time keeping via the NTP protocol and adjust automatically after the leap second has been inserted.</p>

<p>Next, computer clocks usually report the time as <em>seconds since the epoch</em>. It'd be up to the <code>datetime</code> module to adjust its accounting when converting that second count to include leap seconds. It doesn't do this at present. <code>time.time()</code> will just report a time count based on the seconds-since-the-epoch.</p>

<p>So, <em>nothing different</em> will happen when the leap second is officially in effect, other than that your computer clock will be 1 second of for a little while.</p>

<p>The issues with <code>datetime</code> only cover <em>representing</em> a leap second timestamp, which it can't. It won't be asked to do so anyway.</p>
"
39687055,3051961.0,2016-09-25T12:52:57Z,39685757,3,"<p>As far as applying a custom kernel to a given image you may simply use <a href=""http://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html"" rel=""nofollow"">filter2D</a> method to feed in a custom filter. You may also copy the following code to get you going. But The results with current filter seem a bit weird:</p>

<pre><code>import cv2
import numpy as np

# Create a dummy input image.
canvas = np.zeros((100, 100), dtype=np.uint8)
canvas = cv2.circle(canvas, (50, 50), 20, (255,), -1)

kernel = np.array([[-1, -1, -1],
                   [-1, 4, -1],
                   [-1, -1, -1]])

dst = cv2.filter2D(canvas, -1, kernel)
cv2.imwrite(""./filtered.png"", dst)
</code></pre>

<p>Input image:</p>

<p><a href=""http://i.stack.imgur.com/D03Bq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/D03Bq.png"" alt=""enter image description here""></a></p>

<p>Output Image:</p>

<p><a href=""http://i.stack.imgur.com/wQ2u8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wQ2u8.png"" alt=""enter image description here""></a></p>
"
39687059,1542806.0,2016-09-25T12:53:07Z,39677462,1,"<p>You can use morphology: dilate, and then erode with same kernel size. </p>
"
39687099,2151446.0,2016-09-25T12:57:33Z,39685757,2,"<p>You can just adapt the code at <a href=""http://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html"" rel=""nofollow"">http://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html</a>.
That is an OpenCV 3 page but it will work for OpenCV 2.</p>

<p>The only difference in the code below is how the kernel is set.</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread('opencv_logo.png')

kernel = np.ones((3,3),np.float32) * (-1)
kernel[1,1] = 8
print(kernel)
dst = cv2.filter2D(img,-1,kernel)

plt.subplot(121),plt.imshow(img),plt.title('Original')
plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(dst),plt.title('Filters')
plt.xticks([]), plt.yticks([])
plt.show()
</code></pre>

<p>Notice that I have used 8 for the centre pixel instead of 4, because using 4 darkens the results too much. Here is the result from the code above:</p>

<p><a href=""http://i.stack.imgur.com/o5bNQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o5bNQ.png"" alt=""enter image description here""></a></p>
"
39687741,100297.0,2016-09-25T14:08:22Z,39687713,4,"<p>There is no real difference, it <a href=""https://hg.python.org/cpython/diff/c3a0197256ee/Objects/descrobject.c"" rel=""nofollow"">just got renamed</a>.</p>

<p>When it was proposed to expose the type in the <code>typing</code> module in <a href=""http://bugs.python.org/issue14386"" rel=""nofollow"">issue #14386</a>, the object was renamed too:</p>

<blockquote>
  <p>I'd like to bikeshed a little on the name. I think it should be
  MappingProxy. (We don't use ""view"" much but the place where we do use
  it, for keys/values/items views, is very different I think. Also
  collections.abc already defines MappingView as the base class for
  KeysView and friends.)</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Anyway, you are not the first one who remarks that we already use ""view"" to define something else, so I wrote a new patch to use the ""mappingproxy"" name (exposed as types.MappingProxyType).</p>
</blockquote>

<p>The change <a href=""https://docs.python.org/3/whatsnew/3.3.html#types"" rel=""nofollow"">made it into Python 3.3</a>, so in Python 3.2 you'll still see the old name.</p>
"
39688379,100297.0,2016-09-25T15:06:38Z,39688358,4,"<p><code>results[""albums""][""items""]</code> is simply an empty list, so there is no element at index <code>0</code>. You could test for that before trying to index into it:</p>

<pre><code>if not results[""albums""][""items""]:
    # no albums found, so no tracks either
    return []

album_id = results[""albums""][""items""][0][""uri""]
</code></pre>
"
39688614,4400558.0,2016-09-25T15:31:51Z,39662847,-3,"<p>I see you have a lot of edits relating to solutions you may have encountered. I'm just going to address your original post which I believe to be: ""I want to create a wrapper around the unicode constructor that checks input"".</p>

<p>The <a href=""https://docs.python.org/2/library/functions.html#unicode"" rel=""nofollow""><code>unicode</code></a> method is part of Python's standard library. You will <em>decorate</em> the <code>unicode</code> method to add checks to the method.</p>

<pre><code>def add_checks(fxn):
    def resulting_fxn(*args, **kargs):
        # this is where whether the input is of type str
        if type(args[0]) is str:
            # do something
        # this is where the encoding/errors parameters are set to the default values
        encoding = 'utf-8'

        # Set default error behavior
        error = 'ignore'

        # Print any information (i.e. traceback)
        # print 'blah'
        # TODO: for traceback, you'll want to use the pdb module
        return fxn(args[0], encoding, error)
    return resulting_fxn
</code></pre>

<p>Using this will look like this:</p>

<pre><code>unicode = add_checks(unicode)
</code></pre>

<p>We overwrite the existing function name so that you don't have to change all the calls in the large project. You want to do this very early on in the runtime so that subsequent calls have the new behavior.</p>
"
39688751,901925.0,2016-09-25T15:44:57Z,39685168,2,"<p>Starting with <code>@Dthal's</code> sample:</p>

<pre><code>In [92]: a = sparse.csr_matrix([[1.,0,2],[1,2,3]])
In [93]: a.A
Out[93]: 
array([[ 1.,  0.,  2.],
       [ 1.,  2.,  3.]])

In [94]: sums=np.squeeze(a.sum(1).A)
# sums=a.sum(1).A1   # shortcut
In [95]: counts=np.diff(a.tocsr().indptr)
In [96]: means=sums/counts
In [97]: sums
Out[97]: array([ 3.,  6.])
In [98]: counts
Out[98]: array([2, 3], dtype=int32)
In [99]: means
Out[99]: array([ 1.5,  2. ])
</code></pre>

<p><code>repeat</code> lets us replicate the <code>means</code>, producing an array that matches the matrix <code>data</code> in size.</p>

<pre><code>In [100]: mc = np.repeat(means, counts)
In [101]: mc
Out[101]: array([ 1.5,  1.5,  2. ,  2. ,  2. ])
</code></pre>

<p>This <code>mc</code> is the same as <code>@Dthal's</code> <code>(b*d).data</code>.</p>

<p>Now just subtract it from <code>data</code>.</p>

<pre><code>In [102]: a.data -= mc
In [103]: a.A
Out[103]: 
array([[-0.5,  0. ,  0.5],
       [-1. ,  0. ,  1. ]])
</code></pre>
"
39688990,2698552.0,2016-09-25T16:08:30Z,39624223,0,"<p>I had similar problem. I ended up creating a separate script to manage workers. </p>

<p>Create an infinite while loop, sleep for required time and then create/kill worker process.</p>

<pre><code>while True:
    time.sleep(5)
    # create workers if possible
    # kill stale workers if not required.
</code></pre>
"
39689182,6874950.0,2016-09-25T16:28:57Z,39689099,9,"<p>Think back to how slices work: <code>a[beginning:end]</code>.
If you do not supply one of them, then you get all the list from <code>beginning</code> or all the way to <code>end</code>.</p>

<p>What that means is if I ask for <code>a[2:]</code>, I will get the list from the index <code>2</code> all the way to the end of the list and <code>len(a)</code> is an index right after the last element of the array... so <code>a[len(a):]</code> is basically an empty array positioned right after the last element of the array.</p>

<p>Say you have <code>a = [0,1,2]</code>, and you do <code>a[3:] = [3,4,5]</code>, what you're telling Python is that right after <code>[0,1,2</code> and right before <code>]</code>, there should be <code>3,4,5</code>.
Thus <code>a</code> will become <code>[0,1,2,3,4,5]</code> and after that step <code>a[3:]</code> will indeed be equal to <code>[3,4,5]</code> just as you declared.</p>

<p>Edit: as chepner commented, any index greater than or equal to <code>len(a)</code> will work just as well. For instance, <code>a = [0,1,2]</code> and <code>a[42:] = [3,4,5]</code> will also result in <code>a</code> becoming <code>[0,1,2,3,4,5]</code>.</p>
"
39689190,1136895.0,2016-09-25T16:29:57Z,39688927,0,"<p>For huge data sets, there is a solution called <strong>Text Clustering By Concept</strong>. search engines use this Technic,</p>

<p>At first step, you cluster your documents to some groups(e.g 50 cluster), then each cluster has a representative document(that contain some words that have  some useful information about it's cluster)<br />
At second step, for calculating cosine similarity between New Document and you data set, you loop through all representative(50 numbers) and find top near representatives(e.g 2 representative)<br />
At final step, you can loop through all documents in selected representative and find nearest cosine similarity</p>

<p>With this Technic, you can reduce the number of loops and improve performace,
You can read more tecnincs in some chapter of this book: <a href=""http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html"" rel=""nofollow"">http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html</a></p>
"
39689210,4952130.0,2016-09-25T16:32:12Z,39689099,5,"<p>One could generally state that <code>l[len(l):] = [1]</code> is similar to <code>append</code>, <em><a href=""https://docs.python.org/3/tutorial/datastructures.html"" rel=""nofollow"">and that is what is stated in the docs</a></em>, but, that is a <em>special case</em> that holds true only when the right hand side <em>has a single element</em>. </p>

<p>In the more <em>general case</em> it is safer to state that it is equivalent to <code>extend</code> for the following reasons:</p>

<p>Append takes an <em>object</em> and appends that to the end; with slice assignment you <strong>extend</strong> a list with the given iterable on the right hand side:</p>

<pre><code>l[len(l):] = [1, 2, 3]
</code></pre>

<p>is equivalent to:</p>

<pre><code>l.extend([1, 2, 3])
</code></pre>

<p>The same argument to <code>append</code> would cause <code>[1, 2, 3]</code> to be appended as  an object at the end of <code>l</code>. In this scenario <code>len(l)</code> is simply used in order for the extending of the list to be performed at the end of <code>l</code>.</p>

<p>Some examples to illustrate their difference:</p>

<pre><code>l = [1, 2]
l[len(l):] = [1, 2]   # l becomes [1, 2, 1, 2]
l.extend([1, 2])      # l becomes [1, 2, 1, 2, 1, 2]
l.append([1, 2])      # l becomes [1, 2, 1, 2, 1, 2, [1, 2]]
</code></pre>

<p>As you note, <code>l.append(&lt;iterable&gt;)</code> doesn't actually append each value in the iterable, it appends the iterable itself.</p>
"
39689556,904393.0,2016-09-25T17:08:03Z,39641568,4,"<p>Each Heroku dyno gets allocated a single public network port, which you can get from the <code>$PORT</code> variable, pre-assigned by Heroku before launching your app. The unfortunate side effect of this is that you can have only one public web server running in a dyno.</p>

<p>I think the first thing that you need to do is route all requests to your application through your Flask server. For example, you can add a <code>/bokeh/&lt;path:path&gt;</code> route to your Flask app that forwards all requests to the bokeh server using <a href=""http://docs.python-requests.org/en/master/"" rel=""nofollow"">requests</a>, then sends the response back to the client. With this change, you now have a single public web server, and the bokeh server can run as a background service without public access.</p>

<p>So now you can deploy your Flask app to Heroku, and have it receive both its own requests and the requests for the bokeh server. The next step is to figure out where to deploy the bokeh server.</p>

<p>The proper way to do this is to deploy bokeh on a separate dyno. The Flask dyno will know how to forward requests to bokeh because you will have a configuration item with the bokeh server URL.</p>

<p>If you want to have everything hosted on a single dyno I think you can as well, though I have never tried this myself and can't confirm it is viable. The all in one configuration is less ideal and not what Heroku recommends, but according to the <a href=""https://blog.heroku.com/new_dyno_networking_model#networking-improvements"" rel=""nofollow"">dyno networking documentation</a> it appears you can privately listen on any network ports besides <code>$PORT</code>. Those are not exposed publicly, but the doc seems to suggest the processes running inside a dyno can communicate through private ports. So for example, you can start the Flask app on <code>$PORT</code>, and the bokeh server on <code>$PORT + 1</code>, and have Flask internally forward all the bokeh requests to <code>$PORT + 1</code>.</p>
"
39690411,507078.0,2016-09-25T18:36:30Z,39689555,2,"<p>I'm not too familiar with Brython, but right away I can tell you that to port it to RapydScript you simply need to drop most of the unnecessary abstractions that I see the code importing, since RapydScript is closer to native JavaScript. As far as having the RapydScript code in the browser, you have a couple options:</p>

<ul>
<li>(recommended) compile your code ahead of time and include the .js file in your html (similar to Babel, UglifyJS, etc.), this way the code will run much faster and won't require you to include the compiler in the page</li>
<li>use an in-browser RapydScript compiler (this one seems to be the most up-to-date if you don't want to tinker with compilation: <a href=""https://github.com/adousen/RapydScript-pyjTransformer"" rel=""nofollow"">https://github.com/adousen/RapydScript-pyjTransformer</a>) and include the code inside <code>&lt;script type=""text/pyj""&gt;</code> tag similar to what you have done in the example.</li>
</ul>

<p>Now, assuming you pick the recommended option above, the next step is to drop the Brython boilerplate from the code, here is what your logic would look like in RapydScript (note that I also refactored it a little, removing the unnecessary 2-stage rotate method resolution and unneeded lambda call):</p>

<pre><code>t0 = Date.now()

def rotate_property():
   """""" Helper function mapping user agents to transform proeprty names """"""
   if 'Chrome' in window.navigator.userAgent: return 'webkitTransform'
   elif 'Firefox' in window.navigator.userAgent: return 'MozTransform'
   elif 'MSIE' in window.navigator.userAgent: return 'msTransform'
   elif 'Opera' in window.navigator.userAgent: return 'OTransform'
   return 'transform'

degrees = 0
def animation_step(elem_id='img1'):
   """""" Called every 30msec to increase the rotatation of the element. """"""
   nonlocal degrees

   # Get the right property name according to the useragent
   prop = rotate_property()

   # Get the element by id
   el = document.getElementById(elem_id)

   # Set the rotation of the element
   el.style[prop] = ""rotate("" + degrees + ""deg)""
   document.getElementById('status').innerHTML = ""rotate("" + degrees + ""deg)""

   # Increase the rotation
   degrees += 1
   if degrees &gt; 360:
       # Stops the animation after 360 steps
       clearInterval(tm)
       degrees = 0

# Start the animation
tm = setInterval(animation_step, 30)
document.getElementById('status3').innerHTML = ""Time of execution python code("" + (Date.now() - t0) + "" ms)""
</code></pre>

<p>A few things to note:</p>

<ul>
<li>imports are no longer needed, RapydScript doesn't need boilerplate to interact with JavaScript</li>
<li>Pythonic <code>timer.set_interval</code> and <code>timer.clear_interval</code> have been replaced with JavaScript equivalents (setInterval and clearInterval)</li>
<li><code>document</code> you see in my code is the DOM itself, <code>document</code> you have in Brython code is a wrapper around it, hence accessing it works a bit differently</li>
<li>RapydScript dropped <code>global</code> a long time ago in favor of Python 3's safer <code>nonlocal</code>, which is what I used in the code instead</li>
<li>instead of <code>time</code> module, RapydScript has direct access to JavaScript's <code>Date</code> class, which I used in the code to time it</li>
<li>I would also recommend moving <code>prop = rotate_property()</code> call outside the function, since the user agent won't change between function calls (in this case the operation is relatively cheap, but for more complex logic this will improve your performance)</li>
<li>you seem to be launching Brython from HTML via body <code>onload</code>, get rid of that as well as the line that says <code>&lt;script&gt;animation_step(""img1"",30);&lt;/script&gt;</code>, the above code should trigger for you automatically as soon as page loads courtesy of setInterval call</li>
<li>since RapydScript uses unicode characters to avoid name collisions, you need to tell HTML to treat the document as unicode by adding this line to head: <code>&lt;meta charset=""UTF-8""&gt;</code></li>
<li>for future reference, none of your <code>onload</code> calls to RapydScript will work, because unlike Brython, RapydScript protects itself in its own scope, invisible to the outside (this has been the accepted practice in JavaScript world for a long time), your options for making <code>onload</code> work are:

<ul>
<li>(not recommended) compile the file with <code>-b</code> flag to omit the self-protecting scope</li>
<li>(recommended) inside your code, attach your functions to the global <code>window</code> object if you want them accessible from outside</li>
</ul></li>
</ul>

<p>Your actual html code that calls the compiled version of the above code would then look like this:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta charset=""UTF-8""&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;img id=""img1"" src=""cog1.png"" alt=""cog1""&gt;
&lt;h2 style=""width:200px;"" id=""status""&gt;&lt;/h2&gt;
&lt;h2 style=""width:800px;"" id=""status3""&gt;&lt;/h2&gt;
&lt;script src=""myfile.js""&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
"
39690425,3030305.0,2016-09-25T18:38:10Z,39690301,5,"<p>This approach uses the <code>+</code> or <code>(</code> to signal the beginning of a phone number.  It does not require multiple-spaces:</p>

<pre><code>&gt;&gt;&gt; phones = '+35(123) 456 78 90 (123) 555 55 55  (908)985 88 89   (593)592 56 95'
&gt;&gt;&gt; re.split(r' +(?=[(+])', phones)
['+35(123) 456 78 90', '(123) 555 55 55', '(908)985 88 89', '(593)592 56 95']
</code></pre>

<p>This splits the string based on one-or-more spaces followed by either <code>(</code> or <code>+</code>.</p>

<p>In the regular expression, <code>+</code> matches one or more spaces.  <code>(?=[(+])</code> is a look-ahead.  It requires that the spaces be followed by either <code>(</code> or <code>+</code> but does not consume the <code>(</code> or <code>+</code>.  Because we are using a look-ahead instead of a plain match, the the leading <code>(</code> and <code>+</code> remain part of the phone number.</p>
"
39691213,3510736.0,2016-09-25T19:56:51Z,39691164,2,"<p>(First, note that your code is missing</p>

<pre><code>import numpy as np
import numpy.random as rand
</code></pre>

<p>or something like that. The second one is a bit annoying to guess.)</p>

<p>Your two lines </p>

<pre><code>gb = df.groupby(['A'])
q=gb.quantile(np.arange(0,1.1,.1))
</code></pre>

<p>Are equivalent to </p>

<pre><code>q=df.groupby(['A']).quantile(np.arange(0,1.1,.1))
</code></pre>

<p>In other words, <code>q</code> is already the <em>result</em> of applying the quantile on each group. The result you're looking for is <code>q</code> itself:</p>

<pre><code>&gt;&gt;&gt; q
                B     C
A                      
bar 0.0 -1.311556  13.0
    0.1 -1.188745  13.2
    0.2 -1.065935  13.4
    0.3 -0.943124  13.6
    0.4 -0.820313  13.8
    0.5 -0.697503  14.0
    0.6 -0.662497  14.4
    0.7 -0.627492  14.8
    0.8 -0.592486  15.2
    0.9 -0.557481  15.6
    1.0 -0.522475  16.0
foo 0.0  0.032946   1.0
    0.1  0.051352   4.0
    0.2  0.069759   7.0
    0.3  0.088166  10.0
    0.4  0.106572  13.0
    0.5  0.124979  16.0
    0.6  0.388895  16.2
    0.7  0.652811  16.4
    0.8  0.916728  16.6
    0.9  1.180644  16.8
    1.0  1.444560  17.0
</code></pre>
"
39691298,3185459.0,2016-09-25T20:06:05Z,39691091,4,"<p>The following approach would be the most simple one, I suppose ...</p>

<pre><code>s = 'This, I think,., Ú©Ø¨Ø§Ø¨ MAKES , some sense '
pattern = '([\.,\s]+)'
splitted = [i.strip() for i in re.split(pattern, s) if i.strip()]
</code></pre>

<p>The output:</p>

<pre><code>['This', ',', 'I', 'think', ',.,', 'Ú©Ø¨Ø§Ø¨', 'MAKES', ',', 'some', 'sense']
</code></pre>
"
39691408,1020526.0,2016-09-25T20:17:19Z,39691091,0,"<p><em>Update based on OP's last edit</em></p>

<p>Python 3.*:</p>

<pre><code>list(filter(None, re.split('([.,]+(?:\s+[.,]+)*)|\s', s)))
</code></pre>

<p>Output:</p>

<pre><code>['This', ',', 'I', 'think', ',.,', 'Ú©Ø¨Ø§Ø¨', 'MAKES', ',', 'some', 'sense']
</code></pre>
"
39691418,1658617.0,2016-09-25T20:17:58Z,39691091,0,"<p>I believe this is the most efficient option regarding memory, and really efficient regarding computation time:</p>

<pre><code>import re
from itertools import chain
from operator import methodcaller

input_str = 'This, I think,., ???? MAKES , some sense '

iterator = filter(None,  # Filter out all 'None's
                  chain.from_iterable(  # Flatten the tuples into one long iterable
                    map(methodcaller(""groups""),  # Take the groups from each match.
                        re.finditer(""(.*?)(?:([\.,]+)|\s+|$)"", input_str))))

# If you want a list:
list(iterator)
</code></pre>
"
39691433,219241.0,2016-09-25T20:19:01Z,39669718,0,"<p>you're doing almost everything right; you just didn't get the Clock part correctly.</p>

<p><strong>twisted.internet.task.Clock</strong> is a deterministic implementation of IReactorTime, which is mostly used in unit/integration testing for getting a deterministic output from your code; you shouldn't use that in production.</p>

<p>So, what should you use in production? <strong>reactor</strong>! In fact, all production reactor implementations implement the IReactorTime interface.</p>

<p>Just use the following import and function call:</p>

<pre><code>from twisted.internet import reactor
# (omissis)
self.dtime = task.deferLater(reactor, 5, self.printData)
</code></pre>

<p>Just some sidenotes: </p>

<p>in your text above the snippet, you say that you want to invoke cancelTest after five seconds, but in the code you actually invoke printData; of course if printData just prints something, doesn't raise and returns an immediate value, this will cause the cancelTest function to be executed immediately after since it's a chained callcack; <strong>but if you want to actually be 100% sure, you should call cancelTest within deferLater, not printData</strong>.</p>

<p>Also, I don't understand if this is a kind of ""timeout""; please be advised that such callback will be triggered in all situations, even if the tests take less than five seconds. If you need a cancelable task, you should use reactor.callLater directly; that will NOT return a deferred you can use, but will let you cancel the scheduled call.</p>
"
39691438,1643939.0,2016-09-25T20:20:12Z,39688985,0,"<p>You do this by using <code>layer.set_weights(weights)</code>. From the documentation:</p>

<blockquote>
<pre><code>weights: a list of Numpy arrays. The number
         of arrays and their shape must match
         number of the dimensions of the weights
         of the layer (i.e. it should match the
         output of `get_weights`).
</code></pre>
</blockquote>

<p>You don't just put the weights for the filter in there but for <em>each</em> parameter the layer has. The order in which you have to put in the weights depends on <code>layer.weights</code>. You may look at the code or print the names of the weights of the layer by doing something like</p>

<pre><code> print([p.name for p in layer.weights])
</code></pre>
"
39691580,829496.0,2016-09-25T20:36:02Z,39691091,2,"<p><strong>NOTE:</strong> According to the new edit on the question, I've improved my old regex. The new one is quite long but trust me, it's work!</p>

<p>I suggest a pattern below as a delimiter of the function <code>re.split()</code>:</p>

<pre><code>(?&lt;![,\.\ ])(?=[,\.]+)|(?&lt;=[,\.])(?![,\.\ ])|(?&lt;=[,\.])\ +(?![,\.\ ])|(?&lt;![,\.\ ])\ +(?=[,\.][,\.\ ]+)|(?&lt;![,\.\ ])\ +(?![,\.\ ])
</code></pre>

<p>My workaround here doesn't require any pre/post space modification. The thing that make regex work is about how you order the regex expressions with <code>or</code>. My cursory strategy is any patterns that dealing with a space-leading will be evaluated last. </p>

<p>See <a href=""https://regex101.com/r/eW8rZ6/3"" rel=""nofollow"">DEMO</a> </p>

<p><strong>Additional</strong></p>

<p>According to @revo's comment he provided an another shorten version of mine which is</p>

<pre><code>\s+(?=[^.,\s])|\b(?:\s+|(?=[,.]))|(?&lt;=[,.])\b
</code></pre>

<p>See <a href=""https://regex101.com/r/eW8rZ6/5"" rel=""nofollow"">DEMO</a></p>
"
39692784,2912349.0,2016-09-25T23:26:21Z,39691936,2,"<p>An RGBA image has 4 channels, one for each color and one for the alpha value. The binary file seems to have a single channel, as you don't report an error when performing the <code>data.reshape(shape)</code> operation (the shape for the corresponding RGBA image would be (430, 430, 4)). 
I see two potential reasons:</p>

<ol>
<li><p>The image actual does have colour information but when you are grabbing the data you are only grabbing one of the four channels.</p></li>
<li><p>The image is actually a gray-scale image, but the embedded device shows a pseudocolor image, creating the illusion of colour information. Without knowing what the colourmap is being used, it is hard to help you, other than point you towards <code>matplotlib.pyplot.colormaps()</code>, which lists all already available colour maps in matplotlib.</p></li>
</ol>

<p>Could you </p>

<p>a) explain the exact source / type of imaging modality, and </p>

<p>b) show a photo of the output of the embedded device?</p>

<p>PS: Also, at least in my hands, the pasted binary file seems to have a size of 122629, which is incongruent with an image shape of (430,430).</p>
"
39692812,509824.0,2016-09-25T23:31:13Z,39692769,0,"<pre><code>import numpy as np

x = np.array(range(1, 16))
y = np.vstack([x[0::5], x[1::5]]).T.ravel()
y
// =&gt; array([ 1,  2,  6,  7, 11, 12])
</code></pre>

<p>Taking the first <code>N</code> rows of every block of <code>M</code> rows in the array <code>[1, 2, ..., K</code>]:</p>

<pre><code>import numpy as np

K = 30
M = 5
N = 2

x = np.array(range(1, K+1))
y = np.vstack([x[i::M] for i in range(N)]).T.ravel()
y
// =&gt; array([ 1,  2,  6,  7, 11, 12, 16, 17, 21, 22, 26, 27])
</code></pre>

<p>Notice that <code>.T</code> and <code>.ravel()</code> are fast operations: they don't copy any data, but just manipulate the dimensions and strides of the array.</p>

<p>If you insist on getting your slice using fancy indexing:</p>

<pre><code>import numpy as np

K = 30
M = 5
N = 2

x = np.array(range(1, K+1))
fancy_indexing = [i*M+n for i in range(len(x)//M) for n in range(N)]
x[fancy_indexing]
// =&gt; array([ 1,  2,  6,  7, 11, 12, 16, 17, 21, 22, 26, 27])
</code></pre>
"
39692899,5067311.0,2016-09-25T23:43:51Z,39692769,0,"<p>I first thought you need this to work for 2d arrays due to your phrasing of ""first N rows of every block of M rows"", so I'll leave my solution as this.</p>

<p>You could work some magic by reshaping your array into 3d:</p>

<pre><code>M = 5 # size of blocks
N = 2 # number of columns to cut
x = np.arange(3*4*M).reshape(4,-1) # (4,3*N)-shaped dummy input
x = x.reshape(x.shape[0],-1,M)[:,:,:N+1].reshape(x.shape[0],-1) # (4,3*N)-shaped output
</code></pre>

<p>This will extract every column according to your preference. In order to use it for your 1d case you'd need to make your 1d array into a 2d one using <code>x = x[None,:]</code>.</p>
"
39692907,2823755.0,2016-09-25T23:45:05Z,39692769,1,"<p>Reshape the array to multiple rows of five columns then take (slice) the first two columns of each row.</p>

<pre><code>&gt;&gt;&gt; x
array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])
&gt;&gt;&gt; x.reshape(x.shape[0] / 5, 5)[:,:2]
array([[ 1,  2],
       [ 6,  7],
       [11, 12]])
</code></pre>

<p>Or</p>

<pre><code>&gt;&gt;&gt; x.reshape(x.shape[0] / 5, 5)[:,:2].flatten()
array([ 1,  2,  6,  7, 11, 12])
&gt;&gt;&gt; 
</code></pre>

<p>It only works with 1-d arrays that have a length that is a multiple of five.</p>
"
39693035,459.0,2016-09-26T00:05:29Z,39692999,1,"<p>First I'm going to change the <code>elif</code> to just <code>else</code> because your if condition is already testing the elif condition and I don't see how that would be changing while in this code.</p>

<pre><code>for i in nuke.allNodes():
    if not i.knob(""tempMb""):
        if sum0 == 0:
            nuke.message(""first tmp knob created"")
            i.addKnob(t)
        else:
            nuke.message(""second tmp knob created"")
    else: 
        nuke.message(""no nob created"")
</code></pre>

<p>Second I'm assuming that <code>i.knob(string)</code> doesn't check for the existence of a knob by that name, or your code would behave more as you expected. So when I read <a href=""https://docs.thefoundry.co.uk/nuke/80/pythonreference/nuke-module.html#allNodes"" rel=""nofollow"">the docs</a> it seems like a couple of things may happen:</p>

<ol>
<li>The nodes might or might not be knobs in the list returned. If you know you only want knobs you could filter by class type. I think that might look like <code>nuke.allNodes(nuke.Knob)</code>.</li>
<li>I don't think a nuke.Knob.knob(str) is a test for its name or label. I read the docs as implying that your test should be: <code>if i.name != ""tempMb"":</code> or possibly <code>if i.label != ""tempMb""</code> it depends on how you created <code>t</code>.</li>
</ol>

<p>Moving on though, I think there may be a logical error here. If you have 2 nodes (and if you make the above changes, let's assume they're both knobs), and as you loop over all nodes the first one is the <code>tempMb</code>, then when you check the second one it won't be that and you'll try to add <code>t</code>, which I assume is named <code>tempMb</code> too. So that's why it looks to you as though the negative condition is always occurring.</p>

<p>You need to restructure in one of two ways:</p>

<ol>
<li>Before the loop, set a false boolean, in the loop set it to true when the knob is <code>tempMb</code> is found; you may as well exit the loop as soon as this occurs. After the loop check the boolean to see if it's safe to add <code>t</code>.</li>
<li>I see a possible function <a href=""https://docs.thefoundry.co.uk/nuke/80/pythonreference/nuke-module.html#exists"" rel=""nofollow""><code>nuke.exists(s)</code></a> which tells you if you have any ""item"" named <code>s</code>.</li>
</ol>

<p>Maybe remove the loop and write the following:</p>

<pre><code> if not nuke.exists(""tempMb""):
     # Add your knob. I'm actually not seeing `addKnob` in the docs.
</code></pre>
"
39693100,1966294.0,2016-09-26T00:18:09Z,39692999,0,"<p><code>nuke.exists(""Name of Knob"")</code> will check if your knob exists. Try using that in your if statement.</p>

<p>More details on <a href=""http://community.thefoundry.co.uk/discussion/topic.aspx?f=190&amp;t=101399"" rel=""nofollow"">Nuke forum</a>.</p>

<p>See also <a href=""https://www.thefoundry.co.uk/products/nuke/developers/100/pythonreference/nuke-module.html#exists"" rel=""nofollow"">Documentation on nuke.exists</a></p>
"
39693127,1016216.0,2016-09-26T00:23:22Z,39693056,3,"<p>This is not super-easy, because a) you want overlapping matches, and b) you want greedy and non-greedy and everything inbetween.</p>

<p>As long as the strings are fairly short, you can check every substring:</p>

<pre><code>import re
s = ""ATGTCAGGTAAGCTTAGGGCTTTAGGATT""
p = re.compile(r'ATG.*TA[GA]$')

for start in range(len(s)-6):  # string is at least 6 letters long
    for end in range(start+6, len(s)):
        if p.match(s, pos=start, endpos=end):
            print(s[start:end])
</code></pre>

<p>This prints:</p>

<pre><code>ATGTCAGGTAA
ATGTCAGGTAAGCTTAG
ATGTCAGGTAAGCTTAGGGCTTTAG
</code></pre>

<p>Since you appear to work with DNA sequences or something like that, make sure to check out <a href=""http://biopython.org"" rel=""nofollow"">Biopython</a>, too.</p>
"
39693157,5109041.0,2016-09-26T00:27:15Z,39693126,5,"<p>Django let's you follow relationships with the double underscore (__) as deep as you like. So in your case <code>F('parent__factor')</code> should do the trick.</p>

<p>The full queryset:</p>

<pre><code>Child.objects.aggregate(value=Sum(F('field_a') * F('field_b') * F('parent__factor'), output_field=DecimalField()))
</code></pre>
"
39693262,3599135.0,2016-09-26T00:45:00Z,39670940,2,"<p>I found the solution to my problem. I had to install <code>python-opencv</code> as follows:</p>

<pre><code>sudo apt-get install python-opencv
</code></pre>

<p>After that OpenCV works fine.</p>
"
39693824,2705542.0,2016-09-26T02:18:58Z,39693056,0,"<p>I like the accepted answer just fine :-)  That is, I'm adding this for info, not looking for points.</p>

<p>If you have heavy need for this, trying a match on <code>O(N^2)</code> pairs of indices may soon become unbearably slow.  One improvement is to use the <code>.search()</code> method to ""leap"" directly to the only starting indices that can possibly pay off.  So the following does that.</p>

<p>It also uses the <code>.fullmatch()</code> method so that you don't have to artificially change the ""natural"" regexp (e.g., in your example, no need to add a trailing <code>$</code> to the regexp - and, indeed, in the following code doing so would no longer work as intended).  Note that <code>.fullmatch()</code> was added in Python 3.4, so this code also requires Python 3!</p>

<p>Finally, this intends to generalize the <code>re</code> module's <code>finditer()</code> function/method.  While you don't need match objects (you just want strings), they're far more generally applicable, and returning a generator is often friendlier than returning a list too.</p>

<p>So, no, this doesn't do exactly what you want, but does things from which you can <em>get</em> what you want, in Python 3, faster:</p>

<pre><code>def finditer_overlap(regexp, string):
    start = 0
    n = len(string)
    while start &lt;= n:
        # don't know whether regexp will find shortest or
        # longest match, but _will_ find leftmost match
        m = regexp.search(string, start)
        if m is None:
            return
        start = m.start()
        for finish in range(start, n+1):
            m = regexp.fullmatch(string, start, finish)
            if m is not None:
                yield m
        start += 1
</code></pre>

<p>Then, e.g.,</p>

<pre><code>import re
string2 = ""ATGTCAGGTAAGCTTAGGGCTTTAGGATT""
pat = re.compile(""ATG.*(TAA|TAG)"")
for match in finditer_overlap(pat, string2):
    print(match.group())
</code></pre>

<p>prints what you wanted in your example.  The other ways you tried to write a regexp should also work.  In this example it's faster because the second time around the outer loop <code>start</code> is 1, and <code>regexp.search(string, 1)</code> fails to find another match, so the generator exits at once (so skips checking <code>O(N^2)</code> other index pairs).</p>
"
39694401,3430943.0,2016-09-26T03:45:09Z,39691164,0,"<p>You can call specific groups using the index:</p>

<pre><code>q.loc['foo']
</code></pre>
"
39694445,2879498.0,2016-09-26T03:50:29Z,39694155,3,"<p>Pretty complicated given there's no negative look-behind assertions in JavaScript RegExp, and Unicode support is not official yet (only supported in Firefox by a flag at the moment). This uses a library (XRegExp) to handle the unicode classes. If you need the full normal regular expression, <strong><em>it's huge</em></strong>. Just comment and let me know, and I'll update the answer to use the exploded normal RegExp statements that include the Unicode ranges.</p>

<pre><code>const rxLetterToOther = XRegExp('(\\p{L})((?!\\s)\\P{L})','g');
const rxOtherToLetter = XRegExp('((?!\\s)\\P{L})(\\p{L})','g');
const rxNumberToOther = XRegExp('(\\p{N})((?!\\s)\\P{N})','g');
const rxOtherToNumber = XRegExp('((?!\\s)\\P{N})(\\p{N})','g');
const rxPuctToPunct = XRegExp('(\\p{P})(\\p{P})','g');
const rxSep = XRegExp('\\s+','g');

function segmentation(s) {
  return s
    .replace(rxLetterToOther, '$1 $2')
    .replace(rxOtherToLetter, '$1 $2')
    .replace(rxNumberToOther, '$1 $2')
    .replace(rxOtherToNumber, '$1 $2')
    .replace(rxPuctToPunct, '$1 $2')
    .split(rxSep);
}
</code></pre>

<p><a href=""https://jsfiddle.net/a3tf68ae/14/"" rel=""nofollow"">Here it is passing all the test cases!</a></p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>window.onbeforeunload = """";</code></pre>
<pre class=""snippet-code-css lang-css prettyprint-override""><code>* { margin: 0; padding: 0; border: 0; overflow: hidden; }
object { width: 100%; height: 100%; width: 100vw; height: 100vh; }</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;object data=""https://fiddle.jshell.net/a3tf68ae/14/show/"" /&gt;</code></pre>
</div>
</div>
</p>

<p><em>Edit: Updated the test case to print the huge RegExp sources beneath the test results. Run the snippet to see the embedded test case.</em></p>
"
39694457,2288883.0,2016-09-26T03:53:05Z,39694155,1,"<p>I found the answer but is is complex. Does anyone have another simple answer for this</p>

<pre><code>module.exports = (string) =&gt; {
  const segs = string.split(/(\.{2,}|!|""|#|$|%|&amp;|'|\(|\)|\*|\+|,|-|\.|\/|:|;|&lt;|=|&gt;|\?|Â¿|@|[|]|\\|^|_|`|{|\||}|~| )/);

  return segs.filter((seg) =&gt; seg.trim() !== """");
};
</code></pre>
"
39694672,2030966.0,2016-09-26T04:19:31Z,39690742,1,"<p><code>np.NaN</code> is a floating point only kind of thing, so it has to be removed in order to create an integer pd.Series. Jeon's suggestion work's great If 0 isn't a valid value in <code>df['b']</code>. For example:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'a': [1, 5, 5], 'b': [np.NaN, 7200.0, 580.0], 'c': [3, 20, 20]})
print(df, '\n\n')

df['b'] = np.nan_to_num(df['b']).astype(int)

print(df)
</code></pre>

<p>if there are valid 0's, then you could first replace them all with some unique value (e.g., -999999999), the the conversion above, and then replace these unique values with 0's.</p>

<p>Either way, you have to remember that you have 0's where there were once NaNs. You will need to be careful to filter these out when doing various numerical analyses (e.g., mean, etc.)</p>
"
39694727,369874.0,2016-09-26T04:26:39Z,39644514,0,"<p>It's relatively easy to extract the public point from the ASN.1 sequence using <a href=""https://pypi.python.org/pypi/pyasn1"" rel=""nofollow"">pyasn1</a>, but if you want PEM-encrypted PKCS1 (aka ""traditional OpenSSL"") then pyca/cryptography can do that quite easily:</p>

<pre><code>from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import ec

backend = default_backend()

key = ec.generate_private_key(ec.SECP256R1(), backend)
serialized_key = key.private_bytes(
    serialization.Encoding.PEM, 
    serialization.PrivateFormat.TraditionalOpenSSL, 
    serialization.BestAvailableEncryption(b""my_great_password"")
)
</code></pre>

<p>You can find more information about the <a href=""https://cryptography.io/en/latest/hazmat/primitives/asymmetric/ec/#cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKeyWithSerialization.private_bytes"" rel=""nofollow"">private_bytes</a> method in the docs. At this time <code>BestAvailableEncryption</code> will encrypt using <code>AES-256-CBC</code>.</p>
"
39694742,2399154.0,2016-09-26T04:29:10Z,39694670,0,"<p><strike>One question before answering: Is the line <code>carmodel = models.ForeignKey(Model)</code> ok? or should it be <code>ForeignKey(CarModel)</code> instead?</strike></p>

<p>Try this query (this should give you all CarCompany objects whose CarModel's ModelYear's <code>last_availability</code> date is in the future:</p>

<pre><code>from datetime import datetime
CarCompany.objects.filter(carmodel__modelyear__last_availability__gte=datetime.now())
</code></pre>

<p>In order to check both whether <code>last_availability</code> is in the future or blank/null I would use <a href=""https://docs.djangoproject.com/en/dev/topics/db/queries/#complex-lookups-with-q-objects"" rel=""nofollow"">Q objects</a>:</p>

<pre><code>from django.db.models import Q
CarCompany.objects.filter(Q(carmodel__modelyear__last_availability__gte=datetime.now()) |
                          Q(carmodel__modelyear__last_availability__isnull=True))
</code></pre>

<p>For your second example, I can't think but in the same query (It answers your questions <code>Which Teams have open TeamPositions?</code> and <code>Which Teams will have an empty TeamPosition in 30 days</code>)</p>

<p>But I'm not sure what do you mean by <code>Which Teams have no TeamPositions without current Players?</code> If you could explain it a bit...</p>

<pre><code>Team.objects.filter(Q(teamposition__player__left_date__gte=datetime.now()) |
                    Q(teamposition__player__left_date__isnull=True) |
</code></pre>
"
39694974,2901002.0,2016-09-26T04:59:13Z,39694192,1,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow""><code>to_numeric</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.notnull.html"" rel=""nofollow""><code>notnull</code></a> and filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>print (pd.to_numeric(df.b, errors='coerce'))
0    26190.0
1        NaN
2      580.0
Name: b, dtype: float64

print (pd.to_numeric(df.b, errors='coerce').notnull())
0     True
1    False
2     True
Name: b, dtype: bool

df = df[pd.to_numeric(df.b, errors='coerce').notnull()]
print (df)

   a      b
0  1  26190
2  5    580
</code></pre>

<p>Another solution by comment of <a href=""http://stackoverflow.com/questions/39694192/convert-string-column-to-integer/39694974#comment66688077_39694192"">Boud</a> - use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow""><code>to_numeric</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""nofollow""><code>dropna</code></a> and last convert to <code>int</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow""><code>astype</code></a>:</p>

<pre><code>df.b = pd.to_numeric(df.b, errors='coerce')
df = df.dropna(subset=['b'])
df.b = df.b. astype(int)
print (df)
   a      b
0  1  26190
2  5    580
</code></pre>

<hr>

<p>If need check all rows with bad data use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.isnull.html"" rel=""nofollow""><code>isnull</code></a> - filter all data where after applying function <code>to_numeric</code> get <code>NaN</code>:</p>

<pre><code>print (pd.to_numeric(df.b, errors='coerce').isnull())
0    False
1     True
2    False
Name: b, dtype: bool

print (df[pd.to_numeric(df.b, errors='coerce').isnull()])
   a       b
1  5  python
</code></pre>
"
39695130,895932.0,2016-09-26T05:16:27Z,39695046,10,"<p>This is the problem:</p>

<pre class=""lang-py prettyprint-override""><code># initialization for nodes and link them to be a free list
nodes=[StackNode(-1, None)] * MAXSIZE
</code></pre>

<p>When you use the multiply operator, it will create multiple <em>references</em> to the <strong>same</strong> object, as noted <a href=""http://stackoverflow.com/a/2785963/895932"">in this StackOverflow answer</a>. So changing one node's value (as in <code>nodes[i].value = -i</code>) will affect every node since every item in the list points to the same object.</p>

<p>In that same linked answer, the solution is to use <a class='doc-link' href=""http://stackoverflow.com/documentation/python/5265/list-comprehensions#t=201609260515199970598&amp;a=syntax"">list comprehension</a>, like this:</p>

<pre class=""lang-py prettyprint-override""><code>nodes = [StackNode(-1, None) for i in range(MAXSIZE)]
</code></pre>

<p>Also, note that you did not set the value of the last element, so the output (after the fix I suggested above) will be:</p>

<pre>
0, -1, -2, ..., -98, -1
</pre>
"
39695485,2901002.0,2016-09-26T05:49:31Z,39695461,2,"<p>You need <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>sal[sal.employee_name == 'name']
</code></pre>

<p>If need select only some column, use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ix.html"" rel=""nofollow""><code>ix</code></a> with <code>boolean indexing</code>:</p>

<pre><code>sal.ix[sal.employee_name == 'name', 'job_title']
</code></pre>

<p>Sample:</p>

<pre><code>sal = pd.DataFrame({'id':[1,2,3],
                   'employee_name':['name','name1','name2'],
                   'job_title':['titleA','titleB','titleC']},
                    columns=['id','employee_name','job_title'])

print (sal)
   id employee_name job_title
0   1          name    titleA
1   2         name1    titleB
2   3         name2    titleC

print (sal[sal.employee_name == 'name'])
   id employee_name job_title
0   1          name    titleA

print (sal.ix[sal.employee_name == 'name', 'job_title'])
0    titleA
Name: job_title, dtype: object
</code></pre>
"
39695491,3293881.0,2016-09-26T05:49:52Z,39692769,3,"<p><strong>Approach #1</strong> Here's a vectorized one-liner using <a href=""http://docs.scipy.org/doc/numpy-1.10.1/user/basics.indexing.html#boolean-or-mask-index-arrays"" rel=""nofollow""><code>boolean-indexing</code></a> -</p>

<pre><code>x[np.mod(np.arange(x.size),M)&lt;N]
</code></pre>

<p><strong>Approach #2</strong> If you are going for performance, here's another vectorized approach using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html"" rel=""nofollow""><code>NumPy strides</code></a> -</p>

<pre><code>n = x.strides[0]
shp = (x.size//M,N)
out = np.lib.stride_tricks.as_strided(x, shape=shp, strides=(M*n,n)).ravel()
</code></pre>

<p>Sample run -</p>

<pre><code>In [61]: # Inputs
    ...: x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])
    ...: N = 2
    ...: M = 5
    ...: 

In [62]: # Approach 1
    ...: x[np.mod(np.arange(x.size),M)&lt;N]
Out[62]: array([ 1,  2,  6,  7, 11, 12])

In [63]: # Approach 2
    ...: n = x.strides[0]
    ...: shp = (x.size//M,N)
    ...: out=np.lib.stride_tricks.as_strided(x,shape=shp,strides=(M*n,n)).ravel()
    ...: 

In [64]: out
Out[64]: array([ 1,  2,  6,  7, 11, 12])
</code></pre>
"
39695625,2901002.0,2016-09-26T06:00:49Z,39695606,2,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""nofollow""><code>dropna</code></a>:</p>

<pre><code>df = df.dropna(subset=['label'])

print (df)
   reference_word  all_matching_words   label review
10        airport       biz - airport  travel      N
11        airport       cfo - airport  travel      N
12        airport    cfomtg - airport  travel      N
13        airport   meeting - airport  travel      N
14        airport    summit - airport  travel      N
15        airport      taxi - airport  travel      N
16        airport     train - airport  travel      N
17        airport  transfer - airport  travel      N
18        airport      trip - airport  travel      N
</code></pre>

<p>Another solution - <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.notnull.html"" rel=""nofollow""><code>notnull</code></a>:</p>

<pre><code>df = df[df.label.notnull()]

print (df)
   reference_word  all_matching_words   label review
10        airport       biz - airport  travel      N
11        airport       cfo - airport  travel      N
12        airport    cfomtg - airport  travel      N
13        airport   meeting - airport  travel      N
14        airport    summit - airport  travel      N
15        airport      taxi - airport  travel      N
16        airport     train - airport  travel      N
17        airport  transfer - airport  travel      N
18        airport      trip - airport  travel      N
</code></pre>
"
39696192,3088138.0,2016-09-26T06:40:44Z,39695675,1,"<p>A short test demonstrates</p>

<pre><code>In [104]: import numpy as np

In [105]: dt = 0.6

In [106]: np.arange(0, 1+dt, dt)
Out[106]: array([ 0. ,  0.6,  1.2])
</code></pre>

<p>Thus to get meaningful results, either set <code>t[n-1]=1</code> at the start or compute the error as </p>

<pre><code>E = abs(x[n-1]-exact(t[n-1]))
</code></pre>
"
39696863,2849391.0,2016-09-26T07:18:55Z,39696818,14,"<p>instead of concatenating string use <code>os.path.join</code> <code>os.path.expanduser</code> to generate the path and open the file. (assuming you are trying to open a file in your home directory)</p>

<pre><code>with open(os.path.join(os.path.expanduser('~'), 'test.txt')) as fp:
    # do your stuff with file
</code></pre>
"
39696890,2915834.0,2016-09-26T07:21:10Z,39696818,2,"<p>I think the best way would be to us os.path.join() here</p>

<pre><code>import os.path

#â¦
dir = '/home/ashraful/'
fp = open(os.path.join(dir, 'test.txt'), 'r')
</code></pre>
"
39696960,147021.0,2016-09-26T07:25:27Z,39696818,4,"<p>If your goal is to open files/directories, as others have mentioned, you should use the <code>os.path.join()</code> method. However, if you want to format string in Python -- as your question title suggests -- then your first approach should be preferred. To quote from <a href=""https://www.python.org/dev/peps/pep-3101/"" rel=""nofollow"">PEP 310</a>:</p>

<blockquote>
  <p>This PEP proposes a new system for built-in string formatting
      operations, intended as a replacement for the existing '%' string
      formatting operator.</p>
</blockquote>
"
39697003,6848031.0,2016-09-26T07:27:56Z,39696234,1,"<p>First of all this is my first answer. :) </p>

<p>As of I know, PHP is an interpreted language in which only that executable file(php.exe in your php installation directory) is compiled and your code is only interpreted and no inbuilt functions like python.</p>

<p>If you still need compile as of in python, try <a href=""http://hhvm.com/"" rel=""nofollow"" title=""HHVM"">HHVM</a> a virtual machine</p>

<p>(never used that)</p>

<p>and there are some third party compilers like <a href=""http://www.zzee.com/phpexe/"" rel=""nofollow"">ZZEE</a> which compiles into a GUI exe</p>
"
39697033,6874950.0,2016-09-26T07:31:04Z,39696606,2,"<p>Having googled the error message, I found this issue: <a href=""http://bugs.python.org/issue19891"" rel=""nofollow"">http://bugs.python.org/issue19891</a></p>

<p>It seems that the problem often has to do with the current user not having a home directory (which I think is logical for a user called python) or not having the proper permissions on their home directory, but the issue is still open.</p>
"
39698115,100297.0,2016-09-26T08:35:10Z,39698045,3,"<p>Function decoration takes place when the class body is being executed, nothing is known about the class itself or its base classes at this time. This means that <code>modifier</code> decorates the <em>unbound function object</em> and only when <code>func</code> is actually called on an instance will it be bound.</p>

<p>You could return a wrapper function to replace the decorated function, it'll be bound instead and you'll have access to <code>self</code>:</p>

<pre><code>from functools import wraps

def modifier(func):
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        # self is an instance of the class
        self.data
        return func(self, *args, **kwargs)
    return wrapper
</code></pre>

<p>The wrapper will then be called each time <code>method</code> is called on an instance.</p>

<p>If you must have access to the <em>base classes</em> at class creation time, you'll have to wait until the <code>class Child</code> statement has <em>completed execution</em>. Until Python 3.6, that's only possible from a class decorator or metaclass; each is called after the class body is created and you'll have access to (at the least) the base classes.</p>

<p>Using a class decorator, for example:</p>

<pre><code>def modifier(cls):
    # cls is the newly created class object, including class attributes
    cls.data
    return cls

@modifier
class Child(Parent):
    def method(self):
        pass
</code></pre>

<p>Note the location of the decorator now.</p>

<p>Python 3.6 adds an <a href=""https://docs.python.org/3.6/whatsnew/3.6.html#pep-487-simpler-customization-of-class-creation"" rel=""nofollow""><code>__init_subclass__</code> method</a> that could also give you that access; the (class) method is called each time a subclass is created of the current class:</p>

<pre><code>class Parent:
    def __init_subclass__(cls, **kwargs):
        # cls is a subclass of Parent
        super().__init_subclass__(**kwargs)
        cls.data

    data = ""Hi!""

class Child(Parent):
    def method(self):
        pass
</code></pre>
"
39698136,3756159.0,2016-09-26T08:36:07Z,39698045,-1,"<p>Not that the very first arg passed to your decorator's wrapper will be an objects instance (<code>self</code>). Thanks to it you can access w=everything you need. </p>

<p>See Raphaels response: 
<a href=""http://stackoverflow.com/questions/11731136/python-class-method-decorator-w-self-arguments"">Related issue</a></p>

<pre><code>def wrapped(self, *f_args, **f_kwargs):
</code></pre>

<p>self is the very first argument</p>
"
39698226,4093050.0,2016-09-26T08:41:11Z,39698190,3,"<p>Yes, use <code>*args</code>:</p>

<pre><code>In [1]: def func(*args):
   ...:     print(len(args), args)
   ...:     

In [2]: func(1, 2, 3, 4)
4 (1, 2, 3, 4)
</code></pre>

<p>More information can be found <a href=""https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/"" rel=""nofollow"">here.</a></p>
"
39698236,4952130.0,2016-09-26T08:41:48Z,39698190,6,"<p>Since you have a <em>set</em> number of arguments <em>just create an iterable out of them</em>, for example, wrap the argument names in a tuple literal:</p>

<pre><code>for arg in (arg1, arg2, arg3, arg4):
    # do stuff
</code></pre>

<p>If you don't mind your function being capable of being called with more args just use <a href=""https://docs.python.org/3/reference/compound_stmts.html#function-definitions"" rel=""nofollow""><code>*args</code> syntax</a>:</p>

<pre><code>def function1(*args):
    for arg in args:
        arg = function2(arg)
</code></pre>

<p>which then exposes the arguments with which the function was invoked as a tuple which can be iterated over.</p>

<p>If you need to store the results for every invocation, it is better to change the <code>for-loop</code> in a <a href=""https://docs.python.org/3.6/tutorial/datastructures.html#list-comprehensions"" rel=""nofollow""><em>list comprehension</em></a> creating a list of the returned values. For example, given a function that takes a value and simply adds <code>2</code> to it:</p>

<pre><code>def add2(arg):
    return arg + 2

def function1(arg1, arg2, arg3, arg4):
    a1, a2, a3, a4 = [add2(arg) for arg in (arg1, arg2, arg3, arg4)]
    print(a1, a2, a3, a4)
</code></pre>

<p>We create a list comprehension that takes every <code>arg</code> and supplies it to <code>add2</code> and then stores it as an entry in a list, then we unpack to <code>a1,..,aN</code>. The values now contain the results of the function calls:</p>

<pre><code>function1(1, 2, 3, 4)
3, 4, 5, 6
</code></pre>

<p>In the previous examples <code>(arg1, arg2, arg3, arg4)</code> can always be replaced with <code>args</code> if you use the <code>*args</code> syntax when defining the function, the results will be similar.</p>

<hr>

<p>As an addendum, if you're more a fan of functional programming, you could always <strong><a href=""https://docs.python.org/3/library/functions.html#map"" rel=""nofollow""><code>map</code></a></strong> the arguments to the function and save a few characters :-). I'll add a <code>*args</code> definition of <code>function1</code> this time:</p>

<pre><code>def function1(*args):
    a1, a2, a3, a4 = map(add2, args)
    print(a1, a2, a3, a4)
</code></pre>

<p>Same result.</p>
"
39698351,1330293.0,2016-09-26T08:48:00Z,39688927,1,"<p>You should take a look at <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">gensim</a>. Example starting code looks like this:</p>

<pre><code>from gensim import corpora, models, similarities

dictionary = corpora.Dictionary(line.lower().split() for line in open('corpus.txt'))
corpus = [dictionary.doc2bow(line.lower().split()) for line in open('corpus.txt')]

tfidf = models.TfidfModel(corpus)
index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)
</code></pre>

<p>At prediction time you first get the vector for the new doc:</p>

<pre><code>doc = ""Human computer interaction""
vec_bow = dictionary.doc2bow(doc.lower().split())
vec_tfidf = tfidf[vec_bow]
</code></pre>

<p>Then get the similarities (sorted by most similar):</p>

<pre><code>sims = index[vec_tfidf] # perform a similarity query against the corpus
print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples
</code></pre>

<p>This does a linear scan like you wanted to do but they have a more optimized implementation. If the speed is not enough then you can look into approximate similarity search (Annoy, Falconn, NMSLIB).</p>
"
39698943,773609.0,2016-09-26T09:17:13Z,39598666,1,"<p>Solved it with : </p>

<pre><code>class Model(models.Model): 
    objects = models.Manager()  -&gt; objects only access master
    sobjects = ReplicasManager() -&gt; sobjects access either master and replicas

    class Meta: 
        abstract = True  -&gt; so django doesn't create a table
</code></pre>

<p>make every model extend this one instead of models.Model, and then use objects or sobjects whether I want to access only master or if want to access either master or replicas</p>
"
39700526,6550266.0,2016-09-26T10:32:40Z,39694192,0,"<p>This should work</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'a' : [1, 5, 5],
                   'b' : [26190, 'python', 580]})
df
   a       b
0  1   26190
1  5  python
2  5     580

df['b'] = np.where(df.b.str.contains('[a-z]') == True, np.NaN, df.b)
df
   a      b
0  1  26190
1  5    NaN
2  5    580

df = df.dropna()
df
   a      b
0  1  26190
2  5    580
</code></pre>

<p>You use the regex to identify strings, then convert these to <code>np.NaN</code> using <code>np.where</code> then drop them from the df with <code>df.dropna()</code>. </p>
"
39700863,2526441.0,2016-09-26T10:49:39Z,39688927,2,"<p>This problem can be partially addressed by combining the <strong>vector space model</strong> (which is the tf-idf &amp; cosine similarity) together with the <strong>boolean model</strong>. These are concepts of information theory and they are used (and nicely explained) in <a href=""https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html"" rel=""nofollow"">ElasticSearch</a>-  a pretty good search engine.</p>

<p>The idea is simple: you store your documents as inverted indices. Which is comparable to the words present at the end of a book, which hold a reference to the pages (documents) they were mentioned in.</p>

<p>Instead of calculating the tf-idf vector for all document it will only calculate it for documents that have at least one (or specify a threshold) of the words in common. This can be simply done by looping over the words in the queried document, finding the documents that also have this word using the inverted index and calculate the similarity for those.</p>
"
39701542,2328503.0,2016-09-26T11:23:56Z,39646236,1,"<p>I found a <a href=""https://gist.github.com/ibeex/3257877"" rel=""nofollow"">gist</a> which talks about logging in flask. The comment by andyxning (commented on Apr 18, 2015) mentions this - <code>if app.debug is True then all log level above DEBUG will be logged to stderr(StreamHandler)</code>. </p>

<p>The comment also has a link to the source code of <a href=""https://github.com/pallets/flask/blob/master/flask/logging.py"" rel=""nofollow"">flask/logging.py</a>. The <code>create_logger</code> method creates an instance of <code>DebugHandler</code> which inherits from <code>StreamHandler</code> class.</p>

<p>If you print <code>app.logger.handlers</code> you can see that it has an object of <code>flask.logging.DebugHandler</code>. </p>

<pre><code>print app.logger.handlers
[&lt;flask.logging.DebugHandler object at 0x110315090&gt;]
</code></pre>

<p>This DebugHandler is probably used when <code>app.debug is set to true</code> and hence the stack trace gets printed on the console.</p>

<p>Hope this is what you are looking for.</p>
"
39701662,2753095.0,2016-09-26T11:30:05Z,39646236,1,"<p>The normal exception handler is not called when <code>app.debug = True</code>. Looking
in the code of <code>app.py</code> in Flask:</p>

<pre><code>def log_exception(self, exc_info):
    """"""Logs an exception.  This is called by :meth:`handle_exception`
    if debugging is disabled and right before the handler is called.
    ^^^^^^^^^^^^^^^^^^^^^^^^
    The default implementation logs the exception as error on the
    :attr:`logger`.
</code></pre>

<p>Indeed, when setting <code>app.debug = True</code> the exceptions propagation is set
to True explicitly, which prevents <code>log_exception</code> to be called. Here is an excerpt of the documentation (emphasis is mine):</p>

<blockquote>
  <p>PROPAGATE_EXCEPTIONS:     explicitly enable or disable the propagation of exceptions. If not set or explicitly set to None this is <strong>implicitly true</strong> if either TESTING or <strong>DEBUG is true</strong>.</p>
</blockquote>

<p>So, I managed to get both werkzeug debugging and logging working happily
together with a little tweak and the following code:</p>

<pre><code>import logging

from flask import Flask
from werkzeug.serving import run_simple
from werkzeug.wsgi import DispatcherMiddleware
## NEW CODE HERE
import functools
from flask._compat import reraise

def my_log_exception(exc_info, original_log_exception=None):
    original_log_exception(exc_info)
    exc_type, exc, tb = exc_info
    # re-raise for werkzeug
    reraise(exc_type, exc, tb)
## 

def app_builder(app_name, log_file):
    app = Flask(app_name)
    app.debug = True
    app.config.update(PROPAGATE_EXCEPTIONS=False)

    handler = logging.FileHandler(log_file)
    handler.setLevel(logging.DEBUG)
    app.logger.addHandler(handler)

    ## NEW CODE
    app.log_exception = functools.partial(my_log_exception,  original_log_exception=app.log_exception)
    ##

    return app

# rest of your code is unchanged
</code></pre>
"
39702481,100297.0,2016-09-26T12:12:16Z,39702457,10,"<p>You could just use the modulus of the <a href=""https://docs.python.org/3/library/functions.html#hash"" rel=""nofollow""><code>hash()</code> function</a> output:</p>

<pre><code>def onebyte_hash(s):
    return hash(s) % 256
</code></pre>

<p>This is what dictionaries and sets use (hash modulus the internal table size).</p>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; onebyte_hash('abc_123')
182
&gt;&gt;&gt; onebyte_hash('any-string-value')
12
</code></pre>

<p>Caveat: On Python 3.3 and up, <em>hash randomisation</em> is enabled by default, and <em>between restarts of Python</em> you'll get different values. The hash, then, is only stable if you don't restart the Python process or set <a href=""https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED"" rel=""nofollow""><code>PYTHONHASHSEED</code></a> to a fixed decimal number (with <code>0</code> disabling it altogether). On Python 2 and 3.0 through to 3.2 hash randomisation is either not available or only enabled if you set a seed explicitly.</p>

<p>Another alternative is to just <a href=""https://docs.python.org/3/library/functions.html#hash"" rel=""nofollow""><code>hashlib.md5()</code></a> and just take (integer value of) the first byte:</p>

<pre><code>import hashlib

try:
    # Python 2; Python 3 will throw an exception here as bytes are required
    hashlib.md5('')
    def onebyte_hash(s):
        return ord(hashlib.md5(s).digest()[0])
except TypeError:
    # Python 3; encode the string first, return first byte
    def onebyte_hash(s):
        return hashlib.md5(s.encode('utf8')).digest()[0]
</code></pre>

<p>MD5 is a well-establish cryptographic hash, the output is stable across Python versions and independent of hash randomisation.</p>

<p>The disadvantage of the latter would be that it'd be marginally slower; Python caches string hashes on the string object, so retrieving the hash later on is fast and cheap most of the time.</p>
"
39703860,1805634.0,2016-09-26T13:12:39Z,39703241,2,"<p>As the commands may take more than one line its much easier to NOT split the text-file by newlines. I would suggest splitting by '$' instead.</p>

<p>This example code works:</p>

<pre><code>def get_params(fname, desired_command):
    with open(fname,""r"") as f:
        content = f.read()
    for element in content.split('$'):
        element = element.replace('\n', ' ').strip()
        if not element:
            continue
        if ' ' in element:
            command, result = element.split(' ', 1)
        else:
            command, result = element, True
        if desired_command == command or desired_command == '${}'.format(command):
            return result
</code></pre>

<hr>

<p>Here is my edit which works with space containing commands:</p>

<pre><code>import re

COMMAND_RE = re.compile('([A-Z_ ]+[A-Z]) ?(.+)? *')

def get_params(fname, desired_command):
    with open(fname,""r"") as f:
        content = f.read()
    for element in content.split('$'):
        element = element.replace('\n', ' ').strip()
        if not element:
            continue
        command, result = COMMAND_RE.search(element).groups()
        if desired_command == command or desired_command == '${}'.format(command):
            return result or True
</code></pre>
"
39704387,3125566.0,2016-09-26T13:36:27Z,39704084,1,"<blockquote>
  <p>return the <strong>same</strong> list without vowels</p>
</blockquote>

<p>Eh, you're <em>slicing</em> the original list in the recursive calls, so you have a <strong>copy</strong> not the same list. </p>

<p>More so, your code actually works, but since you're passing a <em>slice</em> of the list, the vowel items in the slice (not the original list) are deleted and the original remains unchanged. </p>

<p>You can instead use a <em>non-slicing</em> variant that moves from <em>start</em> to <em>end</em> indices of the original list:</p>

<pre><code>def no_vow(seq, index=0):
    keys = ['a','i','e','o','u']
    if not seq or not isinstance(seq, list) or index &gt;= len(seq):
        return 
    else:
        if seq[index] in keys:
            del seq[index]
            return no_vow(seq, index)
        else:
            return no_vow(seq, index+1)
</code></pre>

<p>Finally, if you're going to print your result, you shouldn't print the output of the function call (which will be <code>None</code>) but the list.</p>

<hr>

<p><strong>Trial</strong>:</p>

<pre><code>li = [""b"", ""c"", ""e"", ""d"", ""a""]

no_vow(li) # list is modified in-place
print(li)
# [""b"", ""c"", ""d""]
</code></pre>
"
39704399,6779307.0,2016-09-26T13:36:52Z,39704084,1,"<pre><code>def no_vowel(seq):
    if not isinstance(seq, list):
        raise ValueError('Expected list, got {}'.format(type(seq)))
    if not seq:
        return []
    head, *tail = seq
    if isinstance(head, list):
        return [[no_vowel(head)]] + no_vowel(tail)
    else:
        if head in 'aeiou':
            return no_vowel(tail)
        else:
            return [head] + novowel(tail)   
</code></pre>

<p>The cool unpacking of the list is a Python 3 feature, and is very similar to functional programmings pattern matching.         </p>
"
39704404,3076091.0,2016-09-26T13:37:00Z,39704084,1,"<p>Your base case returns a None. So whenever you passes the empty list the None is sent up the stack of recursive calls. </p>

<p>Moreover you are not storing the characters which are not vowels, so your else case is wrong.</p>

<p>What you can have is something like this: </p>

<pre><code>&gt;&gt;&gt; def noVow(seq):
...     keys = ['a','i','e','o','u','u']
...     if not seq or not isinstance(seq, list) :
...         return []
...     else:
...         if seq[0] in keys:
...             return noVow(seq[1:])
...         else:
...             return [seq[0]] + noVow(seq[1:])
</code></pre>

<p>Also <code>seq[0:]</code> is equivalent to <code>seq</code>.</p>
"
39704405,459745.0,2016-09-26T13:37:00Z,39703241,1,"<p>Here is my approach: split everything based on white spaces (spaces, tabs and new lines). Then construct a dictionary with command names as keys and parameters as values. From this dictionary, you can look up parameters for any command. This approach opens and reads the file only once:</p>

<pre><code>from collections import deque

def parse_commands_file(filename):
    with open(filename) as f:
        tokens = deque(f.read().split())

    command2parameters = dict()
    while tokens:
        command_name = tokens.popleft()
        # Added
        while tokens and tokens[0].isalpha() and not tokens[0].startswith('$'):
            command_name = command_name + ' ' + tokens.popleft()
        # end added

        parameters = []
        while tokens and not tokens[0].startswith('$'):
            parameters.append(int(tokens.popleft()))
        command2parameters[command_name] = parameters or True

    return command2parameters

if __name__ == '__main__':
    command = parse_commands_file('commands.txt')
    print '$BOOLEAN_COMMAND:', command.get('$BOOLEAN_COMMAND')
    print '$NUMERIC_COMMAND_ALPHA:', command.get('$NUMERIC_COMMAND_ALPHA')
    print '$NUMERIC_COMMAND_BETA:', command.get('$NUMERIC_COMMAND_BETA')
</code></pre>

<p>Output:</p>

<pre><code>$BOOLEAN_COMMAND: True
$NUMERIC_COMMAND_ALPHA: [1, 3, 6, 9, 10]
$NUMERIC_COMMAND_BETA: [2, 7, 9, 10, 15, 25, 40, 900, 2000]
</code></pre>

<h1>Discussion</h1>

<ul>
<li>I use the <code>deque</code> data structure, which stands for <em>double-end queue</em>. This structure behaves like a list, but more efficient in term of insert and pop from both ends</li>
<li>When parsing the parameters, I converted them to <code>int</code>, you can convert them to float or leave them be</li>
<li>The expression <code>parameters or True</code> basically says: if parameters is empty, use <code>True</code>, otherwise, leave it be</li>
</ul>

<h1>Update</h1>

<p>I have added a patch to handle commands with spaces in their names. However, this solution is just a patch, it does not work if you have multiple spaces such as:</p>

<pre><code>$MY      COMMAND HERE
</code></pre>

<p>In this case, multiple spaces got squeezed into one.</p>
"
39704568,4385912.0,2016-09-26T13:44:10Z,39665702,1,"<p>You are not iterating over the same variable in line 92 and 97, since those will always be in the same namespace, at least in the current setting, since you are calling one namespace from within another (since one is embedded in the RNN function). So your effective variable scope will be something like <code>'backward/forward'</code>.</p>

<p>Hence the problem, in my guess, is in lines 89 and 92, since both ""live"" in the same namespace (see above), and both may introduce a variable called <code>RNN/BasicLSTMCell/Linear/Matrix</code>. So you should change your code to the following:</p>

<pre><code># Define a lstm cell with tensorflow
with tf.variable_scope('cell_def'):
    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0)

# Get lstm cell output
with tf.variable_scope('rnn_def'):
    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)
</code></pre>

<p>This makes the LSTMCell initialization live in one namespace - <code>""cell_def/*""</code>, and the initialization of the complete RNN in another - <code>""rnn_def/*""</code>.</p>
"
39704620,165216.0,2016-09-26T13:46:06Z,39702882,1,"<p><code>originalTextFor</code> is not a parse action, but a helper method that attaches a parse action to a defined expression. In your example it is used in two places:</p>

<pre><code># Value
VALUE = (EXPRESSION | pyparsing.Word(pyparsing.alphanums + ""_()+-/*"")
         ).setParseAction(pyparsing.originalTextFor)

FUNCTION = (pyparsing.Optional(PRECISION).setResultsName(""precision"") +
            TYPE.setResultsName(""type"") +
            IDENTIFIER.setResultsName(""name"") +
            LPAREN + pyparsing.Optional(PARAMETERS).setResultsName(""args"") + RPAREN +
            pyparsing.nestedExpr(""{"", ""}"").setParseAction(pyparsing.originalTextFor).setResultsName(""code""))
</code></pre>

<p>change these to:</p>

<pre><code># Value
VALUE = pyparsing.originalTextFor(EXPRESSION |
                                  pyparsing.Word(pyparsing.alphanums + ""_()+-/*""))

FUNCTION = (pyparsing.Optional(PRECISION).setResultsName(""precision"") +
            TYPE.setResultsName(""type"") +
            IDENTIFIER.setResultsName(""name"") +
            LPAREN + pyparsing.Optional(PARAMETERS).setResultsName(""args"") + RPAREN +
            pyparsing.originalTextFor(pyparsing.nestedExpr(""{"", ""}"")).setResultsName(""code""))
</code></pre>

<p>You might find that the more recent pyparsing form of <code>setResultsName</code> to be a little cleaner-looking, but the old form still works fine:</p>

<pre><code>FUNCTION = (pyparsing.Optional(PRECISION)(""precision"") +
            TYPE(""type"") +
            IDENTIFIER(""name"") +
            LPAREN + pyparsing.Optional(PARAMETERS)(""args"") + RPAREN +
            pyparsing.originalTextFor(pyparsing.nestedExpr(""{"", ""}""))(""code""))
</code></pre>

<p>If you make this change, those places that use <code>listAllMatches</code> are handled by adding a '*' to the name argument:</p>

<pre><code>pyparsing.delimitedList(VARIABLE(""variable*""))
</code></pre>
"
39705072,704848.0,2016-09-26T14:06:29Z,39705022,3,"<p>You can just do the following:</p>

<pre><code>In [19]:
(df - df.mean())/df.std()

Out[19]:
              PC1       PC2       PC3       PC4       PC5       PC6       PC7
ind                                                                          
NA06984  0.343471 -0.576088  0.439607 -0.671001 -1.859402 -1.013059 -0.506331
NA06986 -0.673732 -1.136500 -0.929353  0.901369  1.090682  1.079500 -1.027455
NA06989  0.530305 -0.205354 -1.082006  1.308925  0.125133 -0.648825  0.047775
NA06994  0.987008 -0.619197 -0.008506  1.256213  1.628736  0.408167  0.832758
NA07000  1.090804  1.424152  1.197952 -0.760998 -0.143894 -0.670251  1.358280
NA07037 -0.694491 -0.627818 -0.072522 -0.429296  0.051427 -0.344107  0.487541
NA07048 -0.611454 -1.084770  0.016116 -0.475580  0.648445  2.105544  0.366606
NA07051  1.464471 -0.550223  0.592261 -0.956419 -0.493998 -0.113187 -1.876205
NA07056 -0.466140  1.191366  1.724853 -0.908849  0.635547 -1.079716 -1.106613
NA07347 -1.940047  0.743036 -1.727092  1.473490 -0.543749  0.846200  0.619471
NA07357 -0.030195  1.441396 -0.151311 -0.737856 -1.138926 -0.570265  0.804173
</code></pre>

<p>This will operate on the whole df so there is no need to iterate over rows/columns</p>
"
39705506,3809375.0,2016-09-26T14:28:16Z,39702882,0,"<p>Following @Paul McGuire advices, here's a working version which uses python3.5.1 and pyparsing 2.1.9:</p>

<pre><code># -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Copyright (c) 2014, Nicolas P. Rougier
# Distributed under the (new) BSD License. See LICENSE.txt for more info.
# -----------------------------------------------------------------------------
import pyparsing

keywords = (""attribute const uniform varying break continue do for while""
            ""if else""
            ""in out inout""
            ""float int void bool true false""
            ""lowp mediump highp precision invariant""
            ""discard return""
            ""mat2 mat3 mat4""
            ""vec2 vec3 vec4 ivec2 ivec3 ivec4 bvec2 bvec3 bvec4 sampler2D samplerCube""
            ""struct"")
reserved = (""asm""
            ""class union enum typedef template this packed""
            ""goto switch default""
            ""inline noinline volatile public static extern external""
            ""interface flat long short double half fixed unsigned superp""
            ""input output""
            ""hvec2 hvec3 hvec4 dvec2 dvec3 dvec4 fvec2 fvec3 fvec4 sampler1D sampler3D""
            ""sampler1DShadow sampler2DShadow""
            ""sampler2DRect sampler3DRect sampler2DRectShadow""
            ""sizeof cast""
            ""namespace using"")
precision = ""lowp mediump high""
storage = ""const uniform attribute varying""


# Tokens
# ----------------------------------
LPAREN = pyparsing.Literal(""("").suppress()
RPAREN = pyparsing.Literal("")"").suppress()
LBRACK = pyparsing.Literal(""["").suppress()
RBRACK = pyparsing.Literal(""]"").suppress()
LBRACE = pyparsing.Literal(""{"").suppress()
RBRACE = pyparsing.Literal(""}"").suppress()
IDENTIFIER = pyparsing.Word(pyparsing.alphas + '_', pyparsing.alphanums + '_')
TYPE = pyparsing.Word(pyparsing.alphas + '_', pyparsing.alphanums + ""_"")
END = pyparsing.Literal("";"").suppress()
INT = pyparsing.Word(pyparsing.nums)
FLOAT = pyparsing.Regex(
    '[+-]?(((\d+\.\d*)|(\d*\.\d+))([eE][-+]?\d+)?)|(\d*[eE][+-]?\d+)')
STORAGE = pyparsing.Regex('|'.join(storage.split(' ')))
PRECISION = pyparsing.Regex('|'.join(precision.split(' ')))
STRUCT = pyparsing.Literal(""struct"").suppress()


# ------------------------
def get_prototypes(code):
    """"""
    Get all function declarations

    Code example
    ------------

    mediump vec3 function_1(vec4);
    vec3 function_2(float a, float b);
    """"""

    PARAMETER = pyparsing.Group(pyparsing.Optional(PRECISION)(""precision"") +
                                TYPE(""type"") +
                                pyparsing.Optional(IDENTIFIER)(""name""))
    PARAMETERS = pyparsing.delimitedList(PARAMETER)(
        ""arg*"")
    PROTOTYPE = (pyparsing.Optional(PRECISION)(""precision"") +
                 TYPE(""type"") +
                 IDENTIFIER(""name"") +
                 LPAREN + pyparsing.Optional(PARAMETERS)(""args"") + RPAREN +
                 END)
    PROTOTYPE.ignore(pyparsing.cStyleComment)

    for (token, start, end) in PROTOTYPE.scanString(code):
        print(token.precision, token.type, token.name, '(',)
        for arg in token.args:
            print(arg.precision, arg.type, arg.name, ',',)
        print(')')


# ------------------------
def get_functions(code):
    """"""
    Get all function definitions

    Code example
    ------------

    mediump vec3 compute_normal(vec4);
    """"""

    PARAMETER = pyparsing.Group(pyparsing.Optional(PRECISION)(""precision"") +
                                TYPE(""type"") +
                                pyparsing.Optional(IDENTIFIER)(""name""))
    PARAMETERS = pyparsing.delimitedList(PARAMETER)(
        ""arg*"")
    FUNCTION = (pyparsing.Optional(PRECISION)(""precision"") +
                TYPE(""type"") +
                IDENTIFIER(""name"") +
                LPAREN + pyparsing.Optional(PARAMETERS)(""args"") + RPAREN +
                pyparsing.originalTextFor(pyparsing.nestedExpr(""{"", ""}""))(""code""))
    FUNCTION.ignore(pyparsing.cStyleComment)

    for (token, start, end) in FUNCTION.scanString(code):
        print(token.precision, token.type, token.name, '(',)
        for arg in token.args:
            print(arg.precision, arg.type, arg.name, ',',)
        print(') { ... }')

        # print token.code
        # print code[start:end]


# ------------------------
def get_version(code):
    """"""
    Get shader version (if specified)

    Code example
    ------------

    #version 120
    """"""

    VERSION = (
        pyparsing.Literal(""#"") + pyparsing.Keyword(""version"")).suppress() + INT
    for (token, start, end) in VERSION.scanString(code):
        version = token[0]
        # print code[start:end]
    return version

# ------------------------


def get_declarations(code):
    """"""
    Get all declarations prefixed with a storage qualifier.

    Code example
    ------------

    uniform lowp vec4 fg_color = vec4(1),
                      bg_color = vec4(vec3(0),1);
    """"""

    # Callable expression
    EXPRESSION = pyparsing.Forward()
    ARG = pyparsing.Group(EXPRESSION) | IDENTIFIER | FLOAT | INT
    ARGS = pyparsing.delimitedList(ARG)
    EXPRESSION &lt;&lt; IDENTIFIER + \
        pyparsing.Group(LPAREN + pyparsing.Optional(ARGS) + RPAREN)

    # Value
    VALUE = pyparsing.originalTextFor(EXPRESSION |
                                      pyparsing.Word(pyparsing.alphanums + ""_()+-/*""))

    # Single declaration
    VARIABLE = (IDENTIFIER(""name"") +
                pyparsing.Optional(LBRACK +
                                   (INT | IDENTIFIER)(""size"")
                                   + RBRACK) +
                pyparsing.Optional(pyparsing.Literal(""="").suppress() + VALUE(""value"")))

    # Several declarations at once
    DECLARATION = (STORAGE(""storage"") +
                   pyparsing.Optional(PRECISION)(""precision"") +
                   TYPE(""type"") +
                   pyparsing.delimitedList(VARIABLE(""variable*"")) +
                   END)
    DECLARATION.ignore(pyparsing.cStyleComment)

    for (tokens, start, end) in DECLARATION.scanString(code):
        for token in tokens.variable:
            print(tokens.storage, tokens.precision, tokens.type,)
            print(token.name, token.size)


# ------------------------
def get_definitions(code):
    """"""
    Get all structure definitions and associated declarations.

    Code example
    ------------

    uniform struct Light {
        vec4 position;
        vec3 color;
    } light0, light1;
    """"""

    # Single declaration
    DECLARATION = pyparsing.Group(IDENTIFIER(""name"") +
                                  pyparsing.Optional(LBRACK +
                                                     (INT | IDENTIFIER)(""size"") +
                                                     RBRACK))
    # Several declarations at once
    DECLARATIONS = (pyparsing.Optional(PRECISION) +
                    TYPE +
                    pyparsing.delimitedList(DECLARATION) +
                    END)

    # Definition + declarations
    DEFINITION = (STRUCT +
                  IDENTIFIER(""name"") +
                  LBRACE + pyparsing.OneOrMore(DECLARATIONS)('content') + RBRACE +
                  pyparsing.Optional(pyparsing.delimitedList(DECLARATION(""declarations*""))) +
                  END)
    DEFINITION.ignore(pyparsing.cStyleComment)

    for (tokens, start, end) in DEFINITION.scanString(code):
        for token in tokens.declarations:
            print(tokens.name, token.name)
            # print tokens.content


# ----------------
def resolve(code):
    """"""
    Expand const and preprocessor definitions in order to get constant values.

    Return the transformed code
    """"""

    constants = {}

    DEFINITION = (pyparsing.Literal(""#"") + pyparsing.Literal(""define"") +
                  IDENTIFIER(""name"") +
                  pyparsing.restOfLine(""value""))

    VALUE = pyparsing.Word(pyparsing.alphanums + ""_()+-/*"")
    DECLARATION = (pyparsing.Literal(""const"") +
                   TYPE(""type"") +
                   IDENTIFIER(""name"") +
                   pyparsing.Literal(""="") +
                   VALUE(""value"") +
                   pyparsing.Literal("";""))
    REFERENCE = pyparsing.Forward()

    def process_definition(s, l, t):
        value = REFERENCE.transformString(t.value)
        constants[t.name] = value
        REFERENCE &lt;&lt; pyparsing.MatchFirst(
            map(pyparsing.Keyword, constants.keys()))
        return ""#define "" + t.name + "" "" + value

    def process_declaration(s, l, t):
        value = REFERENCE.transformString(t.value)
        constants[t.name] = value
        REFERENCE &lt;&lt; pyparsing.MatchFirst(
            map(pyparsing.Keyword, constants.keys()))
        return ""const "" + t.type + "" "" + t.name + ""="" + value + "";""

    def process_reference(s, l, t):
        return constants[t[0]]

    REFERENCE.setParseAction(process_reference)
    DEFINITION.setParseAction(process_definition)
    DECLARATION.setParseAction(process_declaration)
    EXPANDER = REFERENCE | DEFINITION | DECLARATION

    code = EXPANDER.transformString(code)
    for key, val in constants.items():
        constants[key] = eval(val)

    return code, constants


# -----------------------------------------------------------------------------
if __name__ == '__main__':

    code = """"""
#version 120

#define A (1)
const int B=(A+2);
#define C (B+3)
const int D=C+4;

uniform float array[D];

struct Point {
    vec4 position;
    float size;
};

uniform struct Light {
    vec4 position;
    vec3 color;
} light0, light1;

const float PI = 3.14159265358979323846264;
const float SQRT_2 = 1.4142135623730951;

uniform vec4 fg_color = vec4(1),
             bg_color = vec4(vec3(0),1);

mediump vec3 compute_normal(vec4 position, vec3 orientation);
vec3 /* */ compute_light(vec4, vec3, float intensity)
{
   vec3 hello;
   vec3 hello;
}

""""""
code, _ = resolve(code)
print(""GLSL version: %s\n"" % get_version(code))

get_definitions(code)
get_declarations(code)
get_prototypes(code)
get_functions(code)

# code = """"""
# #if A
# #if B
# #if C
# #endif
# #endif
# #endif
# """"""

# IF = (pyparsing.Literal('#') + (pyparsing.Keyword('if') | pyparsing.Keyword('ifdef') | pyparsing.Keyword('ifndef')))
# ENDIF = (pyparsing.Literal('#') + pyparsing.Keyword('endif'))
# MACRO = (IF + pyparsing.restOfLine() +
#          SkipTo(ENDIF, include=True)).setParseAction(pyparsing.originalTextFor)
# for (tokens, start, end) in MACRO.scanString(code):
#     print tokens
</code></pre>
"
39706264,4099813.0,2016-09-26T15:05:35Z,39706164,1,"<pre><code>if event.type == pygame.KEYDOWN:
    if event.key == pygame.K_LEFT:
        xone_change = -5
    elif event.key == pygame.K_RIGHT: #indented to be part of the keydown events
        xone_change = 5
</code></pre>

<p>You want this. Your indentation was wrong. </p>

<p>You had <code>elif event.key == pygame.K_RIGHT:</code> checking with your event.type which doesn't make sense if you think about it.</p>
"
39706311,52116.0,2016-09-26T15:07:40Z,39706005,0,"<p>CrawlerRunner:</p>

<blockquote>
  <p>This class shouldnât be needed (since Scrapy is responsible of using it accordingly) unless writing scripts that manually handle the crawling process. See Run Scrapy from a script for an example.</p>
</blockquote>

<p>CrawlerProcess:</p>

<blockquote>
  <p>This utility should be a better fit than CrawlerRunner if you arenât running another Twisted reactor within your application.</p>
</blockquote>

<p>It sounds like the CrawlerProcess is what you want unless you're adding your crawlers to an existing Twisted application.</p>
"
39706835,459745.0,2016-09-26T15:32:09Z,39703241,1,"<p>Here is another solution. This one uses regular expression and it does not squeeze multiple spaces within a command:</p>

<pre><code>import re

def parse_commands_file(filename):
    command_pattern = r""""""
        (\$[A-Z _]+)*  # The command, optional
        ([0-9 \n]+)*   # The parameter which might span multiple lines, optional
    """"""
    command_pattern = re.compile(command_pattern, flags=re.VERBOSE)

    with open(filename) as f:
        tokens = re.findall(command_pattern, f.read())
        return {cmd.strip(): [int(n) for n in params.split()] for cmd, params in tokens}

if __name__ == '__main__':
    command = parse_commands_file('commands.txt')
    print '$BOOLEAN_COMMAND:', command.get('$BOOLEAN_COMMAND')
    print '$NUMERIC COMMAND ALPHA:', command.get('$NUMERIC COMMAND ALPHA')
    print '$NUMERIC COMMAND BETA:', command.get('$NUMERIC COMMAND BETA')
</code></pre>

<h1>Discussion</h1>

<p>Basically, the command pattern says each line might contain two parts the command name and the numerical parameters, both are optional. </p>

<p>Note that a command might contain trailing space, that is why we strip them off using the expression <code>cmd.strip()</code>.</p>

<p>Also, the parameters part returned by <code>re.findall()</code> needs to be parsed by splitting them off by white spaces, then convert to <code>int</code> with the expression <code>[int(n) for n in params.split()]</code></p>
"
39707404,2961961.0,2016-09-26T16:01:18Z,39706735,-1,"<p>If you are using one of the latest versions of requests:
Try using the 'json' kwarg (no need to convert to json explicitly) instead of the 'data' kwarg:</p>

<pre><code>response = requests.post(url, json=data, headers=headers)
</code></pre>

<p>Note: Also, this way you can omit the 'Content-type' header.</p>
"
39708191,6207849.0,2016-09-26T16:47:51Z,39707080,1,"<p>I think the way you were trying to use the <code>method=first</code> to rank them after sorting were causing problems. </p>

<p>You could simply use the rank method with <code>first</code> arg on the grouped object itself giving you the desired unique ranks per group.</p>

<pre><code>df['new_rank'] = df.groupby(['weeks','device'])['ranking'].rank(method='first').astype(int)
print (df['new_rank'])

0     2
1     3
2     1
3     4
4     3
5     1
6     2
7     4
8     2
9     3
10    1
11    4
12    2
13    3
14    1
15    4
Name: new_rank, dtype: int32
</code></pre>

<p>Perform pivot operation:</p>

<pre><code>df = df.pivot_table(index=['weeks', 'device'], columns=['new_rank'],
                    values=['adtext'], aggfunc=lambda x: ' '.join(x))
</code></pre>

<p>Choose the second level of the multiindex columns which pertain to the rank numbers:</p>

<pre><code>df.columns = ['rank_' + str(i) for i in df.columns.get_level_values(1)]
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/iMT88.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iMT88.png"" alt=""Image_2""></a></p>

<hr>

<p><strong>Data:</strong>(to replicate)</p>

<pre><code>df = pd.DataFrame({'weeks': ['wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1',
                             'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2'],
                  'device': ['mobile', 'mobile', 'mobile', 'mobile', 'desktop', 'desktop', 'desktop', 'desktop',
                             'mobile', 'mobile', 'mobile', 'mobile', 'desktop', 'desktop', 'desktop', 'desktop'],
                  'website': ['url1', 'url2', 'url3', 'url4', 'url5', 'url2', 'url3', 'url4',
                             'url1', 'url16', 'url3', 'url4', 'url5', 'url2', 'url3', 'url4'],
                  'ranking': [2.1, 2.1, 1.0, 2.9, 2.1, 1.5, 1.5, 2.9, 
                              2.0, 2.1, 1.0, 2.9, 2.1, 2.9, 1.0, 2.9],
                  'adtext': ['string', 'string', 'string', 'string', 'string', 'string', 'string', 'string',
                             'string', 'string', 'string', 'string', 'string', 'string', 'string', 'string']})
</code></pre>

<p>Note: <code>method=first</code> assigns ranks in the order they appear in the array/series.</p>
"
39708266,257465.0,2016-09-26T16:51:09Z,39708247,2,"<p>You can just test the returned value first, then unpack:</p>

<pre><code>value = func(arg)
if value:
    a, b, c, d = value
</code></pre>

<p>Of course, you'll have to deal with what happens in the calling function when <code>abcd</code> don't get assigned. </p>
"
39708308,2867928.0,2016-09-26T16:53:21Z,39708247,2,"<p>Yes it will raise a <code>ValueError</code>. You can take care of that by wrapping the assignment with a <code>try-except</code> statement:</p>

<pre><code>try:
    a, b, c, d = func(arg)
except ValueError:
    # pass or do something else
</code></pre>

<p>Note that you can also check the validation of your returned value but since it's <a href=""https://docs.python.org/3.5/glossary.html"" rel=""nofollow"">Easier to ask for forgiveness than permission</a> as a coding style manner, it's better to use a <code>try-except</code> for handling this. </p>
"
39708327,674039.0,2016-09-26T16:54:21Z,39708247,1,"<p>Functions with heterogeneous return types are awkward for the caller.</p>

<p>Can you refactor?  If the <code>else</code> case is a failure mode, consider using exceptions for this case - python is not golang.  </p>
"
39708334,6779307.0,2016-09-26T16:54:40Z,39708247,2,"<p>Yes, this would cause a problem.  Generally you don't want to do something like this, it often leads to unexpected errors later on that are difficult to catch.  Python allows you to break the rules by returning whatever you want, but you should only do it if you have a good reason. As a general rule, your functions and methods should return the same type no matter what (with the possible exception of <code>None</code>).  </p>
"
39708454,3620003.0,2016-09-26T17:02:20Z,39708247,1,"<p>How about instead of having mixed return types, just refactor to require the function to have a truthy argument and raise an Error otherwise? This way the Error will be raised inside the function and not on assignment, which seems clearer to me.</p>

<pre><code>def func(arg):
    if not arg:
        raise ValueError('func requires a truthy argument')
    return a, b, c, d
</code></pre>
"
39708766,674039.0,2016-09-26T17:21:10Z,39708662,0,"<p>Unless I'm missing something here, the answer is quite obvious: the first case has an attribute assignment which triggers the <code>__setattr__</code> method, in the line:</p>

<pre><code>self.PublicAttribute1 = ""attribute""
</code></pre>

<p>But the second case has no such attribute assignment.  </p>
"
39708827,2063361.0,2016-09-26T17:24:06Z,39708662,0,"<p><code>__setattr__()</code> is called whenever a value is assigned to any of the class's property. Even if the property is initialized in the <code>__init__()</code>, it will make call to <code>__setattr__()</code>. Below is the example to illustrate that:</p>

<pre><code>&gt;&gt;&gt; class X(object):
...     def __init__(self, a, b):
...         self.a = a
...         self.b = b
...     def __setattr__(self, k, v):
...         print 'Set Attr: {} -&gt; {}'.format(k, v)
...
&gt;&gt;&gt; x = X(1, 3)       # &lt;-- __setattr__() called by __init__() 
Set Attr: a -&gt; 1
Set Attr: b -&gt; 3
&gt;&gt;&gt; x.a = 9           # &lt;-- __setattr__() called during external assignment
Set Attr: a -&gt; 9
</code></pre>
"
39708928,6883575.0,2016-09-26T17:29:33Z,39708662,2,"<p>The below can be found in the python documentation</p>

<blockquote>
  <p>Attribute assignments and deletions update the instanceâs dictionary, never a classâs dictionary. If the class has a __setattr__() or __delattr__() method, this is called instead of updating the instance dictionary directly.</p>
</blockquote>

<p>As you can clearly see the <em>__setattr__()</em> changes the instances dictionary when it is called. So whenever you try to assign a <strong>self.instance</strong> it raises your exception  <strong>Exception(""Attribute is Read-Only"")</strong>.
Whereas that's not the case with setting a class's attribute so no  exception is raised.</p>
"
39708960,2781701.0,2016-09-26T17:31:26Z,39706005,2,"<p>Scrapy's documentation does a pretty bad job at giving examples on real applications of both.</p>

<p><code>CrawlerProcess</code> assumes that scrapy is the only thing that is going to use twisted's reactor. If you are using threads in python to run other code this isn't always true. Let's take this as an example.</p>

<pre><code>from scrapy.crawler import CrawlerProcess
import scrapy
def notThreadSafe(x):
    """"""do something that isn't thread-safe""""""
    # ...
class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...

class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...

process = CrawlerProcess()
process.crawl(MySpider1)
process.crawl(MySpider2)
process.start() # the script will block here until all crawling jobs are finished
notThreadSafe(3) # it will get executed when the crawlers stop
</code></pre>

<p>Now, as you can see, the function will only get executed when the crawlers stop, what if I want the function to be executed while the crawlers crawl in the same reactor?</p>

<pre><code>from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
import scrapy

def notThreadSafe(x):
    """"""do something that isn't thread-safe""""""
    # ...

class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...

class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...
runner = CrawlerRunner()
runner.crawl(MySpider1)
runner.crawl(MySpider2)
d = runner.join()
d.addBoth(lambda _: reactor.stop())
reactor.callFromThread(notThreadSafe, 3)
reactor.run() #it will run both crawlers and code inside the function
</code></pre>

<p>The Runner class is not limited to this functionality, you may want some custom settings on your reactor (defer, threads, getPage, custom error reporting, etc)</p>
"
39709070,529630.0,2016-09-26T17:38:10Z,39708662,2,"<p><code>__setattr__</code> is only invoked on <em>instances</em> of a class and not the actual class itself. However, the class is still an object, so when setting the attribute of the class the <code>__setattr__</code> of the class of the class is invoked. </p>

<p>To change how attributes are set on classes you need to look into meta-classes. Metaclasses are overkill for almost any application, and can get confusing very easily. And trying to make a user-defined class/object read-only is normally a bad idea. That said, here is a simple example. </p>

<pre><code>&gt;&gt;&gt; class SampleMetaclass(type): # type as parent, not object
...     def __setattr__(self, name, value):
...         raise AttributeError(""Class is read-only"")
... 
&gt;&gt;&gt; class SampleClass(metaclass=SampleMetaclass):
...     def __setattr__(self, name, value):
...         raise AttributeError(""Instance is read-only"")
... 
&gt;&gt;&gt; SampleClass.attr = 1
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 3, in __setattr__
AttributeError: Class is read-only
&gt;&gt;&gt; s = SampleClass()
&gt;&gt;&gt; s.attr = 1
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""&lt;stdin&gt;"", line 3, in __setattr__
AttributeError: Instance is read-only
</code></pre>
"
39709106,1461210.0,2016-09-26T17:40:04Z,39708568,2,"<p>Here's one approach using binary erosion:</p>

<pre><code>import numpy as np
from scipy import ndimage

eroded = ndimage.binary_erosion(A, np.eye(3))
diff = (A - eroded).astype(np.bool)
print(repr(diff))
# array([[False, False, False, False, False, False],
#        [False,  True,  True,  True,  True, False],
#        [False,  True, False, False,  True, False],
#        [False,  True, False, False,  True, False],
#        [False,  True,  True,  True,  True, False],
#        [False, False, False, False, False, False]], dtype=bool)
</code></pre>

<p>You could also take the Laplacian of your input array and find where it is negative:</p>

<pre><code>lap = ndimage.laplace(A)
print(repr(lap &lt; 0))
# array([[False, False, False, False, False, False],
#        [False,  True,  True,  True,  True, False],
#        [False,  True, False, False,  True, False],
#        [False,  True, False, False,  True, False],
#        [False,  True,  True,  True,  True, False],
#        [False, False, False, False, False, False]], dtype=bool)
</code></pre>
"
39709154,1405065.0,2016-09-26T17:43:01Z,39708662,3,"<p>The <code>__setattr__</code> method defined in a class is only called for attribute assignments on <em>istances</em> of the class. It is not called for class variables, since they're not being assigned on an instance of the class with the method.</p>

<p>Of course, classes are instances too. They're instances of <code>type</code> (or a custom metaclass, usually a subclass of <code>type</code>). So if you want to prevent class variable creation, you need to create a metaclass with a <code>__setattr__</code> method.</p>

<p>But that's not really what you need to make your class do what you want. To just get a read-only attribute that can only be written once (in the <code>__init__</code> method), you can probably get by with some simpler logic. One approach is to set another attribute at the end of the <code>__init__</code> method, which tells <code>__setattr__</code> to lock down assignments after it is set:</p>

<pre><code>class Foo:
    def __init__(self, a, b):
        self.a = a
        self.b = b
        self._initialized = True

    def __setattr__(self, name, value):
        if self.__dict__.get('_initialized'):
            raise Exception(""Attribute is Read-Only"")
        super().__setattr__(name, value)
</code></pre>

<p>Another option would be to use <code>property</code> descriptors for the read-only attributes and store the real values in ""private"" variables that can be assigned to normally. You'd not use <code>__setattr__</code> in this version:</p>

<pre><code>class Foo():
    def __init__(a, b):
        self._a = a
        self._b = b

    @property
    def a(self):
        return self._a

    @property
    def b(self):
        return self._b
</code></pre>
"
39709237,416467.0,2016-09-26T17:48:28Z,39708662,1,"<p><code>__setattr__()</code> applies only to instances of the class. In your second example, when you define <code>PublicAttribute1</code>, you are defining it on the class; there's no instance, so <code>__setattr__()</code> is not called.</p>

<p>N.B. In Python, things you access using the <code>.</code> notation are called attributes, not variables. (In other languages they might be called ""member variables"" or similar.)</p>

<p>You're correct that the class attribute will be shadowed if you set an attribute of the same name on an instance.  For example:</p>

<pre><code>class C(object):
    attr = 42

 c = C()
 print(c.attr)     # 42
 c.attr = 13
 print(c.attr)     # 13
 print(C.attr)     # 42
</code></pre>

<p>Python resolves attribute access by first looking on the instance, and if there's no attribute of that name on the instance, it looks on the instance's class, then that class's parent(s), and so on until it gets to <code>object</code>, the root object of the Python class hierarchy.</p>

<p>So in the example above, we define <code>attr</code> on the class.  Thus, when we access <code>c.attr</code> (the instance attribute), we get 42, the value of the attribute on the class, because there's no such attribute on the instance. When we set the attribute of the instance, then print <code>c.attr</code> again, we get the value we just set, because there is now an attribute by that name on the instance. But the value <code>42</code> still exists as the attribute of the class, <code>C.attr</code>, as we see by the third <code>print</code>.</p>

<p>The statement to set the instance attribute in your <code>__init__()</code> method is handled by Python like any code to set an attribute on an object. Python does not care whether the code is ""inside"" or ""outside"" the class. So, you may wonder, how can you bypass the ""protection"" of <code>__setattr__()</code> when initializing the object? Simple: you call the <code>__setattr__()</code> method of a class that doesn't have that protection, usually your parent class's method, and pass it your instance.</p>

<p>So instead of writing:</p>

<pre><code>self.PublicAttribute1 = ""attribute""
</code></pre>

<p>You have to write:</p>

<pre><code> object.__setattr__(self, ""PublicAttribute1"", ""attribute"")
</code></pre>

<p>Since attributes are stored in the instance's attribute dictionary, named <code>__dict__</code>, you can also get around your <code>__setattr__</code> by writing directly to that:</p>

<pre><code> self.__dict__[""PublicAttribute1""] = ""attribute""
</code></pre>

<p>Either syntax is ugly and verbose, but the relative ease with which you can subvert the protection you're trying to add (after all, if you can do that, so can anyone else) might lead you to the conclusion that Python doesn't have very good support for protected attributes. In fact it doesn't, and this is by design. ""We're all consenting adults here."" You should not think in terms of public or private attributes with Python. All attributes are public. There is a <em>convention</em> of naming ""private"" attributes with a single leading underscore; this warns whoever is using your object that they're messing with an implementation detail of some sort, but they can still do it if they need to and are willing to accept the risks.</p>
"
39709498,5015569.0,2016-09-26T18:05:56Z,39709147,0,"<pre><code># for each ""primary key""
for primary in a.keys():
    # for each ""sub-key""
    for sub_key in a[primary].keys():
        # if the sub-key is also a primary key
        if sub_key in a.keys():
            # assign to the subkey the value of its corresponding primary key
            a[primary][sub_key] = a[sub_key]
</code></pre>

<p>Is this what you're looking for, at least for the first part of your question?</p>
"
39709666,1040092.0,2016-09-26T18:16:07Z,39709527,2,"<p>The <code>add_run</code> method will return a new instance of <code>Run</code> each time it is called. You need create a single instance and then apply <code>italic</code> and <code>bold</code> </p>

<pre><code>import docx

word = 'Dictionary'

doc = docx.Document()
p = doc.add_paragraph()

runner = p.add_run(word)
runner.bold = True
runner.italic = True

doc.save('test.docx')
</code></pre>
"
39709930,1520594.0,2016-09-26T18:32:43Z,39709147,1,"<p>This should work</p>

<pre><code>a = {114907: {114905: 1.4351310915,
              114908: 0.84635577943,
              114861: 61.490648372},
     113820: {113826: 8.6999361654,
              113819: 1.1412795216,
              111068: 1.1964946282,
              117066: 1.5595617822,
              113822: 1.1958951003},
     114908: {114906: 1.279878388,
              114907: 0.77568252572,
              114862: 2.5412545474}
     }

# Lets call the keys leaders and its value is a dict of
# keys ( call them members ) to floats.
# if a member is also a leader, then the two leaders combine.

leaders = set(a.keys())
leaders_to_members = { leader: set(member_dict.keys()) for leader, member_dict in a.items() }
seen_leaders =set()
b = {}
for leader, members in leaders_to_members.items():
    if leader in seen_leaders:
        continue

    members_as_leaders = members.intersection(leaders)
    members_as_leaders.add(leader)

    v = {}
    for member_leader in members_as_leaders:
        v.update(a[member_leader])

    seen_leaders.update(members_as_leaders)

    # if its just one element, you want it as the key directly
    b_key = tuple(members_as_leaders) if len(members_as_leaders) &gt; 1 else members_as_leaders.pop()
    # as per your output, you've removed the key to float value if it is a leader
    b_val = { k: float_val for k, float_val in v.items() if k not in members_as_leaders }
    b[b_key] = b_val

print(b)
</code></pre>

<p>Output</p>

<pre><code>{113820: {111068: 1.1964946282,
          113819: 1.1412795216,
          113822: 1.1958951003,
          113826: 8.6999361654,
          117066: 1.5595617822},
 (114907, 114908): {114861: 61.490648372,
                    114862: 2.5412545474,
                    114905: 1.4351310915,
                    114906: 1.279878388}}
</code></pre>

<blockquote>
  <p>The side question: why does pop in dictionaries return the value only and not the key: value pair?</p>
</blockquote>

<pre><code>&gt;&gt;&gt; a.pop()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: pop expected at least 1 arguments, got 0

&gt;&gt;&gt; help(a.pop)
""""""
Help on built-in function pop:

pop(...) method of builtins.dict instance
    D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.
    If key is not found, d is returned if given, otherwise KeyError is raised
""""""
</code></pre>

<p>As you can see, pop expects the key, so it can pop the value. Since you are required to give it the key, it doesn't have to return the key back to you.</p>
"
39710086,1319541.0,2016-09-26T18:41:23Z,39704367,0,"<p>Your appengine_config.py only needs to contain:</p>

<pre><code>from google.appengine.ext import vendor
vendor.add('lib')
</code></pre>

<p>All the rest you have posted looks fine to me.</p>
"
39710121,6003081.0,2016-09-26T18:43:26Z,39709606,1,"<p>Here is a quickly written (and extremely ugly) answer:</p>

<pre><code>def test(inArr):
    arr=inArr[:] #copy, unnecessary if we use index in a smart way
    result = []
    while len(arr)&gt;1: #as long as there can be an arithmetic progression
        x=[arr[0],arr[1]] #take first two
        arr=arr[2:] #remove from array
        step=x[1]-x[0]
        while len(arr)&gt;0 and x[1]+step==arr[0]: #check if the next value in array is part of progression too
            x[1]+=step #add it
            arr=arr[1:]
        result.append((x[0],x[1],step)) #append progression to result
    if len(arr)==1:
        result.append(arr[0])
    return result

print test([2, 4, 6, 8, 12, 13, 14, 15, 16, 17, 20])
</code></pre>

<p>This returns <code>[(2, 8, 2), (12, 17, 1), 20]</code></p>

<p>Slow, as it copies a list and removes elements from it</p>

<p>It only finds complete progressions, and only in sorted arrays.</p>

<p>In short, it is shitty, but should work ;)</p>

<p>There are other (cooler, more pythonic) ways to do this, for example you could convert your list to a set, keep removing two elements, calculate their arithmetic progression and intersect with the set.</p>

<p>You could also reuse the answer you provided to check for <strong>certain</strong> step sizes. e.g.:</p>

<pre><code>ranges = []
step_size=2
for key, group in groupby(enumerate(data), lambda (index, item): step_size*index - item):
    group = map(itemgetter(1), group)
    if len(group) &gt; 1:
        ranges.append(xrange(group[0], group[-1]))
    else:
        ranges.append(group[0])
</code></pre>

<p>Which finds every group with <strong>step size</strong> of <em>2</em>, but only those.</p>
"
39710128,557022.0,2016-09-26T18:43:58Z,39709147,0,"<p>How about this:</p>

<pre><code>import itertools

b ={}

for k1,v1 in a.items():
    for k2,v2 in v1.items():
        if k2 in a:
            a[k2].pop(k1)
            a[k1].pop(k2)
            dest = dict(itertools.chain(a[k1].items(), a[k2].items())) #python 2.7

            b[(k1,k2)] = dest

print b
</code></pre>

<p>answer:</p>

<pre><code>{(114908, 114907): {114905: 1.4351310915, 114906: 1.279878388, 114861: 61.490648372, 114862: 2.5412545474}}
</code></pre>
"
39710409,5963435.0,2016-09-26T19:01:53Z,39706394,3,"<p>When you just call <code>subl parameters.py</code> it does not block the thread, but you can use the <code>-w</code> flag to do so. I.e. just call</p>

<pre class=""lang-py prettyprint-override""><code>subprocess.Popen([""subl"", ""-w"", ""parameters.py""]).wait()
</code></pre>

<p>and it should work as requested.</p>
"
39710479,6451573.0,2016-09-26T19:05:55Z,39710365,5,"<p>Here's a way to convert to numerical values (casting to <code>int</code> does not work in all cases, unless there's a secret setting somewhere)</p>

<pre><code>from unicodedata import numeric
print(numeric('äº'))
</code></pre>

<p>result: 5.0</p>

<p>Someone noted (and was right) that some arabic or other chars worked fine with <code>int</code>, so a routine with a fallback mechanism could be done:</p>

<pre><code>from unicodedata import numeric

def to_integer(s):
    try:
        r = int(s)
    except ValueError:
        r = int(numeric(s))
    return r
</code></pre>

<p>EDIT: as zvone noted, there are fraction characters that return floating point numbers: ex: <code>numeric('\u00be') is 0.75</code> (3/4 char). So rounding to int is not always safe.</p>

<p>EDIT2: the <code>numeric</code> function only accepts one character. So the ""conversion to numeric"" that could handle most cases without risks of rounding would be</p>

<pre><code>from unicodedata import numeric

def to_float(s):
    try:
        r = float(s)
    except ValueError:
        r = numeric(s)
    return r

print(to_float('ÛµÛµ'))
print(to_float('äº'))
print(to_float('Â¾'))
</code></pre>

<p>result:</p>

<pre><code>55.0
5.0
0.75
</code></pre>

<p>(I don't want to steal user2357112 excellent explanation, but still wanted to provide a solution that tries to cover all cases)</p>
"
39710531,6003081.0,2016-09-26T19:08:57Z,39710365,-1,"<p>The source is incorrect.</p>

<p>From python doc:</p>

<blockquote>
  <p>class int(x, base=10)</p>
  
  <p>Return an integer object constructed from a
  number or string x, or return 0 if no arguments are given. If x is a
  number, return x.__int__(). For floating point numbers, this truncates
  towards zero.</p>
  
  <p>If x is not a number or if base is given, then x must be a string,
  bytes, or bytearray instance <strong>representing an integer literal in radix
  base</strong>. </p>
</blockquote>

<p>And an integer literal is just a string of numbers.</p>

<p>Edit: Was wrong, dug into the source code and found <a href=""http://svn.python.org/projects/python/trunk/Python/mystrtoul.c"" rel=""nofollow"">this</a> function is called when python wants to convert a string to int. There is a <strong>py_CHARMASK</strong> which <em>I guess</em> contains the information we need, but I could not find it :/</p>
"
39710800,2357112.0,2016-09-26T19:25:29Z,39710365,5,"<p><code>int</code> does not accept all ways to write numbers. It understands digit characters used for positional numeral systems, but neither <a href=""http://std.dkuug.dk/jtc1/sc2/wg2/docs/n3087-1.pdf"" rel=""nofollow"">Rumi</a> nor <a href=""https://en.wikipedia.org/wiki/Chinese_numerals#Characters_used_to_represent_numbers"" rel=""nofollow"">Chinese</a> numerals are positional. Neither <code>'äºäº'</code> nor two copies of Rumi numeral 5 would represent 55, so <code>int</code> doesn't accept them.</p>
"
39711282,1658617.0,2016-09-26T19:54:31Z,39711281,5,"<p>The descriptor you've given in the example is a data descriptor.</p>

<p>Upon setting the attribute, as any other data descriptor, it takes the highest priority and is called like so:</p>

<pre><code>type(myinst).__dict__[""attr""].__set__(myinst, 1234)
</code></pre>

<p>This in turn, adds <code>attr</code> to the instance dictionary according to your <code>__set__</code> method.</p>

<p>Upon attribute access, the descriptor is checked for having the <code>__get__</code> method but fails, causing for the search to be redirected to the instance's dictionary like so:</p>

<pre><code>myinst.__dict__[""attr""]
</code></pre>

<p>If it is not found in the instance dictionary, the descriptor itself is returned.</p>

<p>This behavior is shortly documented in the <a href=""https://docs.python.org/3.6/reference/datamodel.html#invoking-descriptors"" rel=""nofollow"">data model</a> like so:</p>

<blockquote>
  <p>If it does not define <code>__get__()</code>, then accessing the attribute will
  return the descriptor object itself unless there is a value in the
  objectâs instance dictionary.</p>
</blockquote>

<p>Common usecases include avoiding <code>{instance: value}</code> dictionaries inside the descriptors, and caching values in an efficient way.</p>

<hr>

<p>In Python 3.6, <code>__set_name__</code> was added to the descriptor protocol thus eliminating the need for specifying the name inside the descriptor. This way, your descriptor can be written like so:</p>

<pre><code>class Desc:
    def __set_name__(self, owner, name):
        self.name = name
    def __set__(self, inst, value):
        inst.__dict__[self.name] = value
        print(""set"", self.name)

class Test:
    attr = Desc()
</code></pre>
"
39711380,364696.0,2016-09-26T19:59:45Z,39711335,3,"<p>The correct solution would be to read until you hit the terminating byte, then convert to UTF-8 at that time (so you have all characters):</p>

<pre><code>mybytes = bytearray()
while True:
    x = read_next_byte()
    if x == 0:
        break
    mybytes.append(x)
my_string = mybytes.decode('utf-8')
</code></pre>

<p>The above is the most direct translation of your original code. Interestingly, this is one of those cases where <a href=""https://docs.python.org/3/library/functions.html#iter"" rel=""nofollow"">two arg <code>iter</code></a> can be used to dramatically simplify the code by making your C-style stateful byte reader function into a Python iterator that lets you one-line the work:</p>

<pre><code># If this were Python 3 code, you'd use the bytes constructor instead of bytearray
my_string = bytearray(iter(read_next_byte, 0)).decode('utf-8')
</code></pre>
"
39711931,5741205.0,2016-09-26T20:36:32Z,39711422,2,"<p>Here is a solution using <code>pivot_table()</code> method:</p>

<pre><code>In [57]: pvt = (df.assign(bins=pd.cut(df.Price, [0,15,30]))
   ....:          .pivot_table(index=['State','# of Boxes'],
   ....:                       columns='bins', aggfunc='size', fill_value=0)
   ....:       )

In [58]: pvt
Out[58]:
bins              (0, 15]  (15, 30]
State # of Boxes
AK    1                 1         0
      2                 2         1
AZ    1                 1         1
      2                 2         0

In [59]: pvt.apply(lambda x: x/pvt.sum(1))
Out[59]:
bins               (0, 15]  (15, 30]
State # of Boxes
AK    1           1.000000  0.000000
      2           0.666667  0.333333
AZ    1           0.500000  0.500000
      2           1.000000  0.000000
</code></pre>
"
39711946,3293881.0,2016-09-26T20:37:43Z,39711838,1,"<p><strong>Approach #1</strong> We could do something like this -</p>

<pre><code># Get the columns indices of the input sparse matrix
C = sp.find(A)[1]

# Use np.in1d to create a mask of non-zero columns. 
# So, we invert it and convert to int dtype for desired output.
out = (~np.in1d(np.arange(A.shape[1]),C)).astype(int)
</code></pre>

<p>Alternatively, to make the code shorter, we can use subtraction -</p>

<pre><code>out = 1-np.in1d(np.arange(A.shape[1]),C)
</code></pre>

<p>Step-by-step run -</p>

<p>1) Input array and sparse matrix from it :</p>

<pre><code>In [137]: arr             # Regular dense array
Out[137]: 
array([[3, 0, 3, 0],
       [0, 0, 2, 0],
       [2, 5, 1, 0],
       [0, 0, 0, 0]])

In [138]: A = sp.coo_matrix(arr) # Convert to sparse matrix as input here on
</code></pre>

<p>2) Get non-zero column indices :</p>

<pre><code>In [139]: C = sp.find(A)[1]

In [140]: C
Out[140]: array([0, 2, 2, 0, 1, 2], dtype=int32)
</code></pre>

<p>3) Use <code>np.in1d</code> to get mask of non-zero columns :</p>

<pre><code>In [141]: np.in1d(np.arange(A.shape[1]),C)
Out[141]: array([ True,  True,  True, False], dtype=bool)
</code></pre>

<p>4) Invert it :</p>

<pre><code>In [142]: ~np.in1d(np.arange(A.shape[1]),C)
Out[142]: array([False, False, False,  True], dtype=bool)
</code></pre>

<p>5) Finally convert to int dtype :</p>

<pre><code>In [143]: (~np.in1d(np.arange(A.shape[1]),C)).astype(int)
Out[143]: array([0, 0, 0, 1])
</code></pre>

<p>Alternative subtraction approach :</p>

<pre><code>In [145]: 1-np.in1d(np.arange(A.shape[1]),C)
Out[145]: array([0, 0, 0, 1])
</code></pre>

<p><strong>Approach #2</strong> Here's another way and possibly a faster one using <code>matrix-multiplication</code> -</p>

<pre><code>out = 1-np.ones(A.shape[0],dtype=bool)*A.astype(bool)
</code></pre>

<hr>

<p><strong>Runtime test</strong> </p>

<p>Let's test out all the posted approaches on a big and really sparse matrix -</p>

<pre><code>In [29]: A = sp.coo_matrix((np.random.rand(4000,4000)&gt;0.998).astype(int))

In [30]: %timeit 1-np.in1d(np.arange(A.shape[1]),sp.find(A)[1])
100 loops, best of 3: 4.12 ms per loop # Approach1

In [31]: %timeit 1-np.ones(A.shape[0],dtype=bool)*A.astype(bool)
1000 loops, best of 3: 771 Âµs per loop # Approach2

In [32]: %timeit 1 - (A.col==np.arange(A.shape[1])[:,None]).any(axis=1)
1 loops, best of 3: 236 ms per loop # @hpaulj's soln

In [33]: %timeit (A!=0).sum(axis=0)==0
1000 loops, best of 3: 1.03 ms per loop  # @jez's soln

In [34]: %timeit (np.sum(np.absolute(A.toarray()), 0) == 0) * 1
10 loops, best of 3: 86.4 ms per loop  # @wwii's soln 
</code></pre>
"
39711997,2901002.0,2016-09-26T20:40:43Z,39711422,3,"<p>I think you can use <code>groupby</code> by columns with binned <code>Series</code> created by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html"" rel=""nofollow""><code>cut</code></a>, aggregated by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.size.html"" rel=""nofollow""><code>size</code></a> and reshape by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html"" rel=""nofollow""><code>unstack</code></a>:</p>

<pre><code>print (pd.cut(df['Price'], bins=[0,15,30]))
0     (0, 15]
1     (0, 15]
2     (0, 15]
3    (15, 30]
4    (15, 30]
5     (0, 15]
6     (0, 15]
7     (0, 15]
Name: Price, dtype: category
Categories (2, object): [(0, 15] &lt; (15, 30]

df1 = df.Price.groupby([df['State'],df['# of Boxes'],pd.cut(df['Price'], bins=[0,15,30])])
              .size()
              .unstack(fill_value=0)

print (df1)
Price             (0, 15]  (15, 30]
State # of Boxes                   
AK    1                 1         0
      2                 2         1
AZ    1                 1         1
      2                 2         0
</code></pre>

<p>Then divide all values by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html"" rel=""nofollow""><code>sum</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.div.html"" rel=""nofollow""><code>div</code></a></p>

<pre><code>df1 = df1.div(df1.sum(axis=1), axis=0)
print (df1)
Price              (0, 15]  (15, 30]
State # of Boxes                    
AK    1           1.000000  0.000000
      2           0.666667  0.333333
AZ    1           0.500000  0.500000
      2           1.000000  0.000000
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>In [135]: %timeit (jez(df))
100 loops, best of 3: 3.51 ms per loop

In [136]: %timeit (maxu(df))
100 loops, best of 3: 6.21 ms per loop

def jez(df):
    df1 = df.Price.groupby([df['State'],df['# of Boxes'],pd.cut(df['Price'], bins=[0,15,30])]).size().unstack(fill_value=0)
    return  df1.div(df1.sum(1), axis=0)

def maxu(df):    
    pvt = df.assign(bins=pd.cut(df.Price, [0,15,30])).pivot_table(index=['State','# of Boxes'], columns='bins', aggfunc='size', fill_value=0)
    return pvt.apply(lambda x: x/pvt.sum(1))
</code></pre>
"
39712057,2049247.0,2016-09-26T20:44:34Z,39711674,0,"<p>According to this post
<a href=""https://github.com/SeleniumHQ/selenium/issues/2739#issuecomment-249479530"" rel=""nofollow"">https://github.com/SeleniumHQ/selenium/issues/2739#issuecomment-249479530</a>
the is that you need to use something called Gecko Driver, found here <a href=""https://github.com/mozilla/geckodriver"" rel=""nofollow"">https://github.com/mozilla/geckodriver</a>. Other people have had success going back to a previous version (before 48) of Firefox as well. I'm also experiencing this problem and don't actually understand how to do either solution and am making slow progress.</p>

<p>Hi Dennis, I'll post my step by step solution now that I got it to work.</p>

<h1>Step By Step solution</h1>

<p>The problem is that Selenium and Firefox don't support one another anymore. I don't actually understand why but hopefully someone can comment and explain in more thorough detail than I. There are two possible solutions. One is install something called Geckodriver. I got that installed but had difficulty adding it to my PATH and generally found myself frustrated. </p>

<p><strong>Instead</strong> I went a simpler route. 
First I uninstalled firefox with the command</p>

<pre><code>sudo apt-get purge firefox
</code></pre>

<p>Then I downloaded <a href=""https://ftp.mozilla.org/pub/firefox/releases/47.0.1/"" rel=""nofollow"">Firefox 47.0.1</a> from here (I selected the english US version). I then moved it from my downloads folder to my home folder. Then I extracted it using this command.</p>

<pre><code>tar xjf firefox-47.0.1.tar.bz2
</code></pre>

<p>Your number Firefox may be different from mine. Then I cd'd into that directory</p>

<pre><code>cd firefox    
</code></pre>

<p>which brought me into that directory. Then all that was left was to run the command </p>

<pre><code>sudo apt install firefox 
</code></pre>

<p>After which the version of Selenium I have worked again. Happily I'm back to writing code not configuring things!</p>
"
39712076,3019689.0,2016-09-26T20:45:35Z,39711838,1,"<p>The actual logical operation can be performed like this:</p>

<pre><code>b = (A!=0).sum(axis=0)==0
# matrix([[False, False, False,  True]], dtype=bool)
</code></pre>

<p>Now, to ensure that I'm answering your question exactly, I'd better tell you how you <em>could</em> convert from booleans to integers (although really, for most applications I can think of, you can do a lot more in <code>numpy</code> and friends if you stick with an array of <code>bool</code>s):</p>

<pre><code>b = b.astype(int)
#matrix([[0, 0, 0, 1]])
</code></pre>

<p>Either way, to then convert from a <code>matrix</code> to a <code>list</code>, you could do this:</p>

<pre><code>c = list(b.flat)
# [0, 0, 0, 1]
</code></pre>

<p>...although again, I'm not sure this is the best thing to do: for most applications I can imagine, I would perhaps just convert to a one-dimensional <code>numpy.array</code> with <code>c = b.A.flatten()</code> instead.</p>
"
39712085,5741205.0,2016-09-26T20:46:35Z,39711229,2,"<p>What about an alternative approach - filtering your data <strong>before</strong> plotting?</p>

<pre><code>In [10]: df.set_index('Date').ix['2010-01-01' : '2010-01-10']
Out[10]:
            Quantity
Date
2010-01-01         1
2010-01-02         0
2010-01-03         0
2010-01-04         2
2010-01-05         3
2010-01-06         1
2010-01-07         0
2010-01-08         1
2010-01-09         1
2010-01-10         2

In [11]: df.set_index('Date').ix['2010-01-01' : '2010-01-10', 'Quantity'].plot.bar(grid=False, legend=False)
</code></pre>
"
39712094,3125566.0,2016-09-26T20:47:06Z,39709606,3,"<p>The <code>itertools</code> <a href=""https://docs.python.org/2.6/library/itertools.html#recipes"" rel=""nofollow"">pairwise recipe</a> is one way to solve the problem. Applied with <a href=""https://docs.python.org/2.6/library/itertools.html#itertools.groupby"" rel=""nofollow""><code>itertools.groupby</code></a>, groups of pairs whose mathematical difference are equivalent can be created. The first and last items of each group are then selected for multi-item groups or the last item is selected for singleton groups:</p>

<pre><code>from itertools import groupby, tee, izip


def pairwise(iterable):
    ""s -&gt; (s0,s1), (s1,s2), (s2, s3), ...""
    a, b = tee(iterable)
    next(b, None)
    return izip(a, b)

def grouper(lst):
    result = []
    for k, g in groupby(pairwise(lst), key=lambda x: x[1] - x[0]):
        g  = list(g)
        if len(g) &gt; 1:
            try:
                if g[0][0] == result[-1]:
                    del result[-1]
                elif g[0][0] == result[-1][1]:
                    g = g[1:] # patch for duplicate start and/or end
            except (IndexError, TypeError):
                pass
            result.append((g[0][0], g[-1][-1], k))
        else:
            result.append(g[0][-1]) if result else result.append(g[0])
    return result
</code></pre>

<hr>

<p><strong>Trial:</strong> <code>input -&gt; grouper(lst) -&gt; output</code></p>

<pre><code>Input: [2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 20]
Output: [(2, 5, 1), (12, 17, 1), 20]

Input: [2, 4, 6, 8, 12, 13, 14, 15, 16, 17, 20]
Output: [(2, 8, 2), (12, 17, 1), 20]

Input: [2, 4, 6, 8, 12, 12.4, 12.9, 13, 14, 15, 16, 17, 20]
Output: [(2, 8, 2), 12, 12.4, 12.9, (13, 17, 1), 20] # 12 does not appear in the second group
</code></pre>

<hr>

<p><strong>Update</strong>: (<em>patch for duplicate start and/or end values</em>)</p>

<pre><code>s1 = [i + 10 for i in xrange(0, 11, 2)]; s2 = [30]; s3 = [i + 40 for i in xrange(45)]

Input: s1+s2+s3
Output: [(10, 20, 2), (30, 40, 10), (41, 84, 1)]

# to make 30 appear as an entry instead of a group change main if condition to len(g) &gt; 2
Input: s1+s2+s3
Output: [(10, 20, 2), 30, (41, 84, 1)]

Input: [2, 4, 6, 8, 10, 12, 13, 14, 15, 16, 17, 20]
Output: [(2, 12, 2), (13, 17, 1), 20]
</code></pre>
"
39712223,2658050.0,2016-09-26T20:55:14Z,39705175,1,"<p>What you are considering here is known in machine learning community as <strong>superset learning</strong>, meaning, that instead of typical supervised setting where you have training set in the form of {(x_i, y_i)} you have {({x_1, ..., x_N}, y_1)} such that you know that at least one element from the set has property y_1. This is not a very common setting, but existing, with some research available, google for papers in the domain.</p>

<p>In terms of your own loss functions - scikit-learn is a no-go. Scikit-learn is about simplicity, it provides you with a small set of ready to use tools with very little flexibility. It is not a research tool, and your problem is researchy. What can you use instead? I suggest you go for any symbolic-differentiation solution, for example <a href=""https://github.com/HIPS/autograd"" rel=""nofollow"">autograd</a> which gives you ability to <strong>differentiate through python code</strong>, simply apply <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"" rel=""nofollow"">scipy.optimize.minimize</a> on top of it and you are done! Any custom loss function will work just fine.</p>

<p>As a side note - minimum operator is not differentiable, thus the model might have hard time figuring out what is going on. You could instead try to do <code>sum((Y - prod_x F(x_1, x_2) )^2)</code> since multiplication is nicely differentiable, and you will still get the similar effect - if at least one element is predicted to be 0 it will remove any ""1"" answer from the remaining ones. You can even go one step further to make it more numerically stable and do:</p>

<pre><code>if Y==0 then loss = sum_x log(F(x_1, x_2 ) )
if Y==1 then loss = sum_x log(1-F(x_1, x_2))
</code></pre>

<p>which translates to</p>

<pre><code>Y * sum_x log(1-F(x_1, x_2)) + (1-Y) * sum_x log( F(x_1, x_2) )
</code></pre>

<p>you can notice similarity with cross entropy cost which makes perfect sense since your problem is indeed a <strong>classification</strong>. And now you have perfect probabilistic loss - you are attaching such probabilities of each segment to be ""bad"" or ""good"" so the probability of the whole object being bad is either high (if Y==0) or low (if Y==1).</p>
"
39712236,5741205.0,2016-09-26T20:55:54Z,39711977,2,"<p>I think you should be able to filter your <code>value_counts</code> series using <code>.ix[]</code> or <code>.loc[]</code> indexers, filtering (indexing) by strings</p>

<p>Demo:</p>

<pre><code>In [27]: df
Out[27]:
                        Count
Admission_Source_Code
1                      246804
2                      135272
5                        8983
8                        3459
4                        3177
6                        1278
9                         522
D                         314
E                          91
0                          29
F                          20

In [28]: df.index.dtype
Out[28]: dtype('O')

In [29]: df.ix['2']
Out[29]:
Count    135272
Name: 2, dtype: int64

In [30]: df.ix[['2','E','5','D']]
Out[30]:
                        Count
Admission_Source_Code
2                      135272
E                          91
5                        8983
D                         314
</code></pre>

<p>List index values:</p>

<pre><code>In [36]: df.index.values
Out[36]: array(['1', '2', '5', '8', '4', '6', '9', 'D', 'E', '0', 'F'], dtype=object)
</code></pre>
"
39712237,298607.0,2016-09-26T20:55:56Z,39709147,1,"<p>In Python3.x, <code>{}.keys()</code> returns a view. You can use set operations on a dict view.</p>

<p>So your algorithm is somewhat simplified to:</p>

<pre><code>outer=a.keys()
deletions=set()
new_a={} 
for k,di in a.items():
    c=outer &amp; di.keys()
    if c:
        c=c.pop()
        if (c,k) not in deletions:
            deletions.add((k,c))
    else:
        new_a[k]=di

for t in deletions:
    del a[t[0]][t[1]], a[t[1]][t[0]]
    new_a[t]=a[t[0]]
    new_a[t].update(a[t[1]])  
&gt;&gt;&gt; new_a
{113820: {113826: 8.6999361654, 
          113819: 1.1412795216, 
          111068: 1.1964946282, 
          117066: 1.5595617822, 
          113822: 1.1958951003}, 
(114908, 114907): {114905: 1.4351310915, 
                   114906: 1.279878388, 
                   114861: 61.490648372, 
                   114862: 2.5412545474}}
</code></pre>

<p>The order of the elements in the tuple may vary depending on the order of iteration and the order of the set operations. Both are unordered with dicts. Since the elements may vary, which dict which is used as the update dict is also unordered. </p>

<p>This function also only works with a single intersection; i.e., there are not tuples created with more than 2 elements as keys. </p>
"
39712446,6486738.0,2016-09-26T21:10:26Z,39706164,1,"<p>The reason why you're getting an error is because different events have different attributes. You can check which event has which attributes at the <a class='doc-link' href=""http://stackoverflow.com/documentation/pygame/5110/event-handling/18046/event-loop#t=201609262052092301037"">Stackoverflow pygame documentation</a> or at the official <a href=""http://www.pygame.org/docs/ref/event.html"" rel=""nofollow"">pygame docs</a>.</p>

<p>Let's take an example between two types of events:</p>

<ol>
<li>QUIT event types has no attributes.</li>
<li>KEYDOWN event types has attributes <em>unicode</em>, <em>key</em> and <em>mod</em>.</li>
</ol>

<p>This means that we cannot check for the <em>key</em> attribute in an event until we've made sure that the event is of type <code>KEYDOWN</code> (or <code>KEYUP</code>).</p>

<p>In your code you have:</p>

<pre><code>for event in pygame.event.get():
    if event.type == pygame.KEYDOWN:
        if event.key == pygame.K_LEFT:
           xone_change = -5
    elif event.key == pygame.K_RIGHT:
        xone_change = 5
</code></pre>

<p>If the event type isn't <code>KEYDOWN</code> then you're checking with the event's <em>key</em> attribute, which will raise an attribute error if the event is of type <code>QUIT</code>, <code>MOUSEBUTTONDOWN</code> or any other event that isn't <code>KEYDOWN</code> or <code>KEYUP</code>.</p>

<p>To correct this you'll do as MooingRawr answered and make sure that you've indented the two last lines inside the first if-statement. Just thought I'd give an answer to why it's wrong.</p>
"
39712481,2141635.0,2016-09-26T21:12:51Z,39709606,1,"<p>You can create an iterator to help grouping and try to pull the next element from the following group which will be the end of the previous group:</p>

<pre><code>def ranges(lst):
    it = iter(lst)
    next(it)  # move to second element for comparison
    grps = groupby(lst, key=lambda x: (x - next(it, -float(""inf""))))
    for k, v in grps:
        i = next(v)
        try:
            step = next(v) - i  # catches single element v or gives us a step
            nxt = list(next(grps)[1])
            yield xrange(i, nxt.pop(0), step)
            # outliers or another group
            if nxt:
                yield nxt[0] if len(nxt) == 1 else xrange(nxt[0], next(next(grps)[1]), nxt[1] - nxt[0])
        except StopIteration:
            yield i  # no seq
</code></pre>

<p>which give you:</p>

<pre><code>In [2]: l1 = [2, 3, 4, 5, 8, 10, 12, 14, 13, 14, 15, 16, 17, 20, 21]

In [3]: l2 = [2, 4, 6, 8, 12, 13, 14, 15, 16, 17, 20]

In [4]: l3 = [13, 14, 15, 16, 17, 18]

In [5]: s1 = [i + 10 for i in xrange(0, 11, 2)]

In [6]: s2 = [30]

In [7]: s3 = [i + 40 for i in xrange(45)]

In [8]: l4 = s1 + s2 + s3

In [9]: l5 = [1, 2, 5, 6, 9, 10]

In [10]: l6 = {1, 2, 3, 5, 6, 9, 10, 13, 19, 21, 22, 23, 24}

In [11]: 

In [11]: for l in (l1, l2, l3, l4, l5, l6):
   ....:         print(list(ranges(l)))
   ....:     
[xrange(2, 5), xrange(8, 14, 2), xrange(13, 17), 20, 21]
[xrange(2, 8, 2), xrange(12, 17), 20]
[xrange(13, 18)]
[xrange(10, 20, 2), 30, xrange(40, 84)]
[1, 2, 5, 6, 9, 10]
[xrange(1, 3), 5, 6, 9, 10, 13, 19, xrange(21, 24)]
</code></pre>

<p>When the step is <code>1</code> it is not included in the xrange output.</p>
"
39713114,2141635.0,2016-09-26T22:02:28Z,39704084,1,"<p>To work for a list of lists containing string and a flat list of strings, you need to iterate over the sequence and then check the type:</p>

<pre><code>def noVow(seq):
    vowels = {'a', 'i', 'e', 'o', 'u', 'u'}
    for ele in seq:
        if isinstance(ele, list):
            # a list so recursively process
            yield [s for s in noVow(ele)]
        # else it has to be a string so just see if it is not a vowel
        elif ele not in vowels:
            yield ele
</code></pre>

<p>You use it like:</p>

<pre><code>In [39]: li
Out[39]: [['b', 'c'], ['d', 'a']]

In [40]: li[:] = noVow(li)

In [41]: print(li)
[['b', 'c'], ['d']]

In [42]: li = [""a"",""b"",""c"",""e""]

In [43]: li[:] = noVow(li)

In [44]: print(li)
['b', 'c']
In [10]: li = [[""b"", ""c""], [""d"", [""a""]]]
In [11]: li[:] = noVow(li)   
In [12]: li
Out[12]: [['b', 'c'], ['d', []]] # works for nested lists
</code></pre>

<p>If you wanted a flat list of all non-vowels and you can use python3, you can use <em>yield from</em>:</p>

<pre><code>def noVow(seq):
    vowels = {'a', 'i', 'e', 'o', 'u', 'u'}
    for ele in seq:
        if isinstance(seq, list):
            yield from noVow(ele)
        elif ele not in vowels:
            yield ele
</code></pre>

<p>You use it the same way:</p>

<pre><code>In [2]: li = [[""b"", ""c""], [""d"", ""a""]]

In [3]: li[:] = noVow(li)

In [4]: li
Out[4]: ['b', 'c', 'd']

In [5]: li = [""a"",""b"",""c"",""e""]

In [6]: li[:] = noVow(li)

In [7]: li
Out[7]: ['b', 'c']
</code></pre>

<p>You can do the same with python2, you just need another loop</p>
"
39713115,901925.0,2016-09-26T22:02:40Z,39711838,1,"<p>Recent</p>

<p><a href=""http://stackoverflow.com/questions/39683931/scipy-sparse-coo-matrix-how-to-fast-find-all-zeros-column-fill-with-1-and-norma"">scipy.sparse.coo_matrix how to fast find all zeros column, fill with 1 and normalize</a></p>

<p>similar, except it wants to fill those columns with 1s and normalize them.</p>

<p>I immediately suggested the <code>lil</code> format of the transpose.  All-0 columns will be empty lists in this format.  But sticking with the <code>coo</code> format I suggested</p>

<pre><code>np.nonzero(~(Mo.col==np.arange(Mo.shape[1])[:,None]).any(axis=1))[0]
</code></pre>

<p>or for this 1/0 format</p>

<pre><code>1 - (Mo.col==np.arange(Mo.shape[1])[:,None]).any(axis=1)
</code></pre>

<p>which is functionally the same as:</p>

<pre><code>1 - np.in1d(np.arange(Mo.shape[1]),Mo.col)
</code></pre>

<p><code>sparse.find</code> converts the matrix to <code>csr</code> to sum duplicates and eliminate duplicates, and then back to <code>coo</code> to get the <code>data</code>, <code>row</code>, and <code>col</code> attributes (which it returns).</p>

<p><code>Mo.nonzero</code> uses <code>A.data != 0</code> to eliminate 0s before returning the <code>col</code> and <code>row</code> attributes.</p>

<p>The <code>np.ones(A.shape[0],dtype=bool)*A.astype(bool)</code> solution requires converting <code>A</code> to <code>csr</code> format for multiplication.  </p>

<p><code>(A!=0).sum(axis=0)</code> also converts to <code>csr</code> because column (or row) sum is done with a matrix multiplication.</p>

<p>So the no-convert requirement is unrealistic, at least within the bounds of sparse formats.  </p>

<p>===============</p>

<p>For Divakar's test case my <code>==</code> version is quite slow; it's ok with small ones, but creates too large of test array with the 1000 columns.</p>

<p>Testing on a matrix that is sparse enough to have a number of 0 columns:</p>

<pre><code>In [183]: Arr=sparse.random(1000,1000,.001)
In [184]: (1-np.in1d(np.arange(Arr.shape[1]),Arr.col)).any()
Out[184]: True
In [185]: (1-np.in1d(np.arange(Arr.shape[1]),Arr.col)).sum()
Out[185]: 367

In [186]: timeit 1-np.ones(Arr.shape[0],dtype=bool)*Arr.astype(bool)
1000 loops, best of 3: 334 Âµs per loop
In [187]: timeit 1-np.in1d(np.arange(Arr.shape[1]),Arr.col)
1000 loops, best of 3: 323 Âµs per loop
In [188]: timeit 1-(Arr.col==np.arange(Arr.shape[1])[:,None]).any(axis=1)
100 loops, best of 3: 3.9 ms per loop
In [189]: timeit (Arr!=0).sum(axis=0)==0
1000 loops, best of 3: 820 Âµs per loop
</code></pre>
"
39713482,4893008.0,2016-09-26T22:39:10Z,39713137,2,"<p>If you don't mind taking a bit of a shortcut and using the median of each date column, this should work:</p>

<pre><code>def order_date_columns(df, date_columns_to_sort):
    x = [(col, df[col].astype(np.int64).median()) for col in date_columns_to_sort]
    return [x[0] for x in sorted(x, key=lambda x: x[1])]
</code></pre>
"
39713580,2336654.0,2016-09-26T22:52:00Z,39713381,3,"<p>Try:</p>

<pre><code>test['TIME'] = pd.to_datetime('2016-09-20') + pd.to_timedelta(time, 'ms')
</code></pre>
"
39713581,509824.0,2016-09-26T22:52:08Z,39713381,3,"<p>This is the raison d'Ètre for <code>pandas.date_range()</code>:</p>

<pre><code>import pandas as pd

test = pd.DataFrame({'TIME': pd.date_range(start='2016-09-20',
                                           freq='10ms', periods=20)})
print(test)
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>                      TIME
0  2016-09-20 00:00:00.000
1  2016-09-20 00:00:00.010
2  2016-09-20 00:00:00.020
3  2016-09-20 00:00:00.030
4  2016-09-20 00:00:00.040
5  2016-09-20 00:00:00.050
6  2016-09-20 00:00:00.060
7  2016-09-20 00:00:00.070
8  2016-09-20 00:00:00.080
9  2016-09-20 00:00:00.090
10 2016-09-20 00:00:00.100
11 2016-09-20 00:00:00.110
12 2016-09-20 00:00:00.120
13 2016-09-20 00:00:00.130
14 2016-09-20 00:00:00.140
15 2016-09-20 00:00:00.150
16 2016-09-20 00:00:00.160
17 2016-09-20 00:00:00.170
18 2016-09-20 00:00:00.180
19 2016-09-20 00:00:00.190
</code></pre>

<p>(Substitute <code>periods=20</code> for <code>periods=172800000</code>)</p>
"
39713787,2336654.0,2016-09-26T23:17:22Z,39713678,3,"<p>you want to use <a href=""http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation"" rel=""nofollow""><code>transform</code></a> after your <code>groupby</code> to get an equivalently sized array.  <code>gt</code> is greater than.  <code>mul</code> is multiply.  I multiply by <code>1</code> to get the boolean results from <code>gt</code> to <code>0</code> or <code>1</code>.  </p>

<p>You can see other examples here 
<a class='doc-link' href=""http://stackoverflow.com/documentation/pandas/1822/grouping-data/14011/using-transform-to-get-group-level-statistics-while-preserving-the-original-data#t=201609262324317180894"">using transform to get group-level statistics while preserving the original dataframe</a></p>

<p>Consider the dataframe <code>df</code></p>

<pre><code>df = pd.DataFrame(dict(labels=np.random.choice(list('abcde'), 100),
                       A=np.random.randn(100)))
</code></pre>

<p>I'd get the indicator like this</p>

<pre><code>df.A.gt(df.groupby('labels').A.transform(pd.Series.quantile, q=.95)).mul(1)
</code></pre>

<hr>

<p>In your case, I'd do</p>

<pre><code>df['INDICATOR'] = df['LENGTH'].gt(df.groupby(['CLIMATE','BIN'])['LENGTH'] \
                                    .transform(pd.Series.quantile, q=.95)).mul(1)
</code></pre>
"
39713902,389289.0,2016-09-26T23:32:42Z,39713773,4,"<p>Those are quotes (â and â). If you just want to get rid of them at the beginning or end of the string, it is easiest to <code>strip</code> them.</p>

<pre><code>&gt;&gt;&gt; u'\u201cAnxiety\u201d'.strip(u'\u201c\u201d')
u'Anxiety'
</code></pre>

<p>If you want to get rid of them anywhere in the string, <code>replace</code> them:</p>

<pre><code>&gt;&gt;&gt; u'\u201cAnxiety\u201d'.replace(u'\u201c', '').replace(u'\u201d', '')
u'Anxiety'
</code></pre>
"
39713915,5046042.0,2016-09-26T23:34:32Z,39713773,0,"<pre><code>dict['track'] = list(map(lambda x: x.replace('\u201c','').replace('\u201d',''), dict['track']))
</code></pre>
"
39714550,525169.0,2016-09-27T00:59:38Z,39714176,1,"<p>Stack your two numpy arrays in ""depth"" using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html"" rel=""nofollow""><code>np.dstack</code></a>, and then modify your <code>foo</code> function, so that it operates on only the last axis of your stacked array. This is easily done using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html"" rel=""nofollow""><code>np.sum</code></a> with parameter <code>axis=-1</code>, instead of using the builtin <code>sum</code>:</p>

<pre><code>import numpy as np

def foo(xy):
    return np.sum(xy, axis=-1) ** np.sin(np.sum(xy, axis=-1))

x = np.arange(-2, 1, 1)          # points in the x axis
y = np.arange( 3, 8, 1)          # points in the y axis
X, Y = np.meshgrid(x, y)         # X, Y : grid
XY = np.dstack((X, Y))
</code></pre>

<p>And now, you should get:</p>

<pre><code>&gt;&gt;&gt; XY.shape
(5, 3, 2)
&gt;&gt;&gt; foo(XY)
array([[ 1.        ,  1.87813065,  1.1677002 ],
       [ 1.87813065,  1.1677002 ,  0.35023496],
       [ 1.1677002 ,  0.35023496,  0.2136686 ],
       [ 0.35023496,  0.2136686 ,  0.60613935],
       [ 0.2136686 ,  0.60613935,  3.59102217]])
</code></pre>

<hr>

<p>If you want to achieve the same effect, but <em>without</em> modifying <code>foo</code>, then you can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_along_axis.html"" rel=""nofollow""><code>np.apply_along_axis</code></a>, which should do exactly what you need:</p>

<pre><code>&gt;&gt;&gt; np.apply_along_axis(foo, -1, XY)
array([[ 1.        ,  1.87813065,  1.1677002 ],
       [ 1.87813065,  1.1677002 ,  0.35023496],
       [ 1.1677002 ,  0.35023496,  0.2136686 ],
       [ 0.35023496,  0.2136686 ,  0.60613935],
       [ 0.2136686 ,  0.60613935,  3.59102217]])
</code></pre>
"
39714563,5771269.0,2016-09-27T01:01:58Z,39704084,1,"<p>I believe this solution correctly implements both of the criteria ""a list of strings or a list of lists of strings"" and ""return the same list"" without any external assistance:</p>

<pre><code>def noVowels(sequence, index=0):
    if not (sequence and type(sequence) is list and index &lt; len(sequence)):
        return 

    vowels = {'a','i','e','o','u'}

    if type(sequence[index]) is list:
        noVowels(sequence[index])
    elif sequence[index] in vowels:
        del sequence[index]
        index -= 1

    noVowels(sequence, index + 1)

    return sequence
</code></pre>

<p><strong>TEST</strong></p>

<pre><code>array = [['a', 'b', 'c', 'd', 'e'], 'f', 'g', 'h', 'i', ['l', 'm', ['n', 'o', 'p'], 'q'], 'r', 's', 't', 'u']

print(array)
print(id(array))
print(id(array[0]))

result = noVowels(array)

print(result)
print(id(result))
print(id(result[0]))
</code></pre>

<p><strong>RESULT</strong></p>

<pre><code>&gt; python3 test.py
[['a', 'b', 'c', 'd', 'e'], 'f', 'g', 'h', 'i', ['l', 'm', ['n', 'o', 'p'], 'q'], 'r', 's', 't', 'u']
4315447624
4315344520
[['b', 'c', 'd'], 'f', 'g', 'h', ['l', 'm', ['n', 'p'], 'q'], 'r', 's', 't']
4315447624
4315344520
&gt;
</code></pre>

<p>Note that the same list, and inner lists, are left intact based on their id not changing.  And it handles a list of lists of lists.</p>

<p>I don't believe it's functional program because it works by side-effect but that contradiction is inherent in the OP's problem description.</p>
"
39714714,5046896.0,2016-09-27T01:29:47Z,39714682,2,"<p>In your code, I think the data type of the element in the dataframe is str, so, try <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna"" rel=""nofollow"">fillna</a>.</p>

<pre><code>In [10]: import pandas as pd
In [11]: import numpy as np
In [12]: df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'], 
                            'B': ['1', '2', '3', np.nan, np.nan]})

In [13]: df.B.fillna('')
Out[13]: 
0    1
1    2
2    3
3     
4     
Name: B, dtype: object

In [14]: df
Out[14]: 
   A    B
0  a    1
1  b    2
2  c    3
3  d  NaN
4  e  NaN

[5 rows x 2 columns]

In [15]: df.B = df.B.fillna('')

In [16]: df[""C""]=df.A+df.B

In [17]: df
Out[17]: 
   A  B   C
0  a  1  a1
1  b  2  b2
2  c  3  c3
3  d      d
4  e      e

[5 rows x 3 columns]
</code></pre>
"
39714744,5015569.0,2016-09-27T01:33:34Z,39714682,0,"<pre><code>df['C'] = pd.Series(df.fillna('').values.tolist()).str.join(' ')
</code></pre>
"
39715163,429982.0,2016-09-27T02:36:29Z,39715121,0,"<p>The issue is there is only one object <code>i</code>, and you're replacing its value every time in the loop.  By the last iteration your (one instance) of i has its value set to the last item you assigned it to.  Each time you append to <code>res</code>, you're basically appending the a pointer to the same object (so you end up with 9 pointers pointing to the same object when your loop is finished).</p>

<pre><code>res = []
seed = [1, 2, 3, 4, 5, 6, 7, 8, 9]
i = {}
for d in seed:
    print(d)
    i[""seed""] = d  # this gets replaced every time through your loop!
    res.append(i.copy())  # need to copy `i`, or else we're updating the same instance
print(res)
</code></pre>
"
39715165,5835068.0,2016-09-27T02:36:35Z,39715121,0,"<p>To answer your first question, it is because thew value of seed is being overwritten, as shown here:</p>

<pre><code>&gt;&gt;&gt; p = {'t':'H'}
&gt;&gt;&gt; p['t'] = 'k'
&gt;&gt;&gt; p
{'t': 'k'}
</code></pre>

<p>I am confused on your second part of the question. Elaborate more?</p>
"
39715172,3792041.0,2016-09-27T02:37:05Z,39715121,1,"<p>The problem is where you are defining i. I <em>think</em> you intended to have a dictionary object named i that has  single attribute named ""seed"". and you want to add those dictionaries to res.</p>

<p>In actual fact you only have one dictionary called i and you just update the value in it every time through the loop. </p>

<p>try this:</p>

<pre><code>res = []
seed = [1, 2, 3, 4, 5, 6, 7, 8, 9]

for d in seed:
    i = {}
    print(d)
    i[""seed""] = d
    res.append(i)
print(res)
</code></pre>

<p>That will create a new instance of i for each loop, and you should get the result you are looking for.</p>
"
39715176,6867263.0,2016-09-27T02:37:23Z,39715121,0,"<p>Curly braces <code>{}</code> in python represent <a href=""http://www.tutorialspoint.com/python/python_dictionary.htm"" rel=""nofollow"">dictionaries</a> which works based on a key and value. Each time you iterate through your loop you overwrite the key: 'seed' with the current value of the list seed. So by the time the loop ends the last value of the list seed is the current value of seed in the dictionary i.</p>
"
39715281,3420092.0,2016-09-27T02:52:54Z,39715227,1,"<p>for Q1 you can pack and unpack arguments:</p>

<pre><code>def foo(*args):

    result = []
    for v in args:
        result.append(v[0] + v[1])

    return result
</code></pre>

<p>This will allow you pass in as many vector arguments as you want, then iterate over them, returning a list of each result. You can also pack and unpack kwargs with **. More info here:</p>

<p><a href=""https://docs.python.org/2/tutorial/controlflow.html#unpacking-argument-lists"" rel=""nofollow"">https://docs.python.org/2/tutorial/controlflow.html#unpacking-argument-lists</a></p>
"
39715470,525169.0,2016-09-27T03:22:29Z,39715227,1,"<p>For Q1, I'm guessing you want to add the innermost dimensions of your arrays, regardless of how many dimensions the arrays have. The simplest way to do this is to use <a href=""http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing"" rel=""nofollow"">ellipsis indexing</a>. Here's a detailed example:</p>

<pre><code>&gt;&gt;&gt; a = np.arange(24).reshape((3, 4, 2))
&gt;&gt;&gt; a
array([[[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7]],

       [[ 8,  9],
        [10, 11],
        [12, 13],
        [14, 15]],

       [[16, 17],
        [18, 19],
        [20, 21],
        [22, 23]]])
&gt;&gt;&gt; a[..., 0]
array([[ 0,  2,  4,  6],
       [ 8, 10, 12, 14],
       [16, 18, 20, 22]])
&gt;&gt;&gt; a[..., 1]
array([[ 1,  3,  5,  7],
       [ 9, 11, 13, 15],
       [17, 19, 21, 23]])
&gt;&gt;&gt; a[..., 0] + a[..., 1]
array([[ 1,  5,  9, 13],
       [17, 21, 25, 29],
       [33, 37, 41, 45]])
</code></pre>

<p>This works equally well for a 1D array:</p>

<pre><code>&gt;&gt;&gt; a = np.array([1, 2])
&gt;&gt;&gt; a[..., 0] + a[..., 1]
3
</code></pre>

<p>So just define <code>foo</code> as:</p>

<pre><code>def foo(V):
    return V[..., 0] + V[..., 1]
</code></pre>

<hr>

<p>For your <code>nGauss</code> function, the simplest solution is to use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_along_axis.html"" rel=""nofollow""><code>np.apply_along_axis</code></a>. For example, you would call it like this:</p>

<pre><code>&gt;&gt;&gt; np.apply_along_axis(nGauss, -1, x1, mu, cov)
</code></pre>
"
39715487,901925.0,2016-09-27T03:24:47Z,39715227,2,"<p>Your function works with both arrays:</p>

<pre><code>In [1]: def foo(V):
   ...:     return V[0]+V[1]
   ...: 
In [2]: foo(np.array([1,3]))
Out[2]: 4
In [3]: foo(np.array([[[1,2],[3,4]], [[5,6],[7,8]]]))
Out[3]: 
array([[ 6,  8],
       [10, 12]])
</code></pre>

<p>This answer is just the sum of these two arrays:</p>

<pre><code>In [4]: np.array([[[1,2],[3,4]], [[5,6],[7,8]]])[0]
Out[4]: 
array([[1, 2],
       [3, 4]])
In [5]: np.array([[[1,2],[3,4]], [[5,6],[7,8]]])[1]
Out[5]: 
array([[5, 6],
       [7, 8]])
</code></pre>

<p>If you expected something else, you'll have to show us.</p>

<p>As for your second question:</p>

<pre><code>In [6]: t1=np.array([[1,2,3], [4,5,6]])
   ...: t2=np.array([1,2,3])
   ...: t3=np.array([[1,2,3], [4,5,6],5])
   ...: 
In [7]: t1.shape
Out[7]: (2, 3)
In [8]: t2.shape
Out[8]: (3,)
In [9]: t3.shape
Out[9]: (3,)
</code></pre>

<p><code>(3,)</code> is a 1 element tuple. Compare these expressions.</p>

<pre><code>In [11]: (3)
Out[11]: 3
In [12]: (3,)
Out[12]: (3,)
</code></pre>

<p>There have been several recent questions about (3,) v (3,1) shape arrays, and <code>np.array([[1,2,3]])</code> v. <code>np.array([1,2,3])</code>.</p>

<p><code>t3</code> is an object dtype array, with 3 elements.  The 3 inputs are different length, so it can't create a 2d array.  Stay away from this type of array for now.  Focus on the simpler arrays.</p>

<pre><code>In [10]: t3
Out[10]: array([[1, 2, 3], [4, 5, 6], 5], dtype=object)
In [13]: t3[0]
Out[13]: [1, 2, 3]
In [14]: t3[2]
Out[14]: 5
</code></pre>

<p><a href=""http://stackoverflow.com/questions/39706277/numpy-why-is-difference-of-a-2-1-array-and-a-vertical-matrix-slice-not-a-2-1"">Numpy: Why is difference of a (2,1) array and a vertical matrix slice not a (2,1) array</a></p>

<p><a href=""http://stackoverflow.com/questions/39694318/difference-between-single-and-double-bracket-numpy-array"">Difference between single and double bracket Numpy array?</a></p>

<p>=====================</p>

<p>With the <code>nGauss</code>:</p>

<pre><code>In [53]: mu=np.array([0,0])
In [54]: cov=np.eye(2)
In [55]: xx=np.array([[[1,2], [5,6]], [[7,8],[9,0]]])
In [56]: np.apply_along_axis(nGauss, -1, xx, mu, cov)
Out[56]: 
array([[ -1.30642333e-02,  -9.03313360e-15],
       [ -4.61510838e-26,  -4.10103631e-19]])
</code></pre>

<p><code>apply_along_axis</code> iterates on the 1st 2 dim, passing each <code>xx[i,j,:]</code> to <code>nGauss</code>.  It's not fast, but is relatively easy to apply.</p>

<pre><code>k = X.shape[0];  # I assume you want
k = X.shape[[1]   # the last dimension
dev = X-mu     # works as long as mu has k terms
</code></pre>

<p>this is a scalar:</p>

<pre><code>p1 = np.power( np.power(np.pi * 2, k) , -0.5);
</code></pre>

<p>so is</p>

<pre><code>p2 = np.power( np.linalg.det(cov)  , -0.5)
</code></pre>

<p>So it comes down to generalizing this expression:</p>

<pre><code>p3 = np.exp( -0.5 * np.dot( np.dot(dev.transpose(), np.linalg.inv(cov)), dev));
</code></pre>

<p>In the simple (2,) <code>x</code> case, <code>dev</code> is 1d, and <code>dev.transpose()</code> does nothing.</p>

<p>It's easier to generalize <code>einsum</code> than <code>dot</code>; I think the equivalent is:</p>

<pre><code>p3 = np.einsum('j,j', np.einsum('i,ij', dev, np.linalg.inv(cov)), dev)
p3 = np.exp( -0.5 * p3)
</code></pre>

<p>which simplifies to</p>

<pre><code>p3 = np.einsum('i,ij,j', dev, np.linalg.inv(cov), dev)
</code></pre>

<p>generalizing to higher dim:</p>

<pre><code>p3 = np.einsum('...i,ij,...j', dev, np.linalg.inv(cov), dev)
</code></pre>

<p>So with:</p>

<pre><code>def nGaussA(X, mu, cov):
    # multivariate negative gaussian.    
    # mu is a vector and cov is a covariance matrix.

    k = X.shape[-1];
    dev = X-mu
    p1 = np.power( np.power(np.pi * 2, k) , -0.5);
    p2 = np.power( np.linalg.det(cov)  , -0.5)
    p3 = np.einsum('...i,ij,...j', dev, np.linalg.inv(cov), dev)
    p3 = np.exp( -0.5 * p3)
    return -1.0 * p1 * p2 * p3;
</code></pre>

<p>matching earlier values:</p>

<pre><code>In [85]: nGaussA(x,mu,cov)
Out[85]: -0.013064233284684921
In [86]: nGaussA(xx,mu,cov)
Out[86]: 
array([[ -1.30642333e-02,  -9.03313360e-15],
       [ -4.61510838e-26,  -4.10103631e-19]])
</code></pre>

<p>So the way to generalize the function is to check each step.  If it produces a scalar, keep it.  If operates with an <code>x</code> keep it.  But if it requires coordinating dimensions with other arrays, use a numpy operation that does that.  Often that involves broadcasting.  Sometimes it helps to study other numpy functions to see how they generalize (e.g. <code>apply_along_axis</code>, <code>apply_over_axes</code>, <code>cross</code>, etc).</p>

<p>An interactive numpy session is essential; allowing me to try ideas with small sample arrays.</p>
"
39715490,771848.0,2016-09-27T03:25:40Z,39715429,3,"<p>Both are not valid XPath expressions, you need to add the tag names after the <code>//</code>. You can also use a wildcard <code>*</code>:</p>

<pre><code>snode_attractions = sel.xpath('//*[contains(@class, ""attraction_element"")]')
</code></pre>

<p>Note that aside from that you second XPath expression that is used in a loop has to be context specific and start with a dot:</p>

<pre><code># Build item index
for snode_attraction in snode_attractions:
    print clean_parsed_string(get_parsed_string(snode_attraction, './/*[@class=""property_title""]/a/@href'))
</code></pre>

<p>Also note that you don't need to instantiate a <code>Selector</code> object and ca use <code>response.xpath()</code> shortcut directly.</p>

<hr>

<p>Note that a more concise and, arguably, more readable version of the same logic implementation would be to use <em>CSS selectors</em>:</p>

<pre><code>snode_attractions = response.css('.attraction_element')
for snode_attraction in snode_attractions:
    print snode_attraction.css('.property_title &gt; a::attr(""href"")').extract_first()
</code></pre>
"
39715773,2823755.0,2016-09-27T04:04:03Z,39711838,0,"<p>Convert to an array or dense matrix, sum the absolute value along the first axis, test the result against zero, convert to int</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; (np.sum(np.absolute(a.toarray()), 0) == 0) * 1
array([0, 0, 0, 1])
&gt;&gt;&gt; (np.sum(np.absolute(a.todense()), 0) == 0) * 1
matrix([[0, 0, 0, 1]])
&gt;&gt;&gt; 
&gt;&gt;&gt; np.asarray((np.sum(np.absolute(a.todense()), 0) == 0), dtype = np.int32)
array([[0, 0, 0, 1]])
&gt;&gt;&gt;
</code></pre>

<hr>

<p>The first is the fastest - 24 uS for your example on my machine.</p>

<p>For a matrix made with <code>np.random.randint(0,3,(1000,1000))</code>, all are right at 25 mS on my machine.</p>
"
39716069,2336654.0,2016-09-27T04:38:45Z,39715950,3,"<p>Use <code>reindex</code></p>

<pre><code>df.groupby('A').B.sum().reindex(list('abcdef'), fill_value=0)

A
a    3
b    0
c    7
d    0
e    5
f    0
Name: B, dtype: int64
</code></pre>
"
39716240,2336654.0,2016-09-27T04:55:41Z,39716210,1,"<p>I'm not sure why the <code>s.drop('B')</code> doesn't work but using the <code>level=0</code> parameter does.</p>

<pre><code>s.drop('B', level=0)

A  0    1
   1    1
   2    1
   2    0
C  0    1
   1    1
   2    1
dtype: int64
</code></pre>
"
39716307,5615811.0,2016-09-27T05:01:23Z,39694646,0,"<p>I built Boost.Python libraries 1.61.0 from source for Python 2.7 using VC 14.0. Then used those in the build process for Netgen (again using VC 14.0) and pointed to the Python 2.7 library and include directories (as opposed to Python 3.5). This has thus far worked in Python 2.7 with my existing code.</p>
"
39716325,3405554.0,2016-09-27T05:02:40Z,39716271,2,"<p>Try :</p>

<pre><code>with open('Log-%s.csv' % filename1, 'a') as log:
</code></pre>
"
39716356,5247911.0,2016-09-27T05:05:32Z,39716271,1,"<p>In <strong>Case 2</strong>, every time <code>write_temp</code> is called, it is populating <code>filename1</code> with timestamp.</p>

<p>So consider,for example, you called it at <code>10:15:13 (hh:mm:ss)</code>, then <code>filename1</code> will be <code>10-15-13.csv</code>. When you will call it again at <code>10:15:14</code> then <code>filename1</code> will be <code>10-15-14.csv</code>.</p>

<p>That's why new file is getting created.</p>

<p><strong>Solution :</strong> Take out <code>filename1</code> from <code>temp_write</code> and pass filename to that function as argument.</p>

<pre><code>from datetime import *
import sys

def write_temp(temperature,file_name):

        print (""In write_temp function - ""+file_name)

        with open(file_name, 'a') as log:
                log.write(""{0},{1}\n"".format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),str(temperature)))


arg = sys.argv[1]
filename1 = str(arg) + ""-"" + datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")+"".csv""
print (""File name is ""+filename1)
write_temp(1,filename1)
</code></pre>

<p><strong>Output on console:</strong></p>

<pre><code>C:\Users\dinesh_pundkar\Desktop&gt;python c.py LOG
File name is LOG-2016-09-27-11-03-16.csv
In write_temp function - LOG-2016-09-27-11-03-16.csv

C:\Users\dinesh_pundkar\Desktop&gt;
</code></pre>

<p><strong>Output of LOG-TimeStamp.csv:</strong></p>

<pre><code>2016-09-27 10:47:06,1
2016-09-27 10:47:06,3
</code></pre>
"
39716580,839957.0,2016-09-27T05:24:51Z,39715910,1,"<p>The classic way would be to use your conditions as indexers:</p>

<pre><code>df1 = pd.DataFrame({'fruit_name':['banana', 'lemon']})
df2 = pd.DataFrame({'fruit_name':['strawberry', 'apple']})
df3 = pd.DataFrame({'fruit_name':['lemon', 'rockmelon', 'apple']})

df3[""color""] = ""unknown""
df3[""color""][df3['fruit_name'].isin(df1['fruit_name'])] = ""yellow""
df3[""color""][df3['fruit_name'].isin(df2['fruit_name'])] = ""red""
df3

#   fruit_name    color
# 0      lemon   yellow
# 1  rockmelon  unknown
# 2      apple      red
</code></pre>

<p>A more functional way would be to write your logic as a function and map it along your series, however this is likely to be quite a bit slower, since a lot of the speed of pandas/numpy comes from using vectorized operations:</p>

<pre><code>def get_fruit_color(x):
    if x in df1['fruit_name'].unique():
        data = ""yellow""
    elif x in df2['fruit_name'].unique():
        data = ""red""
    else:
        data = ""unknown""

    return data

df3[""color""] = df3[""fruit_name""].map(get_fruit_color)
</code></pre>

<p>An SQL-inspired approach would be to store your mappings in a dataframe, and do a join (called a merge in pandas); this should be a very performant option. Specifying <code>how='left'</code> means that it will be a left join, so that if no match is found for the join condition, the row will still remain, with a null value:</p>

<pre><code>colors = ([(x, 'yellow') for x in df1['fruit_name'].unique()] 
           + [(x, 'red') for x in df2['fruit_name'].unique()])
colors_df = pd.DataFrame(colors, columns = ['fruit_name', 'color'])
df3.merge(colors_df, how='left').fillna(""unknown"")
</code></pre>

<p>Finally my favourite method (although maybe it'a a little ""clever"") would be to use a dict to map your values (this is a special pandas trick), this will leave <code>NaN</code> if no match is found, so you can fill these with <code>fillna</code>:</p>

<pre><code>df3[""color""] = df3[""fruit_name""].map(dict(colors)).fillna(""unknown"")
</code></pre>
"
39716593,5771269.0,2016-09-27T05:25:47Z,39715354,0,"<p>One approach would be to calculate a bounding rectangle (or circle) for each snowflake.  Save these as a list or a set.  Whenever you plan to make a new snowflake, first check if its bounding rectangle (or circle) overlaps with the bounds of any previous snowflakes.  If it does, don't draw it.  If it doesn't, draw it and save its bounds too.  An incomplete outline of this approach:</p>

<pre><code>import turtle
import random

def snowflakebranch(n):

    turtle.forward(n * 4)

    for _ in range(3):
        turtle.backward(n)
        turtle.right(45)
        turtle.forward(n)
        turtle.backward(n)
        turtle.left(90)
        turtle.forward(n)
        turtle.backward(n)
        turtle.right(45)

def snowflake(n):

    for _ in range(8):
        snowflakebranch(n)
        turtle.backward(n)
        turtle.right(45)

def overlapping(bounds_list, bounds):
    for previous in bounds_list:
        if overlap(previous, bounds):
            return True

    return False

def overlap(b1, b2):
    # return True or False if these two rectanges or circles overlap
    pass

turtle.penup()

turtle.colormode(255)

turtle.tracer(0)

previous_bounds = []

i = 0

while i &lt; 35:

    x = random.randint(-500, 500)
    y = random.randint(-500, 500)
    turtle.goto(x, y)

    r = random.randint(0, 255)
    g = random.randint(0, 255)
    b = random.randint(0, 255)
    turtle.color(r, g, b)

    turtle.pendown()

    d = random.randint(6, 16)

    # work out the bounding rectangle or circle based on 'd', 'x' &amp; 'y'
    # e.g. (x, y, width &amp; height) or (x, y, radius)
    bounds = ( ... )  

    if not overlapping(previous_bounds, bounds):

        snowflake(d)

        turtle.update()

        previous_bounds.append(bounds)

        i += 1

    turtle.penup()

turtle.done()
</code></pre>

<p>An image of non-overlapping snowflakes using the above logic with the bounding circles also displayed:</p>

<p><a href=""http://i.stack.imgur.com/CuJMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CuJMQ.png"" alt=""enter image description here""></a></p>

<p>I actually like the look of your overlapping snowflakes.  Even if you want overlap, the above logic will allow you to control how much overlap.</p>
"
39716676,4855984.0,2016-09-27T05:33:00Z,39714374,0,"<p>1e^-3 is still fairly high, for the classifier you've described. NaN actually means that the weights have tended to infinity, so I would suggest exploring even lower learning rates, around 1e^-7 specifically. If it continues to diverge, multiply your learning rate by 0.1, and repeat until the weights are finite-valued.</p>
"
39716772,3510736.0,2016-09-27T05:40:05Z,39716492,8,"<p>I would suggest instead of a <code>list</code> of <code>list</code>s, using a <a href=""https://docs.python.org/2/library/collections.html#collections.defaultdict""><code>collections.defaultdict(set)</code></a>. </p>

<p>Say you start with</p>

<pre><code>uniques = collections.defaultdict(set)
</code></pre>

<p>Now the loop can become something like this:</p>

<pre><code>for chunk in data: 
    for col in chunk:
        uniques[col] = uniques[col].union(chunk[col].unique())
</code></pre>

<p>Note that:</p>

<ol>
<li><p><code>defaultdict</code> always has a <code>set</code> for <code>uniques[col]</code> (that's what it's there for), so you can skip <code>initialized</code> and stuff.</p></li>
<li><p>For a given <code>col</code>, you simply update the entry with the union of the current set (which initially is empty, but it doesn't matter) and the new unique elements.</p></li>
</ol>

<p><strong>Edit</strong></p>

<p>As Raymond Hettinger notes (thanks!), it is better to use</p>

<pre><code>       uniques[col].update(chunk[col].unique())
</code></pre>
"
39717262,1563960.0,2016-09-27T06:14:22Z,39717071,2,"<p>the easiest way is to only use double-quotes in your python code, then, in your bash script, wrap all of your python code in one pair of single-quotes, e.g.,</p>

<pre><code>#!/bin/bash

python -c 'import os
import sys

#create number of bytes as specified in the args:
if len(sys.argv) != 3:
    print(""We need a correct number of args : 2 [NUM_BYTES][FILE_NAME]."")
    exit(1)

n = -1
try:
    n = int(sys.argv[1])
except:
    print(""Error casting number : "" + sys.argv[1])
    exit(1)

rand_string = os.urandom(n)

# i changed """"s to ''s below -webb
with open(sys.argv[2], ""wb+"") as f:
    f.write(rand_string)
    f.flush()
    f.close()'
</code></pre>
"
39717446,2901002.0,2016-09-27T06:26:06Z,39717407,3,"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""nofollow""><code>loc</code></a>:</p>

<pre><code>df = pd.DataFrame()

df.loc[0,1] = 10
df.loc[2,8] = 100
print(df)
      1      8
0  10.0    NaN
2   NaN  100.0
</code></pre>

<p>Faster solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""nofollow""><code>DataFrame.set_value</code></a>:</p>

<pre><code>df = pd.DataFrame()
indices = ['A', 'B', 'C']
columns = ['C1', 'C2', 'C3']
for column in columns:
    for index in indices:
       df.set_value(index, column, 1)

print(df)
    C1   C2   C3
A  1.0  1.0  1.0
B  1.0  1.0  1.0
C  1.0  1.0  1.0
</code></pre>
"
39717498,2336654.0,2016-09-27T06:28:49Z,39717407,3,"<p><code>loc</code> works very well, but...<br>
For single assignments use <code>at</code></p>

<pre><code>df = pd.DataFrame()
indices = ['A', 'B', 'C']
columns = ['C1', 'C2', 'C3']
for column in columns:
    for index in indices:
        df.at[index, column] = 1

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/MHVHZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MHVHZ.png"" alt=""enter image description here""></a></p>

<hr>

<h1><code>.at</code> vs <code>.loc</code> vs <code>.set_value</code> timing</h1>

<p><a href=""http://i.stack.imgur.com/wuE0T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wuE0T.png"" alt=""enter image description here""></a></p>
"
39717961,6753099.0,2016-09-27T06:55:31Z,39706242,0,"<p>If the numbers on the vertical axis are the number of observations for the respective class, then the sample size is about 1500. For such a large sample size goodness-of-fit tests are rarely useful. But is it really necessary that your data is perfectly normally distributed? If you want to analyze the data with a statistical method, is this method maybe robust under (""small"") deviations from the normal distribution assumption? 
In practice the question is usually ""Is the normal distribution assumption acceptable"" for my statistical analysis. A perfect normal distribution is very rarly available. 
An additional comment on histograms: One has to be careful by interpreting data from histograms because if the data ""looks normal"" or not may depend on the width of the histogram classes. Histograms are only hints which should be treated with caution.</p>
"
39718151,2336654.0,2016-09-27T07:05:18Z,39717809,4,"<p>Here's a goofy way to do it</p>

<pre><code>cond1 = df.A.gt(1) &amp; df.B.gt(1)
cond2 = df.A.eq(1)
cond3 = df.A.eq(2) &amp; df.C.ne(0)

df['D'] = cond3.map({True: [2, 0]}) \
  .combine_first(cond2.map({True: [0, 2]})) \
  .combine_first(cond1.map({True: [1, 0]})) \

df
</code></pre>

<p><a href=""http://i.stack.imgur.com/f5SLl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/f5SLl.png"" alt=""enter image description here""></a></p>
"
39718403,5317403.0,2016-09-27T07:17:07Z,39683274,0,"<p>Try redefine your <code>Demo1.new_window()</code> as below:</p>

<pre><code>def new_window(self):
    self.master.destroy() # close the current window
    self.master = tk.Tk() # create another Tk instance
    self.app = Demo2(self.master) # create Demo2 window
    self.master.mainloop()
</code></pre>
"
39718863,6633975.0,2016-09-27T07:40:40Z,39718576,0,"<p>To solve IndexError you can use  <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndindex.html#numpy.ndindex"" rel=""nofollow""><code>numpy.ndindex</code></a>:</p>

<pre><code>import numpy as np

mem = b'\x01\x02\xff' #define my input
mem = np.fromstring(mem, dtype=np.uint8) #first convert to int

#print(mem) give me ""[  1   2 255]"" at this piont
mem=np.array(['{0:07b}'.format(mem[b]) for b in np.ndindex(mem.shape)])

data= np.array([list(mem[b]) for b in np.ndindex(mem.shape)]) #finally convert to single bits

print(data)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>[['0', '0', '0', '0', '0', '0', '1'] ['0', '0', '0', '0', '0', '1', '0']
 ['1', '1', '1', '1', '1', '1', '1', '1']]
</code></pre>
"
39718913,3030305.0,2016-09-27T07:43:23Z,39718576,2,"<p>Using unpackbits:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; mem = b'\x01\x02\xff'
&gt;&gt;&gt; x = np.fromstring(mem, dtype=np.uint8)
&gt;&gt;&gt; np.unpackbits(x).reshape(3,8)
array([[0, 0, 0, 0, 0, 0, 0, 1],
       [0, 0, 0, 0, 0, 0, 1, 0],
       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=uint8)
</code></pre>

<h3>Documentation</h3>

<p>From <code>help(np.unpackbits)</code>:</p>

<blockquote>
  <p><strong>unpackbits</strong>(...)<br>
  unpackbits(myarray, axis=None)</p>
  
  <p>Unpacks elements of a uint8 array into a binary-valued output array.</p>
  
  <p>Each element of <code>myarray</code> represents a bit-field that should be unpacked
  into a binary-valued output array. The shape of the output array is either
  1-D (if <code>axis</code> is None) or the same shape as the input array with unpacking
  done along the axis specified.</p>
</blockquote>
"
39718934,5076688.0,2016-09-27T07:44:37Z,39718576,0,"<p>I'm fairly certain the problem with your code is that you're assuming the <code>int</code> in each item in the list will become 8 bits (so <code>2</code> will, in your assumption, return <code>00000010</code>). But it doesn't (<code>2</code> = <code>10</code>), and that screws up your code.</p>

<p>For your last two lines, I think this should be fine:</p>

<pre class=""lang-python prettyprint-override""><code>data = [list(str(bin(x))[2:]) for x in mem]
for a in range(len(data)):
    while len(data[a]) &lt; 8:
        data[a] = ""0"" + data[a]
</code></pre>

<p><code>str(bin(x))[2:]</code> converts to binary (because it returns <code>0b1</code> for <code>1</code>, you need to use <code>[2:]</code> to get <code>1</code>).</p>

<p>The last chunk of code is to ""pad"" out your numbers with extra <code>0</code>'s.</p>
"
39719187,2336654.0,2016-09-27T07:57:29Z,39714682,1,"<p>You can use <code>add</code> method with the <code>fill_value</code> parameter</p>

<pre><code>df['C'] = df.A.add(df.B, fill_value='')
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/JLttG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JLttG.png"" alt=""enter image description here""></a></p>
"
39719208,3293881.0,2016-09-27T07:58:24Z,39719140,2,"<p>As one approach we can get the non-zero indices and get the mean of those as the center of mass, like so -</p>

<pre><code>np.flatnonzero(x).mean()
</code></pre>

<p>Here's another approach using shifted array comparison to get the start and stop indices of that slice and getting the mean of those indices for determining the center of mass, like so -</p>

<pre><code>np.flatnonzero(x[:-1] != x[1:]).mean()+0.5
</code></pre>

<p>Runtime test -</p>

<pre><code>In [72]: x = np.zeros(10000,dtype=int)

In [73]: x[100:2000] = 1

In [74]: %timeit np.flatnonzero(x).mean()
10000 loops, best of 3: 115 Âµs per loop

In [75]: %timeit np.flatnonzero(x[:-1] != x[1:]).mean()+0.5
10000 loops, best of 3: 38.7 Âµs per loop
</code></pre>

<p>We can improve the performance by some margin here with the use of <code>np.nonzero()[0]</code> to replace <code>np.flatnonzero</code> and <code>np.sum</code> in place of <code>np.mean</code> -</p>

<pre><code>In [107]: %timeit (np.nonzero(x[:-1] != x[1:])[0].sum()+1)/2.0
10000 loops, best of 3: 30.6 Âµs per loop
</code></pre>

<p>Alternatively, for the second approach, we can store the start and stop indices and then simply add them to get the center of mass for a bit more efficient approach as we would avoid the function call to <code>np.mean</code>, like so -</p>

<pre><code>start,stop = np.flatnonzero(x[:-1] != x[1:])
out = (stop + start + 1)/2.0
</code></pre>

<p>Timings -</p>

<pre><code>In [90]: %timeit start,stop = np.flatnonzero(x[:-1] != x[1:])
10000 loops, best of 3: 21.3 Âµs per loop

In [91]: %timeit (stop + start + 1)/2.0
100000 loops, best of 3: 4.45 Âµs per loop
</code></pre>

<p>Again, we can experiment with <code>np.nonzero()[0]</code> here.</p>
"
39719532,6193054.0,2016-09-27T08:15:49Z,39718576,0,"<pre><code>mem = b'\x01\x02\xff'
[[int(digit) for digit in ""{0:08b}"".format(byte)] for byte in mem]
</code></pre>

<p>outputs:</p>

<pre><code>[[0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
</code></pre>
"
39719579,2184364.0,2016-09-27T08:18:21Z,39719140,2,"<p>Can't you simply do the following?</p>

<pre><code>center_of_mass = (x*np.arange(len(x))).sum()/x.sum() # 5

%timeit center_of_mass = (x*arange(len(x))).sum()/x.sum()
# 100000 loops, best of 3: 10.4 Âµs per loop
</code></pre>
"
39719580,2901002.0,2016-09-27T08:18:27Z,39717809,3,"<p>Another solution is create <code>Series</code> filled by <code>list</code> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html"" rel=""nofollow""><code>shape</code></a> for generating <code>length</code> of <code>df</code>:</p>

<pre><code>df.loc[(df['A'] &gt; 1) &amp; (df['B'] &gt; 1), ""D""] = pd.Series([[1,0]]*df.shape[0])
df.loc[(df['A'] == 1) , ""D""] = pd.Series([[0,2]]*df.shape[0])
df.loc[(df['A'] == 2) &amp; (df['C'] != 0) , ""D""] = pd.Series([[2,0]]*df.shape[0])
print (df)
   A  B  C       D
0  3  2  1  [1, 0]
1  4  2  3  [1, 0]
2  1  4  1  [0, 2]
3  2  2  3  [2, 0]
</code></pre>
"
39719772,1881878.0,2016-09-27T08:29:19Z,39620547,4,"<p>I have tried your code and it seems to me that your network predicts both classes: </p>

<pre><code>import numpy as np
from sklearn.cross_validation import StratifiedKFold
from neupy.algorithms import PNN

fileName=""dataset_file.csv""
TARGET_COLUMN=35

from numpy import genfromtxt
input_dataset_data = genfromtxt(fileName, delimiter=',', skip_header=0, usecols=(range(0, TARGET_COLUMN-1)))
input_dataset_target = genfromtxt(fileName, delimiter=',', skip_header=0, usecols=(TARGET_COLUMN-1))

kfold_number = 5
skfold = StratifiedKFold(input_dataset_target, kfold_number, shuffle=True)

print(""&gt; Start classify input_dataset dataset"")
for std in [0.2, 0.4,  0.6,  0.8, 1]:
    average_results = []
    for i, (train, test) in enumerate(skfold, start=1):
            pnn_network = PNN(std=std, step=0.2, verbose=False, batch_size=2)
            pnn_network.train(input_dataset_data[train], input_dataset_target[train])
            predictions = pnn_network.predict(input_dataset_data[test])
            print(""Positive in predictions:"", 1 in predictions)
            average_result.append(np.sum(predictions == input_dataset_target[test]) /float(len(predictions)))
    print std, np.average(average_result)
</code></pre>

<p>An example output while tuning std:</p>

<pre><code>1 0.881558441558
('Positive in predictions:', True)
('Positive in predictions:', True)
('Positive in predictions:', True)
('Positive in predictions:', True)
('Positive in predictions:', True)
</code></pre>

<p>That is for <code>std=1</code> the average accuracy in the folds is ~0.88 and 1 is predicted in each of the rounds of stratified cross-validation. I have used the normalized data of your edit.</p>
"
39720329,6661895.0,2016-09-27T08:56:39Z,39717809,0,"<p><strong>Disclaimer</strong>: This is my own question.</p>

<p>Both the answers provided by <a href=""http://stackoverflow.com/users/2901002/jezrael"">jezrael</a> and <a href=""http://stackoverflow.com/users/2336654/pirsquared"">piRSquared</a> work.</p>

<p>I just wanted to add another way of doing it, albeit slightly different from the requirement I posted in the question. Instead of trying to insert a <code>list</code>, you can convert the <code>list</code> into a <code>string</code> and later access it by typecasting. </p>

<pre><code>df.loc[(df['A'] &gt; 1) &amp; (df['B'] &gt; 1), ""D""] = '[1,0]'
df.loc[(df['A'] == 1) , ""D""] = '[0,2]'
df.loc[(df['A'] == 2) &amp; (df['C'] != 0) , ""D""] = '[2,0]'
</code></pre>

<p>This may not be applicable to everyone's use, but I can definitely think of situations where this would suffice.</p>
"
39720343,3074293.0,2016-09-27T08:57:17Z,39712435,3,"<p>This seems to be from the way that pandas is handling nans. When I set <code>skipna=False</code> in the <code>sum</code> method I get the <code>numpy</code> datatype</p>

<pre><code>import pandas as pd
import numpy as np

type(pd.DataFrame({'col1':[.1,.2,.3,.4]}).col1.sum(skipna=True))
#float

type(pd.DataFrame({'col1':[.1,.2,.3,.4]}).col1.sum(skipna=False))
#numpy.float64
</code></pre>

<p>The <code>sum</code> method is calling <code>nansum</code> from <code>pandas/core/nanops.py</code>, which produces the same behaviours. </p>

<pre><code>from pandas.core.nanops import nansum

type(sum(np.arange(10.0)))
# numpy.float64

type(nansum(np.arange(10.0)))
# float
</code></pre>

<p>Why <code>nansum</code> is converting from <code>numpy.float64</code> to <code>float</code>, I couldn't tell you. I've looked at the <code>nansum</code> source code, but none of the functions it itself calls seem to be producing that change.</p>
"
39720615,120163.0,2016-09-27T09:09:59Z,39719729,1,"<p>See my <a href=""http://stackoverflow.com/a/9989663/120163"">SO answer on how to build a control flow graph, using an AST</a>.</p>

<p>The original question asked about CFGs for Java, but the approach is actually pretty generic, and the same approach would work for producing a CFG for Python.</p>

<p>I wouldn't have called this ""quite complex""; the basic idea is pretty simple.</p>
"
39721631,2615075.0,2016-09-27T09:56:10Z,39719567,7,"<p>You can't do that through any API.</p>

<p>Transactions can't be nested while retaining all ACID properties, and not all databases support nested transactions.</p>

<p>Only the outermost atomic block creates a transaction. Inner atomic blocks create a savepoint inside the transaction, and release or roll back the savepoint when exiting the inner block. As such, inner atomic blocks provide atomicity, but as you noted, not e.g. durability.</p>

<p>Since the outermost atomic block creates a transaction, it <em>must</em> provide atomicity, and you can't commit a nested atomic block to the database if the containing transaction is not committed.</p>

<p>The only way to ensure that the inner block is committed, is to make sure that the code in the transaction finishes executing without any errors. </p>
"
39722445,2901002.0,2016-09-27T10:36:21Z,39721800,2,"<p>I think you can use <a href=""http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.from_pandas"" rel=""nofollow""><code>dask.dataframe.from_pandas</code></a>:</p>

<pre><code>from dask import dataframe as dd 
sd = dd.from_pandas(df, npartitions=3)
print (sd)
dd.DataFrame&lt;from_pa..., npartitions=2, divisions=(0, 1, 2)&gt;
</code></pre>

<p>EDIT:</p>

<p>I find <a href=""https://github.com/dask/dask/blob/57285aa1cd721f0845b4f2b3756cb82d5f4597cf/dask/dataframe/tests/test_categorical.py#L14"" rel=""nofollow"">solution</a>:</p>

<pre><code>import pandas as pd
import dask.dataframe as dd
from dask.dataframe.utils import make_meta

df=pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})

dsk = {('x', 0): df}

meta = make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'))
d = dd.DataFrame(dsk, name='x', meta=meta, divisions=[0, 1, 2])
print (d)
dd.DataFrame&lt;x, npartitions=2, divisions=(0, 1, 2)&gt;
</code></pre>
"
39723167,704848.0,2016-09-27T11:11:20Z,39723061,2,"<p>Assuming this is <code>pandas</code> you may need to convert the month column to dtype <code>datetime</code> and then you can use <code>.dt</code> accessor for the year and month attributes:</p>

<pre><code>In [33]:
df['month'] = pd.to_datetime(df['month'])
df.info()

&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 5 entries, 0 to 4
Data columns (total 2 columns):
month    5 non-null datetime64[ns]
items    5 non-null int64
dtypes: datetime64[ns](1), int64(1)
memory usage: 120.0 bytes

In [35]:
years = df['month'].dt.year.tolist()
months = df['month'].dt.month.tolist()
print(years)
print(months)

[1962, 1962, 1962, 1962, 1962]
[1, 2, 3, 4, 5]
</code></pre>
"
39724505,3520792.0,2016-09-27T12:19:44Z,39723925,0,"<p>Just use csv library in python, 
import it and use .</p>

<pre><code>import csv

file_obj = #your_file_object_read_mode
rows = file_obj.readlines()

for raw in csv.DictReader(rows, delimiter="",""):
    print(raw) # the raw will be a dictionary and you can use it well for any need. 
</code></pre>

<p>each raw will look like ie, </p>

<pre><code>{'number3': '88', 'number2': '22', 'name': 'vipul', 'number1': '23'}
</code></pre>

<p>This solves your problem I think, Just give it a try. </p>
"
39724573,2901002.0,2016-09-27T12:22:24Z,39723925,3,"<p>You need <code>quoting=csv.QUOTE_NONE</code> because there are <code>quoting</code> in <code>file</code>:</p>

<pre><code>df = pd.read_csv('TAT_AX1_westbound_style3.csv', quoting=csv.QUOTE_NONE)
print (df)
                    To          New York   Norfolk   Charleston  Savannah 
0       Le Havre (Fri)                 15        18           22       24 
1    ""Rotterdam (Sun)     """"""""         13        16           20      22 ""
2      ""Hamburg (Thu)     """"""""         11        14           18      20 ""
3  ""Southampton (Fri)    """"""""           8        11           15      17 ""
</code></pre>



<pre><code>#remove first column 
df = df.drop(df.columns[0], axis=1)
#remove all "" values to empty string, convert to int
df = df.replace({'""':''}, regex=True).astype(int)
print (df)
                     New York   Norfolk   Charleston   Savannah 
To                                                              
Le Havre (Fri)              15        18           22         24
""Rotterdam (Sun)            13        16           20         22
""Hamburg (Thu)              11        14           18         20
""Southampton (Fri)           8        11           15         17     15       17
</code></pre>
"
39724665,790387.0,2016-09-27T12:26:03Z,39723925,0,"<p>From your sample you have provided, it is clear that the problem is with the data set and pandas is working correctly.</p>

<p>Only the first row is separated correctly, the second row is all in one column; as a single string (pay attention to the <code>""</code>). If I replace the <code>,</code> with <code>|</code>, your problem becomes a bit clearer:</p>

<pre><code>To                                        | |New York |Norfolk |Charleston |Savannah 
Le Havre (Fri)                            | |15       |18      |22         |24 
""Rotterdam (Sun) ,"""""""",13 ,16 ,20 ,22 ""   | 
""Hamburg (Thu) ,"""""""",11 ,14 ,18 ,20 ""     |
""Southampton (Fri) , """""""" ,8 ,11 ,15 ,17 ""|
</code></pre>

<p>Now you have to manually split the second row in order to create the correct data set.</p>

<pre><code>&gt;&gt;&gt; with open('sample2.txt') as f:
...    headers = next(f).split(',')
...    rows = [i.split(',') for i in f]
...
&gt;&gt;&gt; rows = [list(map(str.strip, list(map(lambda x: x.replace('""', ''), i)))) for i in rows]
&gt;&gt;&gt; pd.DataFrame(rows, columns=headers)
                  To   New York Norfolk Charleston Savannah
0     Le Havre (Fri)         15      18         22       24
1    Rotterdam (Sun)         13      16         20       22
2      Hamburg (Thu)         11      14         18       20
3  Southampton (Fri)          8      11         15       17
</code></pre>
"
39724806,1062499.0,2016-09-27T12:33:26Z,39723437,1,"<p>I would guess that, for some reason (related to pipes vs. ttys, see <a href=""http://stackoverflow.com/questions/107705/disable-output-buffering/107717#comment24604506_107717"">this comment</a>), the output of the <code>inner.py</code> Python process is unbuffered the first time you call it, and buffered the second time you call it.  The first time, with unbuffered output, you get the result in the expected order written to your tty.  The second time, with buffering, the output from the <code>echo</code> command gets flushed first (because <code>echo</code> runs and terminates), and then all of the output from the <code>inner.py</code> process shows up at once, when <code>python</code> terminates.  If you disable output buffering for <code>inner.py</code>, you should get the same output in both cases.</p>

<p>Disable output buffering by setting the <code>PYTHONUNBUFFERED</code> environment variable, or by calling python with the <code>-u</code> switch, or by explicitly calling <code>sys.stdout.flush()</code> after every <code>print</code> (or <code>print(...,  flush=True)</code> on Python 3).</p>

<p>The difference between the behaviour of pipes and ttys seems to be a <a href=""https://www.turnkeylinux.org/blog/unix-buffering"" rel=""nofollow"">general behaviour of <code>stdio</code></a>: output to ttys is line-buffered (so, in your code, which reads line by line, it will seem to be unbuffered), whereas output to pipes is buffered.</p>
"
39725042,6215822.0,2016-09-27T12:44:01Z,39683153,1,"<p>Numpy doesn't have built-in support for finite fields. The matrix <code>A</code> in your code is treated as a matrix of real numbers, and hence has rank 2. </p>

<p>If you really need to support finite fields with Numpy, you'll have to define your own data type along with the arithmetic operations yourself, as shown <a href=""http://stackoverflow.com/questions/17044064/how-to-calculate-numpy-arrays-on-galois-field"">here</a>. There are of course the concerns about proper error handling (like divide by zero).</p>

<p>Even then, many common routines will have to be rewritten to support your field data types. For example, from the <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html"" rel=""nofollow"">numpy.linalg.matrix_rank</a> documentation, the routine uses Singular Value Decomposition (SVD), which is not well defined for finite fields, so you'll have to code the rank finding algorithm yourself.</p>

<p>As for the algorithm itself, you could try implementing plain old Gaussian Elimination along <a href=""http://stackoverflow.com/questions/16254654/test-if-matrix-is-invertible-over-finite-field"">these lines</a>, but this can be a pain in the neck and really slow, so you will likely be better off with other tools/packages like <strong>Sage</strong>.</p>
"
39725337,2987899.0,2016-09-27T12:58:44Z,39704367,1,"<p>The package namespace seems to have been changed as pointed out in this <a href=""https://github.com/GoogleCloudPlatform/google-cloud-python/pull/2264"" rel=""nofollow"">github issue</a> and hasn't been fully fixed. You could install the older version (<code>pip install gcloud</code>) which uses a different namespace &amp; use this import statement instead:</p>

<pre><code>from gcloud import storage
</code></pre>

<p>You should also ensure that you're importing vendor libs in your appengine_config.py as pointed out in dyeray's answer.</p>

<p>The issue seems to have been fixed as at version 0.20.0 of google-cloud. So the import statement in the question should work. Just remember to run <code>pip install --upgrade google-cloud</code></p>
"
39725573,4191647.0,2016-09-27T13:10:53Z,39725411,0,"<p>You can try adding another for loop, if you are fine with that.
Something like:</p>

<pre><code>for url in list:  
    for i in range(len(list)):  
      if url[:30] not in list[i]:  
          print(url)  
</code></pre>

<p>That will compare every word with every other word to check for sameness. That's just an example, I'm sure you could make it more robust.</p>
"
39725580,2141635.0,2016-09-27T13:11:02Z,39725411,6,"<p>If you consider any netloc's to be the same you can parse with <a href=""https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlparse"" rel=""nofollow""><code>urllib.parse</code></a></p>

<pre><code>from urllib.parse import  urlparse # python2 from urlparse import  urlparse 

u = ""http://www.myurlnumber1.com/foo+%bar%baz%qux""

print(urlparse(u).netloc)
</code></pre>

<p>Which would give you:</p>

<pre><code>www.myurlnumber1.com
</code></pre>

<p>So to get unique netlocs you could do something like:</p>

<pre><code>unique  = {urlparse(u).netloc for u in urls}
</code></pre>

<p>If you wanted to keep the url scheme:</p>

<pre><code>urls  = [""http://www.myurlnumber1.com/foo+%bar%baz%qux"", ""http://www.myurlnumber1.com""]

unique = {""{}://{}"".format(u.scheme, u.netloc) for u in map(urlparse, urls)}
print(unique)
</code></pre>

<p>Presuming they all have schemes and you don't have http and https for the same netloc and consider them to be the same.</p>

<p>If you also want to add the path:</p>

<pre><code>unique = {u.netloc, u.path) for u in map(urlparse, urls)}
</code></pre>

<p>The table of attributes is listed in the docs:</p>

<pre><code>Attribute   Index   Value   Value if not present
scheme  0   URL scheme specifier    scheme parameter
netloc  1   Network location part   empty string
path    2   Hierarchical path   empty string
params  3   Parameters for last path element    empty string
query   4   Query component empty string
fragment    5   Fragment identifier empty string
username        User name   None
password        Password    None
hostname        Host name (lower case)  None
port        Port number as integer, if present  None
</code></pre>

<p>You just need to use whatever you consider to be the unique parts.</p>

<pre><code>In [1]: from urllib.parse import  urlparse

In [2]: urls = [""http://www.url.com/foo-bar"", ""http://www.url.com/foo-bar?t=baz"", ""www.url.com/baz-qux"",  ""www.url.com/foo-bar?t=baz""]


In [3]: unique = {"""".join((u.netloc, u.path)) for u in map(urlparse, urls)}

In [4]: 

In [4]: print(unique)
{'www.url.com/baz-qux', 'www.url.com/foo-bar'}
</code></pre>
"
39726781,2001031.0,2016-09-27T14:02:13Z,39726577,1,"<p>No this does not exist in general. Some services support an OPTIONS request to the route in question, which should return you documentation about the route. If you are lucky this is machine generated from the same source code that implements the route, so is more accurate than static documentation. However, it may just return a very simple summary, such as which HTTP verbs are supported, which you already know.</p>

<p>Even better, some services may support a machine description of the API using WSDL or WADL, although you probably will only find that if the service also supports XML. This can be better because you will be able to find a library that can parse the description and generate a local object model of the service to use to interact with the API.</p>

<p>However, even if you have OPTIONS or WADL file, the kind of error you are facing could still happen. If the documents are not helping, you probably need to contact the service support team with a demonstration of your problem and request assistance.</p>
"
39726900,2074981.0,2016-09-27T14:07:42Z,39726737,2,"<p>The problem with your current method is that <code>.strip()</code> doesn't really do what you want.  It removes leading and trailing characters (and you want to remove ones within the text), and if you want to specify characters in addition to whitespace, they need to be in a list.</p>

<p>Another problem is that there are many more potential punctuation characters (question marks, exclamations, unicode ellipses, em dashes) that wouldn't get filtered out by your list.  Instead, you can use <code>string.punctuation</code> to get a wide range of characters (note that <code>string.punctuation</code> doesn't include some non-English characters, so its viability may depend on the source of your input):</p>

<pre><code>import string
punctuation = set(string.punctuation)
text = ''.join(char for char in text if char not in punctuation)
</code></pre>

<p>An even faster method (shown in <a href=""http://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python"">other</a> <a href=""http://stackoverflow.com/questions/36464160/what-does-python-string-maketrans"">answers</a> on SO) uses <code>string.translate()</code> to replace the characters:</p>

<pre><code>import string
text = text.translate(string.maketrans('', ''), string.punctuation)
</code></pre>
"
39726956,6162307.0,2016-09-27T14:10:27Z,39726737,0,"<p>I would remove the punctuation marks with the <code>replace</code> function after storing the words in a list like so:</p>

<pre><code>with open(file_name,""r"") as f_r:
    words = []
    for row in f_r:
        words.append(row.split())
punctuation = [',', ';', '.', ':', '-']
words = [x.replace(y, '') for y in punctuation for x in words]
</code></pre>
"
39727047,2531279.0,2016-09-27T14:14:27Z,39726737,1,"<p>You can try using the <code>re</code> module:</p>

<pre><code>import re
with open(file_name) as f:
    for word in re.split('\W+', f.read()):
        print word
</code></pre>

<p>See the <a href=""https://docs.python.org/2/library/re.html"" rel=""nofollow"">re documentation</a> for more details.</p>

<p>Edit: In case of non ASCII characters, the previous code ignore them. In that case the following code can help:</p>

<pre><code>import re
with open(file_name) as f:
    for word in re.compile('\W+', re.unicode).split(f.read().decode('utf8')):
        print word
</code></pre>
"
39727067,6618996.0,2016-09-27T14:15:06Z,39726737,2,"<p><code>strip()</code>only removes characters found at the beginning or end of a string.
So <code>split()</code> first to cut into words, then <code>strip()</code> to remove punctuation.</p>

<pre><code>import string

with open(file_name, ""rt"") as finput:
    for line in finput:
        for word in line.split():
            print word.strip(string.punctuation)
</code></pre>

<p>Or use a natural language aware library like <code>nltk</code>: <a href=""http://www.nltk.org/"" rel=""nofollow"">http://www.nltk.org/</a></p>
"
39727473,2901002.0,2016-09-27T14:34:02Z,39727271,3,"<p>IIUC you need:</p>

<pre><code>short = df.groupby(['ID', 'app_name'])
          .agg({'app_name': len, 
                'active_seconds': lambda x: 100 * x.sum() / df.active_seconds.sum()})
          .rename(columns={'active_seconds': 'count_sec', 'app_name': 'sum_app'})
          .reset_index()

print (short)

                                 ID            app_name  count_sec  sum_app
0  1637ce5a4c4868e694004528c642d0ac              Camera   1.162791        1
1  1637ce5a4c4868e694004528c642d0ac           VKontakte   3.343023        2
2  1ca9bb884462c3ba2391bf669c22d4bd             Twitter   1.453488        1
3  1ca9bb884462c3ba2391bf669c22d4bd           VK Client  58.212209        2
4  a703114aa8a03495c3e042647212fa63           Instagram   8.575581        1
5  a703114aa8a03495c3e042647212fa63           VKontakte   7.194767        1
6  b8f4df3f99ad786a77897c583d98f615           VKontakte  11.555233        4
7  b8f4df3f99ad786a77897c583d98f615  WhatsApp Messenger   8.502907        2
</code></pre>

<p>Another solution:</p>

<pre><code>#you need another name of df, e.g. short1
short1 = df.groupby(['ID', 'app_name'])
           .agg({'app_name': len, 'active_seconds': sum})
           .rename(columns={'active_seconds': 'count_sec', 'app_name': 'sum_app'})
           .reset_index()
short1.count_sec = 100 * short1.count_sec / df.active_seconds.sum()
print (short1)
                                 ID            app_name  count_sec  sum_app
0  1637ce5a4c4868e694004528c642d0ac              Camera   1.162791        1
1  1637ce5a4c4868e694004528c642d0ac           VKontakte   3.343023        2
2  1ca9bb884462c3ba2391bf669c22d4bd             Twitter   1.453488        1
3  1ca9bb884462c3ba2391bf669c22d4bd           VK Client  58.212209        2
4  a703114aa8a03495c3e042647212fa63           Instagram   8.575581        1
5  a703114aa8a03495c3e042647212fa63           VKontakte   7.194767        1
6  b8f4df3f99ad786a77897c583d98f615           VKontakte  11.555233        4
7  b8f4df3f99ad786a77897c583d98f615  WhatsApp Messenger   8.502907        2
</code></pre>
"
39728029,4014959.0,2016-09-27T15:00:52Z,39727689,4,"<p>That many terms is enough to get you over 50 decimal places. The problem is that you are mixing Python floats with Decimals, so your calculations are polluted with the errors in those floats, which are only precise to 53 bits (around 15 decimal digits).</p>

<p>You can fix that by changing </p>

<pre><code>c = pow(decimal.Decimal(1/5), decimal.Decimal(b))
</code></pre>

<p>to </p>

<pre><code>c = pow(1 / decimal.Decimal(5), decimal.Decimal(b))
</code></pre>

<p>or</p>

<pre><code>c = pow(decimal.Decimal(5), decimal.Decimal(-b))
</code></pre>

<p>Obviously, a similar change needs to be made to </p>

<pre><code>c = pow(decimal.Decimal(1/239), decimal.Decimal(b))
</code></pre>

<hr>

<p>You could make your code a <em>lot</em> more readable. For starters, you should put the stuff that calculates the arctan series into a function, rather than duplicating it for arctan(1/5) and arctan(1/239).</p>

<p>Also, you don't need to use Decimal for <em>everything</em>. You can just use simple Python integers for things like <code>count</code> and <code>a</code>. Eg, your calculation for <code>a</code> can be written as</p>

<pre><code>a = (-1) ** count
</code></pre>

<p>or you could just set <code>a</code> to 1 outside the loop and negate it each time through the loop.</p>

<p>Here's a more compact version of your code.</p>

<pre><code>import decimal

decimal.getcontext().prec = 60 #Setting precision

def arccot(n, terms):
    base = 1 / decimal.Decimal(n)
    result = 0
    sign = 1
    for b in range(1, 2*terms, 2):
        result += sign * (base ** b) / b
        sign = -sign
    return result

pi = 16 * arccot(5, 50) - 4 * arccot(239, 11)
print(pi)
</code></pre>

<p><strong>output</strong></p>

<pre><code>3.14159265358979323846264338327950288419716939937510582094048
</code></pre>

<p>The last 4 digits are rubbish, but the rest are fine.</p>
"
39728328,459745.0,2016-09-27T15:14:47Z,39726124,1,"<p>I am trying to guess what you are trying to do since I have no idea what your ""row"" looks like. I assume you have the variable <code>columns</code> which is a list of column names. If that is the case, please consider this code snippet:</p>

<pre><code>class SalesOrder(object):
    def __init__(self, columns, row):
        """""" Transfer all the columns from row to this object """"""
        for name in columns:
            value = getattr(row, name)
            setattr(self, name, value)
        self.long, self.lat = getLongLat(self.post_code)

    def cancel(self):
        self.status = 'cancelled'

    def as_row(self):
        return [getattr(self, name) for name in columns]

    def __repr__(self):
        return repr(self.as_row())

# Create the dictionary of class
orders = {row.order_id: SalesOrder(columns, row) for row in cursor}

# Cancel
cancelled_orders = getCancelledOrders()
for order_id in cancelled_orders:
    orders[order_id].cancel()

# Print all sales orders
for sales_order in orders.itervalues():
    print(sales_order)
</code></pre>

<p>At the lowest level, we need to be able to create a new <code>SalesOrder</code> object from the <code>row</code> object by copying all the attributes listed in <code>columns</code> over. When initializing a <code>SalesOrder</code> object, we also calculate the longitude and latitude as well.</p>

<p>With that, the task of creating the dictionary of class objects become easier:</p>

<pre><code>orders = {row.order_id: SalesOrder(columns, row) for row in cursor}
</code></pre>

<p>Our <code>orders</code> is a dictionary with <code>order_id</code> as keys and <code>SalesOrder</code> as values. Finally, the task up cancelling the orders is the same as your code.</p>

<p>In addition to what you have, I created a method called <code>as_row()</code> which is handy if later you wish to write a <code>SalesOrder</code> object into a CSV or database. For now, I use it to display the ""raw"" row. Normally, the <code>print</code> statement/function will invoke the <code>__str__()</code> method to get a string presentation for an object, if not found, it will attempt to invoke the <code>__repr__()</code> method, which is what we have here.</p>
"
39728895,6394138.0,2016-09-27T15:40:15Z,39726583,1,"<h2>The fix</h2>

<p>Replace the line</p>

<pre><code>ax.plot(pos[:, 0], pos[:, 1], pos[:, 2])
</code></pre>

<p>with</p>

<pre><code>ax.plot(list(pos[:, 0]), list(pos[:, 1]), list(pos[:, 2]))
</code></pre>

<p>and it will work as expected for global <code>pos</code>.</p>

<h2>Explanation</h2>

<p>The problem is that <code>ax.plot(xlist, ylist, zlist)</code> doesn't immediately plot the data. It merely stores references to <code>xlist</code>, <code>ylist</code>, and <code>zlist</code> and uses the data to construct the plot when <code>plt.show()</code> is called. Next, <code>pos[:, 0]</code>, <code>pos[:, 1]</code>, and <code>pos[:, 2]</code> do not return the corresponding columns by value. Instead they return some proxy objects that reference the original matrix.</p>

<p>As a result, actual plotting is performed using the data that ends up in the matrix after the last iteration and all the plots coincide. By wrapping each of <code>pos[:, 0]</code>, <code>pos[:, 1]</code>, and <code>pos[:, 2]</code> in a <code>list()</code> we force the column data to be copied, decoupling the plots from each other.</p>
"
39729862,839829.0,2016-09-27T16:29:05Z,39713137,1,"<p>Since you're using Python 2.7, you can use the <code>cmp</code> keyword argument to <code>sorted</code>. To get the column names in the order that you're looking for, I would do something like:</p>

<pre><code># Returns -1 if first_column[i] &gt; second_column[i] more often.
# Returns 1 if vice versa.
# Returns 0 if equal.
# Assumes df[first_column] and df[second_column] are the same length.
def compare_two(first_column, second_column):
    c1_greater_count = 0
    c2_greater_count = 0
    # Iterate over the two columns in the dataframe. df must be in accessible scope.
    for i in range(len(df[first_column])):
        if df[first_column].iloc(i) &gt; df[second_column].iloc[i]:
            c1_greater_count += 1
        elif df[second_column].iloc[i] &gt; df[first_column].iloc[i]:
            c2_greater_count += 1

    if c1_greater_count &gt; c2_greater_count:
        return -1
    if c2_greater_count &gt; c1_greater_count:
        return 1
    return 0

df = get_dataframe_from_somewhere()
relevant_column_names = get_relevant_column_names(df) # e.g., get all the dates.
sorted_column_names = sorted(relevant_column_names, cmp=compare_two)

# sorted_column_names holds the names of the relevant columns,
# sorted according to the given ordering.
</code></pre>

<p>I'm sure there's a more Pythonic way to do it, but this should work. Note that for Python 3, you can use the <a href=""https://wiki.python.org/moin/HowTo/Sorting#The_Old_Way_Using_the_cmp_Parameter"" rel=""nofollow""><code>cmp_to_key</code></a> utility.</p>
"
39730108,1126841.0,2016-09-27T16:43:07Z,39730037,4,"<p>Make <code>va</code> a property.</p>

<pre><code>class Center():
    def __init__(self):
        self.queue = None
        self._va = 0

    @property
    def va(self):
        return self._va

    @va.setter
    def va(self, value):
        self._va = value
        self.whenChanged()

    def whenChanged(self):
        next = self.queue.dequeue()
        next.function()
</code></pre>
"
39730131,3322400.0,2016-09-27T16:44:20Z,39729787,0,"<p>I believe pip isn't installed for python2.7</p>

<p>try :</p>

<pre><code>pip -V
</code></pre>

<p>On my system for instance it says :</p>

<pre><code>pip 8.1.2 from /usr/lib/python3.4/site-packages (python 3.4)
</code></pre>

<p>So basically using <code>pip uninstall</code> will only remove packages for python3.4 (and not python2.7).</p>

<p>So I don't use pip binary as such, and rather, call pip module from inside python.</p>

<p>In your case :</p>

<pre><code>python2.7 -m pip uninstall tensorflow
</code></pre>
"
39730178,2063361.0,2016-09-27T16:46:09Z,39730037,2,"<p>Whenever a property of class is changed, <a href=""https://docs.python.org/2/library/functions.html#setattr"" rel=""nofollow""><code>setattr()</code></a> function is called. You can override this by defining <a href=""https://docs.python.org/3/reference/datamodel.html#object.__setattr__"" rel=""nofollow""><code>__setattr__(self, property, value)</code></a> function in your class.</p>

<p>You need to make you required function call within this <code>__ setattr__()</code>. Below is the sample example based on your requirement:</p>

<pre><code>class Centre(object):
    def __init__(self):
        self.queue = None
        self.va = 0

    def whenChanged(self):
        next = self.queue.dequeue()
        next.function()

    def __setattr__(self, key, value):
        self.key = value
        self.whenChanged() &lt;-- Your function
</code></pre>

<p>Whenever you will attempt to change the value of any of class's property, this <code>__settattr__</code> function will be called.</p>
"
39730179,2074981.0,2016-09-27T16:46:09Z,39729995,1,"<p>Your problem is that the dash in the dataframe isn't the same as the dash in the dictionary.  The one in the dataframe is an en dash (<code>â</code> or <code>\u2013</code>), while the one in your dictionary is a hyphen (<code>â</code> or <code>\u2010</code>).  They look similar, but they're not the same character, so the strings don't match.</p>
"
39730180,212858.0,2016-09-27T16:46:12Z,39726124,1,"<p>Classes are mostly useful for coupling data to behaviour, and for providing structure (naming and documenting the association of certain properties, for example).</p>

<p>You're not doing either of those here - there's no real behaviour in your class (it doesn't <em>do</em> anything to the data), and all the structure is provided externally. The class instances are just used for their attribute dictionaries, so they're just a fancy wrapper around your old dictionary.</p>

<p>If you <em>do</em> add some real behaviour (above <code>getLongLat</code> and <code>cancelOrder</code>), or some real structure (other than a list of arbitrary column names and field values passed in from outside), then it makes sense to use a class.</p>
"
39730200,113962.0,2016-09-27T16:46:59Z,39730105,2,"<p>The <code>get_object_or_404</code> shortcut uses <code>get()</code>, so it will raise an error if the filtered queryset returns more than one object. </p>

<p>You could slice the queryset to limit it to one object:</p>

<pre><code>dummy_image = get_object_or_404(DummyImage.objects.order_by('?')[:1]).image_url.url
</code></pre>

<p>Alternatively, you could raise the <code>Http404</code> exception manually. This code is a bit longer but you might find it clearer what is going on.</p>

<pre><code>from django.http import Http404

dummy_image = DummyImage.objects.order_by('?').first()
if dummy_image is None:
    raise Http404
else:
    dummy_image = dummy_image.image_url.url
</code></pre>
"
39730307,6779307.0,2016-09-27T16:53:30Z,39730254,0,"<p>Add a <code>break</code> where you want the for loop to end: <a href=""https://docs.python.org/2/tutorial/controlflow.html"" rel=""nofollow"">https://docs.python.org/2/tutorial/controlflow.html</a></p>
"
39730321,100297.0,2016-09-27T16:53:58Z,39730199,4,"<p>The <code>deepcopy()</code> operation is copying lists <em>as they are modified by another thread</em>, and each copy operation takes a small amount of time (longer as the lists grow larger). So between copying the first of the 4 lists and copying the second, the other thread added 2 elements, indicating that copying a list of 8784 elements takes between 0.002 and 0.004 seconds.</p>

<p>That's because there is nothing preventing threading to switch between executing <code>synthesize_data()</code> and the <code>deepcopy.copy()</code> call. In other words, your code is simply not thread-safe.</p>

<p>You'd have to coordinate between your two threads; using a lock for example:</p>

<p>In <code>fileA</code>:</p>

<pre><code># ...
datalock = threading.RLock()
# ...

def synthesize_data():
    while True:
        with datalock:
            for x,y in zip(data,column):
                x.append(y)
            time.sleep(0.002)  # equivalent to 500 Hz
</code></pre>

<p>and in <code>fileB</code>:</p>

<pre><code>with fileA.datalock:
    data = copy.deepcopy(fileA.data)
    for row in data:
        print len(row)
</code></pre>

<p>This ensures that copying only takes place when the thread in <code>fileA</code> is not trying to add more to the lists.</p>

<p>Using locking will slow down your operations; I suspect the pandas assignment operations are already <a href=""http://stackoverflow.com/questions/13592618/python-pandas-dataframe-thread-safe"">subject to locks to keep them thread-safe</a>.</p>
"
39730323,205426.0,2016-09-27T16:54:00Z,39730254,0,"<p>Python has the <code>break</code> and <code>continue</code>statements for loop control.  You can set a boolean that you trigger such that:  </p>

<pre><code>if flag:
    break
#do code
#set flag
</code></pre>
"
39730383,6298477.0,2016-09-27T16:57:09Z,39730254,0,"<p>You can use a <code>break</code> statement as soon as you found a vowel. 
You also do not need to use any <code>split()</code> functions.
One big mistake you did was using <code>char</code> to get the <code>SubString</code>. You need to use the index of that char to get the <code>SubString</code> instead.</p>

<p>Take a look at this:</p>

<pre><code>def convert(s):
    beginning = """"
    index = 0;
    for char in s:
       if char in ('a','e','i','o','u'):
           end = str(s[index:])
           break
       else:
           beginning = beginning + char
       index = index + 1
    return str(end) + ""-"" + beginning + ""ay""
</code></pre>
"
39730516,5741205.0,2016-09-27T17:05:17Z,39730468,3,"<p>I would use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.factorize.html"" rel=""nofollow"">pd.factorize()</a> in this case:</p>

<pre><code>In [8]: cars['color_val'] = pd.factorize(cars.color)[0]

In [9]: cars
Out[9]:
  car_name  color  color_val
0      BMW    RED          0
1      BMW    RED          0
2   ACCURA    RED          0
3   ACCURA    RED          0
4   ACCURA  GREEN          1
5      BMW  BLACK          2
6      BMW   BLUE          3
7      BMW   BLUE          3
</code></pre>
"
39730690,4952130.0,2016-09-27T17:15:56Z,39730524,4,"<p>Pretty sure this isn't possible with a <code>list</code> subclass. It <em>is</em> possible with a <strong><a href=""https://docs.python.org/3/library/collections.html#collections.UserList"" rel=""nofollow""><code>collections.UserList</code></a></strong> subclass (simply <em><a href=""https://docs.python.org/2.7/library/userdict.html#module-UserList"" rel=""nofollow""><code>UserList</code></a></em> in Python <code>2</code>):</p>

<pre><code>from collections import UserList

class MyList(UserList):

    def __init__(self, it=None):
        # keep reference only for list instances
        if isinstance(it, list):
            self.data = it
        else:
            super().__init__(it)

    def my_func(self):
        # do some stuff
        return self
</code></pre>

<p>The fact that <code>UserList</code> exposes a <a href=""https://docs.python.org/3/library/collections.html#collections.UserList.data"" rel=""nofollow""><strong><code>data</code></strong></a> attribute containing the actual list instance makes it easy for us to replace it with the iterable <code>it</code> and essentially just drop the supplied argument there and <em>retain the reference</em>:</p>

<p>By initializing as you did: </p>

<pre><code>l1 = list(range(10))
l2 = MyList(l1)

print(l1, l2, sep='\n')
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</code></pre>

<p>and then mutating:</p>

<pre><code>l1[3] = -5
</code></pre>

<p>The <code>data</code> attribute referencing <code>l1</code> is, of course, mutated:</p>

<pre><code>print(l1, l2, sep='\n')
[0, 1, 2, -5, 4, 5, 6, 7, 8, 9]
[0, 1, 2, -5, 4, 5, 6, 7, 8, 9]
</code></pre>
"
39730704,671543.0,2016-09-27T17:16:53Z,39730254,1,"<p>Break things down one step at a time.</p>

<p>Your first task is to find the first vowel. Let's do that:</p>

<pre><code>def first_vowel(s):
    for index, char in enumerate(s):
        if char in 'aeiou':
            return index
    raise Error('No vowel found')
</code></pre>

<p>Then you need to use that first vowel to split your word:</p>

<pre><code>def convert(s):
    index = first_vowel(s)
    return s[index:] + ""-"" + s[:index] + 'ay'
</code></pre>

<p>Then test it:</p>

<pre><code>print(convert('pig'))
print(convert('string'))
</code></pre>

<p>Full code, runnable, is here: <a href=""https://repl.it/Dijj"" rel=""nofollow"">https://repl.it/Dijj</a></p>

<p>The exception handling, for words that have no vowels, is left as an exercise.</p>
"
39730720,3349443.0,2016-09-27T17:17:54Z,39730184,0,"<p>I think it's hard to do this solely with <code>matplotlib</code> but you can use <code>seaborn</code> which has <code>jointplot</code> function. </p>

<pre><code>import numpy as np
import pandas as pd
import seaborn as sns
sns.set(color_codes=True)

x = np.random.rand(1000,1)
y = np.random.rand(1000,1)
data = np.column_stack((x,y))
df = pd.DataFrame(data, columns=[""x"", ""y""])

sns.jointplot(x=""x"", y=""y"", data=df);
</code></pre>

<p><a href=""http://i.stack.imgur.com/wSjbD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wSjbD.png"" alt=""enter image description here""></a></p>
"
39730922,298607.0,2016-09-27T17:29:34Z,39730254,0,"<p>Side note. You can use a regex:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; cases=['big','string']
&gt;&gt;&gt; for case in cases:
...    print case+'=&gt;', re.sub(r'^([^aeiou]*)(\w*)', '\\2-\\1ay', case)
... 
big=&gt; ig-bay
string=&gt; ing-stray
</code></pre>
"
39731215,3358223.0,2016-09-27T17:48:34Z,39729508,3,"<p>As mentioned in the comment, you can use a contour. Since you are already using a triangulation, you can use <code>tricontourf</code>. See an example below.</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np



## data:

DATA = np.array([
    [-0.807237702464, 0.904373229492, 111.428744443],
    [-0.802470821517, 0.832159465335, 98.572957317],
    [-0.801052795982, 0.744231916692, 86.485869328],
    [-0.802505546206, 0.642324228721, 75.279804677],
    [-0.804158144115, 0.52882485495, 65.112895758],
    [-0.806418040943, 0.405733109371, 56.1627277595],
    [-0.808515314192, 0.275100227689, 48.508994388],
    [-0.809879521648, 0.139140394575, 42.1027499025],
    [-0.810645106092, -7.48279012695e-06, 36.8668106345],
    [-0.810676720161, -0.139773175337, 32.714580273],
    [-0.811308686707, -0.277276065449, 29.5977405865],
    [-0.812331692291, -0.40975978382, 27.6210856615],
    [-0.816075037319, -0.535615685086, 27.2420699235],
    [-0.823691366944, -0.654350489595, 29.1823292975],
    [-0.836688691603, -0.765630198427, 34.2275056775],
    [-0.854984518665, -0.86845932028, 43.029581434],
    [-0.879261949054, -0.961799684483, 55.9594146815],
    [-0.740499820944, 0.901631050387, 97.0261463995],
    [-0.735011699497, 0.82881933383, 84.971061395],
    [-0.733021568161, 0.740454485354, 73.733621269],
    [-0.732821755233, 0.638770044767, 63.3815970475],
    [-0.733876941678, 0.525818698874, 54.0655910105],
    [-0.735055978521, 0.403303715698, 45.90859502],
    [-0.736448900325, 0.273425879041, 38.935709456],
    [-0.737556181137, 0.13826504904, 33.096106049],
    [-0.738278724065, -9.73058423274e-06, 28.359664343],
    [-0.738507612286, -0.138781586244, 24.627237837],
    [-0.738539663773, -0.275090412979, 21.857410904],
    [-0.739099040189, -0.406068448513, 20.1110519655],
    [-0.741152200369, -0.529726022182, 19.7019157715],
])


Xs = DATA[:,0]    
Ys = DATA[:,1]    
Zs = DATA[:,2]


## plot:    
fig = plt.figure()
contour = plt.tricontourf(Xs, Ys, Zs, cmap=""YlGnBu_r"")
fig.colorbar(contour)
fig.savefig('3D.png')
plt.show()
</code></pre>

<p>The results is</p>

<p><a href=""http://i.stack.imgur.com/vFLyC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vFLyC.png"" alt=""enter image description here""></a></p>
"
39731383,2142055.0,2016-09-27T17:59:13Z,39730184,2,"<p>Seaborn is the way to go for quick statistical plots. But if you want to avoid another dependency you can use <a href=""http://matplotlib.org/users/gridspec.html"" rel=""nofollow"" title=""subplot2grid""><code>subplot2grid</code></a> to place the subplots and the keywords <code>sharex</code> and <code>sharey</code> to make sure the axes are synchronized.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

x = np.random.randn(100)
y = np.random.randn(100)

scatter_axes = plt.subplot2grid((3, 3), (1, 0), rowspan=2, colspan=2)
x_hist_axes = plt.subplot2grid((3, 3), (0, 0), colspan=2,
                               sharex=scatter_axes)
y_hist_axes = plt.subplot2grid((3, 3), (1, 2), rowspan=2,
                               sharey=scatter_axes)

scatter_axes.plot(x, y, '.')
x_hist_axes.hist(x)
y_hist_axes.hist(y, orientation='horizontal')
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZtA7q.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZtA7q.png"" alt=""plot""></a></p>

<p>You should always look at the <a href=""http://matplotlib.org/gallery.html"" rel=""nofollow"" title=""Matplotlib gallery"">matplotlib gallery</a> before asking how to plot something, chances are that it will save you a few keystrokes -- I mean you won't have to ask. There are actually two plots like this in the gallery. Unfortunately the code is old and does not take advantage of <code>subplot2grid</code>, <a href=""http://matplotlib.org/examples/pylab_examples/scatter_hist.html"" rel=""nofollow"">the first one</a> uses rectangles and the <a href=""http://matplotlib.org/examples/axes_grid/scatter_hist.html"" rel=""nofollow"">second one</a> uses <code>axes_grid</code>, which is a somewhat weird beast. That's why I posted this answer.</p>
"
39731398,2336654.0,2016-09-27T17:59:58Z,39730737,3,"<p>This is assuming you split the day up into 5 minute windows</p>

<pre><code>df.groupby(df.timestampe.dt.hour.mul(60) \
             .add(df.timestampe.dt.minute) // 5) \
  .apply(pd.DataFrame.reset_index)
</code></pre>

<p><a href=""http://i.stack.imgur.com/7Y76B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Y76B.png"" alt=""enter image description here""></a></p>

<hr>

<pre><code>for name, group in df.groupby(df.timestampe.dt.hour.mul(60).add(df.timestampe.dt.minute) // 5):
    print name
    print group
    print

132
           timestampe  val
0 2016-08-11 11:03:00  0.1
2 2016-08-09 11:04:00  0.5

133
           timestampe  val
1 2016-08-13 11:06:00  0.3
4 2016-08-19 11:09:00  0.8

139
           timestampe  val
3 2016-08-05 11:35:00  0.7

151
           timestampe  val
5 2016-08-21 12:37:00  0.9
</code></pre>
"
39731526,4177078.0,2016-09-27T18:07:19Z,39730737,0,"<p>Since you do not care about the <code>date</code> part of your <code>datetime</code> object, I think that make all <code>date</code> equal is a good trick.</p>

<pre><code>df['time'] = df['timestamp'].apply(lambda x: x.replace(year=2000, month=1, day=1))
</code></pre>

<p>You get:</p>

<pre><code>            timestamp  val                time
0 2016-08-11 11:03:00  0.1 2000-01-01 11:03:00
1 2016-08-13 11:06:00  0.3 2000-01-01 11:06:00
2 2016-08-09 11:04:00  0.5 2000-01-01 11:04:00
3 2016-08-05 11:35:00  0.7 2000-01-01 11:35:00
4 2016-08-19 11:09:00  0.8 2000-01-01 11:09:00
5 2016-08-21 11:37:00  0.9 2000-01-01 11:37:00
</code></pre>

<p>Now you can do what you what on <code>time</code> column. For example, groups on every 5 mins:</p>

<pre><code>grouped = df.groupby(Grouper(key='time', freq='5min'))

grouped.count()

                     timestamp  val
time                               
2000-01-01 11:00:00          2    2
2000-01-01 11:05:00          2    2
2000-01-01 11:10:00          0    0
2000-01-01 11:15:00          0    0
2000-01-01 11:20:00          0    0
2000-01-01 11:25:00          0    0
2000-01-01 11:30:00          0    0
2000-01-01 11:35:00          2    2
</code></pre>

<p>Hope this trick may be suitable for your need. Thanks!</p>
"
39731664,1698152.0,2016-09-27T18:16:05Z,39731477,1,"<p>I think I would probably use namedtuples here, but it's hard to tell from just this snippet.</p>

<p>If you'd like material on how to write more Pythonic code, I recommend checking out Raymond Hettinger's videos, particularly</p>

<p>""Best practices for beautiful intelligible code"" and
""Transforming Code into Beautiful, Idiomatic Python"":  </p>

<p><a href=""http://pyvideo.org/speaker/raymond-hettinger.html"" rel=""nofollow"">http://pyvideo.org/speaker/raymond-hettinger.html</a></p>
"
39731740,6207849.0,2016-09-27T18:20:50Z,39731396,3,"<p>You could use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html"" rel=""nofollow""><code>pivot_table</code></a> by creating an additional column with the value 1 as it doesn't matter either ways (You are only counting them)</p>

<pre><code>df['Y/Ncount'] = 1

df = df.pivot_table(index=['PROD', 'PARAMETER'], columns=['Y/N'], values=['Y/Ncount'], 
                    aggfunc=sum, fill_value=0)

df.columns = [col for col in df.columns.get_level_values(1)]
df.reset_index()
</code></pre>

<p><a href=""http://i.stack.imgur.com/OvPpj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OvPpj.png"" alt=""Image""></a></p>

<hr>

<p>The simplest operation to use under this scenario would be <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html"" rel=""nofollow""><code>crosstab</code></a> which would produce the frequency counts of values present inside the Y/N column:</p>

<pre><code>pd.crosstab([df['PROD'], df['PARAMETER']], df['Y/N'])
</code></pre>

<p><a href=""http://i.stack.imgur.com/1p6eA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1p6eA.png"" alt=""Image""></a></p>
"
39731744,6671342.0,2016-09-27T18:21:09Z,39731477,1,"<p>This question has many interpretations. For example I would simulate results with the following dictionary:</p>

<pre><code>&gt;&gt;&gt; result = {'Game':{'home_team':{'score':20,'id':1}, 'away_team':{'score':15,'id':2}}}
&gt;&gt;&gt; print result['Game']
{'home_team': {'score': 20, 'id': 1}, 'away_team': {'score': 15, 'id': 2}}
&gt;&gt;&gt; print result['Game']['home_team']
{'score': 20, 'id': 1}
&gt;&gt;&gt; print result['Game']['away_team']['score']
15
</code></pre>

<p>There are a lot of ways to simulate your code, the above is just one of them. Of course the code doesn't do what the php code does, just shows a way to access the data.</p>
"
39731750,509824.0,2016-09-27T18:21:30Z,39731396,2,"<p>You want to get the counts of the values in the <code>Y/N</code> column, grouped by <code>PROD</code> and <code>PARAMETER</code>.</p>

<pre><code>import io
import pandas as pd

data = io.StringIO('''\
KEY PROD PARAMETER Y/N
1    AAA    PARAM1   Y
1    AAA    PARAM2   N
1    AAA    PARAM3   N
2    AAA    PARAM1   N
2    AAA    PARAM2   Y
2    AAA    PARAM3   Y
3    CCC    PARAM1   Y
3    CCC    PARAM2   Y
3    CCC    PARAM3   Y
''')
df = pd.read_csv(data, delim_whitespace=True)

res = (df.groupby(['PROD', 'PARAMETER'])['Y/N'] # Group by `PROD` and `PARAMETER`
                                                # and select the `Y/N` column
         .value_counts()                        # Get the count of values
         .unstack('Y/N')                        # Long-to-wide format change
         .fillna(0)                             # Fill `NaN`s with zero
         .astype(int))                          # Cast to integer
print(res)
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>Y/N             N  Y
PROD PARAMETER      
AAA  PARAM1     1  1
     PARAM2     1  1
     PARAM3     1  1
CCC  PARAM1     0  1
     PARAM2     0  1
     PARAM3     0  1
</code></pre>
"
39731770,6005062.0,2016-09-27T18:22:53Z,39711977,0,"<p>I did some tests using your example and filters works well, for example:</p>

<pre><code>df = pandas.read_csv('Yourfile.csv')
df['Admission_Source_Code'].value_counts()

1                      246804
2                      135272
5                        8983
8                        3459
4                        3177
6                        1278
9                         522
D                         314
E                          91
0                          29
F                          20    
Name: Admission_Source_Code, dtype: int64
</code></pre>

<p>If I try:</p>

<pre><code>print (df[(df['Admission_Source_Code']==1)])
</code></pre>

<p>I got:</p>

<pre><code>Empty DataFrame
Columns: [Admission_Source_Code]
Index: []
</code></pre>

<p>However with a <code>list comprehesion</code></p>

<pre><code>df['Admission_Source_Code'] = [str(i) for i in df['Admission_Source_Code']]
</code></pre>

<p>Using a data example:</p>

<p><a href=""http://i.stack.imgur.com/Pmjfc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Pmjfc.png"" alt=""enter image description here""></a></p>

<p>If problems persist, have you considered clean items from csv columns? <em>(i.e. a whitespace)</em>.</p>

<p>For example using the same <code>list comprehesion</code> and <code>strip()</code>:</p>

<pre><code>df['Admission_Source_Code'] = [str(i.strip()) for i in df['Admission_Source_Code']]
</code></pre>
"
39731776,6494623.0,2016-09-27T18:23:07Z,39730787,1,"<p>Have you considered using django-paypal as one of your installed apps?</p>

<p>It looks fit for purpose and may provide what you require. </p>

<p>Take a look:</p>

<p><a href=""https://github.com/spookylukey/django-paypal"" rel=""nofollow"">https://github.com/spookylukey/django-paypal</a>
<a href=""https://django-paypal.readthedocs.io/en/stable/overview.html"" rel=""nofollow"">https://django-paypal.readthedocs.io/en/stable/overview.html</a></p>
"
39731813,2776376.0,2016-09-27T18:25:04Z,39729710,0,"<p>Try a hardcoded Firefox binary
<a href=""https://seleniumhq.github.io/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.firefox_binary.html"" rel=""nofollow"">https://seleniumhq.github.io/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.firefox_binary.html</a></p>

<pre><code>selenium.webdriver.firefox.firefox_binary.FirefoxBinary(""/your/binary/location/firefox"")
driver = webdriver.Firefox(firefox_binary=binary)
</code></pre>
"
39731854,6855437.0,2016-09-27T18:27:15Z,39730787,1,"<p>I am not sure about email, but if you want to access paypal ammounts, this might help.
<a href=""http://stackoverflow.com/questions/34379905/get-amount-from-django-paypal?rq=1"">Get amount from django-paypal</a></p>
"
39732127,15055.0,2016-09-27T18:43:06Z,39732027,8,"<p>This has to do with a smaller number of runs not being accurate enough to get the timing resolution you want.</p>

<p>As you increase the number of runs, the ratio between the times approaches the ratio between the number of runs:</p>

<pre><code>&gt;&gt;&gt; def timeit_ratio(a, b):
...     return timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=a) / timeit('unicodedata.category(chr)', setup = 'import unicodedata, random; chr=unichr(random.randint(0,50000))', number=b)
&gt;&gt;&gt; for i in range(32):
...   r = timeit_ratio(2**(i+1), 2**i)
...   print 2**i, 2**(i+1), r, abs(r - 2)**2  # mean squared error
...
1 2 3.0 1.0
2 4 1.0 1.0
4 8 1.5 0.25
8 16 1.0 1.0
16 32 0.316455696203 2.83432142285
32 64 2.04 0.0016
64 128 1.97872340426 0.000452693526483
128 256 2.05681818182 0.00322830578512
256 512 1.93333333333 0.00444444444444
512 1024 2.01436781609 0.000206434139252
1024 2048 2.18793828892 0.0353208004422
2048 4096 1.98079658606 0.000368771106961
4096 8192 2.11812990721 0.0139546749772
8192 16384 2.15052027269 0.0226563524921
16384 32768 1.93783596324 0.00386436746641
32768 65536 2.28126901347 0.0791122579397
65536 131072 2.18880312306 0.0356466192769
131072 262144 1.8691643357 0.0171179710535
262144 524288 2.02883451562 0.000831429291038
524288 1048576 1.98259818317 0.000302823228866
1048576 2097152 2.088684654 0.00786496785554
2097152 4194304 2.02639479643 0.000696685278755
4194304 8388608 1.98014042724 0.000394402630024
8388608 16777216 1.98264956218 0.000301037692533
</code></pre>
"
39732652,748858.0,2016-09-27T19:18:28Z,39732608,4,"<p>You <em>can</em> put pretty much whatever you want in a list.  It's likely that your <code>sys.argv</code> is too long (even after slicing off the first element).</p>

<p>e.g. if <code>len(sys.argv[1:]) == 6</code> and <code>len(i) == 5</code> than by the time you get to the last element in the <code>for</code> loop, <code>i</code> will be empty.  This appears to be the case from the code you posted in the link.</p>

<p>Also note that you're probably better off using <code>zip</code>:</p>

<pre><code>lst = ['.','.','.',':','']
for v, ii in zip(sys.argv[1:], lst):  # possibly reversed(i) if you meant to pop off the left side of the list rather than the end?
    host += host + str(v)+ str(ii)
</code></pre>

<p>Or (more efficiently):</p>

<pre><code>host = ''.join(j+ii for j, ii in zip(sys.argv[1:], lst))
</code></pre>

<p>Of course, you still likely end up with incorrect output (even with <code>zip</code>) if the input lists aren't the correct lengths -- However, you won't get an exception, just a shorter output string than you might be expecting since <code>zip</code> truncates when one of the iterables is exhausted.</p>
"
39732686,4099813.0,2016-09-27T19:20:17Z,39732608,2,"<pre><code>a=['script','location','00','11','22','33','4444']
i=['.','.','.',':','',''] # added an extra ''

host=''

for v in a[1:]:
    host=host+str(v)+i.pop(0)

print (host)
</code></pre>

<p>Something like this? Changed pop(0) cause you want the start not the end. Your issue was you were trying to pop more than there was. </p>
"
39733039,6418786.0,2016-09-27T19:42:09Z,39729776,0,"<p>Based on the solution by Erba Aitbayev, I found that it suffices to replace the line</p>

<pre><code>ax.cax.colorbar(im,ticks=tick_locations, format=ticker.LogFormatter())
</code></pre>

<p>in the example code originally posted by the line </p>

<pre><code>fig.colorbar(im,ticks=tick_locations, format=ticker.LogFormatter(), cax = ax.cax)
</code></pre>

<p>and everything works without the need to specify explicit dimensions for the colorbar.  I have no idea why one works and the other doesn't, though.  It would be good to add a corresponding comment <a href=""https://stackoverflow.com/a/38940369/6418786"">in the post on sharing colorbars</a>.  I checked and the linear color scale in that example still works if colorbar is called as in the second of the two alternatives above.  (I don't have sufficient reputation to add a comment there.)</p>
"
39733070,1405065.0,2016-09-27T19:44:27Z,39732213,1,"<p>In the documentation you link to, it describes how <code>ndarray.__new__</code> will call <code>__array_finalize__</code> on the arrays it constructs. And your class's <code>__new__</code> method is causing that to happen when you create your instance as a <code>view</code> of an existing array. The <code>view</code> method on the array argument is calling <code>ndarray.__new__</code> for you, and it calls your <code>__array_finalize__</code> method before the instance is returned to you.</p>

<p>You don't see <code>__array_finalize__</code> called twice because you're not calling <code>ndarray.__new__</code> a second time. If your <code>__new__</code> method included a call to <code>super().__new__</code> in addition to the <code>view</code> call, you probably would see <code>__array_finalized__</code> called twice. Such behavior would probably be buggy (or at least, slower than necessary), so it's no surprise you're not doing it!</p>

<p>Python doesn't automatically call overridden methods when the overriding subclass's method gets called. It's up to the overriding method to call (or not call) the overridden version (directly with <code>super</code> or, as in this case, indirectly via another object's <code>view</code> method).</p>
"
39733198,4983450.0,2016-09-27T19:52:00Z,39732957,0,"<p>Based on your logic, you can replace all values between the first False and the last False with False:</p>

<pre><code>def mutate(A):
    ind = np.where(~A)[0]
    if len(ind) != 0:
        A[ind.min():ind.max()] = False
    return A


np.apply_along_axis(mutate, 1, arr)

# array([[ True,  True,  True, False, False, False, False, False,  True,
#          True,  True],
#        [ True,  True,  True, False, False, False, False, False,  True,
#          True,  True],
#        [ True,  True,  True, False, False, False, False, False,  True,
#          True,  True],
#        [ True,  True,  True, False, False, False, False, False,  True,
#          True,  True],
#        [ True,  True,  True, False, False, False, False, False,  True,
#          True,  True]], dtype=bool)
</code></pre>
"
39733220,459745.0,2016-09-27T19:54:00Z,39732608,0,"<p>It seems what you want is simple: join everything using the dot, but the last part, the port number must be joined by colon. Here is another way to do it:</p>

<pre><code>port = sys.argv.pop()
host = '{}:{}'.format('.'.join(sys.argv[1:]), port)
</code></pre>
"
39733305,2074981.0,2016-09-27T19:59:49Z,39733269,2,"<p>It's actually the act of calling <code>in_file.read()</code> twice that's causing your problem.  You can fix it by assigning the result to a variable, as in the original:</p>

<pre><code>indata = in_file.read()
</code></pre>

<p>The reason is that when you call <code>in_file.read()</code>, you ""exhaust"" the file.  Think of the file as a book - as the computer reads it, it leaves a bookmark where it leaves off.  So when it's done, the bookmark is left on the last page of the book.  And when you call <code>in_file.read()</code> the second time, python starts where the bookmark was left - at the end, with no more pages left to read.</p>

<p>If you want to avoid creating a variable for some reason, you could ""move the bookmark"" back to the beginning of the file, as suggested by @WayneWerner in the comments.</p>

<pre><code>in_file.seek(0)
</code></pre>
"
39733317,4799172.0,2016-09-27T20:00:20Z,39733158,2,"<p>No input to test, but try this. Your current approach doesn't include the existing data for each row that already exists in your input data. <code>extend</code> will take the list that represents each row and then add another item to that list... equivalent to adding a column.</p>

<pre><code>import CSV
with open(filename) as csvin:
    readfile = csv.reader(csvin, delimiter=',')
    with open(output, 'w') as csvout:
        writefile = csv.writer(csvout, delimiter=',', lineterminator='\n')
        for row in readfile:
            row.extend([str(row[10]) + ' ' + str(row[11])])
            writefile.writerow(row)
</code></pre>
"
39733436,3923281.0,2016-09-27T20:08:52Z,39732957,1,"<p>Here one idea that's easy to implement and should perform reasonably quickly.</p>

<p>I'll use 0s and 1s so it's a little clearer to look at.</p>

<p>Here's the starting array:</p>

<pre><code>&gt;&gt;&gt; a
array([[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],
       [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],
       [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1],
       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]])
</code></pre>

<p>Accumulate left-to-right using <code>np.logical_and.accumulate</code>, flip left-to-right, do the same again, flip back, and the ""or"" the two arrays together:</p>

<pre><code>&gt;&gt;&gt; andacc = np.logical_and.accumulate
&gt;&gt;&gt; (andacc(a, axis=1) | andacc(a[:, ::-1], axis=1)[:, ::-1]).astype(int)
array([[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]])
</code></pre>

<p>(Leave out <code>.astype(int)</code> to keep a boolean array instead of 0s and 1s.)</p>

<p>Here's a triangle:</p>

<pre><code>&gt;&gt;&gt; b
array([[1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1],
       [1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1],
       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]])

&gt;&gt;&gt; (andacc(b, axis=1) | andacc(b[:, ::-1], axis=1)[:, ::-1]).astype(int)
array([[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],
       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
</code></pre>
"
39733502,1658617.0,2016-09-27T20:13:01Z,39733443,2,"<p>This works:</p>

<pre><code>def sumUp(d):
    new_d = {k: sum(v) for k, v in d.items()}
    print(new_d)
    return new_d
</code></pre>

<p>Keep in mind you are updating the dictionary while iterating over it, which causes all sorts of strange behavior.</p>

<p>The behavior is quite random and depends on the salting of the key's hashes (which modifies the dict order). You can't reproduce it consistently in Python 3.</p>

<p>This is your code fixed:</p>

<pre><code>def sumUp2(d):
    for k in d:
        d[k] = functools.reduce(lambda x, y: x + y, d[k])
    print(d)
</code></pre>

<p>It sets the key instead of updating the dict, which is safe.</p>
"
39733540,998967.0,2016-09-27T20:14:56Z,39729995,0,"<p>Thanks. I changed the dict value but:</p>

<pre><code>In [130]: dataframe = pd.read_csv(
   ...:             lo_csv_path,
   ...:             encoding=encoding_l,
   ...:             sep='|',
   ...:             usecols=[unicode(v) for v in fields_cols_mapping.values()],
   ...:             dtype={ k: object for k in fields_cols_mapping.keys() },
   ...:             )
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call      last)
&lt;ipython-input-130-670241506984&gt; in &lt;module&gt;()
      3             encoding=encoding_l,
      4             sep='|',
----&gt; 5             usecols=[unicode(v) for v in fields_cols_mapping.values()],
      6             dtype={ k: object for k in fields_cols_mapping.keys() },
      7         )

UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 20: ordinal not in range(128)
</code></pre>
"
39733574,459745.0,2016-09-27T20:17:16Z,39733158,0,"<p>I assume that glayne wants to combine column 10 and 11 into one.
In my approach, I concentrate on how to transform a single row first:</p>

<pre><code>def transform_row(input_row):
    output_row = input_row[:]
    output_row[10:12] = [' '.join(output_row[10:12])]
    return output_row
</code></pre>

<p>Once tested to make sure that it works, I can move on to replace all rows:</p>

<pre><code>with open('data.csv') as inf, open('out.csv', 'wb') as outf:
    reader = csv.reader(inf)
    writer = csv.writer(outf)
    writer.writerows(transform_row(row) for row in reader)
</code></pre>

<p>Note that I use the <code>writerows()</code> method to write multiple rows in one statement.</p>
"
39733609,1046449.0,2016-09-27T20:19:39Z,39733476,1,"<p>The reason not to use a variable named <code>sum</code> is because there is a function with the same name.</p>

<p>The assignment asks you to do the work of the <code>sum()</code> function explicitly.  You are on the right track with the <code>if not</code> line but might be more successful if you reverse the logic (like this: <code>if line.startswith</code> ...), then put the handling inside an indented block that follows.</p>

<p>The handling you need is to keep track of how many such lines you handle and the accumulated sum.  Use a term that is a synonym for <code>sum</code> that is not already a Python identifier.  Extract the float value from the end of <code>line</code> and then <code>&lt;your sum variable&gt; += float(the float from ""line"")</code>.</p>

<p>Don't forget to initialize both counter and accumulator before the loop.</p>
"
39733613,3688683.0,2016-09-27T20:19:52Z,39720836,4,"<p>VotingClassifiers are not always guaranteed to have better performance, especially when using soft voting if you have poorly calibrated base models. </p>

<p>For a contrived example, say all of the models are really wrong when they are wrong (say give a probability of .99 for the incorrect class) but are only slightly right when they are right (say give a probability of .51 for the correct class). Furthermore, say 'rf' and 'svc' are always right when 'xgb' is wrong and vice versa and each classifier has an accuracy of 50% on its own. </p>

<p>The voting classifier that you implement would have an accuracy of 0% since you are using soft voting. Here is why:</p>

<ol>
<li>Case 1: 'xgb' right. Then it gives a probability of .51 to the correct class and gets a weight of 2, for a score of 1.02. However, the other models each give a probability of .99 for the incorrect class for a score of 1.98. That class gets chosen by your voting classifier. </li>
<li>Case 2: 'xgb' is wrong. Then it gives a probability of .99 to the incorrect class with a weight of 2 for a score of 1.98. The other two models give a combined score of 1.02 for the correct class. Again, the wrong class is chosen by your classifier. </li>
</ol>
"
39733633,6779307.0,2016-09-27T20:21:27Z,39733476,1,"<pre><code>with open(fname) as f:
    s = 0
    linecount = 0
    for line in f:
        l = line.split()
        try:
            num = float(l[1])
        except ValueError:
            continue
        if l[0] == 'X-DSPAM-Confidence:':
            s += num
            linecount += 1
    print(s/linecount)
</code></pre>

<p>Here's how I would do it.  I'll happily answer any questions.</p>
"
39733688,5666087.0,2016-09-27T20:25:04Z,39733476,0,"<p>@PatrickHaugh beat me to it but here's my answer. It's less complex than his in that it does not implement try/except. I assume you haven't learned try/except yet.</p>

<pre><code>fname = raw_input(""Enter file name: "")
sum = 0
lines = 0
f = open(fname)
for line in f:
    if line.startswith(""X-DSPAM-Confidence:""):
        lines += 1
        line = line.rstrip('\n')  # get rid of the trailing new-line
        number = float(line.split()[-1])  # get last value after splitting the string
        sum += number
f.close()  # Don't forget to close the file!
print (sum / lines)
print ""Done""
</code></pre>
"
39733730,5247911.0,2016-09-27T20:28:23Z,39733476,1,"<p>Using <strong>Regular expression</strong>.</p>

<p><a href=""https://pymotw.com/2/re/"" rel=""nofollow"">More info regarding re module, check here !!!</a></p>

<p><strong>Code:</strong></p>

<pre><code>import re
fname = raw_input(""Enter file name: "")
f = open(fname)
val_list = []
tot = 0
line_cnt = 0
for line in f:
  a = re.findall(""X-DSPAM-Confidence:\s*(\d+\.?\d*)"",line)
  if len(a) != 0:
    tot += float(a[0])
    line_cnt +=1

print (""Line Count is "",line_cnt)
print (""Average is "",tot/line_cnt)
f.close()
</code></pre>

<p><strong>Content of y.txt:</strong></p>

<pre><code>a
X-DSPAM-Confidence:    0.8475
b
X-DSPAM-Confidence:    0.8476
c
X-DSPAM-Confidence:    0.8477
d
X-DSPAM-Confidence:    0.8478
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>C:\Users\dinesh_pundkar\Desktop&gt;python c.py
Enter file name: y.txt
Line Count is  4
Average is  0.84765

C:\Users\dinesh_pundkar\Desktop&gt;
</code></pre>

<p><strong>Points:</strong></p>

<ul>
<li><em>You can open file using <code>with</code> as Patrick has done in his answer. If file is opened using <code>with</code> then no need to close the file explicit.</em></li>
</ul>
"
39733789,3455890.0,2016-09-27T20:33:02Z,39726477,1,"<p><code>QpainterPath.subtracted()</code> doesn't subtract path elements but path areas, 
<a href=""http://doc.qt.io/qt-5/qpainterpath.html#subtracted"" rel=""nofollow"">see documentation</a></p>

<p>same effect if <code>QpainterPath::operator-()</code> is used:</p>

<pre><code>        # path3 = self.path1.subtracted(self.path2)
        path3 = self.path1 â self.path2
</code></pre>

<p>You can identify the elements of a path by something like this</p>

<pre><code>        c = path3.elementCount()
        for i in range(c):
            e = path3.elementAt(i)
            print('Element-nr.: ',  i, 'Type: ',  e.type, 'x: ',   e.x,  'y: ',  e.y)   # type: 0 = MoveTo, 1 = LineTo
</code></pre>

<p>I think, you have to write an own method, which creates path3 from the elements of path1 and path2.</p>
"
39733881,126531.0,2016-09-27T20:38:12Z,39731477,2,"<p><a href=""https://en.wikipedia.org/wiki/Autovivification#Python"" rel=""nofollow"">Python's built-in dict class can be subclassed to implement autovivificious dictionaries simply by overriding the <strong>missing</strong>() method </a>, but that's only part of the solution. If you were to simply implement the <code>Tree</code> example in the Wikipedia link and do something like:</p>

<pre><code>wins = Tree()

wins['team_a']['team_b'] += 1
</code></pre>

<p>You'd run into: <code>TypeError: unsupported operand type(s) for +=: 'Tree' and 'int'</code> because the example code will <code>wins['team_a']['team_b']</code> will have automatically been typed <code>Tree</code> as well.</p>

<p>Whereas:</p>

<pre><code>wins = Tree()

wins['team_a']['team_b'] = 1
</code></pre>

<p>would assign the value 1 properly (as it's a reassignment, not an operation on an existing typed value).</p>

<p>The solution would subclassing to implement autovivification <em>and</em> ensuring that the leaf elements are integers that you can operate on. The following should solve this for you, or at least get you started:</p>

<p>The following should help, or at least get you started:</p>

<pre><code>from collections import defaultdict

def autovivify(levels=1, final=dict):
    return (defaultdict(final) if levels &lt; 2
        else defaultdict(lambda: autovivify(levels - 1, final)))

wins = autovivify(2, int)
losses = autovivify(2, int)

wins['team_a']['team_b'] += 1
losses['team_b']['team_a'] += 1

wins['team_b']['team_c'] += 1
losses['team_c']['team_b'] += 1

wins['team_a']['team_c'] += 1
losses['team_c']['team_a'] += 1

wins['team_a']['team_b'] += 1
losses['team_b']['team_a'] += 1

print(wins['team_a']) # outputs defaultdict(&lt;type 'int'&gt;, {'team_b': 2, 'team_c': 1})
</code></pre>

<p><strong>Source:</strong> <a href=""http://blogs.fluidinfo.com/terry/2012/05/26/autovivification-in-python-nested-defaultdicts-with-a-specific-final-type/"" rel=""nofollow"">http://blogs.fluidinfo.com/terry/2012/05/26/autovivification-in-python-nested-defaultdicts-with-a-specific-final-type/</a></p>

<p>The autovivify function will ensure that the first assignment (<code>team_a</code>) will give you another autovivifying tree, and the second (<code>team_b</code>) will give you an integer. From there, your <code>+= 1</code> will continue to increment the initial value of <code>0</code>.</p>
"
39733950,1330616.0,2016-09-27T20:42:22Z,39733476,1,"<p>Here is a code snippet that suffices your work. I am reading the float values from the line with ""X-DSPAM-Confidence:"" and adding them and in the end, I am taking the mean. Also, since you are a beginner, I suggest to keep in mind that when you are dealing with division and you are expecting a float, either numerator or denominator should be float to give the answer in float. Since in the below code snippet, our number is float, we wont have that issue. </p>

<pre><code>fname = raw_input(""Enter file name: "")
f = open(fname)
cnt = 0
mean_val = 0
for line in f:
    if not line.startswith(""X-DSPAM-Confidence:"") : continue
    mean_val += float(line.split(':')[1])
    cnt += 1
f.close()
mean_val /= cnt
print mean_val
</code></pre>
"
39734013,4530440.0,2016-09-27T20:46:43Z,39729787,1,"<p>It could be because you didn't <a href=""https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation"" rel=""nofollow"">install Tensorflow using <code>pip</code></a>, but using <code>python setup.py develop</code> instead as your <a href=""https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#setting-up-tensorflow-for-development"" rel=""nofollow"">link</a> shows.</p>

<p><code>pip uninstall</code> is likely to fail if the package is installed using <code>python setup.py install</code> as they do not leave behind metadata to determine what files were installed.</p>

<p>Therefore, you should be able to unistall Tensorflow with the option <code>-u</code> or <code>--unistall</code> of <a href=""http://setuptools.readthedocs.io/en/latest/setuptools.html#development-mode"" rel=""nofollow""><code>develop</code></a></p>

<pre><code>cd /home/AIJ/tensorflow/_python_build
python setup.py develop --uninstall
</code></pre>

<p>To answer for the second (interestring) question  about the two <code>dist-package</code> created under <code>/usr/lib/python2.7</code> and <code>/usr/local/lib/python2.7</code> it exists already a <a href=""http://stackoverflow.com/questions/9387928/whats-the-difference-between-dist-packages-and-site-packages"">great Stack Overflow answer</a> on the topic.</p>

<p>PS: Tensorflow is a good library, you should consider <em>not</em> uninstall it :)</p>
"
39734025,639792.0,2016-09-27T20:47:42Z,39729558,0,"<p>It appears to be running the test 4 times, not 3, which makes sense if it's doing an all-combinations run.</p>

<ul>
<li>Run #1: Param 1, Tolerance 1</li>
<li>Run #2: Param 2, Tolerance 1</li>
<li>Run #3: Param 1, Tolerance 2</li>
<li>Run #4: Param 2, Tolerance 2</li>
</ul>

<p>Running four times seems to me like a reasonable approach given the test definition.</p>
"
39734294,3125566.0,2016-09-27T21:04:43Z,39734209,3,"<p>If you're going to exclude the words in quote out of the <em>split</em>, you could use <a href=""https://docs.python.org/2/library/shlex.html#shlex.split"" rel=""nofollow""><code>shlex.split</code></a>:</p>

<pre><code>import shlex

s = ""print 'Hello, world!' times 3""
print(shlex.split(s))
# ['print', 'Hello, world!', 'times', '3']
</code></pre>
"
39734318,2063361.0,2016-09-27T21:06:22Z,39734209,0,"<p><a href=""https://www.tutorialspoint.com/python/string_split.htm"" rel=""nofollow""><code>.split()</code></a> function splits the <code>str</code> based on the delimiter. The default delimiter is a <code>blank space</code>. It doesn't care about the <code>'</code> within your string. In case you want to treat words within <code>'</code> as a single word. You should be using <a href=""https://docs.python.org/2/library/shlex.html#shlex.split"" rel=""nofollow"">shlex</a> library or you may write <code>regex</code> expression. Surely, <code>split()</code> is not what you are looking for.</p>
"
39734358,674039.0,2016-09-27T21:09:52Z,39734252,4,"<p>I don't know how to do it with <code>str.format</code>.  May I propose using <a href=""https://docs.python.org/2/library/string.html#string.zfill"" rel=""nofollow""><code>str.zfill</code></a> instead?</p>

<pre><code>&gt;&gt;&gt; '-10.0040'.zfill(10)
'-0010.0040'
&gt;&gt;&gt; '10.0040'.zfill(10)
'00010.0040'
</code></pre>

<p>If you can bear converting to a number before formatting:</p>

<pre><code>&gt;&gt;&gt; from decimal import Decimal
&gt;&gt;&gt; '{:010.4f}'.format(Decimal('10.0040'))
'00010.0040'
&gt;&gt;&gt; '{:010.4f}'.format(Decimal('-10.0040'))
'-0010.0040'
</code></pre>
"
39734420,1547004.0,2016-09-27T21:15:30Z,39734252,5,"<p>You're problem is that your ""number"" is being represented as a string, so python has no way of knowing whether it's positive or negative, because it doesn't know it's a number.</p>

<pre><code>&gt;&gt;&gt; '{: 010.4f}'.format(10.0400)
' 0010.0400'
&gt;&gt;&gt; '{: 010.4f}'.format(-10.0400)
'-0010.0400'
</code></pre>

<p>This fills with <code>0</code>'s and has a fixed precision.  It will use a <code>space</code> for positive numbers and a <code>-</code> for negative.</p>

<p>You can change the behavior (i.e. <code>+</code> for positive signs, or just fill with an extra <code>0</code>) using the <a href=""https://docs.python.org/2/library/string.html#format-specification-mini-language""><code>sign</code></a> portion of the formatting token</p>
"
39734468,4379038.0,2016-09-27T21:18:29Z,39734252,2,"<p>If you can convert the string to a float, you can do this:</p>

<pre><code>&gt;&gt;&gt; '{:0=10.4f}'.format(float('-10.0040'))
'-0010.0040'
&gt;&gt;&gt; '{:0=10.4f}'.format(float('10.0040'))
'00010.0040'
</code></pre>
"
39734514,2965673.0,2016-09-27T21:22:31Z,39599192,0,"<p>An answer was posted to my <a href=""https://github.com/pydata/pandas/issues/14297"" rel=""nofollow"">bug report</a> that I wanted to share here for completeness. It is not my post, but does just what I had wanted:</p>

<p>Try this (maybe this is what interpolate should do by default, interpolating before re-sampling?)</p>

<pre><code>from scipy.interpolate import interp1d

# fit the interpolation in integer ns-space
f = interp1d(a.index.asi8, a.values)

# generating ending bins
dates = a.resample('15s', base=5).first().index

# apply
pd.Series(f(dates.asi8), dates)
Out[122]: 
2016-05-25 00:00:35    1.000000
2016-05-25 00:00:50    3.000000
2016-05-25 00:01:05    4.000000
2016-05-25 00:01:20    3.500000
2016-05-25 00:01:35    3.000000
2016-05-25 00:01:50    4.000000
2016-05-25 00:02:05    5.000000
2016-05-25 00:02:20    5.272727
2016-05-25 00:02:35    5.545455
2016-05-25 00:02:50    5.818182
2016-05-25 00:03:05    6.083333
2016-05-25 00:03:20    6.333333
2016-05-25 00:03:35    6.583333
2016-05-25 00:03:50    6.833333
2016-05-25 00:04:05    7.041667
2016-05-25 00:04:20    7.166667
2016-05-25 00:04:35    7.291667
2016-05-25 00:04:50    7.416667
2016-05-25 00:05:05    7.541667
2016-05-25 00:05:20    7.666667
2016-05-25 00:05:35    7.791667
2016-05-25 00:05:50    7.916667
Freq: 15S, dtype: float64
</code></pre>
"
39734557,2411802.0,2016-09-27T21:26:03Z,39733158,0,"<p>Each row is read as a list, so you can append to it by adding another list (e.g. the new column value).  Your current code is simply writing the new column value.</p>

<pre><code>with open('data.csv') as fin, open('out.csv', 'wb') as fout:
    reader = csv.reader(fin)
    writer = csv.writer(fout)
    for row in reader:
        new_col_value = [str(row[10]) + ' ' + str(row[11])]
        writer.writerow(row + new_col_value)
</code></pre>
"
39734612,1330616.0,2016-09-27T21:30:04Z,39733158,0,"<p>Below code snippet combines strings in column 10 and column 11 in each row and add that to the end of the each row</p>

<pre><code>import csv
input = 'test.csv'
output= 'output.csv'
with open(input, 'rb') as csvin:
    readfile = csv.reader(csvin, delimiter=',')
    with open(output, 'wb') as csvout:
        writefile = csv.writer(csvout, delimiter=',', lineterminator='\n')
        for row in readfile:
            result = row + [row[10]+row[11]]
            writefile.writerow(result)
</code></pre>
"
39734684,6392709.0,2016-09-27T21:35:37Z,39734209,1,"<p>This regex will capture the quotes, if you want them.</p>

<pre><code>import re

s = ""print 'hello, world!' 3 times""
re.findall(r'(\w+|\'.+\')',s)
</code></pre>
"
39734916,310953.0,2016-09-27T21:53:10Z,39729558,1,"<p><strong>pytest's parameterization is all about getting a fixture and holding on to it for a reasonable lifecycle</strong>.  It does not cache all of the input->output mappings. This is not what you wated here but it makes sense if you consider fixtures being things like database connections or tcp connections (like the smtp in the examples).</p>

<p>You still have a decent argument to make here that enough introspection and optimization on pytest's part would have benefited you (presuming here that run_result is very expensive and you wish to minimize runs).</p>

<p>Why does it do ""the wrong thing"" here?  If you look carefully at the fixtures, tolerance is the ""first order"" or closest parameterized fixture.  </p>

<p>An ugly, inscrutable change that ""works"":</p>

<pre><code>@pytest.fixture(scope=""session"", params=[0.01, 0.0002])
def tol(request):
    """"""Precision in floating point compare.""""""
    return request.param

@pytest.fixture(scope=""session"")
def tolerance(tol):
    """"""Precision in floating point compare.""""""
    return tol
</code></pre>

<p>Why in the world would that work?  It removed the tolerance param to the same ""level"" as the param on the other fixtures.  With this, pytest does in fact only run the run_tests twice.</p>

<pre><code>============================================ test session starts ============================================
&lt;snip&gt;
collected 4 items

test_tolerance.py::test_run_result[1-0.01] Run # 1 param: 1
run_result: 2, expected_result: 2 tolerance: 0.010000
PASSED
test_tolerance.py::test_run_result[1-0.0002]
run_result: 2, expected_result: 2 tolerance: 0.000200
PASSED
test_tolerance.py::test_run_result[2-0.0002] Run # 2 param: 2
run_result: 3, expected_result: 3 tolerance: 0.000200
PASSED
test_tolerance.py::test_run_result[2-0.01]
run_result: 3, expected_result: 3 tolerance: 0.010000
PASSED

========================================= 4 passed in 0.01 seconds ==========================================
</code></pre>

<p><strong>Should you use that code?  Please try not to</strong> as it is too hard to grok, if you use a hack like that, comment heavily to 'splain yourself.  </p>

<p>You asked ""why"" and the key here is that the params for tolerance and paramfixture are at different levels of nesting with the ""closest"" one being the one that will iterate slowest. fixtures aren't cached here, they are just used in a logical order, innermost iterating fastest.</p>
"
39735030,4403872.0,2016-09-27T22:02:09Z,39734836,1,"<pre><code>pd.concat([day.set_index('date'), df.set_index('Date')], axis=1)

&gt;&gt;&gt;

               val     Adj_Close         Close          High  \
2016-01-04  1.3970  12927.200195  12927.200195  12928.900391
2016-01-05  1.3991  12920.099609  12920.099609  12954.900391
2016-01-06  1.4084  12726.799805  12726.799805  12854.599609
2016-01-07  1.4061  12448.200195  12448.200195  12661.200195

                     Low
2016-01-04  12748.500000
2016-01-05  12839.799805
2016-01-06  12701.700195
2016-01-07  12439.099609
</code></pre>

<p>Depending on if you want an inner or outer join, you can specify that with <code>join='inner'</code> or <code>join='outer'</code>.</p>
"
39735093,5741205.0,2016-09-27T22:07:06Z,39735068,3,"<p>you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html"" rel=""nofollow"">pivot_table()</a> method, which uses <code>aggfunc='mean'</code> per-default:</p>

<pre><code>In [46]: df.pivot_table(index='A', columns='B', values='C', fill_value=0)
Out[46]:
B     Ar  Br  Cr
A
one  0.5   0   0
two  1.0   0   0
</code></pre>
"
39735113,2336654.0,2016-09-27T22:08:45Z,39735068,2,"<p>I like <code>groupby</code> and <code>unstack</code></p>

<pre><code>df.groupby(['A', 'B']).C.mean().unstack(fill_value=0)
</code></pre>

<p><a href=""http://i.stack.imgur.com/ek6hU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ek6hU.png"" alt=""enter image description here""></a></p>
"
39735501,459780.0,2016-09-27T22:47:31Z,39646097,1,"<h2>Short answer</h2>

<p>Put <code>include inc/*.hpp</code> in the <code>MANIFEST.in</code> file.</p>

<h2>Long answer</h2>

<p>Based on various blog posts and SO threads, I had tried the suggestion of declaring the files in a <code>MANIFEST.in</code> file. Following <a href=""https://docs.python.org/2/distutils/sourcedist.html"" rel=""nofollow"">these instructions</a>, I added a <code>graft inc/</code> line to <code>MANIFEST.in</code> to include the entire directory. This did not work.</p>

<p>However, replacing this line with <code>include inc/*.hpp</code> did work. Arguably this should have been the first thing I tried, but being unfamiliar with the intricacies and warts of setuptools and distutils, I had no reason to expect that <code>graft</code> wouldn't work.</p>
"
39735604,3337426.0,2016-09-27T22:59:47Z,39721639,3,"<p>I have been searching for the answer and finally I found a solution that solved my problem. </p>

<p>We Just need to add the missing schema to the imports of suds. Below is the code</p>

<pre><code>from suds.xsd.doctor import Import, ImportDoctor
imp=Import('http://www.w3.org/2001/XMLSchema',location='http://www.w3.org/2001/XMLSchema.xsd')
imp.filter.add('http://tempuri.org/')
track_client = Client(TCS_TRACK_URI, doctor=ImportDoctor(imp))
</code></pre>
"
39735754,2320035.0,2016-09-27T23:19:33Z,39735676,0,"<p>Code:</p>

<pre><code>import numpy as np
import pandas as pd

"""""" create some test-data """"""
random_data = np.random.random([3, 3])
random_data[0,0] = 0.0
random_data[1,2] = 0.0

df = pd.DataFrame(random_data,
     columns=['A', 'B', 'C'], index=['first', 'second', 'third'])

print(df)

"""""" binarize """"""
threshold = lambda x: x &gt; 0
df_ = df.apply(threshold).astype(int)

print(df_)
</code></pre>

<p>Output:</p>

<pre><code>A         B         C
first   0.000000  0.610263  0.301024
second  0.728070  0.229802  0.000000
third   0.243811  0.335131  0.863908
A  B  C
first   0  1  1
second  1  1  0
third   1  1  1
</code></pre>

<p>Remarks:</p>

<ul>
<li>get_dummies() analyze each unique value per column and introduces new columns (for each unique value) to mark if this value is active</li>
<li>= if column A has 20 unique values, 20 new columns are added, where exactly one column is true, the others are false</li>
</ul>
"
39735878,509824.0,2016-09-27T23:36:02Z,39735676,4,"<p>Casting to boolean will result in <code>True</code> for anything that is not zero &mdash; and <code>False</code> for any zero entry. If you then cast to integer, you get ones and zeroes.</p>

<pre><code>import io
import pandas as pd

data = io.StringIO('''\
word1 word2 word3
0.0   0.3   1.0
0.1   0.0   0.5
''')
df = pd.read_csv(data, delim_whitespace=True)

res = df.astype(bool).astype(int)
print(res)
</code></pre>

<p>Output:</p>

<pre><code>   word1  word2  word3
0      0      1      1
1      1      0      1
</code></pre>
"
39735879,2912340.0,2016-09-27T23:36:35Z,39735843,6,"<p>Accoring to <a href=""https://www.python.org/dev/peps/pep-0484/"">official PEP</a> to denote list of objects you should use <code>typing.List</code>, not <code>list</code> builtin.</p>

<pre><code>from typing import List


class Something:
    pass


def f(seq: List[Something]):  # no warning
    for o in seq:
        print(o)
</code></pre>
"
39736086,2357112.0,2016-09-28T00:05:49Z,39735843,1,"<p>Åukasz explained how to correct your code. I'll explain why the error message says what it does.</p>

<p><code>list</code> defines <code>__getitem__</code>, true, but that isn't what the error message is complaining about. The error message is saying that <code>type</code> itself, which is the <code>list</code> <em>type's</em> type, doesn't support <code>__getitem__</code>. For <code>list[whatever]</code> to be valid, <code>type</code> would have to define a <code>__getitem__</code> method, not <code>list</code>.</p>
"
39736116,2336654.0,2016-09-28T00:09:23Z,39735676,2,"<p>I would have answered as @Alberto Garcia-Raboso answered but here is an alternative that is very quick and leverages the same idea.</p>

<p>Use <code>np.where</code></p>

<pre><code>pd.DataFrame(np.where(df, 1, 0), df.index, df.columns)
</code></pre>

<p><a href=""http://i.stack.imgur.com/YE8F7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YE8F7.png"" alt=""enter image description here""></a></p>

<hr>

<h1>Timing</h1>

<p><a href=""http://i.stack.imgur.com/EB5Eb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EB5Eb.png"" alt=""enter image description here""></a></p>
"
39736587,674039.0,2016-09-28T01:20:08Z,39736575,1,"<p>No, this is not possible in Python.  You can not add new methods to built-in types in a way that will work reliably.</p>

<p>One thing you could do is subclass string, and define the magic method <code>__invert__</code>.  But it would not work on string literals, only on instances of your subclass.  </p>
"
39736758,1048539.0,2016-09-28T01:46:21Z,39736716,0,"<p>In your <code>Class</code> class, <code>self.stulist</code> instead of <code>Class.stulist</code>. </p>

<p>You are modifying the class itself, not the instance variable.</p>

<pre><code>class Class:
    stulist=[]
    def __init__ (self, classname, numstudents):
        self.classname=classname
        self.numstudents=numstudents
    def addStudent(self, stuNum, stuName, stuGrades):
        self.stulist.append(Student(stuName, stuGrades))
    def getPlace(self):
        print (self.stulist[0].printLn()) #printLn is function in Student 
        print (self.stulist[1].printLn())
        print (self.stulist[2].printLn())
</code></pre>

<p>References to <code>Class</code> are going to modify the actual <code>Class</code> object itself.</p>
"
39736831,674039.0,2016-09-28T01:53:55Z,39736773,2,"<p>Kicking <code>os</code> out of the modules cache can make it freshly importable again:</p>

<pre><code>&gt;&gt;&gt; import sys, os
&gt;&gt;&gt; os.chdir = ""d'oh!""
&gt;&gt;&gt; os.chdir()
TypeError: 'str' object is not callable
&gt;&gt;&gt; del sys.modules['os']
&gt;&gt;&gt; import os
&gt;&gt;&gt; os.chdir
&lt;function posix.chdir&gt;
</code></pre>
"
39736839,5015569.0,2016-09-28T01:55:44Z,39736773,1,"<pre><code>&gt;&gt;&gt; import os
</code></pre>

<p>Assign to the <code>chdir</code> method a string value:</p>

<pre><code>&gt;&gt;&gt; os.chdir = '\some path'
&gt;&gt;&gt; os.chdir
'\some path'
</code></pre>

<p>Use <code>reload</code> to, well, reload the module. <code>reload</code> will reload a <em>previously imported</em> module.</p>

<pre><code>&gt;&gt;&gt; reload(os)
&gt;&gt;&gt; os.chdir
&lt;built-in function chdir&gt;
</code></pre>
"
39736968,4447998.0,2016-09-28T02:12:07Z,39662847,4,"<p>You can register a custom encoding which prints a message whenever it's used:</p>

<p>Code in <code>ourencoding.py</code>:</p>

<pre><code>import sys
import codecs
import traceback

# Define a function to print out a stack frame and a message:

def printWarning(s):
    sys.stderr.write(s)
    sys.stderr.write(""\n"")
    l = traceback.extract_stack()
    # cut off the frames pointing to printWarning and our_encode
    l = traceback.format_list(l[:-2])
    sys.stderr.write("""".join(l))

# Define our encoding:

originalencoding = sys.getdefaultencoding()

def our_encode(s, errors='strict'):
    printWarning(""Default encoding used"");
    return (codecs.encode(s, originalencoding, errors), len(s))

def our_decode(s, errors='strict'):
    printWarning(""Default encoding used"");
    return (codecs.decode(s, originalencoding, errors), len(s))

def our_search(name):
    if name == 'our_encoding':
        return codecs.CodecInfo(
            name='our_encoding',
            encode=our_encode,
            decode=our_decode);
    return None

# register our search and set the default encoding:
codecs.register(our_search)
reload(sys)
sys.setdefaultencoding('our_encoding')
</code></pre>

<p>If you import this file at the start of our script, then you'll see warnings for implicit conversions:</p>

<pre><code>#!python2
# coding: utf-8

import ourencoding

print(""test 1"")
a = ""hello "" + u""world""

print(""test 2"")
a = ""hello âº "" + u""world""

print(""test 3"")
b = u"" "".join([""hello"", u""âº""])

print(""test 4"")
c = unicode(""hello âº"")
</code></pre>

<p>output:</p>

<pre><code>test 1
test 2
Default encoding used
 File ""test.py"", line 10, in &lt;module&gt;
   a = ""hello âº "" + u""world""
test 3
Default encoding used
 File ""test.py"", line 13, in &lt;module&gt;
   b = u"" "".join([""hello"", u""âº""])
test 4
Default encoding used
 File ""test.py"", line 16, in &lt;module&gt;
   c = unicode(""hello âº"")
</code></pre>

<p>It's not perfect as test 1 shows, if the converted string only contain ASCII characters, sometimes you won't see a warning.</p>
"
39737452,6779307.0,2016-09-28T03:16:03Z,39737300,-1,"<p>Pop the first element.  Go through each remaining element and see if the shorter string is a substring of the longer string.  Repeat.  That should be O(n log n)</p>

<p>EDIT: Rough draft of implementation</p>

<pre><code>def not_substrings(l):
    mask = [True]*len(l)
    for i in range(len(l)):
        if not mask[i]:
            continue
        for j in range(i+1, len(l)):
            if len(l[i]) &gt; len(l[j]):
                if l[j] in l[i]:
                    mask[j] = False
            elif l[j] == l[i]:
                mask[j] = False
                mask[i] = False
            else:
                if l[i] in l[j]:
                    mask[i] = False
        if mask[i]:
            print l[i]
</code></pre>

<p>I haven't run this code, but it should be roughly correct.  I don't know if there's a way of doing this without the mask, or what time complexity the <code>[True]*len(l)</code> statement has.  I haven't done any rigorous analysis, but this looks <code>n log n</code> to me, because each iteration only iterates over the remainer of the list, not the entire list.</p>
"
39737741,2225682.0,2016-09-28T03:51:50Z,39737712,2,"<p>If you want to exclude <code>c</code>, <code>d</code> (<code>None</code>s), use <code>is None</code> or <code>is not None</code>:</p>

<pre><code>attrs_present = filter(lambda x: getattr(a_obj, x, None) is not None, a_list)
# NOTE: Added the third argument `None`
#       to prevent `AttributeError` in case of missing attribute 
#       (for example, a_list = ['a', 'e'])
</code></pre>

<p>If you want to include <code>c</code>, <code>d</code>, use <a href=""https://docs.python.org/2/library/functions.html#hasattr"" rel=""nofollow""><code>hasattr</code></a>:</p>

<pre><code>attrs_present = filter(lambda x: hasattr(a_obj, x), a_list)
</code></pre>
"
39737787,4199141.0,2016-09-28T03:56:58Z,39737767,1,"<pre><code>html_body += ""&lt;tr&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;"".\form
                                                                ^
</code></pre>

<p>Right there is your problem.  The backslash is the line-continuation character that the error mentions.  Take it out.</p>
"
39737788,315828.0,2016-09-28T03:57:00Z,39737767,0,"<pre><code>html_body += ""&lt;tr&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;&lt;td&gt;{}&lt;/td&gt;"".\format(p[0],p[1],p[2],p[3])
this backslash is not needed ----------------------------------^
</code></pre>
"
39737983,4323.0,2016-09-28T04:19:39Z,39737941,3,"<p>You can easily eliminate the linear search of <code>small</code> on each iteration by using a set:</p>

<pre><code>smallset = set(small)
results = [row for row in big if row[8] in smallset]
</code></pre>
"
39737987,5382650.0,2016-09-28T04:20:11Z,39732600,0,"<p>This isn't going to work too well [as written]. However, it <em>is</em> possible, so read on ...</p>

<hr>

<p>It helps to know what the actual stack layout is when the <code>main</code> function is called. It's a bit more complicated than most people realize.</p>

<p>Assuming a POSIX OS (e.g. linux), the kernel will set the stack pointer at a fixed address.</p>

<p>The kernel does the following:</p>

<p>It calculates how much space is needed for the environment variable strings (i.e. <code>strlen(""HOME=/home/me"") + 1</code> for all environment variables and ""pushes"" these strings onto the stack in a downward [towards lower memory] direction. It then calculates how many there were (e.g. <code>envcount</code>) and creates an <code>char *envp[envcount + 1]</code> on the stack and fills in the <code>envp</code> values with pointers to the given strings. It null terminates this <code>envp</code></p>

<p>A similar process is done for the <code>argv</code> strings.</p>

<p>Then, the kernel loads the ELF interpreter. The kernel starts the process with the starting address of the ELF interpreter. The ELF interpreter [eventually] invokes the ""start"" function (e.g. <code>_start</code> from <code>crt0.o</code>) which does some init and then calls <code>main(argc,argv,envp)</code></p>

<p>This is [sort of] what the stack looks like when <code>main</code> gets called:</p>

<pre><code>""HOME=/home/me""
""LOGNAME=me""
""SHELL=/bin/sh""

// alignment pad ...

char *envp[4] = {
    // address of ""HOME"" string
    // address of ""LOGNAME"" string
    // address of ""SHELL"" string
    NULL
};

// string for argv[0] ...
// string for argv[1] ...
// ...

char *argv[] = {
    // pointer to argument string 0
    // pointer to argument string 1
    // pointer to argument string 2
    NULL
}

// possibly more stuff put in by ELF interpreter ...

// possibly more stuff put in by _start function ...
</code></pre>

<p>On an <code>x86</code>, the <code>argc</code>, <code>argv</code>, and <code>envp</code> pointer values are put into the first three argument registers of the <code>x86</code> ABI.</p>

<hr>

<p>Here's the problem [problems, plural, actually] ...</p>

<p>By the time all this is done, you have little to no idea what the address of the shell code is. So, any code you write must be RIP-relative addressing and [probably] built with <code>-fPIC</code>.</p>

<p>And, the resultant code can't have a zero byte in the middle because this is being conveyed [by the kernel] as an EOS terminated string. So, a string that has a zero (e.g. <code>&lt;byte0&gt;,&lt;byte1&gt;,&lt;byte2&gt;,0x00,&lt;byte5&gt;,&lt;byte6&gt;,...</code>) would only transfer the first three bytes and <em>not</em> the entire shell code program.</p>

<p>Nor do you have a good idea as to what the stack pointer value is.</p>

<p>Also, you need to <em>find</em> the memory word on the stack that <em>has</em> the return address in it (i.e. this is what the start function's <code>call main</code> asm instruction pushes).</p>

<p>This word containing the return address must be set to the address of the shell code. But, it doesn't always have a fixed offset relative to a <code>main</code> stack frame variable (e.g. <code>buf</code>). So, you can't predict what word on the stack to modify to get the ""return to shellcode"" effect.</p>

<p>Also, on <code>x86</code> architectures, there is special mitigation hardware. For example, a page can be marked <code>NX</code> [no execute]. This is usually done for certain segments, such as the stack. If the RIP is changed to point to the stack, the hardware will fault out.</p>

<hr>

<p>Here's the [easy] solution ...</p>

<p><code>gcc</code> has some intrinsic functions that can help: <code>__builtin_return_address</code>, <code>__builtin_frame_address</code>.</p>

<p>So, get the value of the real return address from the intrinsic [call this <code>retadr</code>]. Get the address of the stack frame [call this <code>fp</code>].</p>

<p>Starting from <code>fp</code> and incrementing (by <code>sizeof(void*)</code>) toward higher memory, find a word that matches <code>retadr</code>. This memory location is the one you want to modify to point to the shell code. It will probably be at offset 0 or 8</p>

<p>So, then do: <code>*fp = argv[1]</code> and return.</p>

<p>Note, extra steps may be necessary because if the stack has the <code>NX</code> bit set, the string pointed to by <code>argv[1]</code> is on the stack as mentioned above.</p>

<hr>

<p>Here is some example code that works:</p>

<pre><code>#define _GNU_SOURCE
#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/syscall.h&gt;

void
shellcode(void)
{
    static char buf[] = ""shellcode: hello\n"";
    char *cp;

    for (cp = buf;  *cp != 0;  ++cp);

    // NOTE: in real shell code, we couldn't rely on using this function, so
    // these would need to be the CPP macro versions: _syscall3 and _syscall2
    // respectively or the syscall function would need to be _statically_
    // linked in
    syscall(SYS_write,1,buf,cp - buf);
    syscall(SYS_exit,0);
}

int
main(int argc,char **argv)
{
    void *retadr = __builtin_return_address(0);
    void **fp = __builtin_frame_address(0);
    int iter;

    printf(""retadr=%p\n"",retadr);
    printf(""fp=%p\n"",fp);

    // NOTE: for your example, replace:
    //   *fp = (void *) shellcode;
    // with:
    //   *fp = (void *) argv[1]

    for (iter = 20;  iter &gt; 0;  --iter, fp += 1) {
        printf(""fp=%p %p\n"",fp,*fp);
        if (*fp == retadr) {
            *fp = (void *) shellcode;
            break;
        }
    }

    if (iter &lt;= 0)
        printf(""main: no match\n"");

    return 0;
}
</code></pre>
"
39738059,6884866.0,2016-09-28T04:27:56Z,39714374,0,"<p>Finally, no more NaN values. The solution is to <strong>scale</strong> my input and output data. The result (accuracy) is still not good, but at least I get some real values for the parameters. I tried feature scaling before in other attempts (where I probably had some other mistakes as well) and assumed it wouldn't help with my problem either.</p>
"
39738062,1330616.0,2016-09-28T04:28:10Z,39737941,1,"<p>A quick and easy way to do is to create a dictionary with 8 column item as the key. Below is a code snippet. </p>

<pre><code>big_dict = {}
for list in big:
    big_dict[list[-1]] = list
output_list = []
for element in small:
    output_list.append(big_dict[element])
</code></pre>
"
39738133,5701438.0,2016-09-28T04:36:00Z,39737941,-1,"<p>Binary search your small list which will reduce the time complexity to O(log n).</p>

<pre><code>smallset = sort(small)
def binarySearch(x):
   #implement binary search
   pass
results = [row for row in big if binarySearch(row[8])]
</code></pre>
"
39738563,3543300.0,2016-09-28T05:15:58Z,39738504,1,"<p>I'm not 100% sure what you're confused about exactly, but here's what I think will help, but please correct me if I'm off track. Your queue has the outputs of both processes on it. You're just getting the one that finished first.</p>

<p>For example in your case it looks like process 1 finishes first, and then process 2.</p>

<p>Thus after both processes are complete the queue will look like</p>

<pre><code>[1,2]
</code></pre>

<p>Then when you call <code>q.get()</code> you get <code>1</code> and the queue now looks like:</p>

<pre><code>`[2]`
</code></pre>

<p>Then when you call <code>q.get()</code> again you will get <code>2</code> and now the queue is empty.</p>
"
39738856,3691303.0,2016-09-28T05:41:21Z,39732600,2,"<p>There are several protections, for the attack straight from the 
compiler. For example your stack may not be executable.</p>

<p><code>readelf -l &lt;filename&gt;</code> </p>

<p>if your output contains something like this:</p>

<p><code>GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4</code></p>

<p>this means that you can only read and write on the stack ( so you should ""return to libc"" to spawn your shell).</p>

<p>Also there could be a canary protection, meaning there is a part of the memory between your variables and the instruction pointer that contains a phrase that is checked for integrity and if it is overwritten by your string the program will exit.</p>

<p>if your are trying this on your own program consider removing some of the protections with gcc commands: </p>

<p><code>gcc -z execstack</code></p>

<p>Also a note on your assembly, you usually include nops before your shell code, so you don't have to target the exact address that your shell code is starting.</p>

<p><code>$(python -c ""print '\x90'*37 +'\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x50\x53\x89\xe1\xb0\x0b\xcd\x80' + 'A'*200 + '\xc8\xf4\xff\xbf'"")</code></p>

<p>Note that in the address that should be placed inside the instruction pointer
you can modify the last hex digits to point somewhere inside your nops and not 
necessarily at the beginning of your buffer.</p>

<p>Of course <code>gdb</code> should become your best friend if you are trying something 
like that. </p>

<p>Hope this helps.</p>
"
39739495,2901002.0,2016-09-28T06:25:09Z,39739432,1,"<p>I think you need:</p>

<pre><code>print (data[0].min(axis=1))
0      3662.1
1      3660.0
2      3659.5
3      3660.0
4      3661.5
5      3662.6
6      3661.5
7      3660.0
8      3661.5
9      3662.1
10     3660.0
11     3661.0
12     3664.1
13     3664.1
14     3661.5
15     3661.0
...
...
</code></pre>

<p>Maybe beter is omit <code>flow = pd.DataFrame(data)</code> and use:</p>

<pre><code>data = [pd.read_csv(f, index_col=None, header=None) for f in temp]

mins = [df.min(axis=1) for df in data[0]]
print (mins[0])
print (mins[1])
</code></pre>
"
39739684,5847976.0,2016-09-28T06:34:42Z,39739093,1,"<p>If I understand the question correctly, you could use a list comprehension to finish reading values in v , like that :</p>

<pre><code>transaction = [ x[1] for x in v ]
</code></pre>

<p>That code will be like grabing all remaining  <code>next(v)[1]</code> until the end of v.</p>

<p><strong>Side note</strong> : calling next(v) all the time is quite ugly and unpractical, you could instead start by converting <code>v</code> to a list and then use simple list slicing to get what you want :</p>

<pre><code> v = list(v)
 id_, name = v[0]
 add_date_1, add_date_2 = [x[1] for x in v[1:3]]
 ...
 transaction = [ x[1] for x in v[1234:] ]
</code></pre>
"
39739820,4165405.0,2016-09-28T06:41:36Z,39739029,0,"<p>As the two fields <code>ch1</code> and <code>ch2</code> are always required in your case, all you need to do is modify your model such that </p>

<pre><code>from django.db import models

class Essai(models.Model):
    ch1 = models.CharField(max_length=100)
    ch2 = models.CharField(max_length=100)
</code></pre>

<p>and include the fields in the form as</p>

<pre><code>class EssaiModelForm(forms.ModelForm):
class Meta:
    model = Essai
    fields = ['ch1', 'ch2']
</code></pre>

<p>Django will automatically raise a validation error for this fields on <code>is_valid()</code></p>

<p>Or if you want to disallow empty string inputs to these fields <code>blank=False</code> will not work</p>

<p>Refer this <a href=""http://stackoverflow.com/questions/6194988/django-how-to-set-blank-false-required-false"">Django - how to set blank = False, required = False</a></p>
"
39739842,6394138.0,2016-09-28T06:42:45Z,39739697,0,"<p>This means that the function performs computations (and most probably contains a loop).</p>

<p>Example:</p>

<pre><code>def foo(n):
    s = 0
    for i in range(n):
        s += i
    print(s)
    return s
</code></pre>

<p><code>print()</code> is a called by <code>foo()</code>, but for a large enough <code>n</code> more time is spent in the <code>for</code> loop (and thus in <code>foo()</code> proper) than in <code>print()</code>. Thus <code>cumtime</code> of <code>foo()</code> should significantly exceed <code>cumtime</code> of <code>print()</code>.</p>
"
39739858,3510736.0,2016-09-28T06:43:27Z,39739697,1,"<p>That actually makes a lot of sense: the difference is the time the code in the function, excluding the calls it makes, takes. For example:</p>

<pre><code>def foo():
    for i in range(99999):
        print 'hello'
    bar()
    baz()
</code></pre>

<p>The cumulative time of <code>foo</code> will be much larger than the sums of times of <code>bar()</code> and <code>baz</code> - it has to do the loop.</p>
"
39739906,3156275.0,2016-09-28T06:46:13Z,39739029,0,"<p>There is a slightly notable difference in model and it's forms. Model.save() performs no validation. So if it fails then probably it raises a database level IntegrityError. For reducing risks of failure at database layer one need to call Model.full_clean() which is also done by ModelForm as documented here in <a href=""https://docs.djangoproject.com/en/1.10/ref/models/instances/#validating-objects"" rel=""nofollow"">django's validating objects</a>. </p>

<p>In my opinion, I expect from django that ModelForms are meant to validate and save data entered by client. Model instantiated programmatically should follow the same control before saving into database.</p>
"
39740629,5847976.0,2016-09-28T07:21:33Z,39740109,0,"<p>The issue here is that you are not actually plotting on the created axes , you are just replacing the content of your list <code>ax</code>.
I'm not very familiar with the object oriented interface for matplotlib but the right syntax would look more like <code>ax[i,j].plot()</code> rather than <code>ax[i] = plot()</code>.
Now the only real solution I can provide is using functional interface, like that :</p>

<pre><code>def boxplotscatter(data):
    f, ax = plt.subplots(2, 1, figsize = (12,6))
    plt.subplot(211)
    scatter_matrix(data, alpha = 0.2, figsize = (6,6), diagonal = 'kde')  
    plt.subplot(212)
    data.boxplot()
</code></pre>
"
39740912,1987319.0,2016-09-28T07:34:26Z,39740814,1,"<p>If it should just be bound by ( and ) you can use the following regex, which ensures starting ( and closing ) and you can have numbers and characters between them. You can add any other symbol also that you want to include.</p>

<p><code>[\(][a-z A-Z 0-9]*[\)]</code></p>

<pre><code>[\(] - starts the bracket
[a-z A-Z 0-9]* - all text inside bracket
[\)] - closes the bracket
</code></pre>

<p>So for input <code>sdfsdfdsf(sdfdsfsdf)sdfsdfsdf</code> , the output will be <code>(sdfdsfsdf)</code>
Test this regex here: <a href=""https://regex101.com/"" rel=""nofollow"">https://regex101.com/</a></p>
"
39740955,5210539.0,2016-09-28T07:36:38Z,39740475,0,"<p>You can store the equations in a dictionary - you will be mapping index of the equation (1, 2, etc) to a tuple, which will contain two items representing two sides of the equation. </p>

<pre><code>equations = dict()
equations[1]  = (pi1  * q[0+1], pi0 * r[0+1])
</code></pre>

<p>You can then call <code>solve(equations[1])</code> and in the <code>solve()</code> fuction do something like this:</p>

<pre><code>def solve(equation):
    left_side_of_equation = equation[0]
    right_side_of_equation = equation[1]
    # Here do everything you need with this equation values.
</code></pre>

<p>Or call function like this: <code>solve(equations[1][0], equations[1][1])</code> and have two parameters in the <code>solve()</code> function.</p>

<p>Edit to answer comment:</p>

<p>If you want to have more than equations with just <code>==</code> (greater, greater equal etc.) you need to save this information in the tuple as well. It can look like this:</p>

<pre><code>equations[42] = (pi1, 0, ""ge"")
</code></pre>

<p>and implement <code>solve()</code> function like this:</p>

<pre><code>def solve(equation):
    left_side_of_equation = equation[0]
    right_side_of_equation = equation[1]
    operator = equation[2]
    if operator == ""ge"": # Greater or equal
        # Do your comparison
    elif operator == ""g"": # Greater
        # Do your comparison
    # Add all remaining possibilities.
</code></pre>
"
39741069,3029117.0,2016-09-28T07:42:37Z,39736330,2,"<p><strong>TL;DR</strong> running HTTP requests are stopped when Ctrl+C is hit on the django dev server</p>

<p>I thought your question is really interesting and investigated:</p>

<p>I made a view that takes 10 seconds to execute and after that sends a response.
To test for your behaviour I stopped the development-server <code>manage.py runserver</code> using Ctrl+C and checked for the results.</p>

<p>My base test:</p>

<pre><code>class TestView ( generic.View ):
    def get ( self, request ):
        import time
        time.sleep(10)
        response = HttpResponse('Done.')
        return response
</code></pre>

<ul>
<li>Normal execute (10s runtime): Displays the msg <code>Done.</code></li>
<li>interrupted execute (Ctrl+C while the request is running): Browser error, the host cannot be reached</li>
</ul>

<p>so far everything as expected. But I played around a little bit, because Ctrl+C in python is <strong>not a full stop</strong>, but actually handled rather conveniently: As soon as Ctrl+C is hit, a <code>KeyboardInterrupt</code> aka <strong>an Exception is risen</strong> (equivalent to this):</p>

<pre><code>raise KeyboardInterrupt()
</code></pre>

<p>so in your command-line based programm you can put the following:</p>

<pre><code>try:
    some_action_that_takes_a_while()
except KeyboardInterrupt:
    print('The user stopped the programm.')
</code></pre>

<p>ported to django the new view looks like that:</p>

<pre><code>def get ( self, request ):
    import time
    slept_for = 0
    try:
        for i in range( 100 ):
            slept_for += 0.1
            time.sleep( 0.1 )
    except KeyboardInterrupt:
        pass

    response = HttpResponse( 'Slept for: ' + str( slept_for ) + 's' )
    return response
</code></pre>

<ul>
<li>Normal execute (10s runtime): Displays the msg <code>Slept for: 10s</code></li>
<li>interrupted execute (Ctrl+C while the request is running): Browser error, the host cannot be reached</li>
</ul>

<p>so no change in behaviour here. out of interest i changed one line, but the result didn't change; i used</p>

<pre><code>slept_for = 1000*1000
</code></pre>

<p>instead of </p>

<pre><code>time.sleep( 0.1 )
</code></pre>

<p>so to finally answer your question: on Ctrl+C the dev server shuts down immediately and <em>running http-requets</em> are <strong>not finished</strong>.</p>
"
39741169,209629.0,2016-09-28T07:47:55Z,39740814,0,"<blockquote>
  <p>I'm learning Python</p>
</blockquote>

<p>If you are learning you should consider alternative implementations, not only regexps. </p>

<p>TO iterate line by line of a text file you just open the file and for over the file handle:</p>

<pre><code>with open('file.txt') as f:
    for line in f:
        do_something(line)
</code></pre>

<p>Each line is a string with the line contents, including the end-of-line char '/n'. To find the start index of a specific substring in a string you can use find:</p>

<pre><code>&gt;&gt;&gt; A = ""hello (world)""
&gt;&gt;&gt; A.find('(')
6
&gt;&gt;&gt; A.find(')')
12
</code></pre>

<p>To get a substring from the string you can use the slice notation in the form:</p>

<pre><code>&gt;&gt;&gt; A[6:12]
'(world'
</code></pre>
"
39741253,476.0,2016-09-28T07:52:05Z,39740632,1,"<p>The bigger issue is that your types aren't sane to begin with. <code>MyMixin</code> makes a hardcoded assumption that it will be mixed into <code>Main</code>, whereas it could be mixed into any number of other classes, in which case it would probably break. If your mixin is hardcoded to be mixed into one specific class, you may as well write the methods directly into that class instead of separating them out.</p>

<p>To properly do this with sane typing, <code>MyMixin</code> should be coded against an <em>interface</em>, or abstract class in Python parlance:</p>

<pre><code>import abc


class MixinDependencyInterface(abc.ABC):
    @abc.abstractmethod
    def foo(self):
        pass


class MyMixin:
    def func2(self: MixinDependencyInterface, xxx):
        self.foo()  # â mixin only depends on the interface


class Main(MixinDependencyInterface, MyMixin):
    def foo(self):
        print('bar')
</code></pre>
"
39741286,2696355.0,2016-09-28T07:53:47Z,39740814,0,"<p>You should use regular expressions which are implemented in the <a href=""https://docs.python.org/2/library/re.html"" rel=""nofollow"">Python re module</a></p>

<p>a simple regex like <code>\(.*\)</code> could match your ""parenthesis string""
but it would be better with a group <code>\((.*)\)</code> which allows to get only the content in the parenthesis.</p>

<pre><code>import re

test_string = """"""cow.jpg : jphide[v5](asdfl;kj88876)
fish.jpg : jphide[v5](65498ghjk;0-)
snake.jpg : jphide[v5](poi098*/8!@#)
test_practice_0707.jpg : jphide[v5](sJ*=tT@&amp;Ve!2)
test_practice_0101.jpg : jphide[v5](nKFdFX+C!:V9)
test_practice_0808.jpg : jphide[v5](!~rFX3FXszx6)
test_practice_0202.jpg : jphide[v5](X&amp;aC$|mg!wC2)
test_practice_0505.jpg : jphide[v5](pe8f%yC$V6Z3)
dog.jpg : negative`""""""

REGEX = re.compile(r'\((.*)\)', re.MULTILINE)

print(REGEX.findall(test_string))
# ['asdfl;kj88876', '65498ghjk;0-', 'poi098*/8!@#', 'sJ*=tT@&amp;Ve!2', 'nKFdFX+C!:V9' , '!~rFX3FXszx6', 'X&amp;aC$|mg!wC2', 'pe8f%yC$V6Z3']
</code></pre>
"
39742223,4453460.0,2016-09-28T08:41:24Z,39663415,1,"<p>As you noticed in your comment, the problem is that <strong>the datapusher is trying to connect to the wrong port</strong>:</p>

<blockquote>
  <p>ConnectionError: HTTPConnectionPool(host='default.ckan.com', <strong>port=80</strong>): Max retries exceeded with url: /api/3/action/resource_show (Caused by : [Errno 111] Connection refused)</p>
</blockquote>

<p>You already found a possible workaround: changing CKAN's port to 80.</p>

<p>As an alternative workaround, I found that adding the port (the same one configured with <code>port</code> in the <code>[server:main]</code> section) to <code>ckan.site_url</code> makes the datapusher use it instead of the default HTTP port (80). So, in your case it could be:</p>

<pre><code>ckan.site_url = http://default.ckan.com:5000
</code></pre>

<p>Of course, this is just another possible workaround; the <em>real</em> solution would be to fix the datapusher so that it correctly reads the port from the configuration ...</p>
"
39742334,2141635.0,2016-09-28T08:46:31Z,39738525,0,"<p>As per the [docs][1, you need to add two more arguments to the tuple, filename and the content type:</p>

<pre><code>#         filed name         filename    file object      content=type
files = {'location[logo]': (""name.png"", open(fileinput),'image/png')}
</code></pre>

<p>You can see a sample an example  below:</p>

<pre><code>In [1]: import requests

In [2]: files = {'location[logo]': (""foo.png"", open(""/home/foo.png""),'image/png')}

In [3]: 

In [3]: ses = requests.session()

In [4]: res = ses.put(""http://httpbin.org/put"",files=files)

In [5]: print(res.request.body[:200])
--0b8309abf91e45cb8df49e15208b8bbc
Content-Disposition: form-data; name=""location[logo]""; filename=""foo.png""
Content-Type: image/png

ï¿½PNG

IHDRï¿½ï¿½:dï¿½tEXtSoftw
</code></pre>

<p>For future reference, <a href=""https://github.com/kennethreitz/requests/issues/1495"" rel=""nofollow"">this comment</a> in a old related issue explains all variations:</p>

<pre><code># 1-tuple (not a tuple at all)
{fieldname: file_object}

# 2-tuple
{fieldname: (filename, file_object)}

# 3-tuple
{fieldname: (filename, file_object, content_type)}

# 4-tuple
{fieldname: (filename, file_object, content_type, headers)}
</code></pre>
"
39744460,6404790.0,2016-09-28T10:11:36Z,39672565,1,"<p>Is there any chance that the category name contains spaces?</p>

<p>Just a tip: You are not using good practice in your code. IMO you should get your javascript code outside of the forloop and remove <code>{{ category_name }}</code> classes. 
<code>catindexlistitem</code> on click should toggle <strong>hidden</strong> class (i noticed you use bootstrap) to it's child ul. </p>

<p>By adding a more generic event listener you simplify your code and by using css you improve performance. In case you want to add effects you still can with css3.</p>
"
39744519,4118756.0,2016-09-28T10:13:39Z,39739644,2,"<p>You can simply hack the x-axis tick labels to achieve what you want.</p>

<pre><code>ticks = myplot.xaxis.get_ticklabels()
new_ticks = ['\n'.join(t.get_text()[1:-1].split(', ')) for t in ticks]
myplot.xaxis.set_ticklabels(new_ticks)
</code></pre>

<p><a href=""http://i.stack.imgur.com/VQ1XE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VQ1XE.png"" alt=""enter image description here""></a></p>
"
39744628,5851016.0,2016-09-28T10:18:01Z,39672565,0,"<p>Try writing the javascript function after the outer for loop, sometimes this might overlap and redirect. And also try providing spaces between the names of category inside ""li"".</p>
"
39745007,5267919.0,2016-09-28T10:35:29Z,39672565,0,"<p>My advice would be to clean up your JS by making a single function to handle all the clicks. You are already using class on clicks, so why not have one function handle all the clicks?</p>

<pre><code>&lt;script&gt;
  $(function() {
    // Hide all elements with a class starting with catlistforum
    $('[class^=""catlistforum""]').hide();

    // Assign clicks to all links whose parent has a class starting with catlistforum
    // (Though why are you hiding the parents? How will they click the links?)
    $('[class^=""catlistforum""] a').on(""click"", function(e) {
      e.preventDefault();

      // Delegate slideToggle to each click target
      $(e.target).slideToggle();

      // This seems redundant, but hopefully it will behave the way you want it to behave
      if(!($(e.target).parent('li').siblings('div').children('ul').children('div').is("":visible""))) {
        $(e.target).parent('li').siblings('div').children('ul').children('div').is("":visible"").slideToggle();
      }
    });
  })
&lt;/script&gt;
</code></pre>

<p>Of course, if I were you, I would just define two new classes (e.g. <code>catlistforum-hide</code> and <code>catlistforum-toggle</code>) which I would apply to all the elements I wanted to hide and toggle, respectively.</p>
"
39745938,205580.0,2016-09-28T11:17:58Z,39736901,5,"<p>To use Unicode in the Windows console for Python 2.7 and 3.x (prior to 3.6), install and enable <a href=""https://pypi.python.org/pypi/win_unicode_console"">win_unicode_console</a>. This uses the wide-character functions <a href=""https://msdn.microsoft.com/en-us/library/ms684958""><code>ReadConsoleW</code></a> and <a href=""https://msdn.microsoft.com/en-us/library/ms687401""><code>WriteConsoleW</code></a>, just like other Unicode-aware console programs such as cmd.exe and powershell.exe. For Python 3.6, a new <code>io._WindowsConsoleIO</code> raw I/O class has been added. It reads and writes UTF-8 encoded text (for cross-platform compatibility with Unix -- ""get a byte"" -- programs), but internally it uses the wide-character API by transcoding to and from UTF-16LE. </p>

<p>The problem you're experiencing with non-ASCII input is reproducible in the console for all Windows versions up to and including Windows 10. The console host process, i.e. conhost.exe, wasn't designed for UTF-8 (codepage 65001) and hasn't been updated to support it consistently. In particular, non-ASCII input causes an empty read. This in turn causes Python's REPL to exit and built-in <code>input</code> to raise <code>EOFError</code>. </p>

<p>The problem is that conhost encodes its UTF-16 input buffer assuming a single-byte codepage, such as the OEM and ANSI codepages in Western locales (e.g. 437, 850, 1252). UTF-8 is a multibyte encoding in which non-ASCII characters are encoded as 2 to 4 bytes. To handle UTF-8 it would need to encode in multiple iterations of <code>M / 4</code> characters, where M is the remaining bytes available from the N-byte buffer. Instead it assumes a request to read N bytes is a request to read N characters. Then if the input has one or more non-ASCII characters, the internal <code>WideCharToMultiByte</code> call fails due to an undersized buffer, and the console returns a 'successful' read of 0 bytes.</p>

<p>You may not observe exactly this problem in Python 3.5 if the pyreadline module is installed. Python 3.5 automatically tries to import <code>readline</code>. In the case of pyreadline, input is read via the wide-character function <a href=""https://msdn.microsoft.com/en-us/library/ms684961""><code>ReadConsoleInputW</code></a>. This is a low-level function to read console input records. In principle it should work, but in practice entering <code>print('Ã¤')</code> gets read by the REPL as <code>print('')</code>. For a non-ASCII character, <code>ReadConsoleInputW</code> returns a sequence of Alt+Numpad <code>KEY_EVENT</code> records. The sequence is a lossy OEM encoding, which can be ignored except for the last record, which has the input character in the <code>UnicodeChar</code> field. Apparently pyreadline ignores the entire sequence.</p>

<p>Prior to Windows 8, output using codepage 65001 is also broken. It prints a trail of garbage text in proportion to the number of non-ASCII characters. In this case the problem is that <code>WriteFile</code> and <code>WriteConsoleA</code> incorrectly return the number of UTF-16 codes written to the screen buffer instead of the number of UTF-8 bytes. This confuses Python's buffered writer, leading to repeated writes of what it thinks are the remaining unwritten bytes. This problem was fixed in Windows 8 as part of rewriting the internal console API to use the ConDrv device instead of an LPC port. Older versions of Windows can use ConEmu or ANSICON to work around this bug.</p>
"
39745979,978961.0,2016-09-28T11:20:36Z,39738423,0,"<p>The problem is how you are creating the list. Right now you're giving it strings that identify the items you want to output. Instead, give it what you want to output. </p>

<pre><code>ABC = [row[0], row[1], row[2]]
</code></pre>

<p>Edit: Since you are trying to print all of the columns in the template you can just iterate over them there. </p>

<pre><code>{% for column in row %}
    &lt;td&gt;{{ column }}&lt;/td&gt;
{% endfor %}
</code></pre>
"
39746080,6463630.0,2016-09-28T11:25:00Z,39740109,1,"<p>I think you just need to pass the axis as an argument of your plotting function. </p>

<pre><code>f, ax = plt.subplots(2, 1, figsize = (12,6))
def boxplotscatter(data, ax):
    ax[0] = scatter_matrix(data, alpha = 0.2, figsize = (6,6), diagonal = 'kde')
    ax[1] = data.boxplot()
</code></pre>
"
39746217,6758673.0,2016-09-28T11:32:04Z,39742127,1,"<p>The approach based on sorting has merit, but can be done a lot simpler:</p>

<pre><code>def pair_counts(a, b):
    n = a.shape[0]  # also b.shape[0]

    counts_a = np.bincount(a)
    counts_b = np.bincount(b)
    sorter_a = np.argsort(a)

    n11 = 0
    same_a_offset = np.cumsum(counts_a)
    for indices in np.split(sorter_a, same_a_offset):
        b_check = b[indices]
        n11 += np.count_nonzero(b_check == b_check[:,None])

    n11 = (n11-n) // 2
    n10 = (np.sum(counts_a**2) - n) // 2 - n11
    n01 = (np.sum(counts_b**2) - n) // 2 - n11
    n00 = n**2 - n - n11 - n10 - n01

    return n11, n10, n01, n00
</code></pre>

<p>If this method is coded efficiently in Cython there's another speedup (probably ~20x) to be gained.</p>

<hr>

<p>Edit:</p>

<p>I found a way to completely vectorize the procedure and lower the time complexity:</p>

<pre><code>def sizes2count(a, n):
    return (np.inner(a, a) - n) // 2

def pair_counts_vec_nlogn(a, b):
    # Size of ""11"" clusters (a[i]==a[j] &amp; b[i]==b[j])
    ab = a * b.max() + b  # make sure max(a)*max(b) fits the dtype!
    _, sizes = np.unique(ab, return_counts=True)

    # Calculate the counts for each type of pairing
    n = len(a)  # also len(b)
    n11 = sizes2count(sizes, n)
    n10 = sizes2count(np.bincount(a), n) - n11
    n01 = sizes2count(np.bincount(b), n) - n11
    n00 = n**2 - n - n11 - n10 - n01

    return n11, n10, n01, n00

def pair_counts_vec_linear(a, b):
    # Label ""11"" clusters (a[i]==a[j] &amp; b[i]==b[j])
    ab = a * b.max() + b

    # Calculate the counts for each type of pairing
    n = len(a)  # also len(b)
    n11 = sizes2count(np.bincount(ab), n)
    n10 = sizes2count(np.bincount(a), n) - n11
    n01 = sizes2count(np.bincount(b), n) - n11
    n00 = n**2 - n - n11 - n10 - n01

    return n11, n10, n01, n00
</code></pre>

<p>Sometimes the O(n log(n)) algorithm is faster than the linear one, because the linear one uses <code>max(a)*max(b)</code> storage. Naming can probably be improved, I'm not really familiar with the terminology.</p>
"
39747414,5483526.0,2016-09-28T12:23:22Z,39742127,1,"<p>To compare two clusterings <code>A</code> and <code>B</code> in linear time:</p>

<ol>
<li>Iterate through clusters in <code>A</code>.  Let the size of each cluster be <code>a_i</code>. The total number of pairs in the same cluster in <code>A</code> is the total of all <code>a_i*(a_i-1)/2</code>.</li>
<li>Partition each A-cluster according to its cluster in <code>B</code>.  Let the size of each partition be <code>b_j</code>.  The total number of pairs in the same cluster in both <code>A</code> and <code>B</code> is the total of all <code>b_j *(b_j-1)/2</code>.</li>
<li>The difference between the two is the total number of pairs that are in the same cluster in A but not B</li>
<li>Iterate through the custers in <code>B</code> to get the total number of pairs in the same cluster in <code>B</code>, and subtract the result from (2) to get pairs in the same cluster in <code>B</code> but not <code>A</code>.</li>
<li>The sum of the above 3 results is the number of pairs that are the same in either A or B.  Subtract from n*(n-1)/2 to get the total number of pairs that are in different clusters in A and B</li>
</ol>

<p>The partitioning in step (2) is done by making a dictionary mapping item -> cluster for B and then looking up each item in each A-cluster.  If you're cross-comparing lots of clusterings, you can save a lot of time by computing these maps just once for each clustering and keeping them around.</p>
"
39748135,3236102.0,2016-09-28T12:52:15Z,39662847,2,"<p>What you can do is the following:</p>

<p>First create a custom encoding. I will call it ""lascii"" for ""logging ASCII"":</p>

<pre><code>import codecs
import traceback

def lascii_encode(input,errors='strict'):
    print(""ENCODED:"")
    traceback.print_stack()
    return codecs.ascii_encode(input)


def lascii_decode(input,errors='strict'):
    print(""DECODED:"")
    traceback.print_stack()
    return codecs.ascii_decode(input)

class Codec(codecs.Codec):
    def encode(self, input,errors='strict'):
        return lascii_encode(input,errors)
    def decode(self, input,errors='strict'):
        return lascii_decode(input,errors)

class IncrementalEncoder(codecs.IncrementalEncoder):
    def encode(self, input, final=False):
        print(""Incremental ENCODED:"")
        traceback.print_stack()
        return codecs.ascii_encode(input)

class IncrementalDecoder(codecs.IncrementalDecoder):
    def decode(self, input, final=False):
        print(""Incremental DECODED:"")
        traceback.print_stack()
        return codecs.ascii_decode(input)

class StreamWriter(Codec,codecs.StreamWriter):
    pass

class StreamReader(Codec,codecs.StreamReader):
    pass

def getregentry():
    return codecs.CodecInfo(
        name='lascii',
        encode=lascii_encode,
        decode=lascii_decode,
        incrementalencoder=IncrementalEncoder,
        incrementaldecoder=IncrementalDecoder,
        streamwriter=StreamWriter,
        streamreader=StreamReader,
    )
</code></pre>

<p>What this does is basically the same as the ASCII-codec, just that it prints a message and the current stack trace every time it encodes or decodes from unicode to lascii.</p>

<p>Now you need to make it available to the codecs module so that it can be found by the name ""lascii"". For this you need to create a search function that returns the lascii-codec when it's fed with the string ""lascii"". This is then registered to the codecs module:</p>

<pre><code>def searchFunc(name):
    if name==""lascii"":
        return getregentry()
    else:
        return None

codecs.register(searchFunc)
</code></pre>

<p>The last thing that is now left to do is to tell the sys module to use 'lascii' as default encoding:</p>

<pre><code>import sys
reload(sys) # necessary, because sys.setdefaultencoding is deleted on start of Python
sys.setdefaultencoding('lascii')
</code></pre>

<p><strong>Warning:</strong>
This uses some deprecated or otherwise unrecommended features. It might not be efficient or bug-free. Do not use in production, only for testing and/or debugging.</p>
"
39748141,771848.0,2016-09-28T12:52:39Z,39748092,1,"<p>You can locate the text node and get the <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#next-sibling-and-previous-sibling"" rel=""nofollow"">next sibling</a>:</p>

<pre><code>In [1]: from bs4 import BeautifulSoup

In [2]: data = """"""&lt;i class=""fa fa-circle align-middle font-80"" style=""color: #45C414; margin-right: 15px""&gt;&lt;/i&gt;Departu
   ...: re for &lt;a href=""/en/ais/details/ports/17787/port_name:TEKIRDAG/_:3525d580eade08cfdb72083b248185a9"" title=""Vie
   ...: w details for: TEKIRDAG""&gt;TEKIRDAG&lt;/a&gt; &lt;/td&gt;""""""
   ...:     

In [3]: soup = BeautifulSoup(data, ""html.parser"")

In [4]: soup.find(text=""Departure for "").next_sibling.get_text()
Out[4]: u'TEKIRDAG'
</code></pre>
"
39748722,4723732.0,2016-09-28T13:17:45Z,39748267,-2,"<p>PYTHONPATH is environmental variable which you can set how ever you want,to add additional directories.You should not install Python packages manually,use <code>pip</code>.On older Ubuntu,probably you have manually installed modules before the upgrade.</p>
"
39748989,509824.0,2016-09-28T13:29:35Z,39748916,0,"<pre><code>from operator import itemgetter

a = [[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968], [12587982, 0.88], [12587984, 0.8484848484848485], [12587992, 0.7777777777777778], [12587995, 0.8070175438596491], [12588015, 0.4358974358974359], [12588023, 0.8985507246376812], [12588037, 0.5555555555555555], [12588042, 0.9473684210526315]]

max(a, key=itemgetter(1))[0]
// =&gt; 12588042
</code></pre>
"
39749005,1903116.0,2016-09-28T13:30:00Z,39748916,7,"<p>Use the <a href=""https://docs.python.org/3/library/functions.html#max"" rel=""nofollow""><code>max</code></a> function and its <code>key</code> parameter, to use only the second element to compare elements of the list.</p>

<p>For example,</p>

<pre><code>&gt;&gt;&gt; data = [[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968].... [12588042, 0.9473684210
526315]]
&gt;&gt;&gt; max(data, key=lambda item: item[1])
[12588042, 0.9473684210526315]
</code></pre>

<p>Now, if you want just the first element, then you can simply get the first element alone, or just unpack the result, like this</p>

<pre><code>&gt;&gt;&gt; index, value = max(data, key=lambda item: item[1])
&gt;&gt;&gt; index
12588042
&gt;&gt;&gt; value
0.9473684210526315
</code></pre>

<hr>

<p>Edit: If you want to find the maximum index (first value) out of all elements with the maximum value (second value), then you can do it like this</p>

<pre><code>&gt;&gt;&gt; _, max_value = max(data, key=lambda item: item[1])
&gt;&gt;&gt; max(index for index, value in data if value == max_value)
</code></pre>

<p>You can do the same in a single iteration, like this</p>

<pre><code>max_index = float(""-inf"")
max_value = float(""-inf"")

for index, value in data:
      if value &gt; max_value:
          max_value = value
          max_index = index
      elif value == max_value:
          max_index = max(max_index, index)
</code></pre>
"
39749019,6779307.0,2016-09-28T13:30:28Z,39748916,3,"<p>Use <code>max</code> with a key.</p>

<pre><code>l = [[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968], [12587982, 0.88], [12587984, 0.8484848484848485], [12587992, 0.7777777777777778], [12587995, 0.8070175438596491], [12588015, 0.4358974358974359], [12588023, 0.8985507246376812], [12588037, 0.5555555555555555], [12588042, 0.9473684210526315]]
max_sub = max(l, key=lambda x: x[1])
max_val = max_sub[1]
max_index = max_sub[0]
</code></pre>
"
39749148,3224757.0,2016-09-28T13:35:30Z,39748916,-1,"<p>Simple </p>

<pre><code>list = [[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968], [12587982, 0.88], [12587984, 0.8484848484848485], [12587992, 0.7777777777777778], [12587995, 0.8070175438596491], [12588015, 0.4358974358974359], [12588023, 0.8985507246376812], [12588037, 0.5555555555555555], [12588042, 0.9473684210526315]]
list2 = []

for x in list:
    list2.append(x[1])
print ""index-&gt;"" + str(list[list2.index(max(list2))][0])
print ""max value-&gt;"" + str(list[list2.index(max(list2))][1])
</code></pre>
"
39749397,246534.0,2016-09-28T13:46:25Z,39749242,1,"<p>Because <code>self</code> isn't working the way you think it is. <code>self</code> is just another local variable: assigning to it inside <code>pop()</code> won't change the object into another thing. See <a href=""http://stackoverflow.com/questions/28531939/python-assignment-to-self-in-constructor-does-not-make-object-the-same"">this question</a> for more details.</p>
"
39749420,5813357.0,2016-09-28T13:47:19Z,39749242,0,"<p>It's moslty because you can't change self directly.<br>
If you think about pointers, you can't change the pointer address, except if you use a pointer on this pointer. Here, if you consider self as a pointer, when you assign another value to self, you don't really change the self pointer.</p>

<p><a href=""http://stackoverflow.com/a/1216361/5813357"">See this answer</a></p>

<p>The second code ""works"" (not in all cases), because you aren't changing self itself, but the references on which it's pointing. Then your instance is updated to remove its old value and update itself with the next value.</p>
"
39749442,2901002.0,2016-09-28T13:47:59Z,39749152,2,"<p>I think you can use <code>parse_cols</code> for parse first column and then filter out all columns from 205 to 1000 by <code>skiprows</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html"" rel=""nofollow""><code>read_excel</code></a>:</p>

<pre><code>df = pd.read_excel('test.xls', 
                   sheet_name='Sheet1', 
                   parse_cols=0, 
                   skiprows=list(range(205,1000)))
print (df)
</code></pre>

<p>Last use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.tolist.html"" rel=""nofollow""><code>tolist</code></a> for convert first column to <code>list</code>:</p>

<pre><code>print({""1"": df.iloc[:,0].tolist()})
</code></pre>

<hr>

<p>The simpliest solution is parse only first column and then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html"" rel=""nofollow""><code>iloc</code></a>:</p>

<pre><code>df = pd.read_excel('test.xls', 
                   parse_cols=0)

print({""1"": df.iloc[:206,0].astype(str).tolist()})
</code></pre>
"
39749520,4177078.0,2016-09-28T13:51:02Z,39749152,1,"<p>I am not familiar with excel, but pandas could easily handle this problem.</p>

<p>First, read the excel to a DataFrame</p>

<pre><code>import pandas as pd
df = pd.read_excel(filename)
</code></pre>

<p>Then, print as you like</p>

<pre><code>print({""1"": list(df.iloc[0:N]['A'])})
</code></pre>

<p>where <code>N</code> is the amount you would like to print. That is it. If the list is not a string list, you need to cast the int to string.</p>

<p>Also, there are a lot parameters that can control the load part of excel <code>read_excel</code>, you can go through the document to set suitable parameters.</p>

<p>Hope this would be helpful to you.</p>
"
39749733,5987688.0,2016-09-28T13:58:52Z,39748916,0,"<pre><code>allData = [[12587961, 0.7777777777777778], [12587970, 0.5172413793103449], [12587979, 0.3968253968253968], [12587982, 0.88], [12587984, 0.8484848484848485], [12587992, 0.7777777777777778], [12587995, 0.8070175438596491], [12588015, 0.4358974358974359], [12588023, 0.8985507246376812], [12588037, 0.5555555555555555], [12588042, 0.9473684210526315]]

listOfSecondData = [i[1] for i in allData]
result = allData[listOfSecondData.index(max(listOfSecondData))][0]

print(result)
#Output: 12588042
</code></pre>
"
39749744,772649.0,2016-09-28T13:59:34Z,39748976,1,"<p>You can fill zeros after the <code>merge</code>:</p>

<pre><code>res = pd.merge(A, B, how=""outer"")
res.loc[~res.key.isin(A.key), A.columns] = 0
</code></pre>

<p><strong>EDIT</strong></p>

<p>to skip <code>key</code> column:</p>

<pre><code>res.loc[~res.key.isin(A.key), A.columns.drop(""key"")] = 0
</code></pre>
"
39750657,6582873.0,2016-09-28T14:38:47Z,39748267,-1,"<p>You can set the <code>PYTHONPATH</code> to content of <code>Ubuntu A</code></p>

<pre><code>env PYTHONPATH=""/usr/local/bin:/usr/lib/python2.7:/usr/lib/python2.7/plat-x86_64-linux-gnu:..."" python
</code></pre>
"
39750977,1643939.0,2016-09-28T14:51:45Z,39731669,1,"<p><code>std</code> is no Keras layer so it does not satisfy the layer input/output shape interface. The solution to this is to use a <a href=""https://keras.io/layers/core/#lambda"" rel=""nofollow""><code>Lambda</code></a> layer wrapping <code>K.std</code>:</p>

<pre><code>from keras.layers import merge, Input, Dense, Lambda
from keras.layers import Convolution1D, Flatten
from keras import backend as K

input_img = Input(shape=(64, 4))

x = Convolution1D(48, 3, activation='relu', init='he_normal')(input_img)
x = Flatten()(x)

std = Lambda(lambda x: K.std(x, axis=1))(input_img)
x = merge([x, std], mode='concat', concat_axis=1)

output =  Dense(100, activation='softmax', init='he_normal')(x)
</code></pre>
"
39751039,3293881.0,2016-09-28T14:54:57Z,39749807,1,"<p><strong>Approach #1</strong></p>

<p>We are working with large sized datasets and memory is an issue, so I will try to optimize the computations within the loop. Now, we can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html"" rel=""nofollow""><code>np.einsum</code></a> to replace <code>np.linalg.norm</code> part and <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html"" rel=""nofollow""><code>np.argpartition</code></a> in place of actual sorting with <code>np.argsort</code>, like so -</p>

<pre><code>out = np.empty((m,))
for i, ps in enumerate(p_fine):
    subs = ps-p
    sq_dists = np.einsum('ij,ij-&gt;i',subs,subs)
    out[i] = data_coarse[np.argpartition(sq_dists,k)[:k]].sum()
out = out/k
</code></pre>

<p><strong>Approach #2</strong></p>

<p>Now, as another approach we can also use <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html"" rel=""nofollow""><code>Scipy's cdist</code></a> for a fully vectorized solution, like so -</p>

<pre><code>from scipy.spatial.distance import cdist
out = data_coarse[np.argpartition(cdist(p_fine,p),k,axis=1)[:,:k]].mean(1)
</code></pre>

<p>But, since we are memory bound here, we can perform these operations in chunks. Basically, we would get chunks of rows from that tall array <code>p_fine</code> that has millions of rows and use <code>cdist</code> and thus at each iteration get chunks of output elements instead of just one scalar. With this, we would cut the loop count by the length of that chunk.</p>

<p>So, finally we would have an implementation like so -</p>

<pre><code>out = np.empty((m,))
L = 10 # Length of chunk (to be used as a param)
num_iter = m//L
for j in range(num_iter):
    p_fine_slice = p_fine[L*j:L*j+L]
    out[L*j:L*j+L] = data_coarse[np.argpartition(cdist\
                           (p_fine_slice,p),k,axis=1)[:,:k]].mean(1)
</code></pre>

<p><strong>Runtime test</strong></p>

<p>Setup -</p>

<pre><code># Setup inputs
m,n = 20000,100
p_fine = np.random.rand(m,3)
p = np.random.rand(n,3)
data_coarse = np.random.rand(n)
k = 5

def original_approach(p,p_fine,m,n,k):
    data_fine = np.empty((m,))
    for i, ps in enumerate(p_fine):
        data_fine[i] = np.mean(data_coarse[np.argsort(np.linalg.norm\
                                                 (ps-p,axis=1))[:k]])
    return data_fine

def proposed_approach(p,p_fine,m,n,k):    
    out = np.empty((m,))
    for i, ps in enumerate(p_fine):
        subs = ps-p
        sq_dists = np.einsum('ij,ij-&gt;i',subs,subs)
        out[i] = data_coarse[np.argpartition(sq_dists,k)[:k]].sum()
    return out/k

def proposed_approach_v2(p,p_fine,m,n,k,len_per_iter):
    L = len_per_iter
    out = np.empty((m,))    
    num_iter = m//L
    for j in range(num_iter):
        p_fine_slice = p_fine[L*j:L*j+L]
        out[L*j:L*j+L] = data_coarse[np.argpartition(cdist\
                               (p_fine_slice,p),k,axis=1)[:,:k]].sum(1)
    return out/k
</code></pre>

<p>Timings -</p>

<pre><code>In [134]: %timeit original_approach(p,p_fine,m,n,k)
1 loops, best of 3: 1.1 s per loop

In [135]: %timeit proposed_approach(p,p_fine,m,n,k)
1 loops, best of 3: 539 ms per loop

In [136]: %timeit proposed_approach_v2(p,p_fine,m,n,k,len_per_iter=100)
10 loops, best of 3: 63.2 ms per loop

In [137]: %timeit proposed_approach_v2(p,p_fine,m,n,k,len_per_iter=1000)
10 loops, best of 3: 53.1 ms per loop

In [138]: %timeit proposed_approach_v2(p,p_fine,m,n,k,len_per_iter=2000)
10 loops, best of 3: 63.8 ms per loop
</code></pre>

<p>So, there's about <strong><code>2x</code></strong> improvement with the first proposed approach  and <strong><code>20x</code></strong> over the original approach with the second one at the sweet spot with the <code>len_per_iter</code> param set at <code>1000</code>. Hopefully this will bring down your 25 minutes runtime to little over a minute. Not bad I guess!</p>
"
39751863,674039.0,2016-09-28T15:32:04Z,39751705,2,"<p>There are better tools for this job than regex, you could try for example:</p>

<pre><code>&gt;&gt;&gt; line
'(13)\xc2\xb5\xc2\xb1\xc2\xbe\xc3\xa2p\xc3\xb4\xc2\x8d(5)example(3)com(0)'
&gt;&gt;&gt; line.decode('ascii', 'ignore')
u'(13)p(5)example(3)com(0)'
</code></pre>

<p>That skips non-ascii characters.  Or with replace, you can swap them for a '?' placeholder:</p>

<pre><code>&gt;&gt;&gt; print line.decode('ascii', 'replace')
(13)ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½pï¿½ï¿½ï¿½ï¿½(5)example(3)com(0)
</code></pre>

<p>But the best solution is to find out what erroneous encoding/decoding caused the <a href=""https://en.wikipedia.org/wiki/Mojibake"" rel=""nofollow"">mojibake</a> to happen in the first place, so you can recover data by using the correct code pages.  </p>

<p>There is an excellent answer about unbaking emojibake <a href=""http://stackoverflow.com/a/24141326/674039"">here</a>.  Note that it's an inexact science, and a lot of the crucial information is actually in the comment thread under that answer. </p>
"
39752124,2414194.0,2016-09-28T15:44:38Z,39751945,2,"<p>Would this work for you?</p>

<pre><code>In [6]: my_list = [1,1,1,2,1,1,1]
In [7]: different = [ii for ii in set(my_list) if my_list.count(ii) == 1]
In [8]: different
Out[8]: [2]
</code></pre>
"
39752163,2956066.0,2016-09-28T15:46:44Z,39751705,-2,"<p>what about this?</p>

<pre><code>line = '(13)\xc2\xb5\xc2\xb1\xc2\xbe\xc3\xa2p\xc3\xb4\xc2\x8d(5)example(3)com(0)'

pattern = r'\\x.+'
re.sub(pattern, r'?', line)
</code></pre>
"
39752186,2901002.0,2016-09-28T15:48:06Z,39751866,2,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow""><code>reindex</code></a>:</p>

<pre><code>print (df.groupby(['product','Sales_band'])['Hour_id','sales']
         .apply(lambda x: x.set_index('Hour_id').reindex(range(1, 11), fill_value=0))
         .reset_index())

   product Sales_band  Hour_id  sales
0   prod_1       HIGH        1    200
1   prod_1       HIGH        2      0
2   prod_1       HIGH        3    100
3   prod_1       HIGH        4    300
4   prod_1       HIGH        5      0
5   prod_1       HIGH        6      0
6   prod_1       HIGH        7      0
7   prod_1       HIGH        8      0
8   prod_1       HIGH        9      0
9   prod_1       HIGH       10      0
10  prod_1  VERY HIGH        1      0
11  prod_1  VERY HIGH        2    100
12  prod_1  VERY HIGH        3      0
13  prod_1  VERY HIGH        4      0
14  prod_1  VERY HIGH        5    253
15  prod_1  VERY HIGH        6    234
16  prod_1  VERY HIGH        7      0
17  prod_1  VERY HIGH        8      0
18  prod_1  VERY HIGH        9      0
19  prod_1  VERY HIGH       10      0
</code></pre>
"
39752197,3124746.0,2016-09-28T15:48:26Z,39751945,2,"<p>You can use <code>Counter</code> from <code>collections</code> package</p>

<pre><code>from collections import Counter

a = [1,2,3,4,3,4,1]
b = Counter(a)  # Counter({1: 2, 2: 1, 3: 2, 4: 2})
elem = list(b.keys())[list(b.values()).index(1)]  # getting elem which is key with value that equals 1
print(a.index(elem))
</code></pre>

<p>Another possible solution that just differently compute <code>elem</code></p>

<pre><code>a = [1,2,3,4,3,4,1]
b = Counter(a)  # Counter({1: 2, 2: 1, 3: 2, 4: 2})
elem = (k for k, v in b.items() if v == 1)
print(a.index(next(elem)))
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Time consumption: </p>

<p>As @Jblasco mentioned, Jblasco's method is not really efficient one, and i was curious to measure it.</p>

<p>So the initial data is array with 200-400 elements, with only one unique value. The code that generate that array is. At the end of snipped there is 100 first elements that prove that it has one unique</p>

<pre><code>import random
from itertools import chain
f = lambda x: [x]*random.randint(2,4)
a=list(chain.from_iterable(f(random.randint(0,100)) for _ in range(100)))
a[random.randint(1, 100)] = 101
print(a[:100])
# [5, 5, 5, 84, 84, 84, 46, 46, 46, 46, 6, 6, 6, 68, 68, 68, 68, 38,
# 38, 38, 44, 44, 61, 61, 15, 15, 15, 15, 36, 36, 36, 36, 73, 73, 73, 
# 28, 28, 28, 28, 6, 6, 93, 93, 74, 74, 74, 74, 12, 12, 72, 72, 22, 
# 22, 22, 22, 78, 78, 17, 17, 17, 93, 93, 93, 12, 12, 12, 23, 23, 23, 
# 23, 52, 52, 88, 88, 79, 79, 42, 42, 34, 34, 47, 47, 1, 1, 1, 1, 71,
# 71, 1, 1, 45, 45, 101, 45, 39, 39, 50, 50, 50, 50]
</code></pre>

<p>That's the code that show us results, i choose to execute 3 times with 10000 executions:</p>

<pre><code>from timeit import repeat


s = """"""\
import random
from itertools import chain
f = lambda x: [x]*random.randint(2,4)
a=list(chain.from_iterable(f(random.randint(0,100)) for _ in range(100)))
a[random.randint(1, 100)] = 101
""""""

print('my 1st method:', repeat(stmt=""""""from collections import Counter
b=Counter(a)
elem = (k for k, v in b.items() if v == 1)
a.index(next(elem))"""""",
             setup=s, number=10000, repeat=3)

print('my 2nd method:', repeat(stmt=""""""from collections import Counter
b = Counter(a)
elem = list(b.keys())[list(b.values()).index(1)]
a.index(elem)"""""",
             setup=s, number=10000, repeat=3))

print('@Jblasco method:', repeat(stmt=""""""different = [ii for ii in set(a) if a.count(ii) == 1]
different"""""", setup=s, number=10000, repeat=3))

# my 1st method: [0.303596693000145, 0.27322746600111714, 0.2701447969993751]
# my 2nd method: [0.2715420649983571, 0.28590541199810104, 0.2821485950007627]
# @Jblasco method: [3.2133491599997797, 3.488262927003234, 2.884892332000163]
</code></pre>
"
39752296,6502500.0,2016-09-28T15:53:31Z,39751945,1,"<p>I would try maybe something like this:</p>

<pre><code>newList = list(set(my_list))
print newList.pop()
</code></pre>

<p>Assuming there's only 1 different value and the rest are all the same.
There's a little bit of ambiguity in your question which makes it difficult to answer but that's all I could think of optimally.</p>
"
39752370,1126841.0,2016-09-28T15:56:41Z,39752005,4,"<p>This is a little beyond the scope of what <code>argparse</code> can do, as the ""type"" of the first argument isn't known ahead of time. I would do something like</p>

<pre><code>import argparse

p = argparse.ArgumentParser()
p.add_argument(""file_or_phone"", help=""MMS File or phone number"")
p.add_argument (""mmsid"", nargs=""?"", help=""MMS-Transaction-ID"")

args = p.parse_args()
</code></pre>

<p>To determine whether <code>args.file_or_phone</code> is intended as a file name or a phone number, you need to check whether or not <code>args.mmsid</code> is <code>None</code>.</p>
"
39752628,1210053.0,2016-09-28T16:09:29Z,39750879,7,"<p>Using dynamic programming, you can construct a list <code>(l0, l1, l2, ... ln-1)</code>, where <code>n</code> is the number of characters in your input string and <code>li</code> is the minimum number of chunks you need to arrive at character <code>i</code> of the input string. The overall structure would look as follows:</p>

<pre><code>minValues := list with n infinity entries
for i from 0 to n-1
    for every choice c that is a suffix of input[0..i]
        if i - len(c) &lt; 0
            newVal = 1
        else
            newVal = minValues[i - len(c)] + 1
        end if
        if(newVal &lt; minValues[i])
            minValues[i] = newVal
            //optionally record the used chunk
        end if
    next
next
</code></pre>

<p>The minimum number of chunk for your entire string is then <code>ln-1</code>. You can get the actual chunks by tracking back through the list (which requires to record the used chunks).</p>

<p>Retrieving the choices that are suffixes can be sped up using a trie (of the reverse choice strings). The worst case complexity will still be <code>O(n * c * lc)</code>, where <code>n</code> is the length of the input string, <code>c</code> is the number of choices, and <code>lc</code> is the maximum length of the choices. However, this complexity will only occur for choices that are nested suffixes (e.g. <code>0</code>, <code>10</code>, <code>010</code>, <code>0010</code>...). In this case, the trie will degenerate to a list. In average, the run time should be much less. Under the assumption that the number of retrieved choices from the trie is always a small constant, it is <code>O(n * lc)</code> (actually, the <code>lc</code> factor is probably also smaller).</p>

<p>Here is an example:</p>

<pre><code>choices = [""0"",""1"",""10"",""100""]
text = ""10010""

algorithm step    content of minValues
                   0      1       2        3      4
---------------------------------------------------------
initialize        (â,     â ,     â ,      â ,    â     )
i = 0, c = ""1""    (1 ""1"", â ,     â ,      â ,    â     )
i = 1, c = ""0""    (1 ""1"", 2 ""0"",  â ,      â ,    â     )
i = 1, c = ""10""   (1 ""1"", 1 ""10"", â ,      â ,    â     )
i = 2, c = ""0""    (1 ""1"", 1 ""10"", 2 ""0"",   â ,    â     )
i = 2, c = ""100""  (1 ""1"", 1 ""10"", 1 ""100"", â ,    â     )
i = 3, c = ""1""    (1 ""1"", 1 ""10"", 1 ""100"", 2 ""1"", â     )
i = 4, c = ""0""    (1 ""1"", 1 ""10"", 1 ""100"", 2 ""1"", 3 ""0"" )
i = 4, c = ""10""   (1 ""1"", 1 ""10"", 1 ""100"", 2 ""1"", 2 ""10"")
</code></pre>

<p>Meaning: We can compose the string with 2 chunks. Tracing back gives the chunks in reverse order: ""10"", ""100"".</p>
"
39752785,5190654.0,2016-09-28T16:18:45Z,39736887,1,"<p>Use SymPy matrices:</p>

<pre><code>In [5]: p = sm.Matrix(p)
</code></pre>

<p>Then the exponent by a symbol will work:</p>

<pre><code>In [6]: p**k
Out[6]: 
â¡                                     k                                      k                        k                                          k                                     
â¢ 0.216542364659101â-0.154508497187474  + 0.419821271704536â0.404508497187474  + 0.363636363636364â1.0     - 0.350372906022699â-0.154508497187474  + 0.259463815113608â0.40450849718747
â¢                                                                                                                                                                                      
â¢                                      k                                      k                        k                                       k                                       
â¢- 0.507064433090879â-0.154508497187474  + 0.143428069454515â0.404508497187474  + 0.363636363636364â1.0    0.820447487227238â-0.154508497187474  + 0.0886434218636708â0.404508497187474
â¢                                                                                                                                                                                      
â¢                                       k                                      k                        k                                        k                                     
â£- 0.0598508375909206â-0.154508497187474  - 0.303785526045443â0.404508497187474  + 0.363636363636364â1.0    0.0968406894772593â-0.154508497187474  - 0.18774978038635â0.404508497187474

 k                         k                                       k                                      k                        k  â¤
4  + 0.0909090909090909â1.0    0.133830541363598â-0.154508497187474  - 0.679285086818143â0.404508497187474  + 0.545454545454545â1.0   â¥
                                                                                                                                      â¥
k                         k                                         k                                      k                        k â¥
  + 0.0909090909090909â1.0     - 0.31338305413636â-0.154508497187474  - 0.232071491318186â0.404508497187474  + 0.545454545454545â1.0  â¥
                                                                                                                                      â¥
k                         k                                          k                                      k                        kâ¥
  + 0.0909090909090909â1.0    - 0.0369898518863388â-0.154508497187474  + 0.491535306431793â0.404508497187474  + 0.545454545454545â1.0 â¦
</code></pre>

<p>At this point you could apply the limit to the single components:</p>

<pre><code>In [9]: (p**k).applyfunc(lambda x: limit(x, k, sm.oo))
Out[9]: 
â¡â  â  ââ¤
â¢       â¥
â¢â  â  ââ¥
â¢       â¥
â£â  â  ââ¦
</code></pre>

<p>This could be either a floating-point approximation error or a bug. Trying another way:</p>

<pre><code>In [15]: p**10000000000000
Out[15]: 
â¡0.363636363636364  0.0909090909090909  0.545454545454545â¤
â¢                                                        â¥
â¢0.363636363636364  0.0909090909090909  0.545454545454545â¥
â¢                                                        â¥
â£0.363636363636364  0.0909090909090909  0.545454545454546â¦
</code></pre>

<p>I strongly recommend to avoid such operations with floating points, better define a Matrix of SymPy numbers:</p>

<pre><code>In [19]: p = Matrix([[sm.S.One/2, sm.S.One/4, sm.S.One/4], [sm.S.One/2, 0, sm.S.One/2], [sm.S.One/4, 0, sm.S.One*3/4]])

In [20]: p
Out[20]: 
â¡1/2  1/4  1/4â¤
â¢             â¥
â¢1/2   0   1/2â¥
â¢             â¥
â£1/4   0   3/4â¦
</code></pre>

<p>Now if you try <strong>simplify(p</strong>k)** you'll get the closed expressions for your matrix power:</p>

<pre><code>In [27]: simplify(p**k)
Out[27]: 
â¡  -k â      6âk           6âk       3âk         k       3âk            k        3âk          k       3âk             k         k           kâ    -k â      6âk          6âk       3âk 
â¢64  ââ- 36â2   ââ5 + 120â2    - 55â2   â(1 + â5)  + 33â2   ââ5â(1 + â5)  - 120â2   â(-â5 + 1)  + 58â2   ââ5â(-â5 + 1)  - 220â64  + 88ââ5â64 â   64  ââ- 42â2   ââ5 + 30â2    - 44â2   â
â¢âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ  âââââââââââââââââââââââââââââââââââââââ
â¢                                                               11â(-25 + 13ââ5)                                                                                                       
â¢                                                                                                                                                                                      
â¢        â                                                           3âk          k          3âk             k            k               kâ                                           
â¢     -k â         3âk            k          3âk         k   161301â2   â(-â5 + 1)    72136â2   ââ5â(-â5 + 1)    408950â64    182888ââ5â64 â                                           
â¢ 2â64  ââ- 23184â2   ââ5â(1 + â5)  + 51841â2   â(1 + â5)  - ââââââââââââââââââââââ + ââââââââââââââââââââââââ - ââââââââââ + ââââââââââââââ      -k â      6âk          6âk        3âk
â¢        â                                                             11                        11                  11             11     â     64  ââ- 42â2   ââ5 + 30â2    - 242â2   
â¢ ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ    âââââââââââââââââââââââââââââââââââââââ
â¢                                                             -204475 + 91444ââ5                                                                                                       
â¢                                                                                                                                                                                      
â¢                         â       6âk       6âk                                                      k                 kâ                                                              
â¢                      -k â  100â2      52â2   ââ5                k             k   41ââ5â(-8ââ5 + 8)    89â(-8ââ5 + 8) â                                                              
â¢                    64  ââ- ââââââââ + ââââââââââ - â5â(8 + 8ââ5)  + (8 + 8ââ5)  - ââââââââââââââââââ + ââââââââââââââââ                          -k â      6âk          6âk       3âk
â¢                         â     11          11                                              11                  11      â                        -64  ââ- 42â2   ââ5 + 30â2    - 33â2   
â¢                    ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ                       âââââââââââââââââââââââââââââââââââââââ
â£                                                                -25 + 13ââ5                                                                                                           

           k        3âk         k       3âk          k       3âk             k        k           kâ     -k â     3âk            k      3âk         k      3âk          k      3âk     
â5â(1 + â5)  + 110â2   â(1 + â5)  - 85â2   â(-â5 + 1)  + 31â2   ââ5â(-â5 + 1)  - 55â64  + 55ââ5â64 â    64  ââ- 2â2   ââ5â(1 + â5)  - 3â2   â(1 + â5)  - 3â2   â(-â5 + 1)  + 2â2   ââ5â(
ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ   ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
                       11â(-25 + 13ââ5)                                                                                                                11                              



         k        3âk            k      3âk          k       3âk             k        k           kâ            -k â    6âk                k                k                 k        
â(1 + â5)  + 110â2   ââ5â(1 + â5)  - 8â2   â(-â5 + 1)  + 20â2   ââ5â(-â5 + 1)  - 55â64  + 55ââ5â64 â           64  ââ30â2    - 15â(8 + 8ââ5)  + â5â(8 + 8ââ5)  - 15â(-8ââ5 + 8)  - â5â(-
ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ          âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
                       11â(-25 + 13ââ5)                                                                                                                55                              



         k       3âk            k       3âk             k       3âk          k        k           kâ       -6âk â    6âk                k                   k                    k     
â(1 + â5)  + 11â2   ââ5â(1 + â5)  - 24â2   ââ5â(-â5 + 1)  + 58â2   â(-â5 + 1)  - 55â64  + 55ââ5â64 â       2    ââ60â2    + 25â(8 + 8ââ5)  + 13ââ5â(8 + 8ââ5)  - 13ââ5â(-8ââ5 + 8)  + 25
âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ     âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
                       11â(-13ââ5 + 25)                                                                                                               110                              

        k       kââ¤
-â5 + 1)  + 6â64 â â¥
âââââââââââââââââââ¥
                  â¥
                  â¥
                  â¥
                  â¥
         kâ       â¥
8ââ5 + 8) â        â¥
âââââââââââ       â¥
                  â¥
                  â¥
                  â¥
                  â¥
            kâ    â¥
â(-8ââ5 + 8) â     â¥
ââââââââââââââ    â¥
                  â¦
</code></pre>

<p>Unfortunately there are some bugs in SymPy that do not allow the calculation of this limit. Don't know if this expression could be useful to you.</p>
"
39752802,2901002.0,2016-09-28T16:19:54Z,39752729,2,"<p>Unfortunately if need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow""><code>reindex</code></a> with <code>MultiIndex</code>, need all levels:</p>

<pre><code>mux = pd.MultiIndex.from_product([list('AB'), np.arange(4), list('XY')],
                                 names=['one', 'two', 'three'])

print (s.reindex(mux, fill_value=0))
one  two  three
A    0    X        0
          Y        0
     1    X        0
          Y        1
     2    X        0
          Y        0
     3    X        2
          Y        3
B    0    X        0
          Y        0
     1    X        4
          Y        5
     2    X        0
          Y        0
     3    X        6
          Y        7
dtype: int32
</code></pre>

<p>EDIT by comment:</p>

<pre><code>idx = pd.MultiIndex.from_tuples([('A', 1, 'X'), ('B', 3, 'Y')],
                                 names=['one', 'two', 'three'])
s = pd.Series([5,6], idx)
print (s)
one  two  three
A    1    X        5
B    3    Y        6
dtype: int64

mux = pd.MultiIndex.from_tuples([('A', 0, 'X'), ('A', 1, 'X'), 
                                 ('A', 2, 'X'), ('A', 3, 'X'), 
                                 ('B', 0, 'Y'), ('B', 1, 'Y'), 
                                 ('B', 2, 'Y'), ('B', 3, 'Y')],
                                 names=['one', 'two', 'three'])

print (s.reindex(mux, fill_value=0))
one  two  three
A    0    X        0
     1    X        5
     2    X        0
     3    X        0
B    0    Y        0
     1    Y        0
     2    Y        0
     3    Y        6
dtype: int64
</code></pre>

<hr>

<p><strong><em>Direct Solution</em></strong>  </p>

<pre><code>new_lvl = np.arange(4)
mux = [(a, b, c) for b in new_lvl for a, c in s.reset_index('two').index.unique()]
s.reindex(mux, fill_value=0).sort_index()

one  two  three
A    0    X        0
          Y        0
     1    X        0
          Y        1
     2    X        0
          Y        0
     3    X        2
          Y        3
B    0    X        0
          Y        0
     1    X        4
          Y        5
     2    X        0
          Y        0
     3    X        6
          Y        7
dtype: int64
</code></pre>
"
39753758,129600.0,2016-09-28T17:11:24Z,39753605,2,"<p>just use a set of guessed letters and do something like the below.</p>

<pre><code>In [164]: word = 'Australia'
In [165]: guessed = set()
In [166]: guessed.add('a')
In [167]: print(''.join(c if c.lower() in guessed else '*' for c in word))
A****a**a
</code></pre>
"
39753779,2709093.0,2016-09-28T17:12:33Z,39753605,1,"<p>With regular expressions:</p>

<pre><code>import re
correct = re.sub(r'[^a]',""*"",""australia"")  # returns ""a****a**a""
</code></pre>

<p>This the regular expression <code>[^a]</code> matches all characters except ""a"". The <code>re.sub</code> function replaces each ""not a"" with a ""*""</p>

<p><em>edit</em></p>

<p>Your comments make be think that you are trying to change the <code>game.correct</code> string by replacing characters in that string. Remember that strings in python are immutable. You can't change individual letters in them. So rather than try to change the stars to ""a"", you have to make a new string. Regular expressions are one way to do this.</p>
"
39753802,102441.0,2016-09-28T17:13:29Z,39753605,0,"<p><code>replace</code> will not help here, since you are not changing the string you are searching.</p>

<p>There are two options here:</p>

<ol>
<li><p>Use <code>str.find</code> repeatedly:</p>

<pre><code>pos = 0
while True:
    pos = game.target.find(request.guess, start=pos)
    if pos == -1: break
    game.correct[pos] = request.guess.upper()
</code></pre></li>
<li><p>Build the string from scratch each time:</p>

<pre><code>game.correct = ''.join(
    letter if letter in request.guesses else '*'
    for letter in game.target
)
</code></pre></li>
</ol>
"
39753850,4128833.0,2016-09-28T17:16:33Z,39753605,0,"<p>I'd suggest you invest some time in learning <a href=""https://docs.python.org/2/library/re.html"" rel=""nofollow"">regular expressions</a>, they come in handy in many similar situations and are often more expressive (and efficient) than ad-hoc implementations. Plus you can reuse (with some quirks) your understanding of them in other languages and OS shells, which makes them very powerful.</p>

<p>You can build a regular expression from your pattern that will match anything except the characters you are looking for: <code>r'[^{}]'.format(pattern)</code>; then use it to do the substitution:</p>

<pre><code>import re
pattern = request.guess
result = re.sub(r'[^{}]'.format(pattern),""*"",""adam"")
</code></pre>

<p><a href=""https://ideone.com/7eDYFz"" rel=""nofollow"">Example on IdeOne</a>:</p>

<pre><code>import re
print re.sub(r'[^{}]'.format('ad'),""*"",""adam"")
print re.sub(r'[^{}]'.format('a'),""*"",""adam"")
print re.sub(r'[^{}]'.format('m'),""*"",""adam"")
</code></pre>

<p>Prints:</p>

<pre><code>ada*
a*a*
***m
</code></pre>
"
39754208,298607.0,2016-09-28T17:36:03Z,39751945,1,"<p>This is a great use for <a href=""http://www.numpy.org"" rel=""nofollow"">numpy</a></p>

<p>Given some random uniform list with a single uniquely different number in it:</p>

<pre><code>&gt;&gt;&gt; li=[1]*100+[200]+[1]*250
</code></pre>

<p>If the uniform value is known (in this case 1 and the unknown value is 200) you can use <code>np.where</code> on an array to get that value:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; a=np.array(li)
&gt;&gt;&gt; a[a!=1]
array([200])
</code></pre>

<p>If the uniform values are not known, you can use <code>np.uniques</code> to get the counts of uniques:</p>

<pre><code>&gt;&gt;&gt; np.unique(a, return_counts=True)
(array([  1, 200]), array([350,   1]))
</code></pre>

<p>For a pure Python solution, use a generator with <code>next</code> to get the first value that is different than all the others:</p>

<pre><code>&gt;&gt;&gt; next(e for i, e in enumerate(li) if li[i]!=1)
200
</code></pre>

<p>Or, you can use <a href=""https://docs.python.org/2/library/itertools.html#itertools.dropwhile"" rel=""nofollow"">dropwhile</a> from itertools:</p>

<pre><code>&gt;&gt;&gt; from itertools import dropwhile
&gt;&gt;&gt; next(dropwhile(lambda e: e==1, li))
200
</code></pre>

<p>If you do not know what the uniform value is, use a Counter on a slice big enough to get it:</p>

<pre><code>&gt;&gt;&gt; uniform=Counter(li[0:3]).most_common()[0][0]
&gt;&gt;&gt; uniform
1
&gt;&gt;&gt; next(e for i, e in enumerate(li) if li[i]!=uniform)
200
</code></pre>

<p>In these cases, <code>next</code> will short-circuit at the first value that satisfies the condition. </p>
"
39754322,6207849.0,2016-09-28T17:43:33Z,39751636,2,"<p>Set the <code>keys</code> to be the index of the two <code>DF's</code>:</p>

<pre><code>def index_set(frame, keys=['key1', 'key2']):
    frame.set_index(keys, inplace=True)
    return frame
</code></pre>

<p>Subset the <code>DF's</code> containing <code>NaN</code> values:</p>

<pre><code>def nulls(frame):
    nulls_in_frame = frame[frame.isnull().any(axis=1)].reset_index()
    return nulls_in_frame
</code></pre>

<p>Join the two <code>Df's</code>. Concatenate the joined <code>DF</code> with each of the subset of <code>NaN</code> containing <code>DF's</code> and drop the duplicated values filling the remaining <code>NaN</code> left with 0's.</p>

<p>Then, using <code>combine_first</code> to patch the values using chaining operation with the joined <code>DF</code>.</p>

<pre><code>def perform_join(fr_1, fr_2, keys=['key1', 'key2']):
    fr_1 = index_set(fr_1); frame_2 = index_set(fr_2)
    frame = fr_1.join(fr_2, how='outer').reset_index()
    cat_fr_1 = pd.concat([frame, nulls(fr_1)]).drop_duplicates(keys, keep=False).fillna(0)
    cat_fr_2 = pd.concat([frame, nulls(fr_2)]).drop_duplicates(keys, keep=False).fillna(0)
    fr_1_join = frame.combine_first(frame.fillna(cat_fr_1[fr_1.columns]))
    joined_frame = fr_1_join.combine_first(frame.fillna(cat_fr_2[fr_2.columns]))
    return joined_frame
</code></pre>

<p>Finally,</p>

<pre><code>perform_join(A, B)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6WWjf.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6WWjf.png"" alt=""Image""></a></p>
"
39754324,131187.0,2016-09-28T17:43:41Z,39726737,1,"<p>The following code preserves apostrophes and blanks, and could easily be modified to preserve double quotations marks, if desired. It works by using a translation table based on a subclass of the string object. I think  the code is fairly easy to understand. It might be made more efficient if necessary.</p>

<pre><code>class SpecialTable(str):
    def __getitem__(self, chr):
        if chr==32 or chr==39 or 48&lt;=chr&lt;=57 \
            or 65&lt;=chr&lt;=90 or 97&lt;=chr&lt;=122:
            return chr
        else:
            return None

specialTable = SpecialTable()


with open('temp2.txt') as inputText:
    for line in inputText:
        print (line)
        convertedLine=line.translate(specialTable)
        print (convertedLine)
        print (convertedLine.split(' '))
</code></pre>

<p>Here's typical output.</p>

<pre><code>This! is _a_ single (i.e. 1) English sentence that won't cause any trouble, right?

This is a single ie 1 English sentence that won't cause any trouble right
['This', 'is', 'a', 'single', 'ie', '1', 'English', 'sentence', 'that', ""won't"", 'cause', 'any', 'trouble', 'right']
'nother one.

'nother one
[""'nother"", 'one']
</code></pre>
"
39754382,5701438.0,2016-09-28T17:46:46Z,39754222,0,"<p>Convert each alphabetical character in the string to empty char """"</p>

<pre><code>import re
num_string = []* len(s)
for i, string in enumerate(s):
    num_string[i] = re.sub('[a-zA-Z]+', '', string)
</code></pre>
"
39754472,1231450.0,2016-09-28T17:53:11Z,39754222,0,"<p>You could go for a combination of <code>locale</code> and regular expressions:</p>

<pre><code>import re, locale
from locale import atof

# or whatever else
locale.setlocale(locale.LC_NUMERIC, 'en_GB.UTF-8')

s = [
      ""12.45-280"", # need to convert to 12.45280
      ""A10.4B2"", # need to convert to 10.42
]

rx = re.compile(r'[A-Z-]+')

def convert(item):
    """"""
    Try to convert the item to a float
    """"""
    try:
        return atof(rx.sub('', item))
    except:
        return None

converted = [match
            for item in s
            for match in [convert(item)]
            if match]

print(converted)
# [12.4528, 10.42]
</code></pre>
"
39754501,771848.0,2016-09-28T17:55:26Z,39754222,1,"<p>You can also remove all non-digits and non-dot characters, then convert the result to float:</p>

<pre><code>In [1]: import re
In [2]: s = [
   ...:       ""12.45-280"", # need to convert to 12.45280
   ...:       ""A10.4B2"", # need to convert to 10.42
   ...: ]

In [3]: for item in s:
   ...:     print(float(re.sub(r""[^0-9.]"", """", item)))
   ...:     
12.4528
10.42
</code></pre>

<p>Here <code>[^0-9.]</code> would match any character except a digit or a literal dot. </p>
"
39754917,6394138.0,2016-09-28T18:19:53Z,39753972,2,"<blockquote>
  <p><a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.FancyBboxPatch"" rel=""nofollow""><code>matplotlib.patches.FancyBboxPatch</code></a> class is similar to <a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.Rectangle"" rel=""nofollow""><code>matplotlib.patches.Rectangle</code></a> class, but it draws
  a fancy box around the rectangle.</p>
  
  <p><a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.FancyBboxPatch.get_width"" rel=""nofollow""><code>FancyBboxPatch.get_width()</code></a> returns the width of the (inner) rectangle</p>
</blockquote>

<p>However, <a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.FancyBboxPatch"" rel=""nofollow""><code>FancyBboxPatch</code></a> is a subclass of <a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.Patch"" rel=""nofollow""><code>matplotlib.patches.Patch</code></a> which provides a <a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.Patch.get_extents"" rel=""nofollow""><code>get_extents()</code></a> method:</p>

<blockquote>
  <p><a href=""http://matplotlib.org/api/patches_api.html#matplotlib.patches.Patch.get_extents"" rel=""nofollow""><code>matplotlib.patches.Patch.get_extents()</code></a></p>
  
  <p>Return a Bbox object defining the axis-aligned extents of the Patch.</p>
</blockquote>

<p>Thus, what you need is <strong><code>textbox.get_bbox_patch().get_extents().width</code></strong>.</p>
"
39755202,5067311.0,2016-09-28T18:35:43Z,39755045,3,"<p>You can easily do this with <code>numpy</code> by sorting both lists (to get a mapping between the two lists) and by inverting one of the sorting permutations:</p>

<pre><code>import numpy as np

a = [7, 14, 0, 9, 19, 9]
b = [45, 42, 0, 1, -1, 0]

a = np.array(a)
b = np.array(b)

ai = np.argsort(a)
bi = np.argsort(b)
aiinv = np.empty(ai.shape,dtype=int)
aiinv[ai] = np.arange(a.size)  # inverse of ai permutation

b_new = b[bi[aiinv]]
# array([ 0, 42, -1,  0, 45,  1])
</code></pre>

<p><a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html"" rel=""nofollow""><code>numpy.argsort</code></a> gives the indices (permutation) that will sort your array. This needs to be inverted to be used inside <code>b</code>, which can be done by the inverted assignment</p>

<pre><code>aiinv[ai] = np.arange(a.size)
</code></pre>
"
39755220,4952130.0,2016-09-28T18:36:25Z,39755171,5,"<p>You were close, you limited your comprehension to the contents of <code>x[0].split</code>, i.e you were missing one <code>for</code> loop through the elements of <code>l</code>:</p>

<pre><code>list_words = ['&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a,a) for x in l for a in x.split()]
</code></pre>

<p>this works because <code>""string"".split()</code> yields a one element list.</p>

<p>This can look <em>way prettier</em> if you define the format string outside the comprehension and use a positional index <code>{0}</code> informing <code>format</code> of the argument (so you don't need to do <code>format(a, a)</code>):</p>

<pre><code>fs = '&lt;a href=""http://url.com/{0}""&gt;{0}&lt;/a&gt;'
list_words = [fs.format(a) for x in l for a in x.split()]
</code></pre>

<p>With <code>map</code> you can get an ugly little duckling too if you like:</p>

<pre><code>list(map(fs.format, sum(map(str.split, l),[])))
</code></pre>

<p>Here we <code>sum(it, [])</code> to flatten the list of lists <code>map</code> with <code>split</code> produces and then map <code>fs.format</code> to the corresponding flattened list. Results are the same:</p>

<pre><code>['&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;',
 '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;',
 '&lt;a href=""http://url.com/fox""&gt;fox&lt;/a&gt;',
 '&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;',
 '&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;',
 '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;']
</code></pre>

<p>Go with the comprehension, <em>obviously</em>.</p>
"
39755230,6779307.0,2016-09-28T18:36:55Z,39755171,2,"<pre><code>list_words = ['&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a,a) for item in l for a in item.split(' ')]
</code></pre>
"
39755246,6671342.0,2016-09-28T18:37:51Z,39755171,2,"<p>In <strong>one-liner</strong>:</p>

<pre><code>list_words = ['&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a,a) for a in [i for sub in [i.split() for i in l] for i in sub]]
</code></pre>

<p><strong>In steps</strong></p>

<p>You can split the list:</p>

<pre><code>l = [i.split() for i in l]
</code></pre>

<p>and then flatten it:</p>

<pre><code>l = [i for sub in l for i in sub]
</code></pre>

<p>result:</p>

<pre><code>&gt;&gt;&gt; l
['the', 'quick', 'fox', 'the', 'the', 'quick']
</code></pre>

<p>Then:</p>

<pre><code>list_words = ['&lt;a href=""http://url.com/{}""&gt;{}&lt;/a&gt;'.format(a,a) for a in l]
</code></pre>

<p>You will finally take:</p>

<pre><code>&gt;&gt;&gt; list_words
['&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;', '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;', '&lt;a href=""http://url.com/fox""&gt;fox&lt;/a&gt;', '&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;', '&lt;a href=""http://url.com/the""&gt;the&lt;/a&gt;', '&lt;a href=""http://url.com/quick""&gt;quick&lt;/a&gt;']
</code></pre>
"
39755265,953482.0,2016-09-28T18:38:58Z,39755045,5,"<pre><code>a = [7, 14, 0, 9, 19, 9]
b = [45, 42, 0, 1, -1, 0]
print zip(*sorted(zip(sorted(b), sorted(enumerate(a), key=lambda x:x[1])), key=lambda x: x[1][0]))[0]
#or, for 3.x:
print(list(zip(*sorted(zip(sorted(b), sorted(enumerate(a), key=lambda x:x[1])), key=lambda x: x[1][0])))[0])
</code></pre>

<p>result:</p>

<pre><code>(0, 42, -1, 0, 45, 1)
</code></pre>

<p>You sort <code>a</code>, using <code>enumerate</code> to keep track of each item's original index. You zip the result with <code>sorted(b)</code>, then re-sort the whole thing based on <code>a</code>'s original indices. Then you call <code>zip</code> once more to extract just <code>b</code>'s values.</p>
"
39755404,6451573.0,2016-09-28T18:46:40Z,39755232,1,"<p>First method is bound to fail: you don't want to add new lines but new columns. So back to second method:</p>

<p>You insert the title OK, but then you're looping through the results on each row, whereas you need to iterate on them.</p>

<p>For this, i create an iterator from the <code>final_results</code> list (with <code>__iter__()</code>), then I call <code>it.next</code> and append to each row (no need to insert in the end, just append)  </p>

<p>I removed the <code>all</code> big list, because 1) you can write one line at a time, saves memory, and 2) <code>all</code> is a predefined function. Avoid to use that as a variable.</p>

<pre><code>final_results = ['0.1065599566767107', '0.0038113334533441123', '20.061623176440904']

# Method2
with open(""test.csv"", 'rb') as input, open('temp.csv', 'wb') as output:
    reader = csv.reader(input, delimiter = ',')
    writer = csv.writer(output, delimiter = ',')


    row = next(reader)  # read title line
    row.append(""Results"")
    writer.writerow(row)  # write enhanced title line

    it = final_results.__iter__()  # create an iterator on the result

    for row in reader:
        if row:  # avoid empty lines that usually lurk undetected at the end of the files
            try:
                row.append(next(it))  # add a result to current row
            except StopIteration:
                row.append(""N/A"")     # not enough results: pad with N/A
            writer.writerow(row)
</code></pre>

<p>result:</p>

<pre><code>Type,Id,TypeId,CalcValues,Results
B,111K,111Kequity(long) 111K,116.211768,0.1065599566767107
C,111N,B(long) 111N,0.106559957,0.0038113334533441123
B,111J,c(long) 111J,20.061634,20.061623176440904
</code></pre>

<p>Note: had we included <code>""Results""</code> in the <code>final_results</code> variable, we wouldn't even have needed to process first line differently.</p>

<p>Note2: the values seem wrong: <code>final_results</code> seems not in the same order as the expected output. And the <code>Result</code> column has turned to <code>ID</code>, but that's easy to correct.</p>
"
39755652,1735756.0,2016-09-28T19:01:00Z,39755232,1,"<pre><code>import csv

HEADER = ""Type,Id,TypeId,CalcValues,ID""
final_results = ['0.1065599566767107', '20.061623176440904', '0.0038113334533441123']

with open(""test.csv"") as inputs, open(""tmp.csv"", ""wb"") as outputs:
    reader = csv.reader(inputs, delimiter="","")
    writer = csv.writer(outputs, delimiter="","")

    reader.next()  # ignore header line
    writer.writerow(HEADER.split("",""))  

    for row in reader:
        writer.writerow(row + [final_results.pop(0)])
</code></pre>

<p>I store the header fields into <code>HEADER</code> and switch 2nd and 3rd elements of <code>final_results</code>, use <code>pop(0)</code> to remove and return the first element of <code>final_results</code></p>

<p>output:</p>

<pre><code>Type,Id,TypeId,CalcValues,ID
B,111K,111Kequity(long) 111K,116.211768,0.1065599566767107
C,111N,B(long) 111N,0.106559957,20.061623176440904
B,111J,c(long) 111J,20.061634,0.0038113334533441123
</code></pre>
"
39755862,113962.0,2016-09-28T19:14:09Z,39755289,4,"<p>The article relies on the undocumented feature that <code>Q()</code> accepts args as well as kwargs.</p>

<p>If you look at <a href=""https://github.com/django/django/blob/9c522d2ed8e752932bfff62d6e2940e56dee700b/django/db/models/query_utils.py#L53"" rel=""nofollow"">source code for the <code>Q</code> class</a>, you can see that it does the following in the <code>__init__</code> method.</p>

<pre><code>class Q(tree.Node):
    ...
    def __init__(self, *args, **kwargs):
        super(Q, self).__init__(children=list(args) + list(kwargs.items()))
</code></pre>

<p>If you call <code>Q(question__contains=dinner)</code> then <code>args</code> in an empty tuple <code>()</code> and <code>kwargs</code> is a dictionary <code>{'question__contains': 'dinner'}</code>. In the <code>super()</code> call, the <code>children</code> variable is</p>

<pre><code>children = list(args) + list(kwargs.items())
</code></pre>

<p>which evaluates to</p>

<pre><code>children = list(()) + list(('question__contains', 'dinner'),)
</code></pre>

<p>which simplifies to</p>

<pre><code>children = [('question__contains', 'dinner')]
</code></pre>

<p>Note that you could also get this result if you use <code>Q(('question__contains', 'dinner'))</code>. In this case, <code>args</code> is a tuple <code>(('question__contains', 'dinner'),)</code> and <code>kwargs</code> is an empty dictionary <code>{}</code>.</p>

<p>In the <code>super()</code> call, the <code>children</code> variable evaluates to</p>

<pre><code>children = list((('question__contains', 'dinner'),)) + list([])
</code></pre>

<p>which simplifies to the same result as before,</p>

<pre><code>children = [('question__contains', 'dinner')]
</code></pre>

<p>We have shown that <code>Q(question__contains=dinner)</code> is equivalent to <code>Q(('question__contains', 'dinner'))</code>, and therefore you can generate the list of <code>Q()</code> objects by looping over a list of 2-tuples in a list comprehension.</p>

<pre><code>&gt;&gt;&gt; predicates = [('question__contains', 'dinner'), ('question__contains', 'meal')]
&gt;&gt;&gt; q_list = [Q(x) for x in predicates]
</code></pre>

<p>Personally, I might prefer to write</p>

<pre><code>&gt;&gt;&gt; predicates = [{'question__contains': 'dinner'},  {'question__contains': 'meal'}]
&gt;&gt;&gt; q_list = [Q(**kwargs) for kwargs in predicates]
</code></pre>

<p>This way, you are not relying on the behaviour of the <code>__init__</code> method of the <code>Q</code>.</p>
"
39756074,2336654.0,2016-09-28T19:27:47Z,39755981,2,"<p>First things first:<br>
What you wanted was merge</p>

<pre><code>df1.merge(df2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/HnoA8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HnoA8.png"" alt=""enter image description here""></a></p>

<hr>

<p><code>join</code> defaults to merging on the <code>index</code>.  You can specify the <code>on</code> parameter which only says which column from left side to match with the index of the right side.  </p>

<p>These might help illustrate</p>

<pre><code>df1.set_index('key').join(df2.set_index('key'))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Mpw4c.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Mpw4c.png"" alt=""enter image description here""></a></p>

<pre><code>df1.join(df2.set_index('key'), on='key')
</code></pre>

<p><a href=""http://i.stack.imgur.com/9QQgK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9QQgK.png"" alt=""enter image description here""></a></p>

<hr>

<p>Your example is matching the index of <code>df2</code> which looks like <code>[0, 1, 2, 3]</code> with the <code>key</code> column of <code>df1</code> which looks like <code>[1, 2, 3, 4]</code><br>
That's why you get <code>NaN</code> in <code>col2</code> when <code>key_l</code> is <code>4</code></p>

<pre><code>df1.join(df2, on='key', lsuffix='_l', how='outer')
</code></pre>

<p><a href=""http://i.stack.imgur.com/Wg93O.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Wg93O.png"" alt=""enter image description here""></a></p>
"
39756350,2781701.0,2016-09-28T19:42:55Z,39754822,2,"<p>It's highly advised to use a distributed crawling software but if you really want to do it like this just for some dirty testing here it is</p>

<pre><code>import subprocess

project_path=""/Users/name/intellij-workspace/crawling/scrape""
subprocess.Popen([""scrapy"",""runspider"",""scrape/spiders/some-spider.py""],cwd=project_path)
</code></pre>
"
39756668,2336654.0,2016-09-28T20:02:17Z,39755742,3,"<p>you can pass <code>kwds</code> to hist and it will pass them along to appropriate sub processes.  The relevant ones here are <code>sharex</code> and <code>sharey</code></p>

<pre><code>data = pd.Series(np.random.randn(1000))
data.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4),
          sharex=True, sharey=True)
</code></pre>

<p><a href=""http://i.stack.imgur.com/q86d4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/q86d4.png"" alt=""enter image description here""></a></p>
"
39757025,674039.0,2016-09-28T20:25:52Z,39756947,2,"<p>Another job for plain old for loop:</p>

<pre><code>for url in testurl1, testurl2, testurl3
    req = Request(url)
    try:
        response = urlopen(req)
    except HttpError as err:
        if err.code == 404:
            continue
        raise
    else:
        # do what you want with successful response here (or outside the loop)
        break
else:
    # They ALL errored out with HTTPError code 404.  Handle this?
    raise err
</code></pre>
"
39757114,6226488.0,2016-09-28T20:31:16Z,39756947,0,"<p>Hmmm maybe something like this?</p>

<pre><code>from urllib2 import Request, urlopen
from urllib2 import URLError, HTTPError

def checkfiles():
    req = Request('http://testurl1')
    try:
        response = urlopen(req)
        url1=('http://testurl1')

    except HTTPError, URLError:
        try:
            url1 = ('http://testurl2')
        except HTTPError, URLError:
            url1 = ('http://testurl3')
    print url1
    finalURL='wget '+url1+'/testfile.tgz'

    print finalURL

checkfiles()
</code></pre>
"
39757180,6671342.0,2016-09-28T20:35:14Z,39757126,1,"<p>Try that:</p>

<pre><code> &gt;&gt;&gt; l = list(set(list1)-set(compSet))
 &gt;&gt;&gt; l
 ['this', 'and']
</code></pre>
"
39757182,4211135.0,2016-09-28T20:35:20Z,39757126,8,"<p>There is a simple way to check for the intersection of two lists: convert them to a set and use <code>intersection</code>:</p>

<pre><code>&gt;&gt;&gt; list1 = ['this', 'and', 'that' ]
&gt;&gt;&gt; compSet = [ 'check','that','thing' ]
&gt;&gt;&gt; set(list1).intersection(compSet)
{'that'}
</code></pre>

<p>You can also use bitwise operators:</p>

<p>Intersection:</p>

<pre><code>&gt;&gt;&gt; set(list1) &amp; set(compSet)
{'that'}
</code></pre>

<p>Union:</p>

<pre><code>&gt;&gt;&gt; set(list1) | set(compSet)
{'this', 'and', 'check', 'thing', 'that'}
</code></pre>

<p>You can make any of these results a list using <code>list()</code>.</p>
"
39757388,646543.0,2016-09-28T20:48:54Z,39740632,1,"<p>There isn't a hugely elegant way to handle import cycles in general, I'm afraid. Your choices are to either redesign your code to remove the cyclic dependency, or if it isn't feasible, do something like this:</p>

<pre><code># some_file.py

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from main import Main

class MyObject(object):
    def func2(self, some_param: 'Main'):
        ...
</code></pre>

<p>The <code>TYPE_CHECKING</code> constant is always <code>False</code> at runtime, so the import won't be evaluated, but mypy (and other type-checking tools) will evaluate the contents of that block.</p>

<p>We also need to make the <code>Main</code> type annotation into a string, effectively forward declaring it since the <code>Main</code> symbol isn't available at runtime.</p>

<p>All that said, using mixins with mypy will likely require a bit more structure then you currently have. Mypy <a href=""https://github.com/python/mypy/issues/1996"" rel=""nofollow"">recommends an approach</a> that's basically what <code>deceze</code> is describing -- to create an ABC that both your <code>Main</code> and <code>MyMixin</code> classes inherit. I wouldn't be surprised if you ended up needing to do something similar in order to make Pycharm's checker happy.</p>
"
39757585,2314737.0,2016-09-28T21:01:11Z,39757188,1,"<p>You need to set <code>set_picker(True)</code> to enable a pick event  or give a tolerance in points as a float (see <a href=""http://matplotlib.org/api/artist_api.html#matplotlib.artist.Artist.set_picker"" rel=""nofollow"">http://matplotlib.org/api/artist_api.html#matplotlib.artist.Artist.set_picker</a>).</p>

<p>So in your case <code>ax1.set_picker(True)</code> if you want  pick event to be fired whenever if mouseevent is over <code>ax1</code>.</p>

<p>You can enable pick events on the elements of the candlestick chart. I read the documentation and <a href=""http://matplotlib.org/api/finance_api.html#matplotlib.finance.candlestick2_ohlc"" rel=""nofollow""><code>candlestick2_ohlc</code></a> returns a tuple of two objects: a <code>LineCollection</code> and a <code>PolyCollection</code>. So you can name these objects and set the picker to true on them</p>

<pre><code>(lines,polys) = candlestick2_ohlc(ax1, ...)
lines.set_picker(True) # collection of lines in the candlestick chart
polys.set_picker(True) # collection of polygons in the candlestick chart
</code></pre>

<p>The index of the event <code>ind = event.ind[0]</code> will tell you which element in the collection contained the mouse event (<code>event.ind</code> returns a list of indices since a mouse event might pertain more than one item).</p>

<p>Once you trigger a pick event on a candlestick you can print the data from the original dataframe.</p>

<p>Here's some working code</p>

<pre><code>import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection, PolyCollection
from matplotlib.text import Text
from matplotlib.finance import candlestick2_ohlc
import numpy as np
import pandas as pd

np.random.seed(0)
dates = pd.date_range('20160101',periods=7)
df = pd.DataFrame(np.reshape(1+np.random.random_sample(42)*0.1,(7,6)),index=dates,columns=[""PX_BID"",""PX_ASK"",""PX_LAST"",""PX_OPEN"",""PX_HIGH"",""PX_LOW""])
df['PX_HIGH']+=.1
df['PX_LOW']-=.1

fig, ax1 = plt.subplots()

ax1.set_title('click on points', picker=20)
ax1.set_ylabel('ylabel', picker=20, bbox=dict(facecolor='red'))

(lines,polys) = candlestick2_ohlc(ax1, df['PX_OPEN'],df['PX_HIGH'],df['PX_LOW'],df['PX_LAST'],width=0.4)
lines.set_picker(True)
polys.set_picker(True)

def onpick1(event):
    if isinstance(event.artist, (Text)):
        text = event.artist
        print 'You clicked on the title (""%s"")' % text.get_text()
    elif isinstance(event.artist, (LineCollection, PolyCollection)):   
        thisline = event.artist
        mouseevent = event.mouseevent
        ind = event.ind[0]
        print 'You clicked on item %d' % ind
        print 'Day: ' + df.index[ind].normalize().to_datetime().strftime('%Y-%m-%d')
        for p in ['PX_OPEN','PX_OPEN','PX_HIGH','PX_LOW']:
            print p + ':' + str(df[p][ind])    
        print('x=%d, y=%d, xdata=%f, ydata=%f' %
          ( mouseevent.x, mouseevent.y, mouseevent.xdata, mouseevent.ydata))



fig.canvas.mpl_connect('pick_event', onpick1)
plt.show()
</code></pre>
"
39757962,4695774.0,2016-09-28T21:28:16Z,39757901,1,"<p>i got a solution <a href=""http://stackoverflow.com/questions/30926670/pandas-add-multiple-empty-columns-to-dataframe"">here</a>. </p>

<pre><code>df.reindex(columns = list['cd'])
</code></pre>

<p>will do the trick.</p>

<p>actually it will be:</p>

<pre><code>df.reindex(columns = list['abcd'])
</code></pre>
"
39758551,2336654.0,2016-09-28T22:19:29Z,39758449,3,"<p>consider <code>pd.Series</code> <code>s</code></p>

<pre><code>s = pd.Series(np.random.choice([3, 4, 5, 6, np.nan], 100))
s.hist()
</code></pre>

<p><a href=""http://i.stack.imgur.com/hzlrF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hzlrF.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Option 1</em></strong><br>
Min Max Scaling</p>

<pre><code>new = s.sub(s.min()).div((s.max() - s.min()))
new.hist()
</code></pre>

<p><a href=""http://i.stack.imgur.com/ThfOU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ThfOU.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong>NOT WHAT OP ASKED FOR</strong><br>
I put these in because I wanted to</p>

<p><strong><em>Option 2</em></strong><br>
sigmoid</p>

<pre><code>sigmoid = lambda x: 1 / (1 + np.exp(-x))

new = sigmoid(s.sub(s.mean()))
new.hist()
</code></pre>

<p><a href=""http://i.stack.imgur.com/W4nZv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/W4nZv.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Option 3</em></strong><br>
tanh (hyperbolic tangent)</p>

<pre><code>new = np.tanh(s.sub(s.mean())).add(1).div(2)
new.hist()
</code></pre>

<p><a href=""http://i.stack.imgur.com/7Q0WT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Q0WT.png"" alt=""enter image description here""></a></p>
"
39758675,2336654.0,2016-09-28T22:33:32Z,39757901,1,"<p><strong><em><code>pd.concat</code></em></strong></p>

<pre><code>pd.concat([df, pd.DataFrame(0, df.index, list('cd'))], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/4L9rZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4L9rZ.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em><code>join</code></em></strong></p>

<pre><code>df.join(pd.DataFrame(0, df.index, list('cd')))
</code></pre>

<p><a href=""http://i.stack.imgur.com/4L9rZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4L9rZ.png"" alt=""enter image description here""></a></p>
"
39762595,2901002.0,2016-09-29T06:07:49Z,39762576,2,"<p>You need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.notnull.html"" rel=""nofollow""><code>notnull</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ix.html"" rel=""nofollow""><code>ix</code></a> for selecting columns:</p>

<pre><code>b = df.ix[((df['actual']==1) &amp; (df['pred']==0)) &amp; (df['house'].notnull()), ['name', 'house']]
</code></pre>

<p>Sample:</p>

<pre><code>df = pd.DataFrame({'house':[None,'a','b'],
                   'pred':[0,0,5],
                   'actual':[1,1,5],
                   'name':['J','B','C']})

print (df)
   actual house name  pred
0       1  None    J     0
1       1     a    B     0
2       5     b    C     5

b = df.ix[((df['actual']==1) &amp; (df['pred']==0)) &amp; (df['house'].notnull()), ['name', 'house']]
print (b)
  name house
1    B     a
</code></pre>

<p>You can also check <a href=""http://pandas.pydata.org/pandas-docs/stable/missing_data.html#values-considered-missing"" rel=""nofollow"">pandas documentation</a>:</p>

<p><strong>Warning</strong></p>

<blockquote>
  <p>One has to be mindful that in python (and numpy), the nan's donât compare equal, but None's do. Note that Pandas/numpy uses the fact that np.nan != np.nan, and treats None like np.nan.</p>
</blockquote>

<pre><code>In [11]: None == None
Out[11]: True

In [12]: np.nan == np.nan
Out[12]: False
</code></pre>

<blockquote>
  <p>So as compared to above, a scalar equality comparison versus a None/np.nan doesnât provide useful information.</p>
</blockquote>

<pre><code>In [13]: df2['one'] == np.nan
Out[13]: 
a    False
b    False
c    False
d    False
e    False
f    False
g    False
h    False
Name: one, dtype: bool
</code></pre>
"
39762726,1060350.0,2016-09-29T06:16:36Z,39742127,1,"<p>You do <strong>not</strong> need to <em>enumerate and count</em> the pairs.</p>

<p>Instead, compute a <em>confusion matrix</em> containing the intersection sizes of each cluster from the first clustering with every cluster of the second clustering (this is one loop over all objects), then compute the number of pairs from this matrix using the equation <code>n*(n-1)/2</code>.</p>

<p>This reduces your runtime from O(n^2) to O(n), so it should give you a <em>considerable</em> speedup.</p>
"
39763472,2901002.0,2016-09-29T06:58:51Z,39763436,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.groupby.html"" rel=""nofollow""><code>groupby</code></a> by <code>index</code> divide by <code>k=2</code>:</p>

<pre><code>k = 2
print (s.index // k)
Int64Index([0, 0, 1, 1], dtype='int64')

print (s.groupby([s.index // k]).mean())
   name
0   3.0
1   9.5
</code></pre>
"
39763606,2336654.0,2016-09-29T07:05:44Z,39763436,2,"<p>If you are using this over large series and many times, you'll want to consider a fast approach.  This solution uses all numpy functions and will be fast.</p>

<p>Use <code>reshape</code> and construct new <code>pd.Series</code></p>

<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>s = pd.Series([1, 5, 20, -1])
</code></pre>

<hr>

<p><strong><em>generalized function</em></strong></p>

<pre><code>def mean_k(s, k):
    pad = (k - s.shape[0] % k) % k
    nan = np.repeat(np.nan, pad)
    val = np.concatenate([s.values, nan])
    return pd.Series(np.nanmean(val.reshape(-1, k), axis=1))
</code></pre>

<p><strong><em>demonstration</em></strong></p>

<pre><code>mean_k(s, 2)

0    3.0
1    9.5
dtype: float64
</code></pre>

<hr>

<pre><code>mean_k(s, 3)

0    8.666667
1   -1.000000
dtype: float64
</code></pre>
"
39763641,5173575.0,2016-09-29T07:07:57Z,39763436,1,"<p>You can do this:</p>

<pre><code>(s.iloc[::2].values + s.iloc[1::2])/2
</code></pre>

<p>if you want you can also reset the index afterwards, so you have 0, 1 as the index, using:</p>

<pre><code>((s.iloc[::2].values + s.iloc[1::2])/2).reset_index(drop=True)
</code></pre>
"
39764116,5702941.0,2016-09-29T07:33:45Z,39598666,0,"<p>IN master replica conf the new data will take few millisecond to replicate the data on all other replica server/database.</p>

<p>so whenever u tried to read after write it wont gives you correct result.</p>

<p>Instead of reading from replica you can use master to read immediately after  write by using <code>using('primary')</code> keyword with your get query.</p>
"
39764285,1603156.0,2016-09-29T07:42:41Z,39763091,3,"<p>I was recently just solving very similar problem - I needed to extract subject(s), action, object(s). And I open sourced my work so you can check this library:
<a href=""https://github.com/krzysiekfonal/textpipeliner"" rel=""nofollow"">https://github.com/krzysiekfonal/textpipeliner</a></p>

<p>This based on spacy(opponent to nltk) but it also based on sentence tree.</p>

<p>So for instance let's get this doc embedded in spacy as example:</p>

<pre><code>import spacy
nlp = spacy.load(""en"")
doc = nlp(u""The Empire of Japan aimed to dominate Asia and the "" \
               ""Pacific and was already at war with the Republic of China "" \
               ""in 1937, but the world war is generally said to have begun on "" \
               ""1 September 1939 with the invasion of Poland by Germany and "" \
               ""subsequent declarations of war on Germany by France and the United Kingdom. "" \
               ""From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered "" \
               ""or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. "" \
               ""Under the Molotov-Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and "" \
               ""annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. "" \
               ""The war continued primarily between the European Axis powers and the coalition of the United Kingdom "" \
               ""and the British Commonwealth, with campaigns including the North Africa and East Africa campaigns, "" \
               ""the aerial Battle of Britain, the Blitz bombing campaign, the Balkan Campaign as well as the "" \
               ""long-running Battle of the Atlantic. In June 1941, the European Axis powers launched an invasion "" \
               ""of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part "" \
               ""of the Axis' military forces into a war of attrition. In December 1941, Japan attacked "" \
               ""the United States and European territories in the Pacific Ocean, and quickly conquered much of "" \
               ""the Western Pacific."")
</code></pre>

<p>You can now create a simple pipes structure(more about pipes in readme of this project):</p>

<pre><code>pipes_structure = [SequencePipe([FindTokensPipe(""VERB/nsubj/*""),
                                 NamedEntityFilterPipe(),
                                 NamedEntityExtractorPipe()]),
                   FindTokensPipe(""VERB""),
                   AnyPipe([SequencePipe([FindTokensPipe(""VBD/dobj/NNP""),
                                          AggregatePipe([NamedEntityFilterPipe(""GPE""), 
                                                NamedEntityFilterPipe(""PERSON"")]),
                                          NamedEntityExtractorPipe()]),
                            SequencePipe([FindTokensPipe(""VBD/**/*/pobj/NNP""),
                                          AggregatePipe([NamedEntityFilterPipe(""LOC""), 
                                                NamedEntityFilterPipe(""PERSON"")]),
                                          NamedEntityExtractorPipe()])])]

engine = PipelineEngine(pipes_structure, doc, [0,1,2])
engine.process()
</code></pre>

<p>And in the result you will get:</p>

<pre><code>&gt;&gt;&gt;[([Germany], [conquered], [Europe]),
 ([Japan], [attacked], [the, United, States])]
</code></pre>

<p>Actually it based strongly (the finding pipes) on another library - grammaregex. You can read about it from a post:
<a href=""https://medium.com/@krzysiek89dev/grammaregex-library-regex-like-for-text-mining-49e5706c9c6d#.zgx7odhsc"" rel=""nofollow"">https://medium.com/@krzysiek89dev/grammaregex-library-regex-like-for-text-mining-49e5706c9c6d#.zgx7odhsc</a></p>

<p><strong>EDITED</strong></p>

<p>Actually the example I presented in readme discards adj, but all you need is to adjust pipe structure passed to engine according to your needs.
For instance for your sample sentences I can propose such structure/solution which give you tuple of 3 elements(subj, verb, adj) per every sentence:</p>

<pre><code>import spacy
from textpipeliner import PipelineEngine
from textpipeliner.pipes import *

pipes_structure = [SequencePipe([FindTokensPipe(""VERB/nsubj/NNP""),
                                 NamedEntityFilterPipe(),
                                 NamedEntityExtractorPipe()]),
                       AggregatePipe([FindTokensPipe(""VERB""),
                                      FindTokensPipe(""VERB/xcomp/VERB/aux/*""),
                                      FindTokensPipe(""VERB/xcomp/VERB"")]),
                       AnyPipe([FindTokensPipe(""VERB/[acomp,amod]/ADJ""),
                                AggregatePipe([FindTokensPipe(""VERB/[dobj,attr]/NOUN/det/DET""),
                                               FindTokensPipe(""VERB/[dobj,attr]/NOUN/[acomp,amod]/ADJ"")])])
                      ]

engine = PipelineEngine(pipes_structure, doc, [0,1,2])
engine.process()
</code></pre>

<p>It will give you result:</p>

<pre><code>[([Donald, Trump], [is], [the, worst])]
</code></pre>

<p>A little bit complexity is in the fact you have compound sentence and the lib produce one tuple per sentence - I'll soon add possibility(I need it too for my project) to pass a list of pipe structures to engine to allow produce more tuples per sentence. But for now you can solve it just by creating second engine for compounded sents which structure will differ only of VERB/conj/VERB instead of VERB(those regex starts always from ROOT, so VERB/conj/VERB lead you to just second verb in compound sentence):</p>

<pre><code>pipes_structure_comp = [SequencePipe([FindTokensPipe(""VERB/conj/VERB/nsubj/NNP""),
                                 NamedEntityFilterPipe(),
                                 NamedEntityExtractorPipe()]),
                   AggregatePipe([FindTokensPipe(""VERB/conj/VERB""),
                                  FindTokensPipe(""VERB/conj/VERB/xcomp/VERB/aux/*""),
                                  FindTokensPipe(""VERB/conj/VERB/xcomp/VERB"")]),
                   AnyPipe([FindTokensPipe(""VERB/conj/VERB/[acomp,amod]/ADJ""),
                            AggregatePipe([FindTokensPipe(""VERB/conj/VERB/[dobj,attr]/NOUN/det/DET""),
                                           FindTokensPipe(""VERB/conj/VERB/[dobj,attr]/NOUN/[acomp,amod]/ADJ"")])])
                  ]

engine2 = PipelineEngine(pipes_structure_comp, doc, [0,1,2])
</code></pre>

<p>And now after you run both engines you will get expected result :)</p>

<pre><code>engine.process()
engine2.process()
[([Donald, Trump], [is], [the, worst])]
[([Hillary], [is], [better])]
</code></pre>

<p>This is what you need I think. Of course I just quickly created a pipe structure for given example sentence and it won't work for every case, but I saw a lot of sentence structures and it will already fulfil quite nice percentage, but then you can just add more FindTokensPipe etc for cases which won't work currently and I'm sure after a few adjustment you will cover really good number of possible sentences(english is not too complex so...:)</p>
"
39764707,2901002.0,2016-09-29T08:05:44Z,39764652,4,"<p>You need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow""><code>concat</code></a>:</p>

<pre><code>print (pd.concat([df1[['Type','Breed','Behaviour']], 
                  df2[['Type','Breed','Behaviour']]], ignore_index=True))

             Type   Breed   Behaviour
0          Golden     Big         Fun
1           Corgi   Small       Crazy
2         Bulldog  Medium      Strong
3             Pug   Small      Sleepy
4  German Shepard     Big        Cool
5          Puddle   Small  Aggressive
</code></pre>

<p>More general is use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.intersection.html"" rel=""nofollow""><code>intersection</code></a> for columns of both <code>DataFrames</code>:</p>

<pre><code>cols = df1.columns.intersection(df2.columns)
print (cols)
Index(['Type', 'Breed', 'Behaviour'], dtype='object')

print (pd.concat([df1[cols], df2[cols]], ignore_index=True))
             Type   Breed   Behaviour
0          Golden     Big         Fun
1           Corgi   Small       Crazy
2         Bulldog  Medium      Strong
3             Pug   Small      Sleepy
4  German Shepard     Big        Cool
5          Puddle   Small  Aggressive
</code></pre>

<p>More general if <code>df1</code> and <code>df2</code> have no <code>NaN</code> values use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""nofollow""><code>dropna</code></a> for removing columns with <code>NaN</code>:</p>

<pre><code>print (pd.concat([df1 ,df2], ignore_index=True))
     Bark Sound   Behaviour   Breed Common Color Other Color            Type
0        NaN         Fun     Big         Gold       White          Golden
1        NaN       Crazy   Small        Brown       White           Corgi
2        NaN      Strong  Medium        Black        Grey         Bulldog
3         Ak      Sleepy   Small          NaN         NaN             Pug
4       Woof        Cool     Big          NaN         NaN  German Shepard
5         Ek  Aggressive   Small          NaN         NaN          Puddle               


print (pd.concat([df1 ,df2], ignore_index=True).dropna(1))
    Behaviour   Breed            Type
0         Fun     Big          Golden
1       Crazy   Small           Corgi
2      Strong  Medium         Bulldog
3      Sleepy   Small             Pug
4        Cool     Big  German Shepard
5  Aggressive   Small          Puddle
</code></pre>
"
39764866,2336654.0,2016-09-29T08:13:25Z,39764652,4,"<p>using <code>join</code> dropping columns that don't overlap</p>

<pre><code>df1.T.join(df2.T, lsuffix='_').dropna().T.reset_index(drop=True)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wb6AL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wb6AL.png"" alt=""enter image description here""></a></p>
"
39765337,2901002.0,2016-09-29T08:35:23Z,39765264,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>#convert to string and compare last value
print ((df.Col1.astype(str).str[-1] != '0') &amp; (df.Col1.notnull()))
0     True
1    False
2     True
3    False
4     True
5    False
Name: Col1, dtype: bool

print (df[(df.Col1.astype(str).str[-1] != '0') &amp; (df.Col1.notnull())])
   Col1
0   0.7
2   1.1
4   9.5
</code></pre>

<p>Another solution for comparing converted value to <code>Ã¬nt</code>, but first need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html"" rel=""nofollow""><code>fillna</code></a>:</p>

<pre><code>s = df.Col1.fillna(1)
print (df[s.astype(int) != s])
   Col1
0   0.7
2   1.1
4   9.5
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>#[30000 rows x 1 columns]
df = pd.concat([df]*10000).reset_index(drop=True)

def jez2(df):
    s = df.Col1.fillna(1)
    return (df[s.astype(int) != s])

In [179]: %timeit (df[(df.Col1.astype(str).str[-1] != '0') &amp; (df.Col1.notnull())])
10 loops, best of 3: 80.2 ms per loop

In [180]: %timeit (jez2(df))
1000 loops, best of 3: 1.16 ms per loop

In [181]: %timeit (df[df.Col1 // 1 != df.Col1].dropna())
100 loops, best of 3: 3.04 ms per loop

In [182]: %timeit (df[df['Col1'].mod(1) &gt; 0].dropna())
100 loops, best of 3: 2.58 ms per loop
</code></pre>
"
39765379,2336654.0,2016-09-29T08:37:25Z,39765264,3,"<p>use <code>//</code> division</p>

<pre><code>df[df.Col1 // 1 != df.Col1].dropna()
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZhlRy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZhlRy.png"" alt=""enter image description here""></a></p>
"
39765388,704848.0,2016-09-29T08:38:02Z,39765264,2,"<p>Another method is to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mod.html#pandas.Series.mod"" rel=""nofollow""><code>mod(1)</code></a> to calculate the modulo with 1:</p>

<pre><code>In [60]:
df[df['Col1'].mod(1) &gt; 0].dropna()

Out[60]:
   Col1
0   0.7
2   1.1
4   9.5
</code></pre>

<p>here we see the effect of <code>mod</code>, whole numbers become <code>0</code> whilst fractional portions will remain:</p>

<pre><code>In [62]:
df['Col1'].mod(1)

Out[62]:
0    0.7
1    0.0
2    0.1
3    0.0
4    0.5
5    NaN
Name: Col1, dtype: float64
</code></pre>
"
39766887,76705.0,2016-09-29T09:44:28Z,39766218,3,"<p>You can substitute the formatting function with your own. Below is just a demo of how it works, you can tune it to your own needs:</p>

<pre><code>def formatfunc(*args, **kwargs):
    value = args[0]
    if value &gt;= 0:
        return '${:,.2f}'.format(value)
    else:
        return '-${:,.2f}'.format(abs(value))

with pd.option_context('display.float_format', formatfunc):
    print(s)
</code></pre>

<p>And you get:</p>

<pre><code>0   -$1.23
1    $4.56
dtype: float64
</code></pre>
"
39766947,704848.0,2016-09-29T09:47:03Z,39766886,4,"<p>use <code>&amp;</code> with parentheses (due to operator precedence), <code>and</code> doesn't understand how to treat an array of booleans hence the warning:</p>

<pre><code>In [64]:
df = pd.DataFrame([{1:1,2:2,3:6},{1:9,2:9,3:10}])
(df &gt; 2) &amp; (df &lt; 10)

Out[64]:
       1      2      3
0  False  False   True
1   True   True  False
</code></pre>

<p>It's possible to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.between.html"" rel=""nofollow""><code>between</code></a> with <code>apply</code> but this will be slower for a large df:</p>

<pre><code>In [66]:
df.apply(lambda x: x.between(2,10, inclusive=False))

Out[66]:
       1      2      3
0  False  False   True
1   True   True  False
</code></pre>

<p>Note that this warning will get raised whenever you try to compare a df or series using <code>and</code>, <code>or</code>, and <code>not</code>, you should use <code>&amp;</code>, <code>|</code>, and <code>~</code> respectively as these bitwise operators understand how to treat arrays correctly</p>
"
39767338,6223328.0,2016-09-29T10:06:19Z,39749807,2,"<p>First of all thanks for the detailed help. </p>

<p>First, Divakar, your solutions gave substantial speed-up. With my data, the code ran for just below 2 minutes depending a bit on the chunk size. </p>

<p>I also tried my way around sklearn and ended up with </p>

<pre><code>def sklearnSearch_v3(p, p_fine, k):
    neigh = NearestNeighbors(k)
    neigh.fit(p)
    return data_coarse[neigh.kneighbors(p_fine)[1]].mean(axis=1)
</code></pre>

<p>which ended up being quite fast, for my data sizes, I get the following</p>

<pre><code>import numpy as np
from sklearn.neighbors import NearestNeighbors

m,n = 2000000,20000
p_fine = np.random.rand(m,3)
p = np.random.rand(n,3)
data_coarse = np.random.rand(n)
k = 3
</code></pre>

<p>yields</p>

<pre><code>%timeit sklearv3(p, p_fine, k)
1 loop, best of 3: 7.46 s per loop
</code></pre>
"
39767451,2336654.0,2016-09-29T10:12:05Z,39766886,1,"<p><code>between</code> is a convenient method for this.  However, it is only for series objects.  we can get around this by either using <code>apply</code> which operates on each row (or column) which is a series.  Or, reshape the dataframe to a series with <code>stack</code></p>

<p>use <code>stack</code>, <code>between</code>, <code>unstack</code></p>

<pre><code>df.stack().between(2, 10, inclusive=False).unstack()
</code></pre>

<p><a href=""http://i.stack.imgur.com/aP2fY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aP2fY.png"" alt=""enter image description here""></a></p>
"
39770093,5270506.0,2016-09-29T12:16:19Z,39769958,1,"<p>Your list comprehension was slightly off. Also if you want <code>0.4</code> when the hour is between <code>7</code> and <code>22</code>, you need <code>7&lt;= hour &lt;= 22</code>:</p>

<pre><code>import numpy as np
hourOfDay = np.mod(range(0, 100), 24)

newList = [0.4 if 7 &lt;= i &lt;= 22 else 0.2 for i in hourOfDay]

&gt;&gt;&gt; newList
[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2]
</code></pre>
"
39770117,2867928.0,2016-09-29T12:17:16Z,39769958,1,"<p>You can use a mask, but note that for refusing of type casting you should create the first array with data type float.:</p>

<pre><code>In [15]: hourOfDay = np.mod(range(0, 100), 24, dtype=np.float)

In [16]: mask = np.logical_or(hourOfDay &lt;= 7, hourOfDay &gt;= 22)

In [17]: hourOfDay[mask] = 0.4

In [19]: hourOfDay[~mask] = 0.2

In [20]: hourOfDay
Out[20]: 
array([ 0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.2,  0.2,  0.2,
        0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,
        0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.2,
        0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,
        0.2,  0.2,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,
        0.4,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,
        0.2,  0.2,  0.2,  0.2,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,  0.4,
        0.4,  0.4,  0.4,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.2,
        0.2,  0.2,  0.2,  0.2,  0.2,  0.2,  0.4,  0.4,  0.4,  0.4,  0.4,
        0.4])
</code></pre>
"
39770304,3196458.0,2016-09-29T12:25:46Z,39768848,1,"<p>Post your configuration if you can.</p>

<p>The documentation says that the <em><strong><code>LoadPlugin</code></strong></em> configuration goes in the collectd.conf file (not that that seems to matter in your case from your log file).</p>

<p>Put your foobar.pm module at <strong><em><code>/path/to/perl/plugins/Collectd/Plugins/FoorBar.pm</code></em></strong> matching it with the path that you specified ... (match the case of names of plugin and plugin pm file).</p>

<pre><code>LoadPlugin perl
# ...
&lt;Plugin perl&gt;
  IncludeDir ""/path/to/perl/plugins""
  BaseName ""Collectd::Plugins""
  EnableDebugger """"
  LoadPlugin ""FooBar""

  &lt;Plugin FooBar&gt;
    Foo ""Bar""
  &lt;/Plugin&gt;
&lt;/Plugin&gt;
</code></pre>
"
39770349,2063361.0,2016-09-29T12:27:53Z,39769958,1,"<p>One of the alternative approach is to use <code>map()</code> as:</p>

<pre><code>map(lambda x: 0.4 if 7 &lt;= x &lt;= 22 else 0.2, hourOfDay)
</code></pre>
"
39771165,3293881.0,2016-09-29T13:05:25Z,39770863,1,"<p>Here's an approach -</p>

<pre><code>def indexing_with_clipping(arr, indices, clipping_value=0):
    idx = np.where(indices &lt; arr.shape,indices,clipping_value)
    return arr[idx[:, 0], idx[:, 1]]
</code></pre>

<p>Sample runs -</p>

<pre><code>In [266]: arr
Out[266]: 
array([[0, 1, 2, 3],
       [5, 6, 7, 8]])

In [267]: indices
Out[267]: 
array([[0, 0],
       [1, 1],
       [1, 9]])

In [268]: indexing_with_clipping(arr,indices,clipping_value=0)
Out[268]: array([0, 6, 5])

In [269]: indexing_with_clipping(arr,indices,clipping_value=1)
Out[269]: array([0, 6, 6])

In [270]: indexing_with_clipping(arr,indices,clipping_value=2)
Out[270]: array([0, 6, 7])

In [271]: indexing_with_clipping(arr,indices,clipping_value=3)
Out[271]: array([0, 6, 8])
</code></pre>

<hr>

<p>With focus on memory and performance efficiency, here's an approach that modifies the indices within the function -</p>

<pre><code>def indexing_with_clipping_v2(arr, indices, clipping_value=0):
    indices[indices &gt;= arr.shape] = clipping_value
    return arr[indices[:, 0], indices[:, 1]]
</code></pre>

<p>Sample run -</p>

<pre><code>In [307]: arr
Out[307]: 
array([[0, 1, 2, 3],
       [5, 6, 7, 8]])

In [308]: indices
Out[308]: 
array([[0, 0],
       [1, 1],
       [1, 9]])

In [309]: indexing_with_clipping_v2(arr,indices,clipping_value=2)
Out[309]: array([0, 6, 7])
</code></pre>
"
39771199,4128833.0,2016-09-29T13:07:32Z,39684300,0,"<p>Since tensorflow uses Eigen, try to use an MKL enabled version of Eigen as described <a href=""https://eigen.tuxfamily.org/dox/TopicUsingIntelMKL.html"" rel=""nofollow"">here</a>:</p>

<blockquote>
  <ol>
  <li>define the EIGEN_USE_MKL_ALL macro before including any Eigen's header</li>
  <li>link your program to MKL libraries (see the <a href=""http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/"" rel=""nofollow"">MKL linking advisor</a>)</li>
  <li>on a 64bits system, you must use the LP64 interface (not the ILP64 one)</li>
  </ol>
</blockquote>

<p>So one way to do it is to follow the above steps to modify the source of tensorflow, recompile and install on your machine. While you're at it you should also try the Intel compiler, which might provide a decent performance boost by itself, if you <a href=""https://software.intel.com/en-us/articles/step-by-step-optimizing-with-intel-c-compiler"" rel=""nofollow"">set the correct flags</a>: <code>-O3 -xHost -ipo</code>.</p>
"
39771262,4108461.0,2016-09-29T13:10:10Z,39770863,1,"<p>You can use list comprehension:</p>

<pre><code>b[
    [min(x,len(b[0])-1) for x in a[:,0]],
    [min(x,len(b[1])-1) for x in a[:,1]]
]
</code></pre>

<p><strong>edit</strong> I used last array value as your clipping value, but you can replace the <code>min()</code> function with whatever you want (e.g. trenary operator)</p>

<p><strong>edit2</strong> OK, based on clarification in comments and all python-fu that I could put together, this snipped finally does what you need:</p>

<pre><code>clipping_value = -1
tmp=np.append(b,[[clipping_value],[clipping_value]],axis=1)
tmp[zip(*[((x,y) if (x&lt;b.shape[0] and y&lt;b.shape[1]) else (0,b.shape[1])) for (x,y) in zip(a.transpose()[0],a.transpose()[1])])]
</code></pre>

<p>It is the same as above, just creates ndarray <code>tmp</code>, which is a copy of <code>b</code> but contains the <code>clipping_value</code> as its last element and then uses my previous solution to set indices so, that they point to the last element if either of the indices is bigger than dimensions of <code>b</code>.</p>

<p>I learned that there is reverse to the <code>zip</code> function and that the numpy arrays accept lists as indices. It was fun. Thanks.</p>
"
39771892,613246.0,2016-09-29T13:37:46Z,39673377,1,"<p>The <a href=""https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP"" rel=""nofollow"">numpy_indexed</a> package (disclaimer: I am its author) contains functionality to efficiently perform these type of operations:</p>

<pre><code>import numpy_indexed as npi
npi.group_by(a[:, :2]).split(a)
</code></pre>
"
39772188,2757925.0,2016-09-29T13:49:58Z,39753717,1,"<p>I believe you cannot use <code>QScintilla</code> directly with <code>QTextEdit</code>. </p>

<p>But have a look at this question: stackoverflow.com/questions/20951660/â¦ and if you want to see the usage <code>QTextEdit</code> (or <code>QPlainTextEdit</code>) with <code>QSyntaxHiglighter</code>, see for example this: <a href=""http://wiki.python.org/moin/PyQt/Python%20syntax%20highlighting"" rel=""nofollow"">http://wiki.python.org/moin/PyQt/Python%20syntax%20highlighting</a> or this <a href=""http://carsonfarmer.com/2009/07/syntax-highlighting-with-pyqt/"" rel=""nofollow"">http://carsonfarmer.com/2009/07/syntax-highlighting-with-pyqt/</a> which uses very basic syntax highlighter for Python code.</p>
"
39773284,633958.0,2016-09-29T14:37:24Z,39768934,2,"<p>As you have guessed, the problem is in how you insert the date value. I'll quote the <a href=""https://docs.mongodb.com/manual/core/index-ttl/"" rel=""nofollow"">docs</a>:</p>

<blockquote>
  <p>If the indexed field in a document is not a date or an array that
  holds a date value(s), the document will not expire.</p>
</blockquote>

<p>You are casting the date to a string. If you are using the pymongo driver, he will handle datetimes nicely and convert it to MongoDB native <a href=""https://docs.mongodb.com/manual/reference/bson-types/#date"" rel=""nofollow"">Date type</a>.</p>

<p>This way, the following should work:</p>

<pre><code>""date"": datetime.utcnow()
</code></pre>
"
39773913,3293881.0,2016-09-29T15:07:09Z,39773480,3,"<p>Here's an approach that flattens into a <code>1D</code> array, uses <code>np.unique</code> to assign unique IDs to each element and then splits back into list of arrays -</p>

<pre><code>lens = np.array(map(len,sequences))
seq_arr = np.concatenate(sequences)
ids = np.unique(seq_arr,return_inverse=1)[1]
out = np.split(ids,lens[:-1].cumsum())
</code></pre>

<p>Sample run -</p>

<pre><code>In [391]: sequences = np.array([[12431253, 123412531, 12341234,12431253, 145345],
     ...:                      [5463456, 1244562, 23452],
     ...:                      [243524, 141234,12431253, 456367], 
     ...:                      [456345, 12431253], 
     ...:                      [75635, 14145, 12346,12431253]])

In [392]: out
Out[392]: 
[array([12, 13, 11, 12,  5]),
 array([10,  9,  2]),
 array([ 6,  4, 12,  8]),
 array([ 7, 12]),
 array([ 3,  1,  0, 12])]

In [393]: np.array(map(list,out)) # If you need NumPy array as final o/p
Out[393]: 
array([[12, 13, 11, 12, 5], [10, 9, 2], [6, 4, 12, 8], [7, 12],
       [3, 1, 0, 12]], dtype=object)
</code></pre>
"
39774016,2336654.0,2016-09-29T15:11:26Z,39773480,2,"<p><strong><em>Option 1</em></strong><br>
Using series definition</p>

<pre><code>stop = sequences.sequence.apply(np.size).cumsum()
start = end.shift().fillna(0).astype(int)
params = pd.concat([start, stop], axis=1, keys=['start', 'stop'])
params.apply(lambda x: list(np.arange(**x)), axis=1)

0    [0, 1, 2, 3]
1       [4, 5, 6]
2       [7, 8, 9]
3        [10, 11]
4    [12, 13, 14]
dtype: object
</code></pre>
"
39774354,6754140.0,2016-09-29T15:26:14Z,39773560,4,"<p>Yep! You can call <code>delete()</code> on the table object with an associated whereclause. </p>

<p>Something like this:</p>

<p><code>stmt = Users.__table__.delete().where(Users.id.in_(subquery...))</code></p>

<p>(and then don't forget to execute the statement: <code>engine.execute(stmt)</code>)</p>

<p><a href=""http://docs.sqlalchemy.org/en/latest/core/selectable.html#sqlalchemy.sql.expression.TableClause.delete"" rel=""nofollow"">source</a></p>
"
39774790,1461210.0,2016-09-29T15:48:50Z,39773922,3,"<p>Indexing with a list or array <a href=""http://docs.scipy.org/doc/numpy/user/basics.indexing.html#index-arrays"" rel=""nofollow"">always returns a copy rather than a view</a>:</p>

<pre><code>In [1]: np.may_share_memory(data, data[[1, 3, 4]])
Out[1]: False
</code></pre>

<p>Therefore the assignment <code>data[[1, 3, 4]][""y""] = 22</code> is modifying a <em>copy</em> of <code>data[[1, 3, 4]]</code>, and the original values in <code>data</code> will be unaffected.</p>

<p>On the other hand, referencing a field of a structured array <a href=""http://docs.scipy.org/doc/numpy/user/basics.rec.html#introduction"" rel=""nofollow"">returns a view</a>:</p>

<pre><code>In [2]: np.may_share_memory(data, data[""y""])
Out[2]: True
</code></pre>

<p>so assigning to <code>data[""y""][[1, 3, 4]]</code> <em>will</em> affect the corresponding elements in <code>data</code>.</p>
"
39775311,2336654.0,2016-09-29T16:16:04Z,39774826,2,"<p>Try <code>seaborn</code></p>

<pre><code>import seaborn as sns
import pandas as pd

def r(df):
    return df.loc[df.name].reset_index(drop=True)

data = df.mean().groupby(level=0).apply(r) \
         .rename_axis(['grp', 'cat']).reset_index(name='mu')

ax = sns.barplot(x='grp', y='mu', hue='cat', data=data)

ax.legend_.remove()
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x() + .05, height + 1, df.columns.levels[1][i])
</code></pre>

<p><a href=""http://i.stack.imgur.com/QPVeu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QPVeu.png"" alt=""enter image description here""></a></p>
"
39776042,6207849.0,2016-09-29T16:58:18Z,39774826,2,"<p>You could simply perform <code>unstack</code> after calculating the <code>mean</code> of the <code>DF</code> to render the bar plot.</p>

<pre><code>import seaborn as sns
sns.set_style('white')

#color=0.75(grey)
df.mean().unstack().plot.bar(color=list('rbg')+['0.75'], rot=0, figsize=(8,8)) 
</code></pre>

<p><a href=""http://i.stack.imgur.com/crBu3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/crBu3.png"" alt=""Image""></a></p>

<hr>

<p><strong>Data:</strong> (As per the edited post)</p>

<pre><code>df
</code></pre>

<p><a href=""http://i.stack.imgur.com/DTy03.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DTy03.png"" alt=""Image""></a></p>

<p>Prepare the multiindex <code>DF</code> by creating an extra column by repeating the labels according to the selections of columns(Here, 4).</p>

<pre><code>df_multi_col = df.T.reset_index()
df_multi_col['labels'] = np.concatenate((np.repeat('A', 4), np.repeat('B', 4)))
df_multi_col.set_index(['labels', 'index'], inplace=True)
df_multi_col
</code></pre>

<p><a href=""http://i.stack.imgur.com/fMbiK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fMbiK.png"" alt=""Image""></a></p>

<pre><code>df_multi_col.mean(1).unstack().plot.bar(color=list('rbg')+['0.75'], rot=0, figsize=(6,6), width=2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/uGYVO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uGYVO.png"" alt=""Image""></a></p>
"
39777583,1209622.0,2016-09-29T18:33:24Z,39777348,2,"<p>There is no way to do that because calling Python or bash will run everything within their own context (that ends when the script ends).</p>

<p>You could achieve those results by using <code>source</code>, since that will actually execute your (shell) script in the current shell. i.e., call your example script with <code>source foomaker.bash</code> instead of <code>bash foomaker.bash</code></p>
"
39778489,1513933.0,2016-09-29T19:27:40Z,39778443,1,"<p>Use a loop:</p>

<pre><code>files = [forms.FileInput() for i in range(20)]
</code></pre>
"
39778711,4763235.0,2016-09-29T19:42:23Z,39778443,3,"<p>You can use Django dynamic Form generation</p>

<pre><code>from django import forms

class MyForm(forms.Form):

    def __init__(self, *args, **kwargs):
        super(MyForm, self).__init__(*args, **kwargs)

        for i in range(20):
            self.fields[""file_%d"" % i] = forms.FileInput()
</code></pre>

<p>See Docs <a href=""https://code.djangoproject.com/wiki/CookBookNewFormsDynamicFields"" rel=""nofollow"">here</a>.</p>
"
39778758,4211135.0,2016-09-29T19:45:42Z,39778657,3,"<p>In a function declaration, <code>**kwargs</code> will take all unspecified keyword arguments and convert them into a dictionary.</p>

<pre><code>&gt;&gt;&gt; test_dict = {'a':1, 'b':2}
&gt;&gt;&gt; def test(**kwargs):
...     print (kwargs)
...
&gt;&gt;&gt; test(**test_dict)
{'b': 2, 'a': 1}
</code></pre>

<p>Note that the dictionary object has to be converted using <code>**</code> when it is passed to the function (<code>test(**test_dict)</code>) and when it is received by the function. It is impossible to do the following:</p>

<pre><code>&gt;&gt;&gt; test(test_dict)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: test() takes 0 positional arguments but 1 was given
</code></pre>

<p>So, in your example, the first <code>**kwargs</code> unpacks the keyword arguments into a dictionary, and then the second packs them back up to be sent to the parent.</p>

<p>A function with <code>**kwargs</code> in the signature can either received an unpacked dictionary or unspecified keyword arguments. Here's an example of the second case:</p>

<pre><code>&gt;&gt;&gt; def test(arg1, **kwargs):
...     print (kwargs)
...
&gt;&gt;&gt; test('first', a=1, b=2)
{'b': 2, 'a': 1}
</code></pre>
"
39778764,3244542.0,2016-09-29T19:45:53Z,39778657,2,"<p>Here at your function definition, it is accepting multiple arguments, and parsing them into a dict. <code>def get_context_data(self, **kwargs):</code> </p>

<p>So now, kwargs is a dictionary object. So if you pass it to  <code>.get_context_data(kwargs)</code> it would have to expect only a single incoming argument, and treat it as a dictionary. </p>

<p>So when you do <code>**kwargs</code> a second time, you are blowing up the dictionary back into keyword arguments that will expand into that functions call.</p>
"
39779024,4952130.0,2016-09-29T20:02:36Z,39778978,13,"<p>The easiest and, arguably, best way to do this would be by utilizing the built-in <a href=""https://docs.python.org/3/library/functions.html#isinstance""><code>isinstance</code></a> with the <code>bytes</code> type:</p>

<pre><code>some_str = b'hello world'
if isinstance(some_str, bytes):
    print('bytes')
elif isinstance(some_str, str):
    print('str')
else:
    # handle
</code></pre>

<p>Since, a byte literal will <em>always</em> be an instance of <code>bytes</code>, <code>isinstance(some_str, bytes)</code> will, of course, evaluate to <code>True</code>.</p>
"
39779592,3885739.0,2016-09-29T20:39:52Z,39778978,4,"<p>Just to complement the other answer, the built-in <a href=""https://docs.python.org/3/library/functions.html#type"" rel=""nofollow""><code>type</code></a> also gives you this information. You can use it with <code>is</code> and the corresponding type to check accordingly.</p>

<p>For example, in Python 3:</p>

<pre><code>a = 'foo'
print(type(a) is str)   # prints `True`
a = b'foo'
print(type(a) is bytes) # prints `True` as well
</code></pre>
"
39779933,1634191.0,2016-09-29T20:59:17Z,39779488,1,"<p><code>AlignIO</code> doesn't seem to be the tool you want for this job. You have a file presumably with many sequences, not with many multiple sequence alignments, so you probably want to use <code>SeqIO</code>, not <code>AlignIO</code> (<a href=""http://biopython.org/wiki/AlignIO"" rel=""nofollow"">source</a>).  This is why the shape of your array is (1, 99, 16926), because you have 1 alignment of 99 sequences of length 16926.</p>

<p>If you just want an array of the sequences (which it appears you do from the <code>np.character</code> dtype supplied to <code>np.array</code>), then do the following:</p>

<pre><code>import numpy as np
from Bio import SeqIO
records = SeqIO.parse(""ma-all-mito.fa"", ""fasta"")
align_array = np.array([record.seq for record in records], np.character)
print(""Array shape %i by %i"" % align_array.shape)
# expect to be (99, 16926)
</code></pre>

<p>Note above that technically each element of <code>records</code> is also a BioPython <code>SeqRecord</code> which includes the sequence in addition to metadata.  <code>list(record)</code> is a shortcut for getting the sequence, the other way being <code>record.seq</code>.  Either should work, but I chose using the attribute way since it is more explicit.</p>
"
39781928,771848.0,2016-09-30T00:27:48Z,39781887,7,"<p>Don't miss the <em>bigger picture</em>. Here is a better way to approach the problem in general.</p>

<p>What if you would define the ""mixes"" dictionary where you would have mixes of colors as keys and the resulting colors as values.</p>

<p>One idea for implementation is to use immutable by nature <a href=""https://docs.python.org/3/library/stdtypes.html#frozenset"" rel=""nofollow""><code>frozenset</code></a>s as mapping keys:</p>

<pre><code>mixes = {
    frozenset(['blue', 'yellow']): 'green'
}

color1 = input(""Color 1: "")
color2 = input(""Color 2: "")

mix = frozenset([color1, color2])
if mix in mixes:
    print(""{0} + {1} = {2}"".format(color1, color2, mixes[mix]))
</code></pre>

<p>This way you may easily <em>scale</em> the solution up, add different mixes without having multiple if/else nested conditions.</p>
"
39781933,2867928.0,2016-09-30T00:28:37Z,39781887,19,"<p>You can use <code>set</code>s for comparison.</p>

<blockquote>
  <p>Two sets are equal if and only if every element of each set is contained in the other</p>
</blockquote>

<pre><code>In [35]: color1 = ""blue""

In [36]: color2 = ""yellow""

In [37]: {color1, color2} == {""blue"", ""yellow""}
Out[37]: True

In [38]: {color2, color1} == {""blue"", ""yellow""}
Out[38]: True
</code></pre>
"
39782312,6902061.0,2016-09-30T01:23:40Z,39782108,0,"<pre><code>if record in entries[truck]:
    entries[truck][record].append(amount)
else:
    entries[truck][record] = [amount]
</code></pre>

<p>I believe this is what you would want? Now we are directly accessing the truck's records, instead of trying to check a local dictionary called <code>records</code>. Just like you did if there wasn't any entry of a truck.</p>
"
39783069,6891327.0,2016-09-30T03:12:14Z,39732600,0,"<p>I was having similar problems when trying to perform a stack buffer overflow. I found that my return address in GDB was different than that in a normal process. What I did was add the following:</p>

<pre><code>unsigned long printesp(void){
    __asm__(""movl %esp,%eax"");
}
</code></pre>

<p>And called it at the end of main right before <code>Return</code> to get an idea where the stack was. From there I just played with that value subtracting 4 from the printed ESP until it worked.</p>
"
39784698,1688590.0,2016-09-30T06:01:42Z,39779488,1,"<p>I'm answering to your problem instead of fixing your code. If you want to keep only certain positions, you want to use <code>AlignIO</code>:</p>

<p>FASTA sample <code>al.fas</code>:</p>

<pre><code>&gt;seq1
CATCGATCAGCATCGACATGCGGCA-ACG
&gt;seq2
CATCGATCAG---CGACATGCGGCATACG
&gt;seq3
CATC-ATCAGCATCGACATGCGGCATACG
&gt;seq4
CATCGATCAGCATCGACAAACGGCATACG
</code></pre>

<p>Now suppose you want to keep only certain positions. <a href=""http://biopython.org/DIST/docs/api/Bio.Align.MultipleSeqAlignment-class.html"" rel=""nofollow"" title=""MultipleSeqAlignment"">MultipleSeqAlignment</a> allows you to <em>query</em> the alignment like a numpy array:</p>

<pre><code>from Bio import AlignIO


al = AlignIO.read(""al.fas"", ""fasta"")

# Print the 11th column
print(al[:, 10])

# Print the 12-15 columns
print(al[:, 11:14])
</code></pre>

<p>If you want to know the shape of the alignment, use <code>len</code> and <code>get_alignment_length</code>:</p>

<pre><code>&gt;&gt;&gt; print(len(al), al.get_alignment_length())
4 29
</code></pre>

<hr>

<p>When you use <code>AlignIO.parse()</code> to load an alignment, it assumes the file to be parsed could contain more than one alignment (PHYLIP does this). Thus the parser returns an iterator over each alignment and not over records as your code implies. But your FASTA file only contain one alignment per file and <code>parse()</code> yields only one <code>MultipleSeqAlignment</code>. So the fix to your code is:</p>

<pre><code>alignment = AlignIO.read(""ma-all-mito.fa"", ""fasta"")
align_array = np.array(alignment, np.character)
print(""Array shape %i by %i"" % align_array.shape)
</code></pre>
"
39786003,4226926.0,2016-09-30T07:29:12Z,39742624,0,"<p>That's because of SQL to get table names. Look at db_sqlanywhere_re_grt.py:142, there is:</p>

<pre><code>SELECT st.table_name
FROM SYSTAB st LEFT JOIN SYSUSER su ON st.creator=su.user_id
WHERE su.user_name = '%s' AND st.table_type = 1
</code></pre>

<p>This is valid sql for sql anywhere version &lt; 10, and I guess you have newest version. So you can edit and update that file with compatible SQL. 
Keep in mind there is much more SQL's that also need to be updated in that file.
In meantime, please fill the bug report at <a href=""http://bugs.mysql.com"" rel=""nofollow"">bugs.mysql.com</a>.</p>
"
39786695,5540305.0,2016-09-30T08:11:26Z,39779587,1,"<p><strong>Entropy is a measure of impurity.</strong> So if a node is pure it means entropy is zero. </p>

<p>Have a look at <a href=""https://github.com/HashCode55/ML_from_scratch/blob/master/decision_tree.py"" rel=""nofollow"">this</a> - </p>

<pre><code>def information_gain(data, column, cut_point):
    """"""
    For calculating the goodness of a split. The difference of the entropy of parent and 
    the weighted entropy of children.
    :params:attribute_index, labels of the node t as `labels` and cut point as `cut_point`
    :returns: The net entropy of partition 
    """"""
    subset1, subset2 = divide_data(data, column, cut_point) 
    lensub1, lensub2 = len(subset1), len(subset2)  
    #if the node is pure return 0 entropy
    if len(subset1) == 0 or len(subset2) == 0:
        return (0, subset1, subset2)     
    weighted_ent = (len(subset1)*entropy(subset1) + len(subset2)*entropy(subset2)) / len(data)  
    return ((entropy(data) - weighted_ent), subset1, subset2)
</code></pre>
"
39787233,2901002.0,2016-09-30T08:42:54Z,39787103,2,"<p>You need add <code>[0]</code>, because you need select first item of list:</p>

<pre><code>last_row = df.tail(1).index[0]
print (last_row)
2016-09-15 11:00:00
</code></pre>

<hr>

<pre><code>last_row = df.tail(1).index
print (last_row)
DatetimeIndex(['2016-09-15 11:00:00'], dtype='datetime64[ns]', freq=None)
</code></pre>

<p>Better solution is simple select last value of index by <code>[-1]</code>:</p>

<pre><code>last_row = df.index[-1]
print (last_row)
2016-09-15 11:00:00
</code></pre>
"
39787879,5014455.0,2016-09-30T09:13:42Z,39787787,4,"<p>Sure:</p>

<pre><code>&gt;&gt;&gt; from pandas import Timestamp
&gt;&gt;&gt; Timestamp('9:45')    
Timestamp('2016-09-30 09:45:00')
&gt;&gt;&gt; Timestamp('17:10')
Timestamp('2016-09-30 17:10:00')
&gt;&gt;&gt; Timestamp('17:10') - Timestamp('9:45')
Timedelta('0 days 07:25:00')
&gt;&gt;&gt; td = Timestamp('17:10') - Timestamp('9:45')
&gt;&gt;&gt; td
Timedelta('0 days 07:25:00')
</code></pre>

<p>And if you really need the time in minutes:</p>

<pre><code>&gt;&gt;&gt; td.seconds/60
445
&gt;&gt;&gt;
</code></pre>

<h2> Edit </h2>

<p>Just double-checked the documentation, and the <code>seconds</code> attribute of a Timedelta object returns: <code>Number of seconds (&gt;= 0 and less than 1 day)</code>.</p>

<p>So, to really be safe if you want the minutes, use:</p>

<pre><code>&gt;&gt;&gt; td.delta # returns the number of nanoseconds   
26700000000000
&gt;&gt;&gt; td.delta * 1e-9 / 60
445.0
</code></pre>
"
39788063,1700939.0,2016-09-30T09:22:58Z,39675844,0,"<p>I just had my first look at <code>lua</code>, which included the <a href=""https://www.lua.org/cgi-bin/demo?sieve"" rel=""nofollow""><code>sieve.lua</code> live demo</a>. It is an implementation of the sieve of Erathostenes using coroutines. My immediate thought was: This would look much cleaner in python:</p>

<pre><code>#!/usr/bin/env python3

# sieve.py
# the sieve of Eratosthenes programmed with a generator functions
# typical usage: ./sieve.py 500 | column

import sys

# generate all the numbers from 2 to n
def gen(n):
    for i in range(2,n):
        yield i

# filter the numbers generated by `g', removing multiples of `p'
def filter(p, g):
    for n in g:
        if n%p !=0:
            yield n

N=int(sys.argv[1]) if len(sys.argv)&gt;1 else 500 # from command line
x=gen(N)                     # generate primes up to N
while True:
    try:
        n = next(x)          # pick a number until done
    except StopIteration:
        break
    print(n)                 # must be a prime number
    x = filter(n, x)         # now remove its multiples
</code></pre>

<p>This does not have much to do with the question, but on my machine using <code>Python 3.4.3</code> a stack overflow happens somewhere for <code>N&gt;7500</code>. Using <code>sieve.lua</code> with <code>Lua 5.2.3</code> the stack overflow happens already at <code>N&gt;530</code>.</p>

<p>Generator objects (which represent a suspended coroutine) can be passed around like any other object, and the next() built-in can be applied to it in any place, so coroutines in python are first-class. Please correct me if I am wrong.</p>
"
39788395,5161084.0,2016-09-30T09:40:11Z,39787787,2,"<p>I think you can also use the Python standard library <code>datetime.datetime</code> without installing the pandas.</p>

<pre><code>&gt;&gt;&gt; t1 = datetime.datetime.strptime('09:45', '%H:%M')
&gt;&gt;&gt; t2 = datetime.datetime.strptime('17:10', '%H:%M')
&gt;&gt;&gt; t1
datetime.datetime(1900, 1, 1, 9, 45)
&gt;&gt;&gt; t2
datetime.datetime(1900, 1, 1, 17, 10)
&gt;&gt;&gt; td = t2 - t1
&gt;&gt;&gt; td.total_seconds()
26700.0
&gt;&gt;&gt; str(td)
'7:25:00'
</code></pre>
"
39791114,2756793.0,2016-09-30T12:04:16Z,39712602,3,"<p>This is a bit clunky but it yields the right output at least for your small example:</p>

<pre><code>import pandas as pd

data = {'Employee ID': [""100"", ""100"", ""100"",""100"",""200"",""200"",""200"",""300""],
        'Effective Date': [""2016-01-01"",""2015-06-05"",""2014-07-01"",""2013-01-01"",""2016-01-01"",""2015-01-01"",""2013-01-01"",""2014-01-01""],
        'Desired Output': [""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 1""]}
df = pd.DataFrame(data, columns=['Employee ID','Effective Date','Desired Output'])

df[""Effective Date""] = pd.to_datetime(df[""Effective Date""])
df = df.sort_values([""Employee ID"",""Effective Date""]).reset_index(drop=True)

for i,_ in df.iterrows():
  df.ix[0,""Result""] = ""Unique Leave Event 1""
  if i &lt; len(df)-1:
    if df.ix[i+1,""Employee ID""] == df.ix[i,""Employee ID""]:
      if df.ix[i+1,""Effective Date""] - df.ix[i,""Effective Date""] &gt; pd.Timedelta('365 days'):
        df.ix[i+1,""Result""] = ""Unique Leave Event "" + str(int(df.ix[i,""Result""].split()[-1])+1)
      else:
        df.ix[i+1,""Result""] = df.ix[i,""Result""]
    else:
      df.ix[i+1,""Result""] = ""Unique Leave Event 1""
</code></pre>

<p>Note that this code assumes that the first row always contains the string <code>Unique Leave Event 1</code>.</p>

<p>EDIT: Some explanation.</p>

<p>First I convert the dates to datetime format and then reorder the dataframe such that the dates for every Employee ID are ascending.</p>

<p>Then I iterate over the rows of the frame using the built-int iterator <code>iterrows</code>. The <code>_</code> in <code>for i,_</code> is merely a placeholder for the second variable I do not use because the iterator gives back both row numbers and rows, I only need the numbers here.</p>

<p>In the iterator I'm doing row-wise comparisons so by default I fill in the first row by hand and then assign to the <code>i+1</code>-th row. I do it like this because I know the value of the first row but not the value of the last row. Then I compare the <code>i+1</code>-th row with the <code>i</code>-th row within an <code>if</code>-safeguard because <code>i+1</code> would give an index-error on the last iteration.</p>

<p>In the loop I first check if the <code>Employee ID</code> has changed between the two rows. If it has not then I compare the dates of the two rows and see if they are apart more than 365 days. If this is the case I read the string <code>""Unique Leave Event X""</code> from the <code>i</code>-th row, increase the number by one and write it in the <code>i+1</code>-row. If the dates are closer I just copy the string from the previous row.</p>

<p>If the <code>Employee ID</code> does change on the other hand I just write <code>""Unique Leave Event 1""</code> to start over.</p>

<p>Note 1: <code>iterrows()</code> has no options to set so I can't iterate only over a subset.</p>

<p>Note 2: Always iterate using one of the built-in iterators and only iterate if you can't solve the problem otherwise.</p>

<p>Note 3: When assigning values in an iteration always use <code>ix</code>, <code>loc</code>, or <code>iloc</code>.</p>
"
39791339,1330293.0,2016-09-30T12:16:33Z,39764582,1,"<p>It looks like you can already swap rows preserving sparsity so the missing part is the algorithm to sort the rows. So you need a function that gives you a ""leftness"" score. A heuristic that could work is the following:</p>

<ol>
<li>first get a mask of the non-zero elements (you don't care about the actual values, only about its non-zeroness).</li>
<li><p>Estimate the density distribution of the non-zero values along the column axis:</p>

<pre><code>def density(row, window):
    padded = np.insert(row, 0, 0)
    cumsum = np.cumsum(padded) 
    return (cumsum[window:] - cumsum[:-window]) / window
</code></pre></li>
<li><p>Calculate the leftness score as the column with the maximum left-penalised density (looking from the right):</p>

<pre><code>def leftness_score(row):
    n = len(a)
    window = n / 10   # 10 is a tuneable hyper parameter
    smoothness = 1    # another parameter to play with
    d = density(row)
    penalization = np.exp(-smoothness * np.arange(n))
    return n - (penalization * d).argmax()
</code></pre></li>
</ol>

<p>This algorithm gives higher score to rows having a high density of values as long as the max value of this density is not too far to the right. Some ideas to take it further: improve the density estimation, play with different penalization functions (instead of neg exp), fit the parameters to some synthethic data reflecting your expected sorting, etc. </p>
"
39791949,361427.0,2016-09-30T12:50:13Z,39704298,5,"<p>had the same problem (or something similar). I solved it by doing:</p>

<p>[Warning: dirty solution]</p>

<pre><code>if not hasattr(django, 'apps'):
    django.setup()
</code></pre>

<p>this way it'll be called only once even if it's imported multiple times</p>
"
39793063,1929187.0,2016-09-30T13:47:06Z,39658717,-1,"<p>I don't know much about matplotlib or jupyter. However, Graphs interest me. I just did some googling and came across this <a href=""http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/"" rel=""nofollow"">post</a>. Seems like you have to render the graph as an HTML video to see a dynamic graph. </p>

<p>I tried that post. <a href=""http://www.filedropper.com/displayanimationasvideoavconv"" rel=""nofollow"">This</a> is the notebook, if you wish to try. Note that the kernel (python 2) takes sometime to build the video. You can read more about it <a href=""https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/"" rel=""nofollow"">here</a>.</p>

<p>Now you want to display a graph row to row. I tried <a href=""http://www.filedropper.com/showdownload.php/randomgraphvideo"" rel=""nofollow"">this</a>. In that notebook, I have a <code>dump_data</code> with 10 rows. I randomly take one and plot them and display as video. </p>

<p>It was interesting to learn about jupyter. Hope this helps.</p>
"
39793409,4300257.0,2016-09-30T14:05:14Z,39782955,0,"<p>So, after going through a lot of the documentation, I've finally figured out a few things.</p>

<p>Firstly, the NoneType error was due to the fact that for a bar chart, you need to pass the dataframe as well as the ColumnDataSource, for the bar-chart to display. 
So the code needed to be:</p>

<pre><code>bar = Bar(df_act5236920,values='activities',label='studs',title = ""Activity 5236920 performed by students"",
      xlabel=""Students"",ylabel=""Activity"",legend=False,tools=TOOLS,source=source)
</code></pre>

<p>Notice how the dataframe name and source=source are both mentioned in the Bar() method.
For the second issue of the value not being displayed, I used @height which essentially displayed the height of the selected bar, which in this case was the count value.</p>

<pre><code>hover.tooltips = OrderedDict([
   (""Student Name"", ""@studs""),
   (""Access Count"", ""@height""),
]) 
</code></pre>

<p>For the student name value, @x and @studs both work. But the only thing I still couldn't resolve was that although I have mentioned the ColumnDataSource ""source"", it didn't really accomplish anything for me because when I try to use @activities in hover.tooltips, it still gives me a response of ""???"". So, I'm not sure what that is all about. And it is an issue that I'm sturggling with in another Time Series visualisation that I'm trying to build.</p>
"
39793589,3510736.0,2016-09-30T14:14:11Z,39793508,2,"<p>It depends what you want exactly:</p>

<ol>
<li><p>If you want the two figures overlayed, then you can call <a href=""http://stackoverflow.com/questions/21465988/python-equivalent-to-hold-on-in-matlab""><code>hold</code></a><code>(True)</code> after the first, then plot the second, then call <code>hold(False)</code>.</p></li>
<li><p>If you want the two figures in a single figure, but side by side (or one over the other), then you can use <a href=""http://matplotlib.org/examples/pylab_examples/subplot_demo.html"" rel=""nofollow""><code>subplot</code></a>. E.g., call <code>subplot(2, 1, 1)</code> before plotting the first, then <code>subplot(2, 1, 2)</code> before the second.</p></li>
</ol>
"
39793772,2034301.0,2016-09-30T14:23:03Z,39759240,0,"<p>To do this on the command line, make sure you provide both an input and output symbol table.  The command should be something like</p>

<pre><code>fstdraw --isymbols=input_syms.txt --osymbols=output_syms.txt fst.bin
</code></pre>

<p>I haven't used ""PyFST"", but I would suggest you use the Python bindings that are included in OpenFst 1.5.1 and later.  The Python support has been getting better in versions 1.5.x, so it's best to use 1.5.3 or later.</p>

<p>If using the OpenFST provided Python bindings, make sure you set the symbol tables before attempting to draw.</p>

<pre><code>fst.input_symbols = your_symbol_table
fst.output_symbols = your_symbol_table
fst.draw(""fst.dot"")
</code></pre>

<p>There is more documentation for these Python bindings available here: <a href=""http://www.openfst.org/twiki/bin/view/FST/PythonExtension"" rel=""nofollow"">http://www.openfst.org/twiki/bin/view/FST/PythonExtension</a></p>

<p>If that doesn't help, could you post some example code please?</p>
"
39794082,1648033.0,2016-09-30T14:38:57Z,39782108,2,"<p>For the kind of calculation you are doing here, I highly recommend <a href=""http://pandas.pydata.org/"" rel=""nofollow"">Pandas</a>.</p>

<p>Assuming <code>in.csv</code> looks like this:</p>

<pre><code>truck,type,amount
A,spent,100
A,earned,60
B,earned,48
B,earned,180
A,spent,40
</code></pre>

<p>You can do the totalling with three lines of code:</p>

<pre><code>import pandas
df = pandas.read_csv('in.csv')
totals = df.groupby(['truck', 'type']).sum()
</code></pre>

<p><code>totals</code> now looks like this:</p>

<pre><code>              amount
truck type          
A     earned      60
      spent      140
B     earned     228
</code></pre>

<p>You will find that Pandas allows you to think on a much higher level and avoid fiddling with lower level data structures in cases like this.</p>
"
39794464,636626.0,2016-09-30T14:57:48Z,39748285,5,"<p>The CP product of, for example, 4 matrices</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=X_%7Babcd%7D+%3D+%5Cdisplaystyle%5Csum_%7Bz%3D0%7D%5E%7BZ%7D%7BA_%7Baz%7D+B_%7Bbz%7D+C_%7Bcz%7D+D_%7Bdz%7D%7D+%2B+%5Cepsilon_%7Babcd%7D"" alt=""X_{abcd} = \displaystyle\sum_{z=0}^{Z}{A_{az} B_{bz} C_{cz} D_{dz} + \epsilon_{abcd}}""></p>

<p>can be expressed using <a href=""https://en.wikipedia.org/wiki/Einstein_notation"" rel=""nofollow"">Einstein notation</a> as</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=X_%7Babcd%7D+%3D+A_%7Baz%7D+B_%7Bbz%7D+C_%7Bcz%7D+D_%7Bdz%7D+%2B+%5Cepsilon_%7Babcd%7D"" alt=""X_{abcd} = A_{az} B_{bz} C_{cz} D_{dz} + \epsilon_{abcd}""></p>

<p>or in numpy as</p>

<pre><code>numpy.einsum('az,bz,cz,dz -&gt; abcd', A, B, C, D)
</code></pre>

<p>so in your case you would use</p>

<pre><code>numpy.einsum('az,bz-&gt;ab', P.U[0], P.U[1])
</code></pre>

<p>or, in your 3-matrix case</p>

<pre><code>numpy.einsum('az,bz,cz-&gt;abc', P.U[0], P.U[1], P.U[2])
</code></pre>

<p><code>sktensor.ktensor.ktensor</code> also have a method <code>totensor()</code> that does exactly this:</p>

<pre><code>np.allclose(np.einsum('az,bz-&gt;ab', P.U[0], P.U[1]), P.totensor())
&gt;&gt;&gt; True
</code></pre>
"
39796429,2336654.0,2016-09-30T16:53:31Z,39794016,5,"<p><strong><em>Setup</em></strong>  </p>

<pre><code>d1 = pd.DataFrame(dict(
        year=np.random.choice((2014, 2015, 2016), 100),
        cntry=['United States' for _ in range(100)],
        State=np.random.choice(states, 100),
        Col1=np.random.randint(0, 20, 100),
        Col2=np.random.randint(0, 20, 100),
        Col3=np.random.randint(0, 20, 100),
    ))

df = d1.groupby(['year', 'cntry', 'State']).agg(['size', 'sum'])
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/3igw5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3igw5.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Answer</em></strong><br>
Easiest way would have been to only run <code>size</code> after <code>groupby</code></p>

<pre><code>d1.groupby(['year', 'cntry', 'State']).size()

year  cntry          State        
2014  United States  California       10
                     Florida           9
                     Massachusetts     8
                     Minnesota         5
2015  United States  California        9
                     Florida           7
                     Massachusetts     4
                     Minnesota        11
2016  United States  California        8
                     Florida           8
                     Massachusetts    11
                     Minnesota        10
dtype: int64
</code></pre>

<hr>

<p>To use the calculated <code>df</code></p>

<pre><code>df.xs('size', axis=1, level=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/eiwXV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eiwXV.png"" alt=""enter image description here""></a></p>

<p>And that would be useful if the <code>size</code> were different for each column.  But because the <code>size</code> column is the same for <code>['Col1', 'Col2', 'Col3']</code>, we can just do</p>

<pre><code>df[('Col1', 'size')]

year  cntry          State        
2014  United States  California       10
                     Florida           9
                     Massachusetts     8
                     Minnesota         5
2015  United States  California        9
                     Florida           7
                     Massachusetts     4
                     Minnesota        11
2016  United States  California        8
                     Florida           8
                     Massachusetts    11
                     Minnesota        10
Name: (Col1, size), dtype: int64
</code></pre>

<hr>

<p><strong><em>Combined View 1</em></strong></p>

<pre><code>pd.concat([df[('Col1', 'size')].rename('size'),
           df.xs('sum', axis=1, level=1)], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/gPHTA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gPHTA.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>Combined View 2</em></strong>  </p>

<pre><code>pd.concat([df[('Col1', 'size')].rename(('', 'size')),
           df.xs('sum', axis=1, level=1, drop_level=False)], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/8WiC6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8WiC6.png"" alt=""enter image description here""></a></p>
"
39796737,5061557.0,2016-09-30T17:14:48Z,39794016,2,"<p>piRSquared beat me to it but if you must do it this way and want to keep the alignment with columns and sum or size underneath you could reindex the columns to remove the size value and then add in a new column to contain the size value.</p>

<p>For example:</p>

<pre><code>group = df.groupby(['year', 'cntry','state']).agg(['sum','size'])
mi = pd.MultiIndex.from_product([['Col1','Col2','Col3'],['sum']])
group = group.reindex_axis(mi,axis=1)
sizes = df.groupby('state').size().values
group['Tot'] = 0
group.columns = group.columns.set_levels(['sum','size'], level=1)
group.Tot.size = sizes
</code></pre>

<p>It will end up looking like this:</p>

<pre><code>                 Col1 Col2 Col3  Tot
                  sum  sum  sum size
year cntry State
2015 US    CA      20    0    4    1
           FL      40    3    5    1
           MASS     8    1    3    1
           MN      12    2    3    1
</code></pre>
"
39796814,2539738.0,2016-09-30T17:20:12Z,39796519,0,"<p>well, I would get rid of the ""fout"" line.  You don't seem to write to that file, and it doesn't need to be open to use the ""read_csv"" feature of pandas.  then you can go through each row and find what's zero, and what isn't</p>

<pre><code>returnArray = []
i=0
while i &lt; len(df.values):
    j=14 #since user only cares about column 14-26
    while j &lt; len(df.values[i]):
        if df.values[i][j] == 0:
            returnArray.append([i,j])
        j=j+1
    i=i+1
</code></pre>
"
39796873,2336654.0,2016-09-30T17:23:36Z,39796519,1,"<p>Consider the <code>pd.DataFrame</code> <code>df</code></p>

<pre><code>cols = ['JAN', 'FEB', 'MAR', 'APR',
        'MAY', 'JUN', 'JUL', 'AUG',
        'SEP', 'OCT', 'NOV', 'DEC']

df = pd.DataFrame(np.random.randint(0, 3, (10, 12)), columns=cols)
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/1iTj9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1iTj9.png"" alt=""enter image description here""></a></p>

<hr>

<p>I'll use each rows evaluation of <code>row == 0</code> to be a boolean mask on the columns themselves.  Use <code>list</code> to fit nicely back into a <code>pd.Series</code></p>

<pre><code>df.eq(0).apply(lambda x: list(df.columns[x]), 1)

0                   [FEB, MAR, APR, NOV]
1                        [FEB, OCT, NOV]
2              [JAN, APR, AUG, NOV, DEC]
3                        [MAR, APR, SEP]
4                   [MAY, JUN, NOV, DEC]
5                        [APR, AUG, NOV]
6         [MAR, APR, JUN, OCT, NOV, DEC]
7    [JAN, FEB, APR, JUL, OCT, NOV, DEC]
8              [MAY, JUL, AUG, SEP, OCT]
9         [FEB, MAR, APR, JUN, AUG, SEP]
dtype: object
</code></pre>

<hr>

<p>To get the number of days</p>

<pre><code>days_in_month = pd.Series(dict(
        JAN=31, FEB=28, MAR=31,
        APR=30, MAY=31, JUN=30,
        JUL=31, AUG=31, SEP=30,
        OCT=31, NOV=30, DEC=31
    ))

df.eq(0).dot(days_in_month)

0    119
1     89
2    153
3     91
4    122
5     91
6    183
7    212
8    154
9    180
dtype: int64
</code></pre>
"
39796953,3836111.0,2016-09-30T17:29:38Z,39796852,14,"<p>Well let's take your <code>tooth</code> example - here is what the regex-engine does (a lot simplified for better understanding)</p>

<p>Start with <code>t</code> then look ahead in the string - and fail the lookahead, as there is another <code>t</code>.</p>

<pre><code>tooth
^  Â°
</code></pre>

<p>Next take <code>o</code>, look ahead in the string - and fail, as there is another <code>o</code>.</p>

<pre><code>tooth
 ^Â°
</code></pre>

<p>Next take the second <code>o</code>, look ahead in the string - no other <code>o</code> present - match it, return it, work done.</p>

<pre><code>tooth
  ^
</code></pre>

<p>So your regex doesn't match the first unrepeated character, but the first one, that has no further repetitions towards the end of the string.</p>
"
39796971,4211135.0,2016-09-30T17:30:49Z,39796852,3,"<p>The reason why your regex is not working is that it will not match a character that is <em>followed</em> by the same character, but there is nothing to prevent it from matching a character that isn't <em>followed</em> by the same character, <em>even if</em> it is preceded by the same character.</p>
"
39798947,303931.0,2016-09-30T19:38:21Z,39719567,6,"<p>I agree with knbk's answer that it is not possible: durability is only present at the level of a transaction, and atomic provides that. It does not provide it at the level of save points. Depending on the use case, there may be workarounds.</p>

<p>I'm guessing your use case is something like:</p>

<pre><code>@atomic  # possibly implicit if ATOMIC_REQUESTS is enabled
def my_view():
    run_some_code()  # It's fine if this gets rolled back.
    charge_a_credit_card()  # It's not OK if this gets rolled back.
    run_some_more_code()  # This shouldn't roll back the credit card.
</code></pre>

<p>I think you'd want something like:</p>

<pre><code>@transaction.non_atomic_requests
def my_view():
    with atomic():
        run_some_code()
    with atomic():
        charge_a_credit_card()
    with atomic():
        run_some_more_code()
</code></pre>

<p>If your use case is for credit cards specifically (as mine was when I had this issue a few years ago), my coworker discovered that <a href=""https://support.stripe.com/questions/does-stripe-support-authorize-and-capture"">credit card processors actually provide mechanisms for handling this</a>. A similar mechanism might work for your use case, depending on the problem structure:</p>

<pre><code>@atomic
def my_view():
    run_some_code()
    result = charge_a_credit_card(capture=False)
    if result.successful:
        transaction.on_commit(lambda: result.capture())
    run_some_more_code()
</code></pre>

<p>Another option would be to use a non-transactional persistence mechanism for recording what you're interested in, like a log database, or a redis queue of things to record.</p>
"
39799676,2336654.0,2016-09-30T20:33:57Z,39798594,3,"<p>This one is tricky!</p>

<p>I first evaluate which elements share the same <code>'y'</code> values as it's neighbor.<br>
Then I check who has the same <code>'z'</code> as their neighbor.<br>
A new group is when neither of these things are true.</p>

<pre><code>y_chk = df.y.eq(df.y.shift())
z_chk = df.z.eq(df.z.shift())
grps = (~y_chk &amp; ~z_chk).cumsum()
df['w'] = df.groupby(grps).x.transform(pd.Series.head, n=1)
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/8hAuu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8hAuu.png"" alt=""enter image description here""></a></p>
"
39801000,772649.0,2016-09-30T22:32:49Z,39799821,5,"<p>You can design a bandstop filter:</p>

<pre><code>wc = freq[np.argmax(amplitude)] / (0.5 / dt)
wp = [wc * 0.9, wc / 0.9]
ws = [wc * 0.95, wc / 0.95]
b, a  = signal.iirdesign(wp, ws, 1, 40)
f = signal.filtfilt(b, a, f)
</code></pre>
"
39801665,5395658.0,2016-10-01T00:06:53Z,39799821,1,"<p>I wanted something like this. I know that there is better solution but my hacked works. :) Second peak is removed.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from scipy.fftpack import rfft, irfft, fftfreq, fft
import scipy.fftpack

# Number of samplepoints
N = 500
# sample spacing
T = 0.1

x = np.linspace(0.0, N*T, N)
y = 5*np.sin(x) + np.cos(2*np.pi*x) 

yf = scipy.fftpack.fft(y)
xf = np.linspace(0.0, 1.0/(2.0*T), N/2)
#fft end

f_signal = rfft(y)
W = fftfreq(y.size, d=x[1]-x[0])

cut_f_signal = f_signal.copy()
cut_f_signal[(W&gt;0.6)] = 0

cut_signal = irfft(cut_f_signal)



# plot results
f, axarr = plt.subplots(1, 3)
axarr[0].plot(x, y)
axarr[0].plot(x,5*np.sin(x),'g')

axarr[1].plot(xf, 2.0/N * np.abs(yf[:N//2]))
axarr[1].legend(('numpy fft * dt'), loc='upper right')
axarr[1].set_xlabel(""f"")
axarr[1].set_ylabel(""amplitude"")


axarr[2].plot(x,cut_signal)
axarr[2].plot(x,5*np.sin(x),'g')

plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/zIbct.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zIbct.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/s46SR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/s46SR.png"" alt=""enter image description here""></a></p>
"
39802037,2750492.0,2016-10-01T01:08:40Z,39801978,0,"<p>The <code>stdout</code> encoding is defined by the environment that is executing the python script, e.g.:</p>

<pre><code>$ python -c ""import sys; print(sys.stdout.encoding)""
UTF-8
$ LC_CTYPE=ascii python -c ""import sys; print(sys.stdout.encoding)""
US-ASCII
</code></pre>

<p>Try adjusting your environment before running the script. You can force the encoding value for Python by setting the <a href=""https://docs.python.org/3.5/using/cmdline.html#envvar-PYTHONIOENCODING"" rel=""nofollow""><code>PYTHONIOENCODING</code></a> environment variable.</p>
"
39802235,1768245.0,2016-10-01T01:47:21Z,39590741,0,"<p>Can you try to install opencv on mac using brew?</p>

<pre><code>brew reinstall opencv3 --HEAD --with-python3 --with-ffmpeg --with-tbb --with-contrib
</code></pre>

<p>Worked for me on MAC OS SIERRA.</p>
"
39802359,5846630.0,2016-10-01T02:12:57Z,39714611,2,"<p>I found that to resolve local DNS was taking forever to resolve.</p>

<p>If anyone has the same problem run this commands:</p>

<p>sudo scutil --get LocalHostName
sudo scutil --get HostName</p>

<p>If the result is not the same, use this commands to put them equal:</p>

<p>sudo scutil --set LocalHostName My-MacBook
sudo scutil --set HostName My-MacBook</p>

<p>Problem solved to me.</p>
"
39802679,772649.0,2016-10-01T03:18:19Z,39737300,-1,"<p>Use a <code>set</code> object to keep all the substrings. This is faster but use a lot of memory, if every string is short, you can try this.</p>

<pre><code>import string
import random
from itertools import combinations

def get_substrings(w):
    return (w[s:e] for s, e in combinations(range(len(w)+1), 2))

def get_not_substrings(words):
    words = sorted(set(words), key=len, reverse=True)
    substrings = set()

    for w in words:
        if w not in substrings:
            yield w
            substrings.update(get_substrings(w))

words = ["""".join(random.choice(string.ascii_lowercase) 
    for _ in range(random.randint(1, 12))) for _ in range(10000)]
res = list(get_not_substrings(words))
</code></pre>
"
39802797,364696.0,2016-10-01T03:40:52Z,39737300,0,"<p>Using <a href=""https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm"" rel=""nofollow"">Aho-Corasick</a> should allow you to get asymptotic run time of <code>O(n)</code>, at the expense of adding additional memory usage, and higher fixed multiplier on costs (ignored by big-O notation, but still meaningful). The complexity of the algorithm is the sum of several components, but none of them multiply, so it should be linear by all metrics (number of strings, length of strings, longest string, etc.).</p>

<p>Using <a href=""https://pypi.python.org/pypi/pyahocorasick"" rel=""nofollow""><code>pyahocorasick</code></a>, you'd do an initial pass to make an automaton that can scan for all of the strings at once:</p>

<pre><code>import ahocorasick

# This code assumes no duplicates in mystrings (which would make them mutually
# substrings). Easy to handle if needed, but simpler to avoid for demonstration

mystrings = ['abc','abcd','ab','def','efgd']

# Build Aho-Corasick automaton, involves O(n) (in combined length of mystrings) work
# Allows us to do single pass scans of a string for all strings in mystrings
# at once
aut = ahocorasick.Automaton()
for s in mystrings:
    # mapping string to itself means we're informed directly of which substring
    # we hit as we scan
    aut.add_word(s, s)
aut.make_automaton()

# Initially, assume all strings are non-substrings
nonsubstrings = set(mystrings)

# Scan each of mystrings for substrings from other mystrings
# This only involves a single pass of each s in mystrings thanks to Aho-Corasick,
# so it's only O(n+m) work, where n is again combined length of mystrings, and
# m is the number of substrings found during the search
for s in mystrings:
    for _, substr in aut.iter(s):
        if substr != s:
           nonsubstrings.discard(substr)

# A slightly more optimized version of the above loop, but admittedly less readable:
# from operator import itemgetter
# getsubstr = itemgetter(1)
# for s in mystrings:
#     nonsubstrings.difference_update(filter(s.__ne__, map(getsubstr, aut.iter(s))))

for nonsub in nonsubstrings:
    print(nonsub)
</code></pre>

<p>Note: Annoyingly, I'm on a machine without a compiler right now, so I can't install <code>pyahocorasick</code> to test this code, but I've used it before, and I believe this should work, modulo stupid typos.</p>
"
39803163,1072724.0,2016-10-01T04:52:00Z,39737300,2,"<p>Is memory an issue?  You could turn to the tried and true...TRIE! </p>

<p>Build a suffix tree!</p>

<p>Given your input <code>['abc','abcd','ab','def','efgd']</code></p>

<p>We would have a tree of </p>

<pre><code>              _
            / | \
           a  e  d
          /   |   \
         b*   f    e
        /     |     \
       c*     g      f*
      /       |
     d*       d*
</code></pre>

<p>Utilizing a DFS (Depth-First-Search) search of said tree you would locate the deepest leafs <code>abcd</code>, <code>efgd</code>, and <code>def</code></p>

<p>Tree traversal is pretty straight forward and your time complexity is <code>O(n*m).</code> A much better improvement over the <code>O(n^2)</code> time you had previously.  </p>

<p>With this approach it becomes simple to add new keys and still make it easy to find the unique keys. </p>

<p>Consider adding the key <code>deg</code> </p>

<p>your new tree would be approximately </p>

<pre><code>              _
            / | \
           a  e  d
          /   |   \
         b*   f    e
        /     |   / \
       c*     g  g*   f*
      /       |
     d*       d*
</code></pre>

<p>With this new tree it is still a simple matter of performing a DFS search to obtain the unique keys that are not prefixes of others. </p>

<pre><code>from typing import List


class Trie(object):
    class Leaf(object):
        def __init__(self, data, is_key):
            self.data = data
            self.is_key = is_key
            self.children = []

        def __str__(self):
            return ""{}{}"".format(self.data, ""*"" if self.is_key else """")

    def __init__(self, keys):
        self.root = Trie.Leaf('', False)
        for key in keys:
            self.add_key(key)

    def add_key(self, key):
        self._add(key, self.root.children)

    def has_suffix(self, suffix):
        leaf = self._find(suffix, self.root.children)

        if not leaf:
            return False

        # This is only a suffix if the returned leaf has children and itself is not a key
        if not leaf.is_key and leaf.children:
            return True

        return False

    def includes_key(self, key):
        leaf = self._find(key, self.root.children)

        if not leaf:
            return False

        return leaf.is_key

    def delete(self, key):
        """"""
        If the key is present as a unique key as in it does not have any children nor are any of its nodes comprised of
         we should delete all of the nodes up to the root
        If the key is a prefix of another long key in the trie, umark the leaf node
        if the key is present in the trie and contains no children but contains nodes that are keys we should delete all
         nodes up to the first encountered key
        :param key:
        :return:
        """"""

        if not key:
            raise KeyError

        self._delete(key, self.root.children, None)

    def _delete(self, key, children: List[Leaf], parents: (List[Leaf], None), key_idx=0, parent_key=False):
        if not parents:
            parents = [self.root]

        if key_idx &gt;= len(key):
            return

        key_end = True if len(key) == key_idx + 1 else False
        suffix = key[key_idx]
        for leaf in children:
            if leaf.data == suffix:
                # we have encountered a leaf node that is a key we can't delete these
                # this means our key shares a common branch
                if leaf.is_key:
                    parent_key = True

                if key_end and leaf.children:
                    # We've encountered another key along the way
                    if parent_key:
                        leaf.is_key = False
                    else:
                        # delete all nodes recursively up to the top of the first node that has multiple children
                        self._clean_parents(key, key_idx, parents)
                elif key_end and not leaf.children:
                    # delete all nodes recursively up to the top of the first node that has multiple children
                    self._clean_parents(key, key_idx, parents)

                # Not at the key end so we need to keep traversing the tree down
                parents.append(leaf)
                self._delete(key, leaf.children, parents, key_idx + 1, key_end)

    def _clean_parents(self, key, key_idx, parents):
        stop = False
        while parents and not stop:
            p = parents.pop()

            # Need to stop processing a removal at a branch
            if len(p.children) &gt; 1:
                stop = True

            # Locate our branch and kill its children
            for i in range(len(p.children)):
                if p.children[i].data == key[key_idx]:
                    p.children.pop(i)
                    break
            key_idx -= 1

    def _find(self, key, children: List[Leaf]):
        if not key:
            raise KeyError

        match = False
        if len(key) == 1:
            match = True

        suffix = key[0]
        for leaf in children:
            if leaf.data == suffix and not match:
                return self._find(key[1:], leaf.children)
            elif leaf.data == suffix and match:
                return leaf
        return None

    def _add(self, key, children: List[Leaf]):
        if not key:
            return

        is_key = False
        if len(key) == 1:
            is_key = True

        suffix = key[0]
        for leaf in children:
            if leaf.data == suffix:
                self._add(key[1:], leaf.children)
                break
        else:
            children.append(Trie.Leaf(suffix, is_key))
            self._add(key[1:], children[-1].children)

        return

    @staticmethod
    def _has_children(leaf):
        return bool(leaf.children)


def main():
    keys = ['ba', 'bag', 'a', 'abc', 'abcd', 'abd', 'xyz']
    trie = Trie(keys)
    print(trie.includes_key('ba'))  # True
    print(trie.includes_key('b'))  # False
    print(trie.includes_key('dog'))  # False
    print(trie.has_suffix('b'))  # True
    print(trie.has_suffix('ab'))  # True
    print(trie.has_suffix('abd'))   # False

    trie.delete('abd')  # Should only remove the d
    trie.delete('a')    # should unmark a as a key
    trie.delete('ba')   # should remove the ba trie
    trie.delete('xyz')  # Should remove the entire branch
    trie.delete('bag')  # should only remove the g

    print(trie)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>Please note the above trie implementation does not have a DFS search implemented; however, provides you with some amazing legwork to get started. </p>
"
39803320,2563446.0,2016-10-01T05:23:53Z,39779587,0,"<p>The entropy of a split measures the uncertainty associated with the class labels in that split. In a binary classification problem (classes = {0,1}), the probability of class 1 (in your text, x) can range from 0 to 1. The entropy is maximum (with a value of 1) when x=0.5. Here both classes are equally probable. The entropy is minimum when one of the classes is absent, i.e. either x=0 or x=1. Here, there is no uncertainty regarding the class, hence the entropy is 0.</p>

<p><br/><br/>
Graph of entropy (y-axis) vs x (x-axis):</p>

<p><a href=""http://i.stack.imgur.com/GXnDC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GXnDC.png"" alt=""Graph of entropy (y-axis) vs x (x-axis)""></a></p>

<p><br/><br/>
The following calculation shows how to deal with the entropy calculation mathematically, when x=0 (the case when x=1 is analogous):</p>

<p><a href=""http://i.stack.imgur.com/rCrwe.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rCrwe.png"" alt=""enter image description here""></a></p>

<p><br/>
In your program, you could treat x=0 and x=1 as special cases, and return 0. For other values of x, the above equation can be used directly.</p>
"
39804441,3160881.0,2016-10-01T08:16:27Z,39804375,2,"<p>Try this</p>

<pre><code>from datetime import datetime


print(sorted(persons, key=lambda x: datetime.strptime(x['passport']['birth_info']['date'], ""%d/%m/%Y""))) #reverse=True for descending order.
</code></pre>
"
39804445,2072459.0,2016-10-01T08:17:01Z,39804375,6,"<p>The function <code>sorted()</code> provides the <code>key</code> argument. One can define a callable which returns the key to compare the items:</p>

<pre><code>sorted(persons, key=lambda x: x['passport']['birth_info']['date'])
</code></pre>

<p>The argument x is an item of the given list of persons. </p>

<p>If the dates are strings you could use the <code>datetime</code> module:</p>

<pre><code>sorted(persons, key=lambda x: datetime.datetime.strptime(x['passport']['birth_info']['date'], '%m/%d/%Y'))
</code></pre>
"
39804570,6207849.0,2016-10-01T08:34:30Z,39798594,1,"<p>Make all null strings as <code>NaN</code> values by replacing them. Next, group them according to 'y' and fill all the missing values with the value corresponding to it's first valid index present in 'z'. </p>

<p>Then, perform groupby operation on 'z', by applying sum which aggregates all the values present in 'x' together. Slice it accordingly to fill all the values in that group with that particular value(Here, slice=0).</p>

<p>Convert it to a dictionary to create the mapping and finally assign it back to a new column, 'w' as shown:</p>

<pre><code>df_new = df.replace('Nan', np.NaN)
df_new['z'] = df_new.groupby('y')['z'].transform(lambda x: x.loc[x.first_valid_index()])
df['w'] = df_new['z'].map(df_new.groupby('z')['x'].apply(lambda x: x.sum()[0]).to_dict())
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/V6RLm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/V6RLm.png"" alt=""Image""></a></p>
"
39805488,2431997.0,2016-10-01T10:24:48Z,39662847,2,"<p>Just add:</p>

<pre><code>from __future__ import unicode_literals
</code></pre>

<p>at the beginning of your source code files - it has to be the first import and it has to be in all source code files affected and the headache of using unicode in Python-2.7 goes away. If you didn't do anything super weird with strings then it should get rid of the problem out of the box.<br>
Check out the following Copy&amp;Paste from my console - I tried with the sample from your question:</p>

<pre><code>user@linux2:~$ python
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; someDynamicStr = ""bar"" # could come from various sources

&gt;&gt;&gt;
&gt;&gt;&gt; # works
... u""foo"" + someDynamicStr
u'foobar'
&gt;&gt;&gt; u""foo{}"".format(someDynamicStr)
u'foobar'
&gt;&gt;&gt;
&gt;&gt;&gt; someDynamicStr = ""\xff"" # uh-oh
&gt;&gt;&gt;
&gt;&gt;&gt; # raises UnicodeDecodeError
... u""foo"" + someDynamicStr
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
uUnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)
""&gt;&gt;&gt; u""foo{}"".format(someDynamicStr)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)
&gt;&gt;&gt;
</code></pre>

<p>And now with <code>__future__</code> magic:</p>

<pre><code>user@linux2:~$ python
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from __future__ import unicode_literals
&gt;&gt;&gt; someDynamicStr = ""bar"" # could come from various sources
&gt;&gt;&gt;
&gt;&gt;&gt; # works
... u""foo"" + someDynamicStr
u'foobar'
&gt;&gt;&gt; u""foo{}"".format(someDynamicStr)
u'foobar'
&gt;&gt;&gt;
&gt;&gt;&gt; someDynamicStr = ""\xff"" # uh-oh
&gt;&gt;&gt;
&gt;&gt;&gt; # raises UnicodeDecodeError
... u""foo"" + someDynamicStr
u'foo\xff'
&gt;&gt;&gt; u""foo{}"".format(someDynamicStr)
u'foo\xff'
&gt;&gt;&gt; 
</code></pre>
"
39805998,613246.0,2016-10-01T11:22:43Z,39800223,2,"<p>If I understand your question correctly, the <a href=""https://pypi.python.org/pypi/numpy-indexed"" rel=""nofollow"">numpy_indexed</a> package (disclaimer: I am its author) has a fast and elegant solution to this:</p>

<pre><code># generate a random example graph
n_edges = 50
n_nodes = 10
n_types = 3
edges = np.random.randint(0, n_nodes, size=(n_edges, 2))
node_types = np.random.randint(0, 2, size=(n_nodes, n_types)).astype(np.bool)

# Note; this is for a directed graph
s, e = edges.T
# for undirected, add reversed edges
s, e = np.concatenate([edges, edges[:,::-1]], axis=0).T
import numpy_indexed as npi
node_idx, neighbor_type_count = npi.group_by(s).sum(node_types[e])
</code></pre>

<p>In general, operations on graphs, or algorithms involving jagged-arrays, can often be efficiently and elegantly expressed using grouping-operations.</p>
"
39806897,6758673.0,2016-10-01T12:59:39Z,39776890,1,"<p>You can intepolate to determine the skewness and interpolate again to correct it. </p>

<pre><code>import numpy as np
from scipy.ndimage.interpolation import map_coordinates

m, n = g.shape
j_shift = np.interp(g[:,0], g[0,:], np.arange(n))
pad = int(np.max(j_shift))
i, j = np.indices((m, n + pad))
z = map_coordinates(g, [i, j - j_shift[:,None]], cval=np.nan)
</code></pre>

<p>This works on the example image, but you have to do some additional checks to make it function on other gradients. It does not work on gradients that are nonlinear in the x-direction though. Demo:</p>

<p><a href=""http://i.stack.imgur.com/hYyAb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hYyAb.png"" alt=""demo""></a></p>

<p>Full script:</p>

<pre><code>import numpy as np
from scipy.ndimage.interpolation import map_coordinates

def fix(g):
    x = 1 if g[0,0] &lt; g[0,-1] else -1
    y = 1 if g[0,0] &lt; g[-1,0] else -1
    g = g[::y,::x]

    m, n = g.shape
    j_shift = np.interp(g[:,0], g[0,:], np.arange(n))
    pad = int(np.max(j_shift))
    i, j = np.indices((m, n + pad))
    z = map_coordinates(g, [i, j - j_shift[:,None]], cval=np.nan)

    return z[::y,::x]

import matplotlib.pyplot as plt

i, j = np.indices((50,100))
g = 0.01*i**2 + j

plt.figure(figsize=(6,5))
plt.subplot(211)
plt.imshow(g[::-1], interpolation='none')
plt.title('original')
plt.subplot(212)
plt.imshow(fix(g[::-1]), interpolation='none')
plt.title('fixed')
plt.tight_layout()
</code></pre>
"
39807198,6335503.0,2016-10-01T13:27:19Z,39807008,0,"<p>For me both:</p>

<pre><code>print(id(string[0:5]))
print(id(string[-10:-5]))  
</code></pre>

<p>have different IDs, and as such, those answers:</p>

<pre><code>print(string[0:5] == string[-10:-5])    #True
print(string[0:5] is string[-10:-5])    #False
</code></pre>

<p>Are as they should be expected.</p>

<p>Are u sure u didnt make mistake here?</p>

<p># I would post that as a comment, but cant yet</p>
"
39807630,1903116.0,2016-10-01T14:12:31Z,39807586,4,"<pre><code>result = {}
with open(filetxt, 'r') as f:
    for line in f:
        # split the read line based on whitespace
        idx, c1, c2, c3 = line.split()

        # setdefault will set default value, if the key doesn't exist and
        # return the value corresponding to the key. In this case, it returns a list and
        # you append all the three values as a tuple to it
        result.setdefault(idx, []).append((int(c1), int(c2), int(c3)))
</code></pre>

<hr>

<p>Edit: Since you want to the key also to be an integer, you can <code>map</code> the <code>int</code> function over the split values, like this</p>

<pre><code>        idx, c1, c2, c3 = map(int, line.split())
        result.setdefault(idx, []).append((c1, c2, c3))
</code></pre>
"
39807752,850018.0,2016-10-01T14:26:14Z,39748267,-1,"<p>The simplest way is to use <a href=""https://docs.python.org/2/library/sys.html#sys.path"" rel=""nofollow"">sys.path</a> to make sure that you will have right order of paths added. <code>sys.path</code> gives out the list of paths that are available in the <code>PYHTONPATH</code> environment variable with the right order. If you want any path to have higher priority over others, just add it to the beginning of the list.</p>

<p>And you will also find this in the official docs:</p>

<blockquote>
  <p>A program is free to modify this list for its own purposes.</p>
</blockquote>

<p>WARNING: Even though this gives better control over the priority, just make sure that whatever library you add does not mess with the system libraries. Else, your library will be searched first, as it is in the beginning of the list, and they may replace the system libraries. Just as an example, if you have written a library by the name <code>os</code>, after adding this to the <code>sys.path</code>, that library will be imported instead of Python's in-built. So take all the caution and also with a large amount of salt before jumping to this.</p>
"
39808018,2750492.0,2016-10-01T14:54:32Z,39807586,3,"<p>You are converting your values back to comma separated strings, which you can't use in <code>for first, second, third in data</code> - so just leave them as a list <code>splitLine[1:]</code> (or convert to <code>tuple</code>).<br>
You don't need your initializing <code>for</code> loop with a <code>defaultdict</code>. You also don't need the conditional checks with <code>defaultdict</code> either.</p>

<p>Your code without the superfluous code:</p>

<pre><code>with open(filetxt, 'r') as file:
    for line in file:
       splitLine = line.split()
       info[int(splitLine[0])].append(splitLine[1:])
</code></pre>

<p>One slight difference is if you want to operate on <code>int</code>s I would convert up front:</p>

<pre><code>with open(filetxt, 'r') as file:
    for line in file:
       splitLine = list(map(int, line.split()))   # list wrapper for Py3
       info[splitLine[0]].append(splitLine[1:])
</code></pre>

<p>Actually in Py3, I would do:</p>

<pre><code>       idx, *cs = map(int, line.split())
       info[idx].append(cs)
</code></pre>
"
39808173,3734244.0,2016-10-01T15:10:44Z,39807948,2,"<p>Because iterators have enough details stored in them to enable them generate the next element of a sequence without having that ""next element"" in memory.</p>

<p>To understand what is going on let's create our own fake iterator</p>

<pre><code>class Fakeiterator:
    def __init__(self, range_list):
        self.current = range_list[0]
        self.high = range_list[-1]

    def __iter__(self):
        return self

    def __next__(self):
        if self.current &gt; self.high:
            raise StopIteration
        else:
            self.current += 1
            return self.current - 1
</code></pre>

<p>In our <code>__init__</code> method we've stored enough details (the start point and end point of our iterator) to enable us generate the next element without actually having it in memory. As far as we have this information, even though we're given a list containing 2000 elements we just need to know the start and end point</p>

<p>in our <code>__next__</code> method anytime we ask for the next element in our iterator, The iterator simply increments the current counter and return it back to us.</p>

<p>Lets test our iterator:</p>

<pre><code>&gt;&gt;&gt; x = list(range(5))
&gt;&gt;&gt; y = Fakeiterator(x)
&gt;&gt;&gt; del x
&gt;&gt;&gt; list(y)
[0, 1, 2, 3, 4]
&gt;&gt;&gt;
</code></pre>

<p>The <code>list</code> constructor repeatedly calls <code>__next__</code> until <code>StopIteration</code> is raised by our iterator and that's at the point where the current element is higher than the maximum element we stored at the creation of the iterator.</p>

<p>But in your case calling <code>iter(x)</code> on a list, returns a <code>list_iterator</code> object that <strong>STORES</strong> x internally. <code>x</code> is still stored but not with the name <code>x</code> anymore.</p>

<p>On why <code>getsizeof</code> returns a lower size which as you expected is supposed to be greater or at least equal to the size of the original list. from the docs</p>

<blockquote>
  <p>sys.getsizeof(object[, default]) Return the size of an object in
  bytes. The object can be any type of object. All built-in objects will
  return correct results, but this does not have to hold true for
  third-party extensions as it is implementation specific.</p>
  
  <p><strong>Only the memory consumption directly attributed to the object is
  accounted for, not the memory consumption of objects it refers to.</strong></p>
  
  <p>If given, default will be returned if the object does not provide
  means to retrieve the size. Otherwise a TypeError will be raised.</p>
  
  <p>getsizeof() calls the objectâs <strong>sizeof</strong> method and adds an
  additional garbage collector overhead if the object is managed by the
  garbage collector.</p>
</blockquote>

<p>To demonstrate that let's write a quick script</p>

<pre><code>import sys

x = [1, 2, 3]

print(sys.getsizeof(x))

class storex():
    def __init__(self, param):
        self.param = param

y = storex(x)

print(sys.getsizeof(y))
print(y.param, sys.getsizeof(y.param))
</code></pre>

<p>When you run the script. this is the output (on my machine, but it should be the same with yours)</p>

<pre><code>88
56
[1, 2, 3] 88
</code></pre>

<p>even though the list <code>[1, 2, 2]</code> is 88 bytes long, when we store it as an attribute of <code>storex</code> it doesn't automatically make <code>storex</code> become larger than it. because <code>storex</code> refers to it. it's not part of <code>storex</code> directly</p>

<p>But on printing the size of <code>y.param</code>, we can see that it's still the same size as the original <code>[1, 2, 3]</code> list</p>

<p>Also <code>del</code> doesn't delete the object from memory, it simply unbinds the name <code>x</code> so x won't refer to any object in memory. the value of x will only be discarded (garbage collected) when there is no reference to it again</p>

<p>Here is a demonstration of what i mean</p>

<pre><code>&gt;&gt;&gt; x = [1,2,3]
&gt;&gt;&gt; class y: pass
... 
&gt;&gt;&gt; y.x = x
&gt;&gt;&gt; id(x), id(y.x)
(140177507371016, 140177507371016)
&gt;&gt;&gt; del x
&gt;&gt;&gt; id(y.x)
140177507371016
&gt;&gt;&gt; x
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
&gt;&gt;&gt;
</code></pre>

<p>deleting <code>x</code> doesn't automatically delete <code>[1,2,3]</code> which <code>y.x</code> points to, even though their id's show that they both pointed to the same object in memory.</p>
"
39808275,6588087.0,2016-10-01T15:21:03Z,39807948,1,"<p>According to what I know, del x does not del the value in the memory since your y is still referring it. It is kind of pointer. x and y is referring to the same memory. </p>

<p>when you do del x, python will dereference the x and do garbage collection.</p>

<p>while by doing x=list(y), you are pointing the memory to x again.</p>
"
39808352,448474.0,2016-10-01T15:29:15Z,39719567,6,"<p>This type of <em>durability</em> is <strong>impossible due to ACID</strong>, with one connection. (i.e. that a nested block stays committed while the outer block get rolled back) It is a consequence of ACID, not a problem of Django. Imagine a super database and the case that table <code>B</code> has a foreign key to table <code>A</code>.</p>

<pre><code>CREATE TABLE A (id serial primary key);
CREATE TABLE B (id serial primary key, b_id integer references A (id));
-- transaction
   INSERT INTO A DEFAULT VALUES RETURNING id AS new_a_id
   -- like it would be possible to create an inner transaction
      INSERT INTO B (a_id) VALUES (new_a_id)
   -- commit
-- rollback  (= integrity problem)
</code></pre>

<p>If the inner ""transaction"" should be durable while the (outer) transaction get rolled back then the integrity would be broken. The rollback operation must be always implemented so that it can never fail, therefore no database would implement a nested independent transaction. It would be against the principle of causality and the integrity can not be guarantied after such selective rollback. It is also against atomicity. </p>

<p>The transaction is related to a database connection. If you create <strong>two connections</strong> then two independent transactions are created. One connection doesn't see uncommitted rows of other transactions (it is possible to set this <em>isolation level</em>, but it depends on the database backend) and no foreign keys to them can be created and the integrity is preserved after rollback by the database backend design.</p>

<p>Django supports multiple databases, therefore multiple connections.</p>

<pre><code># no ATOMIC_REQUESTS should be set for ""other_db"" in DATABASES

@transaction.atomic  # atomic for the database ""default""
def my_view():
    with atomic():   # or set atomic() here, for the database ""default""
        some_code()
        with atomic(""other_db""):
            row = OtherModel.objects.using(""other_db"").create(**kwargs)
        raise DatabaseError
</code></pre>

<p>The data in ""other_db"" stays committed.</p>

<p>It is probably possible in Django to create a trick with two connections to the same database like it would be two databases, with some database backends, but I'm sure that it is untested, it would be prone to mistakes, with problems with migrations, bigger load by the database backend that must create real parallel transactions at every request and it can not be optimized. It is better to use two real databases or to reorganize the code.</p>

<p>The setting DATABASE_ROUTERS is very useful, but I'm not sure yet if you are interested in multiple connections.</p>
"
39809089,3832970.0,2016-10-01T16:43:24Z,39751242,0,"<p>I suggest using</p>

<pre><code> Title:\s*(.*?)\s*Procedure|Title:\s*(.*)
</code></pre>

<p>See the <a href=""https://regex101.com/r/a8DXbK/1"" rel=""nofollow"">regex demo</a>.</p>

<p><em>Details</em>:</p>

<ul>
<li><code>Title:</code> - literal text <code>Title:</code></li>
<li><code>\s*</code> - 0+ whitespaces</li>
<li><code>(.*?)</code> - Group 1: any 0+ chars other than linebreak symbols as few as possible up to the first</li>
<li><code>\s*Procedure</code> - 0+ whitespaces + the string <code>Procedure</code></li>
<li><code>|</code> - or</li>
<li><code>Title:\s*</code> - <code>Title:</code> string + 0+ whitespaces</li>
<li><code>(.*)</code> - Group 2: zero or more any chars other than linebreak symbols as many as possible (the rest of the line).</li>
</ul>

<p><a href=""http://ideone.com/QJiZjR"" rel=""nofollow"">Python code</a>:</p>

<pre><code>import re
regex = r""Title:\s*(.*?)\s*Procedure|Title:\s*(.*)""
test_str = (""Title: Anorectal Fistula (Fistula-in-Ano) Procedure Code(s):\n\n""
    ""Effective date: 7/1/07\n\n""
    ""Title:\n\n""
    ""2003247\n\n""
    ""or previous effective dates)\n\n""
    ""Title:\n\n""
    ""ST2 Assay for Chronic Heart Failure\n\n""
    ""Description/Background\n\n""
    ""Heart Failure\n\n""
    ""HF is one among many cardiovascular diseases that comprises a major cause of morbidity and mortality worldwide. The term âheart failureâ (HF) refers to a complex clinical syndrome ."")
res = []
for m in re.finditer(regex, test_str):
    if m.group(1):
        res.append(m.group(1))
    else:
        res.append(m.group(2))
print(res)
# =&gt; ['Anorectal Fistula (Fistula-in-Ano)', '2003247', 'ST2 Assay for Chronic Heart Failure']
</code></pre>
"
39809509,3922042.0,2016-10-01T17:24:54Z,39658717,0,"<p>In addition to @0aslam0 I used code from <a href=""http://jakevdp.github.io/blog/2013/05/12/embedding-matplotlib-animations/"" rel=""nofollow"">here</a>. I've just changed animate function to get next row every next time. It draws animated evolution (M steps) of all N  points.</p>

<pre><code>from IPython.display import HTML
import numpy as np
from matplotlib import animation
N = 5
M = 100
points_evo_array = np.random.rand(M,N)

# First set up the figure, the axis, and the plot element we want to animate
fig = plt.figure()
ax = plt.axes(xlim=(0, M), ylim=(0, np.max(points_evo_array)))
lines = []

lines = [ax.plot([], [])[0] for _ in range(N)]

def init():    
    for line in lines:
        line.set_data([], [])
    return lines

def animate(i):
    for j,line in enumerate(lines):
        line.set_data(range(i), [points_evo_array[:i,j]])
    return lines

# call the animator.  blit=True means only re-draw the parts that have changed.
anim = animation.FuncAnimation(fig, animate,np.arange(1, M), init_func=init, interval=10, blit=True)

HTML(anim.to_html5_video())
</code></pre>

<p>Hope it will be useful</p>
"
39809512,487339.0,2016-10-01T17:26:12Z,39798594,2,"<p>In the general case this is a set consolidation/connected components problem.  While if we assume certain things about your data we can solve a reduced case, it's just a bit of bookkeeping to do the whole thing.</p>

<p>scipy has a connected components function we can use if we do some preparation:</p>

<pre><code>import scipy.sparse

def via_cc(df_in):
    df = df_in.copy()

    # work with ranked version
    dfr = df[[""y"",""z""]].rank(method='dense')
    # give nans their own temporary rank
    dfr = dfr.fillna(dfr.max().fillna(0) + dfr.isnull().cumsum(axis=0))
    # don't let y and z get mixed up; have separate nodes per column
    dfr[""z""] += dfr[""y""].max() 

    # build the adjacency matrix
    size = int(dfr.max().max()) + 1
    m = scipy.sparse.coo_matrix(([1]*len(dfr), (dfr.y, dfr.z)),
                                (size, size))

    # do the work to find the groups
    _, cc = scipy.sparse.csgraph.connected_components(m)

    # get the group codes
    group = pd.Series(cc[dfr[""y""].astype(int).values], index=dfr.index)
    # fill in w from x appropriately
    df[""w""] = df[""x""].groupby(group).transform(min)

    return df
</code></pre>

<p>which gives me</p>

<pre><code>In [230]: via_cc(df0)
Out[230]: 
   x    y    z  w
0  a   jj  NaN  a
1  b   ii   mm  a
2  c   kk   nn  c
3  d   ii  NaN  a
4  e  NaN   oo  e
5  f   jj   mm  a
6  g  NaN   nn  c

In [231]: via_cc(df1)
Out[231]: 
   x   y   z  w
0  a  ii  mm  a
1  b  ii  nn  a
2  c  jj  nn  a
3  d  jj  oo  a
4  e  kk  oo  a
</code></pre>

<p>If you have a set consolidation recipe around, like the one <a href=""https://rosettacode.org/wiki/Set_consolidation#Python:_Iterative"" rel=""nofollow"">here</a>, you can simplify some of the above at the cost of an external function.</p>

<p>(Aside: note that in my df0, the ""Nan""s are really NaNs.  If you have a <em>string</em> ""Nan"" (note how it's different from NaN), then the code will think it's just another string and will assume that you want all ""Nan""s to be in the same group.)</p>
"
39811752,45908.0,2016-10-01T21:45:42Z,39779538,-3,"<p><a href=""https://docs.python.org/3.5/library/ast.html"" rel=""nofollow"">ast</a> only stores the beginning line number for each node.</p>

<p>If your scripts always have another assignment (to <code>author</code>, from your example) after the <code>description</code> assignment, then you could do something like this ...</p>

<ul>
<li>Build a list of line numbers with assignments</li>
<li>When you find a <code>description</code> assignment node, look up the next assignment line</li>
<li>Use that to calculate then end of the <code>description</code> assignment</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>import ast


class Assignment(ast.NodeVisitor):
    lines = []

    def visit_Assign(self, node):
        for target in node.targets:
            self.lines.append(target.lineno)
        self.generic_visit(node)


class DescriptionUpdater(ast.NodeVisitor):

    def visit_Assign(self, node):
        if node.targets[0].id == 'description':
            current = Assignment.lines.index(node.lineno)
            next_line = Assignment.lines[current + 1]
            num_lines = next_line - node.lineno
            # update_description(node, num_lines)
        self.generic_visit(node)


with open('script.py') as script:
    tree = ast.parse(script.read())
    Assignment().visit(tree)
    DescriptionUpdater().visit(tree)
</code></pre>
"
39812014,2074832.0,2016-10-01T22:24:13Z,39811963,0,"<p>The largest number in the lookup table is 26 so you never need to lookup strings of lengths greater than 2. Modify the for loop accordingly. That might be enough to make brute force viable.</p>
"
39812097,1400793.0,2016-10-01T22:38:00Z,39811963,4,"<p>The recurrence relation you have to solve (where <code>s</code> is a string of digits, and <code>a</code> and <code>b</code> are single digits) is this:</p>

<pre><code> S("""") = 1
 S(a) = 1
 S(s + a + b) = S(s+a) + (S(s) if ab is between 10 and 26)
</code></pre>

<p>That can be computed using dynamic programming rather than backtracking. If you do it right, it's O(n) time complexity, and O(1) space complexity.</p>

<pre><code>def seq(s):
    a1, a2 = 1, 1
    for i in xrange(1, len(s)):
        a1, a2 = a1 + (a2 if 9 &lt; int(s[i-1:i+1]) &lt; 27 else 0), a1
    return a1

print seq('2222222222222222222222222222222222222222222222222222222222222222222222')
</code></pre>
"
39812454,2131849.0,2016-10-01T23:31:36Z,39806895,0,"<p>Shortly after posting this question, I thought of the answer, but gave up the phone before I could post it...</p>

<p>So instead of relying on glob to find the royal all files, have it only look for files with extensions.</p>

<p>Here's how to validate if glob is even needed:</p>

<pre><code>path = 'subdirectory/filename' # no extension

files = [ path ] # for consistancy
if not os.path.exists( path ):
    files = glob('%s.*'%path)
    if not files:
        raise IOError(""no files found"")

for f in files:
    # do whatever
</code></pre>

<p>This should work with most names, including weirdly formatted names.</p>
"
39812985,2302878.0,2016-10-02T01:18:55Z,39710796,0,"<p>I suspect the reason there isn't a tool to fix this problem is because there is no general solution, aside from performing the alignment again using software that does not exhibit this problem. In your example, the query sequence aligns perfectly to the reference and so in that case the CIGAR string is not very interesting (just a single <code>M</code>atch operation prefixed by the overall query length). In that case the fix simply requires changing <code>101M</code> to <code>98M</code>. </p>

<p>However, for more complex CIGAR strings (e.g. those that include <code>I</code>nsertions, <code>D</code>eletions, or any other operations), you would have no way of knowing which part of the CIGAR string is too long. If you subtract from the wrong part of the CIGAR string, you'll be left with a misaligned read, which is probably worse for your downstream analysis than just leaving the whole read out. </p>

<p>That said, if it happens to be trivial to get it right (perhaps your broken alignment procedure always adds extra bases to the first or last CIGAR operation), then what you need to know is the correct way to calculate the query length according to the CIGAR string, so that you know what to subtract from it. </p>

<p><code>samtools</code> calculates this using the <code>htslib</code> function <a href=""https://github.com/samtools/htslib/blob/19c189438f852e6e62dbda73f854d465cebb3d9f/sam.c#L325-L332"" rel=""nofollow"">bam_cigar2qlen</a>.</p>

<p>The other functions that <code>bam_cigar2qlen</code> calls are defined in <a href=""https://github.com/samtools/htslib/blob/bf753361dab9b1640cf64f7886dbfe35357a43c5/htslib/sam.h#L76-L105"" rel=""nofollow"">sam.h</a>, including a helpful comment showing the truth table for which operations consume query sequence vs reference sequence.</p>

<p>In short, to calculate the query length of a CIGAR string the way that samtools (really htslib) does it, you should add the given length for CIGAR operations <code>M</code>, <code>I</code>, <code>S</code>, <code>=</code>, or <code>X</code> and ignore the length of CIGAR operations for any of the other operations. </p>

<p>The current version of the python cigar module seem to be using the <a href=""https://github.com/brentp/cigar/blob/master/cigar.py#L68"" rel=""nofollow"">same set of operations</a>, and the algorithm for calculating the query length (which is what <code>len(Cigar(cigar))</code> would return) looks right to me. What makes you think that it isn't giving the correct results?</p>

<p>It looks like you should be able to use the cigar python module to hard clip from either the left or right end using the <code>mask_left</code> or <code>mask_right</code> method with <code>mask=""H""</code>.</p>
"
39813481,674039.0,2016-10-02T03:07:50Z,39813433,4,"<p>First of all, <code>list.append</code> and <code>dict.__setitem__</code> are both O(1) average case.  Of course they will have different coefficients, but there is not really any blanket reason to say that one or the other will be the faster.  The coefficients may change depending on implementation detail, too.</p>

<p>Secondly, a more fair comparison would be to remove the attribute resolution overhead:</p>

<pre><code>&gt;&gt;&gt; timeit.timeit('test[0] = (""test"")', setup='test = {}')
0.0813908576965332
&gt;&gt;&gt; timeit.timeit('test_append(""test"")', setup='test = []; test_append = test.append')
0.06907820701599121
</code></pre>

<p>The lookup of the method name on the instance is relatively expensive, when you are looking at an extremely cheap operation such as <code>append</code>. </p>

<p>I also see lists being consistently a little faster, once there is some data inside.  This example is python 3.5.2:</p>

<pre><code>&gt;&gt;&gt; dict_setup = 'import random; test = {random.random(): None for _ in range(1000)}'
&gt;&gt;&gt; list_setup = 'import random; test = [random.random() for _ in range(1000)]; test_append=test.append'
&gt;&gt;&gt; timeit.timeit('test[0] = ""test""', setup=dict_setup)
0.06155529400166415
&gt;&gt;&gt; timeit.timeit('test_append(""test"")', setup=list_setup)
0.057089386998995906
</code></pre>
"
39813754,2034787.0,2016-10-02T04:05:42Z,39811963,0,"<p>You may have also recognized 308061521170129 as the 71st Fibonacci number. This relationship corresponds with the Fibonacci numbers giving ""the solution to certain enumerative problems. The most common such problem is that of counting the number of compositions of 1s and 2s that sum to a given total n: there are Fn+1 ways to do this"" (<a href=""https://en.wikipedia.org/wiki/Fibonacci_number#Use_in_mathematics"" rel=""nofollow"">https://en.wikipedia.org/wiki/Fibonacci_number#Use_in_mathematics</a>).</p>

<p>Every contiguous subsequence in the string that can be divided into either single or double digit codes represents such an <code>n</code> with multiple possible compositions of 1s and 2s; and thus, for every such subsequence within the string, the result must be multiplied by the (subsequence's length + 1) Fibonacci number (in the case of the 70 2's, we just multiply 1 by the 71st Fibonacci number).</p>
"
39814684,1354939.0,2016-10-02T07:08:22Z,39704367,2,"<p>There is an <a href=""https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/read-write-to-cloud-storage"" rel=""nofollow"">App Engine specific Google Cloud Storage API</a> that ships with the App Engine SDK that you can use to work with Cloud Storage buckets.</p>

<pre><code>import cloudstorage as gcs
</code></pre>

<p>Is there a reason you didn't use this built-in library, which requires no configuration to load?</p>
"
39815014,496940.0,2016-10-02T07:58:51Z,39704367,0,"<p>It looks like the only thing that is required is to include <code>google-cloud</code> into your project <code>requirements.txt</code> file.</p>

<p>Check if this simple sample works for you (you shouldn't get any imports error). 
Create below files and run <code>pip install -r requirements.txt -t lib</code>. Nothing more is required on my site to make it work.</p>

<p><strong>app.yaml</strong></p>

<pre><code>application: mysample
runtime: python27
api_version: 1
threadsafe: true

handlers:
  - url: /.*
    script: main.app
</code></pre>

<p><strong>main.py</strong></p>

<pre><code>import webapp2
from google.cloud import storage


class MainPage(webapp2.RequestHandler):
    def get(self):
        self.response.headers['Content-Type'] = 'text/plain'
        self.response.write('Hello, World!')

app = webapp2.WSGIApplication([
    ('/', MainPage),
], debug=True)
</code></pre>

<p><strong>appengine_config.py</strong></p>

<pre><code>from google.appengine.ext import vendor
import os

# Third-party libraries are stored in ""lib"", vendoring will make
# sure that they are importable by the application.
if os.path.isdir(os.path.join(os.getcwd(), 'lib')):
    vendor.add('lib')
</code></pre>

<p><strong>requirements.txt</strong></p>

<pre><code>google-cloud
</code></pre>
"
39815461,2912340.0,2016-10-02T09:04:02Z,39815135,4,"<p>You can easily do it by creating custom <a href=""https://docs.python.org/2/library/sys.html#sys.excepthook"" rel=""nofollow""><code>sys.excepthook</code></a>:</p>

<pre><code>import sys
import traceback


def report_exception(exc_type, exc_value, exc_tb):
    # just a placeholder, you may send an e-mail here
    print(""Type"", exc_type)
    print(""Value"", exc_value)
    print(""Tb"", ''.join(traceback.format_tb(exc_tb)))


def custom_excepthook(exc_type, exc_value, exc_tb):
    report_exception(exc_type, exc_value, exc_tb)
    sys.__excepthook__(exc_type, exc_value, exc_tb)  # run standard exception hook


sys.excepthook = custom_excepthook

raise RuntimeError(""I want to report exception here..."")
</code></pre>

<p>For pretty-printing traceback objects refer to <a href=""https://docs.python.org/2/library/traceback.html"" rel=""nofollow"">traceback</a> module. </p>
"
39815739,5904928.0,2016-10-02T09:45:47Z,39815695,3,"<p>Your <code>else</code> part will be executed in both cases. 
<code>else</code> part executed when loop terminate when condition didn't found.Which is what is happening in your code. But it will also work same without <code>continue</code> statement.</p>

<p>now what about break statement's else part, Break statement's else part will be executed only if:</p>

<ul>
<li>If the loop completes normally without any break.</li>
<li>If the loop doesn't encounter a break.</li>
</ul>

<p><a href=""http://i.stack.imgur.com/yPLeC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yPLeC.png"" alt=""enter image description here""></a></p>
"
39815753,5679756.0,2016-10-02T09:47:38Z,39815695,4,"<p>With a <code>for</code> loop in Python, the <code>else</code> block is executed when the loop finishes normally, i.e. there is no <code>break</code> statement. A <code>continue</code> does not affect it either way.</p>

<p>If the for loop ends because of a <code>break</code> statement, then <code>else</code> block will not execute. If the loop exits normally (no <code>break</code>), then the <code>else</code> block will be executed.</p>

<p>From the <a href=""https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops"" rel=""nofollow"">docs</a>:</p>

<blockquote>
  <p>When used with a loop, the else clause has more in common with the else clause of a try statement than it does that of if statements: a try statementâs else clause runs when no exception occurs, and a loopâs else clause runs when no break occurs.</p>
</blockquote>

<p>I always remember it because of how Raymond Hettinger <a href=""https://youtu.be/OSGv2VnC0go?t=17m44s"" rel=""nofollow"">describes it</a>. He said it should have been called <code>nobreak</code> instead of <code>else</code>. (That's also a good video that explains the usefulness of the for-else construct)</p>

<p>Example:</p>

<pre><code>numbers = [1,2,3]
for number in numbers:
    if number == 4:
        print(""4 was found"")
        break
else:
    print(""4 was not found"")
</code></pre>

<p>When you run the above code, since <code>4</code> is not in the list, the loop will not <code>break</code> and the <code>else</code> clause will print. If you add <code>4</code> to the list and run it again, it will <code>break</code> and the <code>else</code> will not print. In most other languages, you would have to add some sentinel boolean like <code>found</code> and make it <code>True</code> if you find a <code>4</code>, then only print the statement after the loop if <code>found</code> is <code>False</code>. </p>
"
39816690,6209196.0,2016-10-02T11:49:33Z,39805391,0,"<p>Your problem is that you aren't storing what form you are working on. I would simply assign 0 into a variable and add 1 to it after every iteration. So your code should be:</p>

<pre><code>currentForm = 0
for form in br.forms():
        if not forms.find_control(name = ""email""):
                currentForm += 1
                continue
        print ""Selecting form number %i..."" % currentForm
        br.select_form(nr = currentForm)
        br.form[""email""] = email
        br.submit()
        currentForm += 1
        print ""Success: "", link
</code></pre>

<p>Note: <code>x += y</code> is equal to <code>x = x + y</code></p>

<p>Edit: You should fix your indenting too, you don't need to press tab twice, one press works too!</p>
"
39816915,6109283.0,2016-10-02T12:17:36Z,39816795,2,"<p>It isn't the most pandas-like solution, but you can try the following:</p>

<pre><code>col = np.array([""A"", ""B"", ""A1R"", ""B2"", ""AABB4""])
data = pd.DataFrame(col, columns=[""Before""])
</code></pre>

<p>Now compute the maximum length, the list of individual lengths, and the differences:</p>

<pre><code>max_ = data.Before.map(lambda x: len(x)).max()
lengths_ = data.Before.map(lambda x: len(x))
diffs_ = max_ - lengths_
</code></pre>

<p>Create a new column called <code>After</code> adding the underscores, or any other character:</p>

<pre><code>data[""After""] = data[""Before""] + [""_""*i for i in diffs_]
</code></pre>

<p>All this gives:</p>

<pre><code>  Before  After
0      A  A____
1      B  B____
2    A1R  A1R__
3  AABB4  AABB4
</code></pre>
"
39817006,1005215.0,2016-10-02T12:29:08Z,39816795,2,"<p>Without creating extra columns:</p>

<pre><code>In [63]: data
Out[63]: 
    Col1
0      A
1      B
2    A1R
3     B2
4  AABB4

In [64]: max_length = data.Col1.map(len).max()

In [65]: data.Col1 = data.Col1.apply(lambda x: x + '_'*(max_length - len(x)))

In [66]: data
Out[66]: 
    Col1
0  A____
1  B____
2  A1R__
3  B2___
4  AABB4
</code></pre>
"
39817066,494631.0,2016-10-02T12:36:35Z,39816433,1,"<p>It seems you've forgot to wrap <code>subtasks</code> to <code>group</code>.</p>

<pre><code>def func1(date):
    subtasks = []
    for filepath in all_files:
        kwargs = {'date': date, 'hfile': filepath}
        subtask = mytask1.subtask(kwargs=kwargs)
        subtasks.append(subtask)

    chrd = chord(header=group(subtasks), body=myfinaltask1.subtask(kwargs={'date': date}))
    return chrdr
</code></pre>
"
39817126,100297.0,2016-10-02T12:43:19Z,39817081,5,"<p>Yes, there is a difference. Although in Python 3, all objects are instances of <code>object</code>, including <code>object</code> itself, only <code>Any</code> documents that the return value should be disregarded by the typechecker.</p>

<p>The <code>Any</code> type docstring states that object is a subclass of <code>Any</code> and vice-versa:</p>

<pre><code>&gt;&gt;&gt; import typing
&gt;&gt;&gt; print(typing.Any.__doc__)
Special type indicating an unconstrained type.

    - Any object is an instance of Any.
    - Any class is a subclass of Any.
    - As a special case, Any and object are subclasses of each other.
</code></pre>

<p>However, a proper typechecker (one that goes beyond <code>isinstance()</code> checks, and which inspects how the object is actually <em>used</em> in the function) can readily object to <code>object</code> where <code>Any</code> is always accepted.</p>

<p>From the <a href=""https://docs.python.org/3/library/typing.html#the-any-type"" rel=""nofollow""><code>Any</code> type documentation</a>:</p>

<blockquote>
  <p>Notice that no typechecking is performed when assigning a value of type <code>Any</code> to a more precise type.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Contrast the behavior of <code>Any</code> with the behavior of <code>object</code>. Similar to <code>Any</code>, every type is a subtype of <code>object</code>. However, unlike <code>Any</code>, the reverse is not true: object is not a subtype of every other type.</p>
  
  <p>That means when the type of a value is <code>object</code>, a type checker will reject almost all operations on it, and assigning it to a variable (or using it as a return value) of a more specialized type is a type error.</p>
</blockquote>

<p>and from the mypy documentation section <a href=""http://mypy.readthedocs.io/en/latest/dynamic_typing.html#any-vs-object"" rel=""nofollow""><em>Any vs. object</em></a>:</p>

<blockquote>
  <p>The type <code>object</code> is another type that can have an instance of arbitrary type as a value. Unlike <code>Any</code>, <code>object</code> is an ordinary static type (it is similar to <code>Object</code> in Java), and only operations valid for all types are accepted for object values.</p>
</blockquote>

<p><code>object</code> can be <a href=""http://mypy.readthedocs.io/en/latest/casts.html#casts"" rel=""nofollow"">cast</a> to a more specific type, while <code>Any</code> really means <em>anything goes</em> and a type checker disengages from any use of the object (even if you later assign such an object to a name that <em>is</em> typechecked).</p>

<p>You already painted your function into a an un-typed corner by accepting <code>list</code>, which comes down to being the same thing as <code>List[Any]</code>. The typechecker <em>disengaged there</em> and the return value no longer matters, but since your function accepts a list containing <code>Any</code> objects, the proper return value would be <code>Any</code> here.</p>

<p>To properly participate in type-checked code, you need to mark your input as <code>List[T]</code> (a genericly typed container) for a typechecker to then be able to care about the return value. Which in your case would be <code>T</code> since you are retrieving a value from the list. Create <code>T</code> from a <code>TypeVar</code>:</p>

<pre><code>from typing import TypeVar, List

T = TypeVar('T')

def get_item(L: List[T], i: int) -&gt; T:
    return L[i]
</code></pre>
"
39818021,2336654.0,2016-10-02T14:31:05Z,39816795,6,"<p>consider the <code>pd.Series</code> <code>s</code></p>

<pre><code>s = pd.Series(['A', 'B', 'A1R', 'B2', 'AABB4'])
</code></pre>

<p><strong><em>solution</em></strong><br>
use <code>str.ljust</code></p>

<pre><code>m = s.str.len().max()
s.str.ljust(m, '_')

0    A____
1    B____
2    A1R__
3    B2___
4    AABB4
dtype: object
</code></pre>

<hr>

<p><strong><em>for your case</em></strong>  </p>

<pre><code>m = df.Col1.str.len().max()
df.Col1 = df.Col1.ljust(m '_')
</code></pre>
"
39818123,6911908.0,2016-10-02T14:42:39Z,39796852,5,"<p>Regular expressions are not optimal for the task even if you use alternative implementations of re that do not limit lookbehind by fixed length strings (such as Matthew Barnett's regex).</p>

<p>The easiest way is to count occurrences of letters and print the first one with frequency equal to 1:</p>

<pre><code>import sys
from collections import Counter, OrderedDict

# Counter that remembers that remembers the order entries were added
class OrderedCounter(Counter, OrderedDict):
    pass

# Calling next() once only gives the first entry
first=next

with open(sys.argv[1], 'r') as test_cases:
    for test in test_cases:
        lettfreq = OrderedCounter(test)
        print(first((l for l in lettfreq if lettfreq[l] == 1)))
</code></pre>
"
39818474,1354939.0,2016-10-02T15:21:08Z,39817482,1,"<p>Your contact form handler isn't being hit because you have a catch-all rule that precedes it. Also, your contact form handler needs its own URL rather than it too having a catch-all pattern. Try this:</p>

<pre><code>version: 1  
runtime: python27 
api_version: 1 
threadsafe: true

libraries:
- name: jinja2 
  version: latest

- name: webapp2  
  version: latest

handlers:
- url: /      
  static_files: www/index.html
  upload: www/index.html

- url: /contact  
  script: www/contactForm.app

- url: /(.*)      
  static_files: www/\1
  upload: www/(.*)
</code></pre>

<p>Also, your Python appears invalid due to lack of tabs/spaces to indent your code. Should be more like:</p>

<pre><code>import webapp2
import jinja2
import os
from google.appengine.api import mail

jinja_environment = jinja2.Environment(autoescape=True,
                                       loader=jinja2.FileSystemLoader(
                                       os.path.join(os.path.dirname(__file__),
                                                    'templates')))

class contact(webapp2.RequestHandler):
  def get(self):
    template = jinja_environment.get_template('contact.html')
    self.response.out.write(self.template.render())

  def post(self):

    # takes input from user
    vorname=self.request.get(""vorname"")
    # ...

    if not mail.is_email_valid(userMail):
      self.response.out.write(""Wrong email! Check again!"")

    message=mail.EmailMessage(sender=""authorized sender address"",
                              subject=""Kontaktformular"")

    message.to=""...""
    message.body="""""" Hallo:
    Vorname: %s
    ...
    Text: %s"""""" %(vorname,...,text)
    message.send()

    self.response.out.write(""Message sent!"")

app = webapp2.WSGIApplication([('/contact',contact)], debug=True)
</code></pre>
"
39819278,2901002.0,2016-10-02T16:44:58Z,39818559,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mul.html"" rel=""nofollow""><code>mul</code></a> with <code>list comprehension</code>:</p>

<pre><code>df = df.mul(df.columns.to_series(), axis=1)
print (df)
   blah  meep  zimp
0        meep      
1  blah            
2  blah        zimp

print ([list(filter(None, x)) for x in df.values.tolist()])
[['meep'], ['blah'], ['blah', 'zimp']]

print (pd.Series([list(filter(None, x)) for x in df.values.tolist()], index=df.index))
0          [meep]
1          [blah]
2    [blah, zimp]
dtype: object
</code></pre>
"
39819514,1545014.0,2016-10-02T17:10:38Z,39750041,1,"<p>I could be mistaken, here are some ideas:</p>

<ul>
<li>I don't have <em>any</em> icon on my machine called ""applications-mail"". I did find many ""internet-mail"" icons though. </li>
</ul>

<blockquote>
  <p>/usr/share/icons/Mint-X/categories/96/applications-mail.svg</p>
</blockquote>

<ul>
<li>Also, I believe svg icons should be in a <code>scalable</code> directory. Possibly the svg you did find wasn't recognized as such. Eg. I have:</li>
</ul>

<p><code>/usr/share/icons/Tango/scalable/apps/internet-mail.svg</code></p>

<ul>
<li>I modified your program slightly:</li>
</ul>

<p>Listing:</p>

<pre><code>#!/usr/bin/env python3

from gi.repository import Gtk

class MainWindow(Gtk.Window):
    def __init__(self):
        super(MainWindow, self).__init__()
        self.connect(""delete-event"", Gtk.main_quit)

        #icon_name = ""applications-mail""
        icon_name = ""internet-mail""
        icon_theme = Gtk.IconTheme.get_default()

        found_icons = set()
        for res in range(0, 512, 2):
            icon = icon_theme.lookup_icon(icon_name, res, 0)
            #print(icon)
            if icon != None:
                found_icons.add(icon.get_filename())

        if len(found_icons) &gt; 0:
            print(""\n"".join(found_icons))
            sizes = Gtk.IconTheme.get_default().get_icon_sizes(icon_name)
            max_size = max(sizes)
            print(""max size = {} ({})"".format(max_size, sizes))
            pixbuf = icon_theme.load_icon(icon_name, max_size, 0)
            self.set_default_icon_list([pixbuf])

        self.show_all()

    def run(self):
        Gtk.main()


def main(args):
    mainwdw = MainWindow()
    mainwdw.run()

    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main(sys.argv))
</code></pre>

<p>and I get:</p>

<pre><code>/usr/share/icons/Tango/24x24/apps/internet-mail.png
/usr/share/icons/Tango/scalable/apps/internet-mail.svg
/usr/share/icons/Tango/16x16/apps/internet-mail.png
/usr/share/icons/Tango/32x32/apps/internet-mail.png
/usr/share/icons/Tango/22x22/apps/internet-mail.png
max size = 32 ([22, 16, 24, 32, -1, 0])
</code></pre>

<p>where -1 indicates a scalable icon. (so, don't use <code>max()</code> - look for -1.
This from the <a href=""https://developer.gnome.org/gtk3/stable/GtkIconTheme.html#gtk-icon-theme-get-icon-sizes"" rel=""nofollow"">developers' site</a>: </p>

<blockquote>
  <p>[gtk_icon_theme_get_icon_sizes] Returns an array of integers describing the sizes at which the icon is
  available without scaling. A size of -1 means that the icon is
  available in a scalable format.</p>
</blockquote>

<p><strong>edit</strong>: More ideas:</p>

<ul>
<li><p>Gtk uses gdk-pixbuf.loaders modules to render images such as icons. You might not have the svg driver working correctly. I also seem to recall that the librsvg libraries is necessary.</p></li>
<li><p>Even if another icon is actually working, you might be seeing a copy from the icon cache, and your icon renderer might still be failing.</p></li>
<li><p>There might even be a problem with the icon cache itself. Try <a href=""https://developer.gnome.org/gtk3/stable/gtk-update-icon-cache.html"" rel=""nofollow"">rebuilding the cache</a>.</p></li>
</ul>
"
39820411,624829.0,2016-10-02T18:46:06Z,39818559,2,"<p>I suggest to use <code>dot</code> after building a series of atomic lists:</p>

<pre><code>s = pd.Series([[col] for col in df.columns])

s.index = df.columns

df.dot(s)
Out[35]: 
0          [meep]
1          [blah]
2    [blah, zimp]
dtype: object
</code></pre>
"
39820773,6207849.0,2016-10-02T19:25:23Z,39818559,2,"<p>Another solution using sum of products using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html"" rel=""nofollow""><code>np.sum</code></a> followed by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html"" rel=""nofollow""><code>str.split</code></a> as shown:</p>

<pre><code>sep = ' '
pd.Series((df.values*(df.columns.values + sep)).sum(1)).str.split()

0          [meep]
1          [blah]
2    [blah, zimp]
dtype: object
</code></pre>
"
39820925,5982652.0,2016-10-02T19:40:26Z,39820443,2,"<p>Check out IronPython! :)</p>

<p>Good declaration: 
<a href=""https://blogs.msdn.microsoft.com/charlie/2009/10/25/running-ironpython-scripts-from-a-c-4-0-program/"" rel=""nofollow"">https://blogs.msdn.microsoft.com/charlie/2009/10/25/running-ironpython-scripts-from-a-c-4-0-program/</a></p>

<p>Hope this helps, it's pretty ez ;)</p>

<pre><code>var ipy = Python.CreateRuntime();
dynamic test = ipy.UseFile(""Test.py"");
test.pythonString(); //python-function to execute
</code></pre>

<p>just replace ""pythonString()"" with your function (already did this).</p>
"
39821062,893159.0,2016-10-02T19:54:59Z,39778435,-1,"<p>What about this?</p>

<pre><code>filehandle = None
def get_filehandle(filename):
    if filehandle is None or filehandle.closed():
        filehandle = open(filename, ""r"")
    return filehandle
</code></pre>

<p>You may want to encapsulate this into a class to prevent other code from messing with the <code>filehandle</code> variable.</p>
"
39821179,6779307.0,2016-10-02T20:08:48Z,39821166,4,"<pre><code>[sublist[::-1] for sublist in to_reverse[::-1]]
</code></pre>

<p>List comprehension works here.  <code>[::-1]</code> is basically the same as <code>reversed</code>, but does not modify the list.</p>

<p>EDIT:</p>

<p>As pointed out below, <code>reversed</code> doesn't modify the list.  It returns a <code>listreverseiterator</code> object </p>

<p>More Edit:</p>

<p>If you want a solution for lists of arbitrary depth, try:</p>

<pre><code>def deep_reverse(to_reverse):
    if isinstance(to_reverse, list):
        return list(map(deep_reverse, to_reverse[::-1]))
    else:
        return to_reverse
</code></pre>

<p>Even more Edit:</p>

<p>To mutate a list in a function:</p>

<pre><code>L[:] = new_list 
</code></pre>

<p>Will modify the list in place.</p>
"
39821238,5679756.0,2016-10-02T20:14:27Z,39821166,0,"<p>This looks very familiar :). I'm not going to give the whole working solution  but here are some tips:</p>

<p>As you know, there are two steps, reverse each sub-list and then reverse the outer list (in place, without making a new list, so it will mutate the global <code>L</code>).</p>

<p>So you can loop through the outer list, and mutate each sub-list:</p>

<pre><code>for i in range(len(L)):
    # if L[i] is a list:
        # reverse with [::-1] and update L[i] to the reversed version
# reverse the outer list L, list.reverse() will operate in-place on L
</code></pre>

<p>Now remember, if you loop through the list like this:</p>

<pre><code>for item in list:
    item = 'xxx'
</code></pre>

<p>You can't change <code>item</code> with the above code. <a class='doc-link' href=""http://stackoverflow.com/documentation/python/3553/common-pitfalls/949/changing-the-sequence-you-are-iterating-over#t=201610022012362266711""><code>item</code> is a placeholder value, so changing it doesn't actually modify the list</a>. </p>

<p>You instead need to index the item in <code>L</code>, and enumerate can help with this, or you can use the less-preffered <code>range(len())</code> as above.</p>

<pre><code>for i, item in enumerate(L):
    # do something with L[i]
    L[i] = 'something'
</code></pre>

<p><strong>Edit: since there is so much confusion over this, I'll go ahead and post a working solution based on Stefan Pochmann's very elegant answer:</strong></p>

<pre><code>def deep_reverse(L):
    L.reverse()
    for sublist in L:
        sublist.reverse()
</code></pre>

<p>Notice there is <strong>no return statement, and no print statement</strong>. This will correctly modify <code>L</code> in place. You <strong>cannot</strong> reassign <code>L</code> inside the function because then it will just create a new local version of <code>L</code>, and it will not modify the global <code>L</code>. You can use <code>list.reverse()</code> to modify <code>L</code> <strong>in place</strong> which is necessary based on the specifications. </p>
"
39821239,6884169.0,2016-10-02T20:14:31Z,39821166,-1,"<p>You could make this recursive, so it will work for arbitrarily deep nests.</p>

<p>something like (UNTESTED):</p>

<pre><code>def deep_reverse(L)
    """""" 
    assumes L is a list of lists whose elements are ints
    Mutates L such that it reverses its elements and also 
    reverses the order of the int elements in every element of L. 
    It does not return anything.
    """"""
    for i in reversed(L):
          if len(i) &gt; 1:
              deep_reverse(i)
          else:
              print(i)
</code></pre>
"
39821379,6391535.0,2016-10-02T20:28:50Z,39821166,1,"<p>This should do the trick.</p>

<pre><code>L = [[1, 2], [3, 4], [5, 6, 7]]

def deep_reverse(L):
    for i in range(len(L)):
        L[i]=L[i][::-1]
    L=L[::-1]
    return L
</code></pre>
"
39821459,646543.0,2016-10-02T20:37:09Z,39817081,3,"<p><code>Any</code> and <code>object</code> are superficially similar, but in fact are entirely <em>opposite</em> in meaning.</p>

<p><code>object</code> is the <em>root</em> of Python's metaclass hierarchy. Every single class inherits from <code>object</code>. That means that <code>object</code> is in a certain sense the most restrictive type you can give values. If you have a value of type <code>object</code>, the only methods you are permitted to call are ones that are a part of every single object. For example:</p>

<pre><code>foo = 3  # type: object

# Error, not all objects have a method 'hello'
bar = foo.hello()   

# OK, all objects have a __str__ method
print(str(foo))   
</code></pre>

<p>In contrast, <code>Any</code> is an <em>escape hatch</em> meant to allow you to mix together dynamic and statically typed code. <code>Any</code> is the least restrictive type -- any possible method or operation is permitted on a value of type <code>Any</code>. For example:</p>

<pre><code>from typing import Any
foo = 3  # type: Any

# OK, foo could be any type, and that type might have a 'hello' method
# Since we have no idea what hello() is, `bar` will also have a type of Any
bar = foo.hello()

# Ok, for similar reasons
print(str(foo))
</code></pre>

<p>You should generally try and use <code>Any</code> only for cases where...</p>

<ol>
<li>As a way of mixing together dynamic and statically typed code. For example, if you have many dynamic and complex functions, and don't have time to fully statically type all of them, you could settle for just giving them a return type of Any to nominally bring them into the typechecked work. (Or to put it another way, Any is a useful tool for helping migrate an untypechecked codebase to a typed codebase in stages).</li>
<li>As a way of giving a type to an expression that is difficult to type. For example, Python's type annotations currently do not support recursive types, which makes typing things like arbitrary JSON dicts difficult. As a temporary measure, you might want to give your JSON dicts a type of <code>Dict[str, Any]</code>, which is a bit better then nothing.</li>
</ol>

<p>In contrast, use <code>object</code> for cases where you want to indicate in a typesafe way that a value MUST literally work with any possible object in existence.</p>

<p>My recommendation is to avoid using <code>Any</code> except in cases where there is no alternative. <code>Any</code> is a concession -- a mechanism for allowing dynamism where we'd really rather live in a typesafe world.</p>

<p>For more information, see:</p>

<ul>
<li><a href=""https://docs.python.org/3/library/typing.html#the-any-type"" rel=""nofollow"">https://docs.python.org/3/library/typing.html#the-any-type</a></li>
<li><a href=""http://mypy.readthedocs.io/en/latest/kinds_of_types.html#the-any-type"" rel=""nofollow"">http://mypy.readthedocs.io/en/latest/kinds_of_types.html#the-any-type</a></li>
</ul>

<hr>

<p>For your particular example, I would use <em>TypeVars</em>, rather then either object or Any. What you want to do is to indicate that you want to return the type of whatever is contained within the list. If the list will always contain the same type (which is typically the case), you would want to do:</p>

<pre><code>from typing import List, TypeVar

T = TypeVar('T')
def get_item(L: List[T], i: int) -&gt; T:
    return L[i]
</code></pre>

<p>This way, your <code>get_item</code> function will return the most precise type as possible.</p>
"
39821514,2063361.0,2016-10-02T20:42:43Z,39821166,0,"<p>Alternatively you use <a href=""https://docs.python.org/2/library/functions.html#map"" rel=""nofollow""><code>map()</code></a> to achieve this as:</p>

<pre><code>&gt;&gt;&gt; map(lambda x: x[::-1], L[::-1])       # In Python 2.x
[[7, 6, 5], [4, 3], [2, 1]]

&gt;&gt;&gt; list(map(lambda x: x[::-1], L[::-1])) # In Python 3.x
[[7, 6, 5], [4, 3], [2, 1]]
</code></pre>

<p>Check Blog on <a href=""http://www.python-course.eu/lambda.php"" rel=""nofollow"">Lambda, filter, reduce and map</a> to know how <code>lambda</code> functions and <code>map()</code> works in Python.</p>
"
39821639,1672429.0,2016-10-02T20:57:03Z,39821166,2,"<blockquote>
  <p>I'm trying to create a function that reverses the order of the elements in a list, and also reverses the elements in a sublist.</p>
</blockquote>

<p>Then do exactly those two things:</p>

<pre><code>L.reverse()
for sublist in L:
    sublist.reverse()
</code></pre>

<hr>

<p>Full demo because you seem to be confused about what your function is supposed to do and how to test it:</p>

<pre><code>&gt;&gt;&gt; def deep_reverse(L):
        """""" 
        assumes L is a list of lists whose elements are ints
        Mutates L such that it reverses its elements and also 
        reverses the order of the int elements in every element of L. 
        It does not return anything.
        """"""
        L.reverse()
        for sublist in L:
            sublist.reverse()

&gt;&gt;&gt; L = [[1, 2], [3, 4], [5, 6, 7]]
&gt;&gt;&gt; deep_reverse(L)
&gt;&gt;&gt; print(L)
[[7, 6, 5], [4, 3], [2, 1]]
</code></pre>
"
39822069,6909494.0,2016-10-02T21:51:37Z,39748267,1,"<p>If you peek into Python's <code>site.py</code>, which you can by opening <code>/usr/lib/python2.7/site.py</code> in a text editor.</p>

<p>The sys.path is augmented with directories for packages distributed within the distribution. Local addons go into <code>/usr/local/lib/python/dist-packages</code>, the global addons install into <code>/usr/{lib,share}/python/dist-packages</code>.</p>

<p>You can change the order by overriding this:</p>

<pre><code>def getsitepackages():
    """"""Returns a list containing all global site-packages directories
    (and possibly site-python).

    For each directory present in the global ``PREFIXES``, this function
    will find its `site-packages` subdirectory depending on the system
    environment, and will return a list of full paths.
    """"""
    sitepackages = []
    seen = set()

    for prefix in PREFIXES:
        if not prefix or prefix in seen:
            continue
        seen.add(prefix)

        if sys.platform in ('os2emx', 'riscos'):
            sitepackages.append(os.path.join(prefix, ""Lib"", ""site-packages""))
        elif os.sep == '/':
            sitepackages.append(os.path.join(prefix, ""local/lib"",
                                        ""python"" + sys.version[:3],
                                        ""dist-packages""))
            sitepackages.append(os.path.join(prefix, ""lib"",
                                        ""python"" + sys.version[:3],
                                        ""dist-packages""))
        else:
            sitepackages.append(prefix)
            sitepackages.append(os.path.join(prefix, ""lib"", ""site-packages""))
        if sys.platform == ""darwin"":
            # for framework builds *only* we add the standard Apple
            # locations.
            from sysconfig import get_config_var
            framework = get_config_var(""PYTHONFRAMEWORK"")
            if framework:
                sitepackages.append(
                        os.path.join(""/Library"", framework,
                            sys.version[:3], ""site-packages""))
    return sitepackages
</code></pre>
"
39822174,3510736.0,2016-10-02T22:03:22Z,39821687,1,"<p>In general, I think it's a good idea to make your nontrivial functions and classes as close to mathematical functions (e.g., <code>sin(x)</code>), as possible. Given the same input, a mathematical function gives the same output each time, irrespective of the current date, random choices, and so forth. </p>

<ul>
<li><p>If your function performs nontrivial logic dependent on the current date or time, pass the current date or time externally to it.</p></li>
<li><p>If your function performs random choices, pass it a pseudo-random number generator.</p></li>
</ul>

<p>So, for example, instead of:</p>

<pre><code>import datetime

def foo():
    ...
    now = datetime.datetime.now()
    ...

foo()
</code></pre>

<p>Use </p>

<pre><code>import datetime

def foo(now):
    ...
    ...

foo(datetime.datetime.now())
</code></pre>

<p>This makes your nontrivial code consistent across multiple executions.</p>

<ol>
<li><p>You can predictably test it.</p></li>
<li><p>If it fails in production, it is easier to reconstruct the problem.</p></li>
</ol>
"
39822237,5693803.0,2016-10-02T22:10:02Z,39822188,1,"<p>In your second example, instead of <code>num = request.args.get('num')</code> try to simply use <code>num</code>. Since you specified it as an input to your route/function, you should be able to access it directly. </p>
"
39822258,2063361.0,2016-10-02T22:12:50Z,39822159,0,"<p>I could think of two possible reasons:</p>

<ol>
<li>Database configuration you are using with/without Flask are different</li>
<li>Check the <code>type</code> and value of your <code>user_id</code> in both the code. It might be different.</li>
</ol>
"
39822292,6909494.0,2016-10-02T22:16:45Z,39822188,1,"<p>Try this:</p>

<pre><code>@app.route('/iLike/&lt;int:num&gt;', methods=['GET','POST'])
def single2(num):
  print(num)
</code></pre>
"
39822302,216074.0,2016-10-02T22:17:42Z,39822188,3,"<p>You are mixing route parameters and request arguments here.</p>

<p>Parameters you specify in the route are route parameters and are a way to declare <a href=""http://flask.pocoo.org/docs/0.11/quickstart/#variable-rules"" rel=""nofollow"">variable routes</a>. The values for these parameters are passed as function arguments to the route function. So in your case, for your <code>&lt;num&gt;</code> url part, the value is passed as the function argument <code>num</code>.</p>

<p>Request arguments are independent of routes and are passed to URLs as GET parameters. You can access them through the <a href=""http://flask.pocoo.org/docs/0.11/quickstart/#the-request-object"" rel=""nofollow"">request object</a>. This is what you are doing with <code>request.args.get()</code>.</p>

<p>A full example would look like this:</p>

<pre><code>@app.route('/iLike/&lt;int:num&gt;')
def single2(num):
    print(num, request.args.get('num'))
</code></pre>

<p>Opening <code>/iLike/123</code> would now result in <code>123 None</code>. The request argument is empty because you didnât specify one. You can do that by opening <code>/iLike/123?num=456</code>, which would result in <code>123 456</code>.</p>
"
39822311,6734406.0,2016-10-02T22:19:42Z,39822188,1,"<p>You recieve <code>None</code> here:</p>

<pre><code>num = request.args.get('num')
</code></pre>

<p>because you're not passing <code>num</code> as element of querystring.</p>

<p>When use <code>request.args.get('num')</code>?</p>

<p>If we would have URL like this one:</p>

<pre><code>localhost:8080/iLike?num=2
</code></pre>

<p>But it's not your case. You pass <code>num</code> already to a function as an argument. So in your case just use:</p>

<pre><code>@app.route('/iLike/&lt;num&gt;', methods=['GET','POST'])
def single2(num):
     try:
        location = session.get('location')
        transType = session.get('transType')
        data = session.get('data')

        print(num)
</code></pre>
"
39822369,6734406.0,2016-10-02T22:27:11Z,39822087,1,"<pre><code>pets = db.session.query(Pet, Person.name).join(Person)

for pet, person_name in pets:
    print pet.name, person_name
</code></pre>

<p>Using that type of querying we force SQLAlchemy to use Mapping of <code>pet</code> table to <code>Pet</code> object and get <code>Person</code>'s name as second item in select. Of course you can use something like that:</p>

<pre><code>pets = db.session.query(Pet, Person).join(Person)
</code></pre>

<p>then you'll be able to use it in this way:</p>

<pre><code>for pet, person in pets:
    print pet.name, person.name
</code></pre>

<p>so have reference to a person as object of <code>Person</code> class. But first option is more preferable because it's faster, just because of getting person's name only.</p>
"
39822646,487339.0,2016-10-02T23:06:35Z,39822276,1,"<p>You could do this using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html"" rel=""nofollow""><code>np.einsum</code></a>:</p>

<pre><code>In [142]: old = orig(a,b)

In [143]: new = np.einsum('ijk,ikl-&gt;ijl', a, b)

In [144]: np.allclose(old, new)
Out[144]: True
</code></pre>

<p>One advantage of using <code>einsum</code> is that you can almost read off what it's doing from the indices: leave the first axis alone (<code>i</code>), and perform a matrix multiplication on the last two <code>(jk,kl-&gt;jl)</code>).</p>
"
39822673,2336654.0,2016-10-02T23:10:40Z,39818559,1,"<p>use <code>where</code> and <code>stack</code> to drop <code>0</code>s then grab indices left over</p>

<pre><code># number of levels in columns
num = df.columns.nlevels
# handy list for stacking
lvls = list(range(num))
# reverse (sort of) list for unstacking
rlvls = [x * -1 - 1 for x in lvls]

# get just levels in index that used to be columns
xsl = lambda x: x.xs(x.name).index.tolist()

# where is faster than replace
# when I stack, I'll drop all np.nan
# then just grab the indices that are left
df.where(df, np.nan).stack(lvls).groupby(level=lvls).apply(xsl)

0          [meep]
1          [blah]
2    [blah, zimp]
dtype: object
</code></pre>
"
39822961,6592720.0,2016-10-03T00:00:37Z,39822944,1,"<p>Try using the tab character to format your spaces better.</p>

<pre><code>print(name + ""\t"" + score)
</code></pre>

<p>This should give you something closer to your desired output. But you may need to use two if some names are long.</p>
"
39823003,6912578.0,2016-10-03T00:05:51Z,39822944,1,"<p>You can add the names and the scores to a list and then print it as a table as </p>

<pre><code>import numpy as np
name_list = ['jane doe' ,'sally smooth']
score = np.array([[102,],[106,]]) #make a numpy array 
row_format =""{:&gt;15}"" * (len(name_list))
for name, row in zip(name_list, score):
    print(row_format.format(name, *row))
</code></pre>

<p>Note: This depends on str.format() </p>

<p>This code will output:</p>

<pre><code>           jane doe            102
       sally smooth            106
</code></pre>
"
39823782,2734863.0,2016-10-03T02:19:36Z,39736000,0,"<p>Uninstall Anaconda and everything works... I conclude that you simply cannot have Anaconda installed and use the standard Python 3.5 compiler at the same time if you're using Pyinstaller. Maybe <a href=""http://stackoverflow.com/questions/39728108/running-pyinstaller-after-anaconda-install-results-in-importerror-no-module-nam"">this</a> is related. </p>

<p>This is not the <a href=""http://stackoverflow.com/a/37398710/2734863"">first</a> time that uninstalling Anaconda appears to solve my issues... If I should report this issue somewhere please comment below. I don't know where.</p>
"
39829092,6518219.0,2016-10-03T10:06:09Z,39828366,1,"<p>you can use also comm command as below</p>

<pre><code>comm -12 &lt;(pip freeze) &lt;(pip3.4 freeze)
</code></pre>

<p>to search for intersections;</p>

<pre><code>grep -f &lt;(pip freeze) &lt;(pip3.4 freeze)
</code></pre>
"
39830224,1665365.0,2016-10-03T11:11:24Z,39829473,22,"<p>There is a bug with PyCParser - See <a href=""https://github.com/pyca/cryptography/issues/3187"">https://github.com/pyca/cryptography/issues/3187</a></p>

<p>The work around is to use another version or to not use the binary distribution.</p>

<pre><code>pip install git+https://github.com/eliben/pycparser@release_v2.14
</code></pre>

<p>or</p>

<pre><code>pip install --no-binary pycparser
</code></pre>
"
39830304,494631.0,2016-10-03T11:16:01Z,39830235,5,"<p>It is not a bug or a feature (more likely a feature), it is just misconfiguration. </p>

<p>As the <a href=""http://docs.celeryproject.org/en/latest/userguide/optimizing.html#prefetch-limits"">documentation</a> says, the worker can reserve some tasks for himself to hasten the processing messages. But this makes sense only for small and fast tasks - it does not ask the broker for the new message but immediately starts reserved one.</p>

<p>But for the long tasks this may lead to the case described in your question.</p>

<blockquote>
  <p>If you have many tasks with a long duration you want the multiplier value to be 1, which means it will only reserve one task per worker process at a time.</p>
  
  <p>If you have a combination of long- and short-running tasks, the best option is to use two worker nodes that are configured separately, and route the tasks according to the run-time.</p>
</blockquote>

<p>So, you need to set <code>CELERYD_PREFETCH_MULTIPLIER = 1</code> in the celery's settings.</p>

<p>But,</p>

<blockquote>
  <p>When using early acknowledgement (default), a prefetch multiplier of 1 means the worker will reserve at most one extra task for every active worker process.</p>
  
  <p>When users ask if itâs possible to disable âprefetching of tasksâ, often what they really want is to have a worker only reserve as many tasks as there are child processes.</p>
</blockquote>

<p>I also may recommend to set <code>CELERY_ACKS_LATE = True</code> to send ACK command only after the task get completed. This way the worker won't reserve any additional tasks at all, but currently executing task will be marked as reserved only.</p>

<p>Although this has a side effect - if the worker get crashed/terminated in the middle of executing of your task, the task will be marked again as not-started and any other worker may start it again from the beginning. So make sure you have <code>idempotent</code> tasks. See <a href=""http://docs.celeryproject.org/en/latest/userguide/optimizing.html#reserve-one-task-at-a-time"">docs</a> again about this.</p>
"
39832796,3555845.0,2016-10-03T13:26:39Z,39832721,3,"<p>Assigning the dictionary <code>self</code> to <code>__dict__</code> allows attribute access and item access:</p>

<pre><code>&gt;&gt;&gt; c = Config()
&gt;&gt;&gt; c.abc = 4
&gt;&gt;&gt; c['abc']
4
</code></pre>
"
39832824,5349916.0,2016-10-03T13:27:52Z,39832773,2,"<p>If your values are unique, just use the <code>list.index</code> method. For example, you can do this:</p>

<pre><code>import random
l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
start_l = l[:]
random.shuffle(l)
for elem in l:
    print(elem, '-&gt;', start_l.index(elem))
</code></pre>

<p>Of course, in your example this is trivial - each element is already it's initial index.</p>

<pre><code># gives the same result as above.
l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
random.shuffle(l)
for elem in l:
    print(elem, '-&gt;', elem)
</code></pre>

<p>In fact, the best method depends strongly on what you want to do. If you have other data, it might be simplest to just shuffle indices, not data. This avoids any problems of duplication etc. Basically you get a permutation list, where each element is the index the position is shifted to. For example, <code>[2, 1, 0]</code> is the permutation for reversing a list.</p>

<pre><code>l = list(random.randint(0, 10) for _ in range(10))
l_idx = list(range(len(l)))  # list of indices in l
random.shuffle(l_idx)
for new_idx, old_idx in enumerate(l_idx):
    print(l[old_idx], '@', old_idx, '-&gt;', new_idx)
</code></pre>
"
39832854,953482.0,2016-10-03T13:28:48Z,39832773,12,"<p>You could pair each item with its index using <code>enumerate</code>, then shuffle that.</p>

<pre><code>&gt;&gt;&gt; import random
&gt;&gt;&gt; l = [4, 8, 15, 16, 23, 42]
&gt;&gt;&gt; x = list(enumerate(l))
&gt;&gt;&gt; random.shuffle(x)
&gt;&gt;&gt; indices, l = zip(*x)
&gt;&gt;&gt; l
(4, 8, 15, 23, 42, 16)
&gt;&gt;&gt; indices
(0, 1, 2, 4, 5, 3)
</code></pre>

<p>One advantage of this approach is that it works regardless of whether <code>l</code> contains duplicates.</p>
"
39832887,1832539.0,2016-10-03T13:30:32Z,39832773,1,"<p>To keep track of everything using a dictionary, one can do this: </p>

<p>Use <code>enumerate</code> in your dictionary comprehension to have index and value in your iteration, and then assign value as key, and index as value. </p>

<pre><code>import random

l = [5, 3, 2, 0, 8, 7, 9, 6, 4, 1]
d = {v: i for i, v in enumerate(l)}
print(d) # current state
random.shuffle(l)
</code></pre>

<p>The advantage here is that you get <code>O(1)</code> lookup for retrieving your index for whatever value you are looking up. </p>

<p>However, if your list will contain duplicates, <a href=""http://stackoverflow.com/a/39832854/1832539"">this</a> answer from Kevin should be referred to.</p>
"
39832896,2063361.0,2016-10-03T13:30:52Z,39832773,1,"<p>Create a copy of the original <code>list</code> and shuffle the copy:</p>

<pre><code>&gt;&gt;&gt; import random    
&gt;&gt;&gt; l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
&gt;&gt;&gt; l_copy = list(l)  # &lt;-- Creating copy of the list
&gt;&gt;&gt; random.shuffle(l_copy)  # &lt;-- Shuffling the copy
&gt;&gt;&gt; l_copy   # &lt;-- Shuffled copy               
[8, 7, 1, 3, 6, 5, 9, 2, 0, 4]
&gt;&gt;&gt; l   # &lt;-- original list
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
&gt;&gt;&gt; 
</code></pre>
"
39833012,2063361.0,2016-10-03T13:36:20Z,39832721,2,"<p>As per <a href=""https://docs.python.org/2/library/stdtypes.html#object.__dict__"" rel=""nofollow"">Python Document</a>, <code>object.__dict__</code> is:</p>

<blockquote>
  <p>A dictionary or other mapping object used to store an objectâs (writable) attributes.</p>
</blockquote>

<p>Below is the sample example:</p>

<pre><code>&gt;&gt;&gt; class TestClass(object):
...     def __init__(self):
...         self.a = 5
...         self.b = 'xyz'
... 
&gt;&gt;&gt; test = TestClass()
&gt;&gt;&gt; test.__dict__
{'a': 5, 'b': 'xyz'}
</code></pre>
"
39833238,6734406.0,2016-10-03T13:47:49Z,39750879,2,"<pre><code>def find_shortest_path(graph, start, end, path=[]):
    path = path + [start]
    if start == end:
        return path
    if start not in graph:
        return None
    shortest = None
    for node in graph[start]:
        if node not in path:
            newpath = find_shortest_path(graph, node, end, path)
            if newpath:
                if not shortest or len(newpath) &lt; len(shortest):
                    shortest = newpath
    return shortest


def get_it_done(choices, number):
    mapping = {}
    graph = {} 

    for choice in choices:
        if choice in number:
            _from = number.index(choice)
            _to = _from + len(choice)
            mapping.setdefault((_from, _to), choice)

    items = sorted(mapping.items(), key=lambda x: x[0])
    for _range, value in items:
        _from, _to = _range
        graph.setdefault(_from, []).append(_to)
    start = 0
    end = _range[1] #this is hack, works only in python 2.7
    path = find_shortest_path(graph, start, end) 
    ranges = [tuple(path[i:i+2]) for i in range(len(path) - 1)]
    if len(ranges) == 1:
        items = sorted(choices, key=len, reverse=True)
        number_length = len(number) 
        result = ''
        for item in items:
            result += item
            if len(result) == number_length: 
                return result 
    return [mapping[_range] for _range in ranges]


if __name__ == ""__main__"":
    examples = [
        # Example1 -&gt;
        # Solution ['012345678910203040506070', '80', '90', '100', '200', '300', '400', '500', '600', '700', '800', '900']
        (
            [
                ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"",
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""100"", ""200"", ""300"", ""400"", ""500"", ""600"", ""700"", ""800"", ""900"",
                ""012345678910203040506070""
            ],
            ""0123456789102030405060708090100200300400500600700800900""
        ),
        ## Example2
        ## Solution ['100']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""100""
        ),
        ## Example3
        ## Solution ['101234567891020304050', '6070809010020030040050', '0600700800900']
        (
            [
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""012345678910203040506070"",
                ""101234567891020304050"",
                ""6070809010020030040050"",
                ""0600700800900""
            ],
            ""10123456789102030405060708090100200300400500600700800900""
        ),
        ### Example4
        ### Solution ['12', '34', '56', '78', '90']
        (
            [
                ""12"", ""34"", ""56"", ""78"", ""90"",
                ""890"",
            ],
            ""1234567890""
        ),
        ## Example5
        ## Solution ['12', '34']
        (
            [
                ""1"", ""2"", ""3"",
                ""12"", ""23"", ""34""
            ],
            ""1234""
        ),
        # Example6
        ## Solution ['100', '10']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""10010""
        )
    ]

    score = 0
    for (choices, large_number) in examples:
        res = get_it_done(choices, large_number)
        flag = """".join(res) == large_number
        print(""{0}\n{1}\n{2} --&gt; {3}"".format(
            large_number, """".join(res), res, flag))
        print('-' * 80)
        score += flag

    print(
        ""Score: {0}/{1} = {2:.2f}%"".format(score, len(examples), score / len(examples) * 100))
</code></pre>

<p><code>get_it_done</code> function creates at first <code>mapping</code>, where keys are occurency ranges of each <code>choice</code> in a <code>number</code>. Then sorts it by first item in each key of <code>mapping</code> dict. Next step is creating <code>graph</code>. Then using <code>find_shortest_path</code> function, we can find the shortest path to build result in the most optimal way. So at the end we can use <code>mapping</code> again, to return <code>choices</code> according to their ranges. If there is one range, we have situation when all numbers consists the same two values, so rules are different. We can collect numbers direct from <code>choices</code> (sorted descending) until length of the result will be the same as length of a <code>number</code>.</p>
"
39833394,1967396.0,2016-10-03T13:56:05Z,39832773,1,"<p>A more intuitive alternative to the other answers:</p>

<p>Shuffle a range of indices, and use that to get a shuffled list of the original values.</p>
"
39837081,4349370.0,2016-10-03T17:24:19Z,39590741,0,"<p>I have the same error and none of the solutions proposed worked for me.</p>
"
39837394,2781701.0,2016-10-03T17:43:21Z,39836404,0,"<p>This is how I would go on about it:</p>

<pre><code>def get_p_token(self, response):
    # generate token
    ...
    yield Request(url = response.url, callback=self.no_captcha, method = ""GET"",priority=1, meta = response.meta, dont_filter=True)


def get_ad(self, response):
    temp_dict = AppextItem()
    try:
        Selector(response).xpath('//div[@class=""messagebox""]').extract()[0]
        print(""Captcha found when scraping ID ""+ response.meta['id'] + "" LINK: ""+response.meta['link'])
        self.p_token = ''

        yield Request(url = url_, callback=self.get_p_token, method = ""GET"",priority=1, meta = response.meta)

    except Exception:
        print(""Captcha was not found"")
        yield Request(url = url_, callback=self.no_captcha, method = ""GET"",priority=1, meta = response.meta)
</code></pre>

<p>You haven't provided working code so this is only a demonstration of the problem...The logic here is pretty simple:</p>

<p>If a captcha is found it goes to <code>get_p_token</code> and after generating the token, it requests the url that you were requesting before. If no captcha is found it goes on as normal.</p>
"
39837467,674039.0,2016-10-03T17:46:59Z,39837416,4,"<p>I would do that like this:</p>

<pre><code>s1 &lt;= s2 if s1 else False
</code></pre>

<p>It should be faster, because it uses the built-in operators supported by sets rather than using more expensive function calls and attribute lookups.  It's logically equivalent.</p>
"
39837492,65624.0,2016-10-03T17:48:10Z,39837416,2,"<p>Instead of using an <code>if</code> you can force the result to be a <code>bool</code> by doing this:</p>

<pre><code>def check_set(s1, s2):
    return bool(s1 and s1.issubset(s2))
</code></pre>
"
39837506,6640099.0,2016-10-03T17:49:15Z,39837416,1,"<p>Why not just return the value? That way, you avoid having to write <code>return True</code> or <code>return False</code>.</p>

<pre><code>def check_set(s1, s2):
    return bool(s1 and s1.issubset(s2))
</code></pre>
"
39837564,584846.0,2016-10-03T17:52:38Z,39837416,-1,"<p>Instead of using an empty set, you could use a set with an empty value:</p>

<p><code>s1 = set([''])</code> or <code>s1 = set([None])</code></p>

<p>Then your <code>print</code> statement would work as you expected.</p>
"
39838310,355230.0,2016-10-03T18:41:34Z,39837416,2,"<p>You can take advantage of how Python evaluates the truthiness of an object <strong>plus</strong> how it short-circuits boolean <code>and</code> expressions with:</p>

<pre><code>bool(s1) and s1 &lt;= s2
</code></pre>

<p>Essentially this means: if <code>s1</code> is something not empty AND it's a subset of <code>s2</code></p>
"
39838609,2999346.0,2016-10-03T19:00:34Z,39836953,1,"<p>A backward iteration can be performed to remove the increasing parts in <code>precision</code>. Then, vertical and horizontal lines can be plotted as specified in the answer of Bennett Brown to <a href=""http://stackoverflow.com/questions/16930328/vertical-horizontal-lines-in-matplotlib"">vertical &amp; horizontal lines in matplotlib</a> . </p>

<p>Here is a sample code:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import matplotlib.pyplot as plt

#just a dummy sample
recall=np.linspace(0.0,1.0,num=42)
precision=np.random.rand(42)*(1.-recall)
precision2=precision.copy()
i=recall.shape[0]-2

# interpolation...
while i&gt;=0:
    if precision[i+1]&gt;precision[i]:
        precision[i]=precision[i+1]
    i=i-1

# plotting...
fig, ax = plt.subplots()
for i in range(recall.shape[0]-1):
    ax.plot((recall[i],recall[i]),(precision[i],precision[i+1]),'k-',label='',color='red') #vertical
    ax.plot((recall[i],recall[i+1]),(precision[i+1],precision[i+1]),'k-',label='',color='red') #horizontal

ax.plot(recall,precision2,'k--',color='blue')
#ax.legend()
ax.set_xlabel(""recall"")
ax.set_ylabel(""precision"")
plt.savefig('fig.jpg')
fig.show()
</code></pre>

<p>And here is a result:</p>

<p><a href=""http://i.stack.imgur.com/pEi0e.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pEi0e.jpg"" alt=""enter image description here""></a></p>
"
39838796,3125566.0,2016-10-03T19:12:58Z,39838419,0,"<p>The following pretty much does what you want. </p>

<ul>
<li><p>The tricky part is the computation of the intervals which was done
using <a href=""https://docs.python.org/2/library/functions.html#enumerate"" rel=""nofollow""><code>enumerate</code></a> to subtract the current item from the item at the next index, and subtracting the last item from the first.</p></li>
<li><p>The cumulatives are taken by <em>summing</em> the slices of the computed <code>sintervals</code>. The stop index of each slice is increased by 1 than that of the previous:</p></li>
<li><p>The last part is the formatting part which uses <code>.join</code> to <em>join</em> the list items, which are then formatted.</p></li>
</ul>

<hr>

<pre><code>def prime_form(*pitches, tones_in_octave=12):
    # pitches  = set(pitches)
    spitches = sorted([p % tones_in_octave for p in pitches])
    lth = len(spitches)

    # Question 1
    intervals = [(spitches[0 if (i+1) &gt;= lth else (i+1)]-x) % tones_in_octave for i, x in enumerate(spitches)]    
    sintervals = sorted(intervals) 

    # Question 2
    p_form = [sum(sintervals[:i]) for i in range(len(intervals))]

    print('({})\nthis is the prime form!'.format(' '.join(map(str, p_form))))
</code></pre>

<hr>

<p><strong>Tests</strong>:</p>

<pre><code>&gt;&gt;&gt; prime_form(9, 1, 5)
(0 4 8)
this is the prime form!

&gt;&gt;&gt; prime_form(11, 1, 0)
(0 1 2)
this is the prime form!

&gt;&gt;&gt; prime_form(1, 3, 4)
(0 1 3)
this is the prime form!
</code></pre>
"
39839283,510937.0,2016-10-03T19:46:38Z,39839119,1,"<p>This problem can be easily solved by first simplifying it and thinking recursively.</p>

<p>So let's first assume that all the elements in the input sequence are unique, then the set of ""unique"" permutations is simply the set of permutations.</p>

<p>Now to find the rank of the sequence <code>a_1, a_2, a_3, ..., a_n</code> into its set of permutations we can:</p>

<ol>
<li><p>Sort the sequence to obtain <code>b_1, b_2, ..., b_n</code>. This permutation by definition has rank <code>0</code>.</p></li>
<li><p>Now we compare <code>a_1</code> and <code>b_1</code>. If they are the same then we can simply remove them from the problem: the rank of <code>a_1, a_2, ..., a_n</code> will be the same as the rank of just <code>a_2, ..., a_n</code>.</p></li>
<li><p>Otherwise <code>b_1 &lt; a_1</code>, but then <strong>all</strong> permutations that start with <code>b_1</code> are going to be smaller than <code>a_1, a_2, ..., a_n</code>. The number of such permutations is easy to compute, it's just <code>(n-1)! = (n-1)*(n-2)*(n-3)*...*1</code>.</p>

<p>But then we can continue looking at our sequence <code>b_1, ..., b_n</code>. If <code>b_2 &lt; a_1</code>, again all permutations starting with <code>b_2</code> will be smaller.
So we should add <code>(n-1)!</code> again to our rank.</p>

<p>We do this until we find an index <code>j</code> where <code>b_j == a_j</code>, and then we end up at point 2.</p></li>
</ol>

<p>This can be implemented quite easily:</p>

<pre><code>import math

def permutation_rank(seq):
    ref = sorted(seq)
    if ref == seq:
        return 0
    else:
        rank = 0
        f = math.factorial(len(seq)-1)
        for x in ref:
            if x &lt; seq[0]:
                rank += f
            else:
                rank += permutation_rank(seq[1:]) if seq[1:] else 0
                return rank
</code></pre>

<p>The solution is pretty fast:</p>

<pre><code>In [24]: import string
    ...: import random
    ...: seq = list(string.ascii_lowercase)
    ...: random.shuffle(seq)
    ...: print(*seq)
    ...: print(permutation_rank(seq))
    ...: 
r q n c d w s k a z b e m g u f i o l t j x p h y v
273956214557578232851005079
</code></pre>

<p>On the issue of equal elements: the point where they come into play is that <code>(n-1)!</code> is the number of permutations, considering each element as different from the others. If you have a sequence of length <code>n</code>, made of symbol <code>s_1, ..., s_k</code> and symbol <code>s_j</code> appears <code>c_j</code> times then the number of unique permutations is `(n-1)! / (c_1! * c_2! * ... * c_k!).</p>

<p>This means that instead of just adding <code>(n-1)!</code> we have to divide it by that number, and also we want to decrease by one the count <code>c_t</code> of the current symbol we are considering.</p>

<p>This can be done in this way:</p>

<pre><code>import math
from collections import Counter
from functools import reduce
from operator import mul

def permutation_rank(seq):
    ref = sorted(seq)
    counts = Counter(ref)

    if ref == seq:
        return 0
    else:
        rank = 0
        f = math.factorial(len(seq)-1)
        for x in sorted(set(ref)):
            if x &lt; seq[0]:
                counts_copy = counts.copy()
                counts_copy[x] -= 1
                rank += f//(reduce(mul, (math.factorial(c) for c in counts_copy.values()), 1))
            else:
                rank += permutation_rank(seq[1:]) if seq[1:] else 0
                return rank
</code></pre>

<p>I'm pretty sure there is a way to avoid copying the counts dictionary, but right now I'm tired so I'll let that as an exercise for the reader.</p>

<p>For reference, the final result:</p>

<pre><code>In [44]: for i,x in enumerate(sorted(set(it.permutations('aabc')))):
    ...:     print(i, x, permutation_rank(x))
    ...:     
0 ('a', 'a', 'b', 'c') 0
1 ('a', 'a', 'c', 'b') 1
2 ('a', 'b', 'a', 'c') 2
3 ('a', 'b', 'c', 'a') 3
4 ('a', 'c', 'a', 'b') 4
5 ('a', 'c', 'b', 'a') 5
6 ('b', 'a', 'a', 'c') 6
7 ('b', 'a', 'c', 'a') 7
8 ('b', 'c', 'a', 'a') 8
9 ('c', 'a', 'a', 'b') 9
10 ('c', 'a', 'b', 'a') 10
11 ('c', 'b', 'a', 'a') 11
</code></pre>

<p>And to show that it is efficient:</p>

<pre><code>In [45]: permutation_rank('zuibibzboofpaoibpaybfyab')
Out[45]: 246218968687554178
</code></pre>
"
39839340,2041077.0,2016-10-03T19:51:16Z,39839119,0,"<p>Here's some Ruby code I wrote to do exactly this. You'd need to modify it if you have repeated elements (and decide how you want to handle them). </p>

<p>This takes advantage that if we have n elements, each selection of k elements will show up exactly (n-k)! times. E.g., [a,b,c,d] -- if we look at all permutations, (4-1)! = 3! of them will start with each of 'a', 'b', 'c', and 'd'. In particular, the first 3! will start with 'a', the next 3! with b, and so on. Then you recurse on the remaining elts.</p>

<pre><code>  def get_perm_id(arr)
    arr_len = arr.length
    raise ""get_perm_id requires an array of unique elts"" if arr_len != arr.uniq.length
    arr_sorted = arr.sort
    perm_num = 0
    0.upto(arr_len - 2) do |i|
      arr_elt = self[i]
      sorted_index = arr_sorted.find_index(arr_elt)
      sorted_right_index = arr_sorted.length - sorted_index - 1
      right_index = arr_len - i - 1
      left_delta = [0, right_index - sorted_right_index].max
      perm_num += left_delta * (arr_len - i - 1).factorial
      arr_sorted.slice!(sorted_index)
    end
    perm_num
  end
</code></pre>
"
39839381,2359772.0,2016-10-03T19:53:52Z,39839119,0,"<p><strong>Let's see how index of string can be calculated without finding all permutation of the string.</strong> </p>

<p>Consider string <code>s = ""cdab"".</code> Now, before string <code>s</code> (in lexical order), strings <code>""a***"",</code> <code>""b***""</code> would be there. (<code>*</code> denotes remaining characters).</p>

<p>That's <code>2*3!</code> strings. So any strings starting with <code>c</code> will have index more than this. </p>

<p>After <code>""a***""</code> and <code>""b***"",</code> string starting with <code>'c'</code>will begin.
Index of the string <code>s = 2*3! + index(""dab"")</code>.</p>

<p>Now recursively find the index for <code>""dab""</code></p>

<p>Just for illustration, order of string goes as follows :   </p>

<pre><code>    a*** --&gt; 3! 
    b*** --&gt; 3!
    ca** --&gt; 2!
    cb** --&gt; 2!
    cdab --&gt; 1  
</code></pre>

<p>Following is the python code : </p>

<pre><code>import math

def index(s):
    if(len(s)==1):
        return 1
    first_char = s[0]
    character_greater = 0
    for c in s:
        if(first_char&gt;c):
            character_greater = character_greater+1
    return (character_greater*math.factorial((len(s)-1)) + index(s[1:len(s)])    
</code></pre>
"
39839489,4124317.0,2016-10-03T20:00:59Z,39834523,0,"<p>That is interesting. I'm used to draw interactive plots a little bit differently: </p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from time import sleep

x = np.array([0])
y = np.array([0])

plt.ion()
fig = plt.figure()
ax=fig.add_subplot(111)
ax.set_xlim([0,50])
ax.set_ylim([0,2500])
line,  = ax.plot(x,y)
plt.show()
for i in range(51):
    x = np.append(x,[x[-1]+1])
    y = np.append(y,[x[-1]**2])
    line.set_data(x,y)
    plt.draw()
    sleep(0.01)
</code></pre>

<p>Can you (or anyone) check if this shows the same problems in Matplotlib 1.5?</p>
"
39840352,3293881.0,2016-10-03T21:00:37Z,39800223,1,"<p>You can simply use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.at.html"" rel=""nofollow""><code>np.add.at</code></a> -</p>

<pre><code>out = np.zeros_like(nodes)
np.add.at(out, edges[:,0],nodes[edges[:,1]])
np.add.at(out, edges[:,1],nodes[edges[:,0]])
</code></pre>
"
39840755,1016216.0,2016-10-03T21:32:49Z,39840625,0,"<p>First of all: store the values in a dictionary, that's way less work.</p>

<p>Then, just iterate over the words in the text and add the values:</p>

<pre><code>def get_text_value(text):
    lv = {'a': 1, 'b': 4, 'c': 2, 'd': 3, 'e': 1, 'f': 4, 'g': 3, 'h': 4, 'i': 1, 'j': 7, 'k': 7, 'l': 4, 'm': 6, 'n': 6, 'o': 1, 'p': 3, 'q': 9, 'r': 2, 's': 2, 't': 2, 'u': 1, 'v': 8, 'w': 5, 'x': 9, 'y': 9, 'z': 9}
    return sum(
              lv.get(word[:1], 0) + lv.get(word[-1:], 0)
              for word in text.lower().split("" "")
              )
</code></pre>
"
39840885,1016216.0,2016-10-03T21:43:16Z,39840845,3,"<p>I dont see where you're getting the <code>p</code> from. As far as I can see you can almost literally translate it from Haskell:</p>

<pre><code>def orbit(x, y):
    u, v = 0, 0
    while True:
        u, v = u**2 â v**2 + x, 2*u*v + y
        yield u, v
</code></pre>

<p>In their example, calling the function as <code>orbit(1, 2)</code>, <code>u</code> will be bound to <code>1</code> and <code>v</code> to <code>2</code> in the first round, then that (<code>(1, 2)</code>) is yielded. In the next iteration, <code>u = 1**2 - 2**2 + 1 = 1 - 4 + 1 = -2</code> and <code>v = 2*1*2 + 2 = 6</code>.</p>
"
39842057,5626112.0,2016-10-03T23:38:43Z,39841204,1,"<p>allright so I had to create a similar df because I did not have access to your <code>a</code> variable. I change your <code>a</code> variable for a list from 0 to 99... so t will be 0 to 99</p>

<p>you could do this :</p>

<pre><code>a = range(0, 100)
DF_data = pd.DataFrame([a, [np.sin(x)for x in a], [np.cos(x)for x in a]], index=[""t"", ""sine"", ""cosine""], columns=[""t_%d""%_ for _ in range(100)]).T
DF_data[""desc""] = [""info about this"" for _ in DF_data.index]

df = pd.melt(DF_data, id_vars=['t','desc'])
df.head(5)
</code></pre>

<p>this should return what you are looking for.</p>

<pre><code>     t             desc variable     value
0  0.0  info about this     sine  0.000000
1  1.0  info about this     sine  0.841471
2  2.0  info about this     sine  0.909297
3  3.0  info about this     sine  0.141120
4  4.0  info about this     sine -0.756802
</code></pre>
"
39842259,2399397.0,2016-10-04T00:01:37Z,39613476,0,"<p>@roman: Nice challenge you have there.</p>

<p>I have being in a similar scenario before so here is my <em>2 cents</em>: unless this consumer only <em>""read""</em> and <em>""write""</em> the message, without do any real proccessing of it, you could <em>re-design</em> this consumer as a consumer/producer that will <em>consume</em> the message, it will process the message and then will put result in another queue, that queue (processed messages for say) could be read by 1..N non-pooled asynchronous processes that would have open the DB connection in it's own entire life-cycle.</p>

<p>I can extend my answer, but I don't know if this approach fits for your needs, if so, I can give you more detail about the extended design.</p>
"
39842412,267540.0,2016-10-04T00:23:36Z,39842386,1,"<p>That error does not come from your template as you seem to think. It comes from your view </p>

<pre><code>def get_queryset(self):
    query = request.GET.get('q')
</code></pre>

<p>It should be</p>

<pre><code>    query = self.request.GET.get('q') 
</code></pre>
"
39842733,642070.0,2016-10-04T01:06:02Z,39842253,3,"<p><a href=""https://docs.python.org/3/library/os.html#os.walk"" rel=""nofollow"">os.walk</a> iterates through subdirectories yielding a 3-tuple <code>(dirpath, dirnames, filenames)</code> for each subdirectory visited. When you do:</p>

<pre><code>for dirs, folders, files in os.walk('.'):
    for subDirs in dirs:
</code></pre>

<p>things go badly wrong. <code>dirs</code> is the name of the subdirectory in each iteration which means that <code>for subDirs in dirs:</code> is really enumerating the characters in the directory name. It so happens that the first directory you iterate is <code>"".""</code> and just by luck its a single character directory name so your for loop appears to work.</p>

<p>As soon as you walk into another subdirectory (lets call it 'foo'), your code will try to find subdirectories called <code>foo\f</code>, <code>foo\o</code> and <code>foo\o</code> a second time. That doesn't work.</p>

<p>But you shouldn't be re-enumerating the subdirectories yourself. <code>os.walk</code> already does that. Boiling your code down to the enumeration part, this will find all of the <code>.docx</code> in the subtree.</p>

<pre><code>#!/usr/bin/env python3

import os

for dirpath, dirnames, filenames in os.walk('.'):
    docx_files = [fn for fn in filenames if fn.endswith('.docx')]
    for docx_file in docx_files:
        filename = os.path.join(dirpath, docx_file)
        print(filename)
</code></pre>
"
39843275,6902790.0,2016-10-04T02:30:01Z,39842758,0,"<p>After couple more hours of digging, I think it is best to keep a single User model and use permissions and roles for regulations. </p>

<p>There are ways that can make multiple different user model authentications work, such as describe in here: <a href=""http://stackoverflow.com/questions/3206856/how-to-have-2-different-admin-sites-in-a-django-project][1]"">How to have 2 different admin sites in a Django project?</a> But I decided it wasn't worth it for my purposes.</p>
"
39843324,3358074.0,2016-10-04T02:36:34Z,39842758,2,"<p>The recommended Django practice is to create a OneToOne field pointing to the User, rather than extending the User object - this way you build on top of Django's User by decorating only the needed new model properties (for example):</p>

<pre><code>class Profile(models.Model):
    user = models.OneToOneField(User,parent_link=True,blank=True,null=True)
    profile_image_path =  models.CharField(max_length=250,blank=True, null=True)
    phone = models.CharField(max_length=250,blank=True, null=True)
    address = models.ForeignKey(Address,blank=True,null=True)
    is_admin = models.NullBooleanField(default=False,blank=True,null=True)

    class Meta:
       verbose_name = 'Profile'
       verbose_name_plural = 'Profiles'
</code></pre>
"
39843502,4079962.0,2016-10-04T03:02:18Z,39824273,1,"<p>You could try instantiating the browser in the worker:</p>

<pre><code>def worker(queue):
    browser = webdriver.Chrome()
    try:
        while True:
            id_ = queue.get(True)
            obj = ReviewID(id_)
            obj.search(browser)
            if obj.exists(browser):
                print(obj.get_url(browser))
            else:
                print(""Nothing"")
    finally:
        brower.quit()
</code></pre>
"
39843523,674039.0,2016-10-04T03:04:16Z,39843488,61,"<p>Since python integers are unbounded, you have to do this with a custom class:</p>

<pre><code>import functools

@functools.total_ordering
class NeverSmaller(object):
    def __le__(self, other):
        return False

class ReallyMaxInt(NeverSmaller, int):
    def __repr__(self):
        return 'ReallyMaxInt()'
</code></pre>

<p>Here I've used a mix-in class <code>NeverSmaller</code> rather than direct decoration of <code>ReallyMaxInt</code>, because on Python 3 the action of <code>functools.total_ordering</code> would have been prevented by existing ordering methods inherited from <code>int</code>.  </p>

<p>Usage demo:</p>

<pre><code>&gt;&gt;&gt; N = ReallyMaxInt()
&gt;&gt;&gt; N &gt; sys.maxsize
True
&gt;&gt;&gt; isinstance(N, int)
True
&gt;&gt;&gt; sorted([1, N, 0, 9999, sys.maxsize])
[0, 1, 9999, 9223372036854775807, ReallyMaxInt()]
</code></pre>

<p>Note that in python2, <code>sys.maxint + 1</code> is bigger than <code>sys.maxint</code>, so you can't rely on that.  </p>

<p><em>Disclaimer</em>: This is an integer in the <a href=""https://en.wikipedia.org/wiki/Object-oriented_programming#Inheritance_and_behavioral_subtyping"" rel=""nofollow"">OO</a> sense, it is not an integer in the mathematical sense.  Consequently, arithmetic operations inherited from the parent class <code>int</code> may not behave sensibly.  If this causes any issues for your intended use case, then they can be disabled by implementing <code>__add__</code> and friends to just error out.</p>
"
39844035,664345.0,2016-10-04T04:10:19Z,39840625,0,"<p>Looks like you're just getting started with Python. Welcome! Couple things that might be worth getting more familiar with.</p>

<ol>
<li>Dictionaries</li>
</ol>

<p>Dictionaries are good for storing a key value pair. In your case, a dictionary would work really well as you have a letter (your key) and a numerical value (the value). As mentioned in the other answer, a dictionary is going to significantly clean up your code.</p>

<ol start=""2"">
<li>String methods.</li>
</ol>

<p>The function you need to go from getting the first and last letter of the entire sentence to the first and last of each word is <code>split</code>. It will split one string into a list of strings where it finds a space.</p>

<pre><code>&gt;&gt;&gt; ""enjoy Today"".split("" "")
['enjoy', 'Today']
&gt;&gt;&gt; ""abracadabra"".split("" "")
['abracadabra']
</code></pre>

<p>You don't have to just split on spaces.</p>

<pre><code>&gt;&gt;&gt; ""abracadabra"".split(""b"")
['a', 'racada', 'ra']
</code></pre>

<ol start=""3"">
<li>List comprehension</li>
</ol>

<p>Just google it as there are lots of articles that will do a better job explaining then I will here.</p>

<p>These 3 methods are a few of the key building blocks that will make Python enjoyable to write and your code concise and easy to read.</p>
"
39844080,1714030.0,2016-10-04T04:16:42Z,39672565,1,"<p>The most probable cause is the use of {{category.name}} for class names. </p>

<p>The code snippet doesn't show what values is accepted for category.name and my guess is it can be user input? 
See <a href=""http://www.w3schools.com/tags/att_global_class.asp"" rel=""nofollow"">naming rules</a> in section Attribute Values what is valid for class names.</p>

<p>It can be solved using template tag <a href=""https://docs.djangoproject.com/es/1.10/ref/templates/builtins/#slugify"" rel=""nofollow"">slugify</a> ({{category.name|slugify}}) but my recommendation is to try re-design the solution a bit.</p>
"
39844229,6096652.0,2016-10-04T04:35:34Z,39840625,0,"<p>do like this:</p>

<pre><code>def get_text_value2(text):
    letters = [chr(i) for i in range(ord('a'), ord('z')+1)]
    letter_values = [1, 4, 2, 3, 1, 4, 3, 4, 1, 7, 7, 4, 6, 6, 1, 3, 9, 2, 2, 2, 1, 8, 5, 9, 9, 9]
    alpha_value_dict = dict(zip(letters, letter_values))
    formatted_text = text.lower()
    return alpha_value_dict.get(text[0], 0) + alpha_value_dict.get(text[-1], 0)
</code></pre>

<p>or if you want keep the same logic of return value with your origional function, you can do:</p>

<pre><code>def get_text_value2(text):
    letters = [chr(i) for i in range(ord('a'), ord('z')+1)]
    letter_values = [1, 4, 2, 3, 1, 4, 3, 4, 1, 7, 7, 4, 6, 6, 1, 3, 9, 2, 2, 2, 1, 8, 5, 9, 9, 9]
    alpha_value_dict = dict(zip(letters, letter_values))
    formatted_text = text.lower()
    try:
        return alpha_value_dict[text[0]] + alpha_value_dict[text[-1]]
    except:
        return 0
</code></pre>
"
39844293,4177078.0,2016-10-04T04:41:43Z,39748267,4,"<p>As we cannot explore into your system, I am trying to analysis your first question by illustrating how <code>sys.path</code> is initialized. Available references are <a href=""http://mikeboers.com/blog/2014/05/23/where-does-the-sys-path-start"" rel=""nofollow"">where-does-sys-path-starts</a> and <a href=""http://pyvideo.org/pycon-us-2011/pycon-2011--reverse-engineering-ian-bicking--39-s.html"" rel=""nofollow"">pyco-reverse-engineering</a>(python2.6).</p>

<p>The <code>sys.path</code> comes from the following variables(in order):</p>

<ol>
<li><code>$PYTHONPATH</code> (highest priority)</li>
<li><code>sys.prefix</code>-ed stdlib</li>
<li><code>sys.exec_prefix</code>-ed stdlib</li>
<li><code>site-packages</code></li>
<li><code>*.pth</code> in site-packages (lowest priority)</li>
</ol>

<p>Now let's describe each of these variables:</p>

<ol>
<li><code>$PYTHONPATH</code>, this is just a system environment variable.</li>
<li>&amp; 3. <code>sys.prefix</code> and <code>sys.exec_prefix</code> are determined before any python script is executed. It is actually coded in the source <a href=""https://hg.python.org/cpython/file/e5d963cb6afc/Modules/getpath.c#l314"" rel=""nofollow"">Module/getpath.c</a>. </li>
</ol>

<p>The logic is like this: </p>

<pre><code>IF $PYTHONHOME IS set:
    RETURN sys.prefix AND sys.exec_prefix as $PYTHONHOME
ELSE:
    current_dir = directory of python executable;
    DO:
        current_dir = parent(current_dir)
        IF FILE 'lib/pythonX.Y/os.py' EXSITS:
            sys.prefix = current_dir
        IF FILE 'lib/pythonX.Y/lib-dynload' EXSITS:
            sys.exec_prefix = current_dir
        IF current_dir IS '/':
            BREAK
    WHILE(TRUE)
    IF sys.prefix IS NOT SET:
       sys.prefix = BUILD_PREFIX
    IF sys.exec_prefix IS NOT SET:
       sys.exec_prefix = BUILD_PREFIX
</code></pre>

<ol start=""4"">
<li><p>&amp; 5. <code>site-packages</code> and <code>*.pth</code> are added by import of <code>site.py</code>. In this module you will find the docs:</p>

<p>This will append site-specific paths to the module search path.  On
Unix (including Mac OSX), it starts with sys.prefix and
sys.exec_prefix (if different) and appends
lib/python/site-packages as well as lib/site-python.
... ...</p>

<p>For Debian and derivatives, this sys.path is augmented with directories
for packages distributed within the distribution. Local addons go
into /usr/local/lib/python/dist-packages, Debian addons
install into /usr/{lib,share}/python/dist-packages.
/usr/lib/python/site-packages is not used.</p>

<p>A path configuration file is a file whose name has the form
.pth; its contents are additional directories (one per line)
to be added to sys.path.
... ...</p></li>
</ol>

<p>And a code snippet for important function <code>getsitepackages</code>:</p>

<pre><code>sitepackages.append(os.path.join(prefix, ""local/lib"",
                            ""python"" + sys.version[:3],
                            ""dist-packages""))
sitepackages.append(os.path.join(prefix, ""lib"",
                            ""python"" + sys.version[:3],
                            ""dist-packages""))
</code></pre>

<p>Now I try to fig out where may be this odd problem comes from:</p>

<ol>
<li><code>$PYTHONPATH</code>, impossible, because it is empty both A and B</li>
<li><code>sys.prefix</code> and <code>sys.exec_prefix</code>, maybe, please check them and as well as <code>$PYTHONHOME</code></li>
<li><code>site.py</code>, maybe, check the file.</li>
</ol>

<p>The <code>sys.path</code> output of B is quite odd, <code>dist-package</code> (<code>site-package</code>) goes before <code>sys.exec_prefix</code> (<code>lib-dynload</code>). Please try to investigate  each step of <code>sys.path</code> initialization of machine B, you may find out something.</p>

<p>Very sorry that I cannot replicate your problem. By the way, about your question title, I think <code>SYS.PATH</code> is better than <code>PYTHONPATH</code>, which makes me misinterpretation as <code>$PYTHONPATH</code> at first glance.</p>
"
39844489,5715298.0,2016-10-04T05:03:01Z,39844100,1,"<p>You can check type of input value like this:</p>

<pre><code>    PyObject* check_type(PyObject*self, PyObject*args) {
    PyObject*any;

    if (!PyArg_ParseTuple(args, ""O"", &amp;any)) {
        PyErr_SetString(PyExc_TypeError, ""Nope."");
        return NULL;
    }

    if (PyFloat_Check(any)) {
        printf(""indeed float"");
    }
    else {
        printf(""\nint\n"");
    }

    Py_INCREF(Py_None);

    return Py_None;
}
</code></pre>

<p>You can extract float value from object using: </p>

<pre><code>double result=PyFloat_AsDouble(any);
</code></pre>

<p>But in this particular situation probably no need to do this, no matter what you parsing int or float you can grab it as a float and check on roundness: </p>

<pre><code>    float target;
    if (!PyArg_ParseTuple(args, ""f"", &amp;target)) {
                PyErr_SetString(PyExc_TypeError, ""Nope."");
                return NULL;
    }

    if (target - (int)target) {
        printf(""\n input is float \n"");
    }
    else {
        printf(""\n input is int \n"");
    }
</code></pre>
"
39844702,2750492.0,2016-10-04T05:22:16Z,39844473,1,"<p>You can simplify this problem with <code>reduce</code> (in <code>functools</code> in Py3)</p>

<pre><code>import functools as ft
from operator import mul

def find(ns):
    if len(ns) == 1 or len(ns) == 2 and 0 in ns:
        return str(max(ns))
    pos = filter(lambda x: x &gt; 0, ns)
    negs = sorted(filter(lambda x: x &lt; 0, ns))
    return str(ft.reduce(mul, negs[:-1 if len(negs)%2 else None], 1) * ft.reduce(mul, pos, 1))

&gt;&gt;&gt; find([-1, -2, -3, 0, 2])
'12'
&gt;&gt;&gt; find([-3, 0])
'0'
&gt;&gt;&gt; find([-1])
'-1'
&gt;&gt;&gt; find([])
'1'
</code></pre>
"
39844742,3322400.0,2016-10-04T05:26:38Z,39844473,0,"<p>Here is another solution that doesn't require libraries :</p>

<pre><code>def find(l):
    if len(l) &lt;= 2 and 0 in l: # This is the missing case, try [-3,0], it should return 0
        return max(l)
    l = [e for e in l if e != 0] # remove 0s    
    r = 1
    for e in l: # multiply all
        r *= e 
    if r &lt; 0: # if the result is negative, remove biggest negative number and retry
        l.remove(max([e for e in l if e &lt; 0]))
        r = find(l)
    return r

print(find([-1, -2, -3, 0, 2])) # 12
print(find([-3, 0])) # 0
</code></pre>

<p><strong>EDIT :</strong></p>

<p>I think I've found the missing case which is when there are only two elements in the list, and the highest is 0.</p>
"
39845004,1832058.0,2016-10-04T05:50:28Z,39843798,1,"<p>You have to open and close file only once - open before first <code>crawl_dfs()</code> and close after first <code>crawl_dfs()</code></p>

<p>Tested:</p>

<pre><code>import urllib
from bs4 import BeautifulSoup
import requests
#import readability
import time
import http.client

# --- functions ---

def get_urls(seed_url):
    r = requests.get(seed_url)
    soup = BeautifulSoup(r.content,""html.parser"")
    links = soup.findAll('a', href=True)
    valid_links = []
    for links in links:
        if 'wiki' in links['href'] and '.' not in links['href'] and ':' not in links['href'] and '#' not in links['href']:
            valid_links.append(root_url + links['href'])
    return valid_links


def crawl_dfs(seed_url, max_depth, file_out):
    if max_depth &gt;= 1:
       children = get_urls(seed_url)
       for child in children:
           if child not in visited:          
               file_out.write(child + ""\n"")                                    
               #time.sleep(1)
               visited.append(child)
               crawl_dfs(child, max_depth-1, file_out)

# --- main ---

seed_url = ""https://en.wikipedia.org/wiki/Sustainable_energy""
root_url = ""https://en.wikipedia.org""
max_limit = 1

visited=[root_url]

file1 = open(""file_crawled.txt"", ""w+"")

crawl_dfs(seed_url, max_limit, file1)

file1.close()
</code></pre>
"
39845124,768675.0,2016-10-04T05:59:20Z,39730467,1,"<p>The actual limits are not known until the figure is drawn.  By adding a canvas draw after setting the <code>xlim</code> and <code>ylim</code>, but before obtaining the <code>xlim</code> and <code>ylim</code>, then one can get the desired limits.</p>

<pre><code>f,ax=plt.subplots(1) #open a figure
ax.axis('equal') #make the axes have equal spacing
ax.plot([0,20],[0,20]) #test data set

#change the plot axis limits
ax.set_xlim([2,18]) 
ax.set_ylim([5,15])

#Drawing is crucial 
f.canvas.draw() #&lt;---------- I added this line

#read the plot axis limits
xlim2=array(ax.get_xlim())
ylim2=array(ax.get_ylim())

#define indices for drawing a rectangle with xlim2, ylim2
sqx=array([0,1,1,0,0])
sqy=array([0,0,1,1,0])

#plot a thick rectangle marking the xlim2, ylim2
ax.plot(xlim2[sqx],ylim2[sqy],lw=3) 
</code></pre>

<p><a href=""http://i.stack.imgur.com/v4XJd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v4XJd.png"" alt=""Figure produced by script""></a></p>
"
39845432,527088.0,2016-10-04T06:23:21Z,39844100,5,"<p>If you declare a C function to accept floats, the compiler won't complain if you hand it an int. For instance, this program produces the answer 2.000000:</p>

<pre><code>#include &lt;stdio.h&gt;

float f(float x) {
  return x+1;
}

int main() {
  int i=1;
  printf (""%f"", f(i));
}
</code></pre>

<p>A python module version, iorf.c:</p>

<pre><code>#include &lt;Python.h&gt;

static PyObject *IorFError;

float f(float x) {
  return x+1;
}


static PyObject *
fwrap(PyObject *self, PyObject *args) {
  float in=0.0;
  if (!PyArg_ParseTuple(args, ""f"", &amp;in))
    return NULL;
  return Py_BuildValue(""f"", f(in));
}

static PyMethodDef IorFMethods[] = {
    {""fup"",  fwrap, METH_VARARGS,
     ""Arg + 1""},
    {NULL, NULL, 0, NULL}        /* Sentinel */
};


PyMODINIT_FUNC
initiorf(void)
{
  PyObject *m;

  m = Py_InitModule(""iorf"", IorFMethods);
  if (m == NULL)
    return;

  IorFError = PyErr_NewException(""iorf.error"", NULL, NULL);
  Py_INCREF(IorFError);
  PyModule_AddObject(m, ""error"", IorFError);
}
</code></pre>

<p>The setup.py:</p>

<pre><code>from distutils.core import setup, Extension

module1 = Extension('iorf',
                    sources = ['iorf.c'])

setup (name = 'iorf',
       version = '0.1',
       description = 'This is a test package',
       ext_modules = [module1])
</code></pre>

<p>An example:</p>

<pre><code>03:21 $ python
Python 2.7.10 (default, Jul 30 2016, 18:31:42)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import iorf
&gt;&gt;&gt; print iorf.fup(2)
3.0
&gt;&gt;&gt; print iorf.fup(2.5)
3.5
</code></pre>
"
39846321,6354622.0,2016-10-04T07:15:44Z,39846207,3,"<pre><code>from random import randint

def create_bombpos():
    global BOMBS, NUM_BOMBS, GRID_TILES
    i = 0
    while i&lt;NUM_BOMBS:
       x = randint(1, GRID_TILES)
       y = randint(1, GRID_TILES)
       if (x,y) not in BOMBS
           BOMBS.append((x, y))
           i = i + 1
    print(BOMBS)
</code></pre>

<p>If the newly generated point is already in the list, then <code>i</code> won't get incremented, and we will find another newly generated point, till it is not present in <code>BOMBS</code>.</p>

<p>Hope it helps!!</p>
"
39846368,15277.0,2016-10-04T07:18:03Z,39844100,1,"<p>Floats are (usually) passed in via registers while ints are (usually) passed in via the stack. This means that you literally cannot, inside the function, check whether the argument is a float or an int.</p>

<p>The only workaround is to use variadic arguments, with the first argument specifying the type as either int or double (not float).</p>

<pre><code>func_int_or_double (uint8_t type, ...) {
va_list ap;
va_start (ap, type);
int intarg;
double doublearg;
if (type==1) {
   intarg = va_arg (ap, int);
}
if (type==2) {
   doublearg = va_arg (ap, double);
}
va_end (ap);
// Your code goes here
}
</code></pre>

<p>Although, I'm not really sure if python can handle calling variadic functions, so YMMV. As a last ditch-effort you can always sprintf the value into a buffer and let your function sscanf float/int from the buffer.</p>
"
39846391,2254704.0,2016-10-04T07:19:28Z,39846207,4,"<p>Searching each time through your whole BOMBS list would cost you <code>O(n)</code> (linear time). Why don't you use a <a href=""https://docs.python.org/2/library/sets.html"" rel=""nofollow"">set</a> instead? a Set guarantees that you ll end up with distinct (in terms of hashing) elements.</p>

<pre><code>from random import randint

def create_bombpos():
BOMBS = set()
i = 0
while i&lt;NUM_BOMBS:
   x = randint(1, GRID_TILES)
   y = randint(1, GRID_TILES)
   if (x,y) not in BOMBS
       BOMBS.add((x, y))
       i = i + 1
print(BOMBS)
</code></pre>

<p>Let me give u an example of a set:</p>

<pre><code>&gt;&gt;&gt; a = set()
&gt;&gt;&gt; a.add((1,2))
&gt;&gt;&gt; a
{(1, 2)}
&gt;&gt;&gt; a.add((1,2))
&gt;&gt;&gt; a.add((1,3))
&gt;&gt;&gt; a.add((1,2))
&gt;&gt;&gt; a
{(1, 2), (1, 3)}
</code></pre>

<p>I can add the same element to a set many times, but only 1 instance will be present.</p>
"
39846442,5771269.0,2016-10-04T07:22:22Z,39844473,1,"<pre><code>from functools import reduce
from operator import mul

def find(array):
    negative = []
    positive = []
    zero = None
    removed = None

    def string_product(iterable):
        return str(reduce(mul, iterable, 1))

    for number in array:
        if number &lt; 0:
            negative.append(number)
        elif number &gt; 0:
            positive.append(number)
        else:
            zero = str(number)

    if negative:
        if len(negative) % 2 == 0:
            return string_product(negative + positive)

        removed = max(negative)

        negative.remove(removed)

        if negative:
            return string_product(negative + positive)

    if positive:
        return string_product(positive)

    return zero or str(removed)
</code></pre>
"
39846474,2336654.0,2016-10-04T07:24:28Z,39844967,4,"<p><strong><em>strategy</em></strong><br>
create <code>pd.Series</code> representing a <code>map</code> from filenames to filenames.<br>
<code>stack</code> our dataframe, <code>map</code>, then <code>unstack</code></p>

<p><strong><em>setup</em></strong>  </p>

<pre><code>import pandas as pd
import numpy as np
from string import letters

media_frame = pd.DataFrame(
    pd.DataFrame(
        np.random.choice(list(letters), 9500 * 800 * 3) \
          .reshape(3, -1)).sum().values.reshape(9500, -1))

u = np.unique(media_frame.values)
from_filenames = pd.Series(u)
to_filenames = from_filenames.str[1:] + from_filenames.str[0]

m = pd.Series(to_filenames.values, from_filenames.values)
</code></pre>

<p><strong><em>solution</em></strong>  </p>

<pre><code>media_frame.stack().map(m).unstack()
</code></pre>

<hr>

<h1>timing</h1>

<p><strong><em>5 x 5 dataframe</em></strong></p>

<p><a href=""http://i.stack.imgur.com/jZbck.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jZbck.png"" alt=""enter image description here""></a></p>

<p><strong><em>100 x 100</em></strong></p>

<p><a href=""http://i.stack.imgur.com/199TE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/199TE.png"" alt=""enter image description here""></a></p>

<p><strong><em>9500 x 800</em></strong></p>

<p><a href=""http://i.stack.imgur.com/117t8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/117t8.png"" alt=""enter image description here""></a></p>

<p><strong><em>9500 x 800</em></strong><br>
<code>map</code> using <code>series</code> vs <code>dict</code><br>
<code>d = dict(zip(from_filenames, to_filenames))</code></p>

<p><a href=""http://i.stack.imgur.com/Sw2V3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Sw2V3.png"" alt=""enter image description here""></a></p>
"
39846480,6609526.0,2016-10-04T07:24:41Z,39846207,0,"<p>Use a python set for this, it will automatically check for duplicates and simply ignore every entry that already is in the list.
I also think the runtime is much better than using a list and checking for duplicates manually.</p>

<p>Link: <a href=""https://docs.python.org/2/library/sets.html"" rel=""nofollow"">https://docs.python.org/2/library/sets.html</a></p>
"
39846562,2902280.0,2016-10-04T07:29:11Z,39845960,2,"<p>You could always divide your computation time by 2, noticing that d(i, i) = 0 and d(i, j) = d(j, i).</p>

<p>But have you had a look at <code>sklearn.metrics.pairwise.pairwise_distances()</code> (in v 0.18, see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html"" rel=""nofollow"">the doc here</a>) ?</p>

<p>You would use it as:</p>

<pre><code>from sklearn.metrics import pairwise
import numpy as np

a = np.array([[0, 0, 0], [1, 1, 1], [3, 3, 3]])
pairwise.pairwise_distances(a)
</code></pre>
"
39846761,6609526.0,2016-10-04T07:40:14Z,39845960,-3,"<p>Forget about numpy, which is only a convenience solution for self expanding arrays.
Use python lists instead which have a very fast indexing access and are about 15 times faster.
Use it like this:</p>

<pre><code>features = list(dataframe)
distances = [[None]*17000]*17000

def sum_diff():
    for i in range(17000):
        for j in range(17000):
            for k in range(300):
                diff = features[i][k] - features[j][k]
                diff = diff*diff
                sumsquares = sumsquares + diff
                distances[i][j] = sumsquares
</code></pre>

<p>I hope this is faster than your solution, just try it and give feedback please.</p>
"
39846872,1531124.0,2016-10-04T07:46:49Z,39846735,1,"<p>Thing is: you let that library method <strong>combinations</strong> do all the ""real"" work for you. </p>

<p>And of course: normally that is exactly the way to go. You do <strong>not</strong> want to re-invent the wheel when there is an existing library function that gives you what you need. Your current code is pretty concise, and good to read (except maybe that you should call your list, well, ""list"", but not ""l"").</p>

<p>But this case is different: obviously, most of the execution time for this program will happen in that call. And it seems that google thinks whatever this call is doing .. can be done <strong>faster</strong>. </p>

<p>So, the answer for you is: you actually want to re-invent the wheel, by rewriting your code in a way that is <strong>better</strong> than what it is doing right now! A first starting point might be to check out the source code of <strong>combinations</strong> to understand if/how that call is doing things that you do not need in your context.</p>

<p><em>Guessing</em>: that call creates <strong>a lot</strong> of permutations that are not ideal. All of that is <strong>wasted</strong> time. You want to step back and consider how to build those lucky triples from your input <strong>without</strong> creating a ton of not so lucky triples!</p>
"
39846996,1984065.0,2016-10-04T07:53:24Z,39846207,3,"<p>You could also use random.sample to achieve this:</p>

<pre><code>from random import sample

GRID_TILES = 100
NUM_BOMBS = 5

indexes = sample(range(GRID_TILES * GRID_TILES), NUM_BOMBS)
BOMBS = [(i // GRID_TILES, i % GRID_TILES) for i in indexes]
</code></pre>
"
39847037,5847976.0,2016-10-04T07:55:34Z,39845960,1,"<p>The big thing with numpy is to avoid using loops and to let it do its magic with the vectorised operations, so there are a few basic improvements that will save you some computation time: </p>

<pre><code>import numpy as np
import timeit

#I reduced the problem size to 1000*300 to keep the timing in reasonable range
n=1000
features = np.random.rand(n,300)
distances = np.zeros((n,n))


def sum_diff():
    for i in range(n):
        for j in range(n):
            diff = np.array(features[i] - features[j])
            diff = np.square(diff)
            sumsquares = np.sum(diff)
            distances[i][j] = sumsquares

#Here I removed the unnecessary copy induced by calling np.array
# -&gt; some improvement
def sum_diff_v0():
    for i in range(n):
        for j in range(n):
            diff = features[i] - features[j]
            diff = np.square(diff)
            sumsquares = np.sum(diff)
            distances[i][j] = sumsquares

#Collapsing of the statements -&gt; no improvement
def sum_diff_v1():
    for i in range(n):
        for j in range(n):
            distances[i][j] = np.sum(np.square(features[i] - features[j]))

# Using brodcasting and vetorized operations -&gt; big improvement
def sum_diff_v2():
    for i in range(n):
        distances[i] = np.sum(np.square(features[i] - features),axis=1)

# Computing only half the distance -&gt; 1/2 computation time
def sum_diff_v3():
    for i in range(n):
        distances[i][i+1:] = np.sum(np.square(features[i] - features[i+1:]),axis=1)
    distances[:] = distances + distances.T

print(""original :"",timeit.timeit(sum_diff, number=10))
print(""v0 :"",timeit.timeit(sum_diff_v0, number=10))
print(""v1 :"",timeit.timeit(sum_diff_v1, number=10))
print(""v2 :"",timeit.timeit(sum_diff_v2, number=10))
print(""v3 :"",timeit.timeit(sum_diff_v3, number=10))
</code></pre>

<p><strong>Edit :</strong> For completeness I also timed Camilleri's solution that is <strong>much faster</strong>:</p>

<pre><code>from sklearn.metrics import pairwise

def Camilleri_solution():
    distances=pairwise.pairwise_distances(features)
</code></pre>

<p><strong>Timing results</strong> (in seconds, function run 10 times with 1000*300 input):</p>

<pre><code>original : 138.36921879299916
v0 : 111.39915344800102
v1 : 117.7582511530054
v2 : 23.702392491002684
v3 : 9.712442981006461
Camilleri's : 0.6131987979897531
</code></pre>

<p>So as you can see we can easily gain an order of magnitude by using the proper numpy syntax. Note that with only 1/20th of the data the function run in about one second so I would expect the whole thing to run in the tens of minutes as the scipt runs in N^2.</p>
"
39847350,5209273.0,2016-10-04T08:13:37Z,39846735,1,"<p>Here is a solution off the top of my head that has O(n^2) time and O(n) space complexity. I think there is a better solution (probably using dynamic programming), but this one beats generating all combinations.</p>

<pre><code>public static int foobar( int[] arr)
{
    int noOfCombinations = 0;
    int[] noOfDoubles = new int[arr.length];

    // Count lucky doubles for each item in the array, except the first and last items
    for( int i = 1; i &lt; arr.length-1; ++i)
    {
        for( int j = 0; j &lt; i; ++j)
        {
            if( arr[i] % arr[j] == 0)
                ++noOfDoubles[i];
        }
    }

    // Count lucky triples
    for( int i = 2; i &lt; arr.length; i++)
    {
        for( int j = 1; j &lt; i; ++j)
        {
            if( arr[i] % arr[j] == 0)
                noOfCombinations += noOfDoubles[j];
        }
    }

    return noOfCombinations;
}
</code></pre>
"
39847753,199754.0,2016-10-04T08:38:29Z,39844967,1,"<p>I got the 60 second task to complete in 10 seconds by removing <code>replace()</code> altogether and using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_value.html"" rel=""nofollow"">set_value()</a> one element at a time.</p>
"
39848100,1874054.0,2016-10-04T08:56:05Z,39847681,0,"<p>How about something like this :</p>

<pre><code>(&lt;image&gt;\n&lt;title&gt;CNN.com - )(.*?)(&lt;\/title&gt;\n.*) 
</code></pre>

<p>Group number 2 would be <code>Technology</code>.</p>
"
39848174,6919239.0,2016-10-04T08:59:13Z,39847681,1,"<p>Your regexp is not bad but you need to escape the slash in <code>&lt;/title&gt;</code> with a backslash and it does not match because of the newlines in your string.</p>

<p>Newlines are whitespaces (like space, tabulation... \s is equivalent to [ \t\n\r\f\v] when the UNICODE flag is not set), so you can use \s to match them.</p>

<p>I assume you're using python3 but it does not matter.</p>

<pre><code>s = """"""&lt;image&gt;
&lt;title&gt;CNN.com - Technology&lt;/title&gt;
&lt;link&gt;http://www.cnn.com/TECH/index.html?eref=rss_tech&lt;/link&gt;""""""
r = r""&lt;image&gt;[\s]*&lt;title[^&gt;]*&gt;CNN.com - (.*?)&lt;\/title&gt;[\s]*&lt;link&gt;""
m = re.search(r, s)
print(m.group(0))
print(m.group(1))
</code></pre>

<p>group(1) is ""Technology"".</p>
"
39848201,6663192.0,2016-10-04T09:00:28Z,39847681,0,"<p>If you use the 'single line' option for regex, you name newlines with <code>.</code>. So, you can do:</p>

<pre><code>&lt;image&gt;.&lt;title[^&gt;]*&gt;CNN.com - (.*?)&lt;/title&gt;.&lt;link&gt;
</code></pre>
"
39848393,2550354.0,2016-10-04T09:11:22Z,39848164,0,"<p>If you need a generic convertor('convert') like you did, this is the way to go.</p>

<p>The biggest downside will be <strong>performance</strong> when you will need to create a lot of instances( I assumed you might, since the class called <code>Vector</code>). This will be slow since python class initiation is slow.</p>

<p>In this case you might consider using <a href=""https://docs.python.org/3/library/collections.html#collections.namedtuple"" rel=""nofollow"">namedTuple</a> you can see the docs have a similar scenario as you have.</p>

<p>As a side note: If that possible, why not creating a dict with the string representation of x and y on the init method? and then keep using the x and y as normal variables without all the converting</p>
"
39849658,2550354.0,2016-10-04T10:12:59Z,39849497,1,"<p>Sounds like the dom elements are not yet loaded when your code try to reach them. </p>

<p>Try to <a href=""http://stackoverflow.com/questions/7781792/selenium-waitforelement"">wait</a> for the elements to be fully loaded and just then replace.</p>

<p>This works for your when you run it command by command because then you let the driver load all the elements before you execute more commands. </p>
"
39850114,512100.0,2016-10-04T10:37:15Z,39849875,-2,"<p>CSRF token gets included in HTML form by calling <code>hidden_tag</code> function on your form object.</p>

<p>For example check this <a href=""https://gist.github.com/srahul07/6507758"" rel=""nofollow"">gist</a>, line number 6. This is how you add form and it's elements in jinja.</p>
"
39850582,2901002.0,2016-10-04T11:01:34Z,39850550,2,"<p>I think you need remove one <code>[]</code>:</p>

<pre><code>test = [1,0,0,0]
df.iloc[1] = test

print (df) 
         this   is    a sentence
this      NaN  NaN  NaN      NaN
is          1    0    0        0
a         NaN  NaN  NaN      NaN
sentence  NaN  NaN  NaN      NaN
</code></pre>
"
39850762,2615075.0,2016-10-04T11:10:20Z,39849875,4,"<p>You should pass a plain dict and the request object to <code>template.render()</code>, not a <code>RequestContext</code>. The template engine will convert it to a <code>RequestContext</code> for you:</p>

<pre><code>template = get_template(""osnowa_app/register.html"")
context = {'form': form}
output = template.render(context, request)
</code></pre>

<p>Right now, the <code>template.render()</code> function sees a dict-like object as the first argument, but no request as the second argument. Without a request as the second argument, it converts the dict-like <code>RequestContext</code> into a plain <code>Context</code> object. Since the <code>Context</code> object doesn't run context processors, your context is missing the csrf token. </p>

<p>Alternatively you can just use the <a href=""https://docs.djangoproject.com/en/1.10/topics/http/shortcuts/#render"" rel=""nofollow""><code>render</code> shortcut</a>, which returns a <code>HttpResponse</code> object with the rendered template as content:</p>

<pre><code>from django.shortcuts import render

def register(request):
    ...
    return render(request, ""osnowa_app/register.html"", {'form': form})
</code></pre>

<p>This particular case is also being discussed in <a href=""https://code.djangoproject.com/ticket/27258"" rel=""nofollow"">ticket #27258</a>. </p>
"
39850892,2648166.0,2016-10-04T11:17:36Z,39850270,1,"<p>This should resolve this. You basically save the old function with different name and give your function as the default post.</p>

<pre><code>setattr(requests, 'old_post', requests.post)

def post(url, data=None, json=None, **kwargs):
    try:
        requests.old_post(url, data, json, kwargs)
    except:
        notify_another_process()

setattr(requests, 'post', post)
</code></pre>
"
39851324,1226028.0,2016-10-04T11:39:15Z,39851196,2,"<p><code>with</code> is for use with context managers.
At the code level, a context manager must define two methods:</p>

<ul>
<li><code>__enter__(self)</code></li>
<li><code>__exit__(self, type, value, traceback)</code>.</li>
</ul>

<p>Be aware that there are class decorators which can turn otherwise simple classes/functions into context managers - see <a href=""https://docs.python.org/2.7/library/contextlib.html"" rel=""nofollow"">contextlib</a> for some examples</p>
"
39851381,2154798.0,2016-10-04T11:41:34Z,39850270,1,"<p>You're almost there, but you should use the self argument</p>

<pre><code>def post_new(self, url, data=None, json=None, **kwargs):
    try:
        return self.request('POST', url, data=data, json=json, **kwargs)
    except:
        notify_another_process()
</code></pre>

<p>Then set the post function to port_new</p>

<pre><code>requests.post = post_new
</code></pre>
"
39851760,4952130.0,2016-10-04T12:01:03Z,39851196,2,"<h3>Regarding when you <em>should</em> use it:</h3>

<p>No one forces you to use the <code>with</code> statement, it's just syntactic sugar that's there to make your life easier. If you use it or not is totally up to you but, it is generally recommended to do so. (We're forgetful and <code>with ...</code> looks ways better than explicit initialize resource/finalize recourse calls).</p>

<h3>When you <em>can</em> use it:</h3>

<p>When you <em>can</em> use it boils down to examining if it defines the context manager protocol. This could be as simple as trying to use <code>with</code> and seeing that it fails :-)</p>

<p>If you dynamically need to check if an object <em>is</em> a context manager, you have two options.</p>

<p>First, wait for the stable release of <code>Python 3.6</code> which defines an <code>ABC</code> for context managers, <code>ContextManager</code>, which can be used in <code>issubclass/isinstance</code> checks:</p>

<pre><code>&gt;&gt;&gt; from typing import ContextManager
&gt;&gt;&gt; class foo:
...     def __enter__(self): pass
...     def __exit__(self): pass
... 
&gt;&gt;&gt; isinstance(foo(), ContextManager)
True
&gt;&gt;&gt; class foo2: pass
... 
&gt;&gt;&gt; isinstance(foo2(), ContextManager)
False
</code></pre>

<p>Or, create your own little function to check for it:</p>

<pre><code>def iscontext(inst):
    cls = type(inst)
    return (any(""__enter__"" in vars(a) for a in cls.__mro__) and
            any(""__exit__"" in vars(a) for a in cls.__mro__))
</code></pre>

<p>As a final note, the <code>with</code> statement is present in <code>Python 2</code> and in <code>3</code>, the use case you saw probably just wasn't aware of it :-).</p>
"
39851863,2063361.0,2016-10-04T12:05:43Z,39851196,0,"<p>You should use <code>with</code> whenever you need to perform some similar action before and after executing the statement. For example:</p>

<ul>
<li><em>Want to execute SQL query?</em> You need to open and close the connections safely.Use <code>with</code>.</li>
<li><em>Want to perform some action on file?</em> You have to open and close the file safely. Use <code>with</code></li>
<li><em>Want to store some data in temporary file to perform some task?</em> You need to create the directory, and clean it up once you are done. Use <code>with</code>, and so on. . . </li>
</ul>

<p>Everything you want to perform before the query execution, add it to the <code>__enter__()</code> method. And the action to be performed after, add it to the <code>__exit__()</code> method.</p>

<p>One of the nice thing about <code>with</code> is, <em><code>__exit__</code> is executed even if the code within <code>with</code> raises any <code>Exception</code></em></p>
"
39851921,2901002.0,2016-10-04T12:08:34Z,39851743,1,"<p>IIUC you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a> and add custom function with <code>apply</code>:</p>

<pre><code>print (d.groupby('n')['ts'].apply(lambda x: (x.min() - pd.Timestamp('2016-07-15')).days))
n
12    0
23    0
31    0
Name: ts, dtype: int64
</code></pre>

<p>In your code you get <code>0</code> too, but values are converted to <code>datetime</code> (<code>1970-01-01</code>), because <code>dtype</code> of <code>ts</code> was <code>datetime</code> before.</p>

<p>I think then need cast <code>datetime</code> to <code>int</code>, but first convert to <code>numpy array</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.values.html"" rel=""nofollow""><code>values</code></a>:</p>

<pre><code>s = pd.pivot_table(d, columns='n', 
                      values='ts', 
                      aggfunc=lambda x: (np.min(x) - pd.Timestamp('2016-07-15')).days)
s = s.values.astype(int)
print (s)
n
12    0
23    0
31    0
Name: ts, dtype: int64
</code></pre>
"
39852190,820410.0,2016-10-04T12:20:43Z,39850270,1,"<p>This is the answer which worked for me. It is inspired by the answers mentioned by <a href=""http://stackoverflow.com/users/2648166/siddharth-gupta"">Siddharth</a> &amp; <a href=""http://stackoverflow.com/users/2154798/lafferc"">lafferc</a> both. This is on top of what both of them mentioned.</p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; def post(self, url, data=None, json=None, **kwargs):
...     try:
...         raise Exception()
...     except:
...         print ""notifying another process""
... 
&gt;&gt;&gt; setattr(requests.Session, 'post_old', requests.Session.post)
&gt;&gt;&gt; setattr(requests.Session, 'post', post)
&gt;&gt;&gt; s = requests.Session()
&gt;&gt;&gt; s.post(""url"")
notifying another process
</code></pre>
"
39852317,2063361.0,2016-10-04T12:26:01Z,39852123,0,"<p>You may use <a href=""https://docs.python.org/2/library/re.html#re.sub"" rel=""nofollow""><code>re.sub()</code></a> of regex to achieve it:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; replace_with = 'HELLO'
&gt;&gt;&gt; new_string  = re.sub('group by\s\w+\(utimestamp\)', ""group_by""+replace_with, query)

# Value of new_string: SELECT  as utimestamp, sum(value) as value 
# from table 
# where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00' 
# group by HELLO, HELLO, id
</code></pre>

<p>where <code>replace_with</code> is the content you need to update with the pattern <code>'\w+\(utimestamp\)'</code> and <code>query</code> is the string you mentioned in the code.</p>

<p>Here, <code>\w+</code> means alphabets with occurence of one or more, whereas <code>\(utimestamp\)</code> along with that denotes, words followed by the string <code>(utimestamp)</code>.</p>

<p><strong>EDIT</strong>:</p>

<p>As is mentioned in the comment, to replace all instances of the <code>timestamp</code> in the <code>query</code>, regex expression should be like:</p>

<pre><code>re.sub('group by\s\w+\(utimestamp\)(,\s*\w+\(utimestamp\))*', ""group_by"" + replace_with, query)

# Returned Value:  
# SELECT DATE(utimestamp) as utimestamp, sum(value) as value from table
# where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00'
# group by HELLO, id
</code></pre>
"
39852346,3728414.0,2016-10-04T12:27:56Z,39852123,0,"<p>From what you asked, I would go for</p>

<pre><code>query = query.split(""group by"")[0] + "" group by MY_COOL_STRING"" + query.split(""(utimestamp)"")[-1]
</code></pre>

<p>It concatenates the part before the <code>group by</code>, then <code>MY_COOL_STRING</code> and then first thing before the first <code>(utimestamp)</code>.</p>
"
39853371,2901002.0,2016-10-04T13:14:59Z,39853311,4,"<p>You can use <code>lreshape</code>:</p>

<pre><code>types = [col for col in df.columns if col.startswith('type')]
location = [col for col in df.columns if col.startswith('location')]

print(pd.lreshape(df, {'Type':types, 'Location':location}, dropna=False))
</code></pre>

<p>Sample:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({
'type1': {0: 1, 1: 4}, 
'id': {0: 'a', 1: 'a'}, 
'type10': {0: 1, 1: 8},
'location1': {0: 2, 1: 9},
'location10': {0: 5, 1: 7}})

print (df)
  id  location1  location10  type1  type10
0  a          2           5      1       1
1  a          9           7      4       8

types = [col for col in df.columns if col.startswith('type')]
location = [col for col in df.columns if col.startswith('location')]

print(pd.lreshape(df, {'Type':types, 'Location':location}, dropna=False))
  id  Location  Type
0  a         2     1
1  a         9     4
2  a         5     1
3  a         7     8
</code></pre>

<hr>

<p>Another solution with double <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html"" rel=""nofollow""><code>melt</code></a>:</p>

<pre><code>print (pd.concat([pd.melt(df, id_vars='id', value_vars=types, value_name='type'),
                  pd.melt(df, value_vars=location, value_name='Location')], axis=1)
         .drop('variable', axis=1))

  id  type  Location
0  a     1         2
1  a     4         9
2  a     1         5
3  a     8         7
</code></pre>
"
39853445,2867928.0,2016-10-04T13:18:59Z,39853321,2,"<p>python datetime can't parse the <code>GMT</code> part (You might want to specify it manually in your format). You can use <code>dateutil</code> instead:</p>

<pre><code>In [16]: s = 'Tue Oct 04 2016 12:13:00 GMT+0200 (CEST)'

In [17]: from dateutil import parser

In [18]: parser.parse(s)
Out[18]: d = datetime.datetime(2016, 10, 4, 12, 13, tzinfo=tzoffset(u'CEST', -7200))
In [30]: d.utcoffset()
Out[30]: datetime.timedelta(-1, 79200)

In [31]: d.tzname()
Out[31]: 'CEST'
</code></pre>
"
39853448,1016216.0,2016-10-04T13:19:12Z,39853321,2,"<p>%z is the <code>+0200</code>, <code>%Z</code> is <code>CEST</code>. Therefore:</p>

<pre><code>&gt;&gt;&gt; s = ""Tue Oct 04 2016 12:13:00 GMT+0200 (CEST)""
&gt;&gt;&gt; datetime.strptime(s, '%a %b %d %Y %H:%M:%S GMT%z (%Z)')
datetime.datetime(2016, 10, 4, 12, 13, tzinfo=datetime.timezone(datetime.timedelta(0, 7200), 'CEST'))
</code></pre>

<p>I also replaced your <code>%m</code> with <code>%d</code>; <code>%m</code> is the month, numerically, so in your case <code>04</code> would be parsed as April.</p>
"
39853528,693154.0,2016-10-04T13:23:05Z,39852123,0,"<p>If I'm not mistaken, you don't want to get rid of the <code>(utimestamp)</code> part, only the <code>YEAR</code>, <code>MONTH</code>, etc. Or maybe I got it wrong but this solution is trivial to adapt in that case: just adapt the <code>rep</code> dict to cover your needs.</p>

<p>In any case, I would use regular expressions for that. This should take care of what you want (I think) in a single pass and in a (fairly) simple way.</p>

<pre class=""lang-python prettyprint-override""><code>import re

rep = {
    'YEAR': 'y',
    'MONTH': 'm',
    'WEEK': 'w',
    'DAY': 'd',
}

query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value from table
where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00'
group by YEAR(utimestamp), MONTH(utimestamp), id """"""

rep = dict((re.escape(k), v) for k, v in rep.iteritems())
pattern = re.compile(""|"".join(rep.keys()))
replaced = pattern.sub(lambda m: rep[re.escape(m.group(0))], query)

print(""Processed query: {}\n"".format(replaced))
</code></pre>

<p>That's just the basic example. Here's a more complete one with comments explaining what the code does, including a test at the end for all the possible patterns you mentioned:</p>

<pre class=""lang-python prettyprint-override""><code>import re

# Several possible patterns like you mentioned.
# Only used for testing further down.
patterns = [
    'YEAR(utimestamp), MONTH(utimestamp), DAY(utimestamp)',
    'YEAR(utimestamp), MONTH(utimestamp), WEEK(utimestamp)',
    'YEAR(utimestamp), MONTH(utimestamp)',
    'YEAR(utimestamp)'
]

# These are the several patterns to be matched and their replacements.
# The keys are the patterns to match and the values are what you want
# to replace them with.
rep = {
    'YEAR': 'y',
    'MONTH': 'm',
    'WEEK': 'w',
    'DAY': 'd',
}

# The query string template, where we'll replace {} with each of the patterns.
query = """""" SELECT DATE(utimestamp) as utimestamp, sum(value) as value from table
where utimestamp BETWEEN '2000-06-28 00:00:00' AND '2000-07-05 00:00:00'
group by {}, id """"""

# A dictionary with escaped patterns (the keys) suitable for use in regex.
rep = dict((re.escape(k), v) for k, v in rep.iteritems())

# We join each possible pattern (the keys in the rep dict) with | so that the
# regex engine considers them all when matching, i.e., ""hey, regex engine,
# please match YEAR or MONTH or WEEK or DAY"". This builds the matching patter
# we'll use and we also pre-compile the regex to make it faster.
pattern = re.compile(""|"".join(rep.keys()))

# This is the trick part: we're using pattern.sub() to replace our pattern from
# above with what we want (the values in the rep dict). We're telling the regex
# engine to call a function for each occurrence of the pattern in order to get
# the value we're replacing it with. In our case, we want to get the value from
# the rep dict, using the key which is the found match. m is the match object,
# m.group(0) is the first match, re.escape() escapes the value and we finally
# use this as the key to fetch the value from the rep dict.
q = query.format(patterns[0])
print(""Query: {}\n"".format(q))
replaced = pattern.sub(lambda m: rep[re.escape(m.group(0))], q)
print(""Processed query: {}\n"".format(replaced))

# Now to test it with the examples you gave let's iterate over the patterns
# dict, form a new query string using each of them and run the regex against
# each one.
print(""###########################"")
print(""Test each pattern:\n"")
print(""---------------------------"")
for p in patterns:
    q = query.format(p)
    print(""Pattern: {}"".format(p))
    print(""Original query: {}\n"".format(q))

    replaced = pattern.sub(lambda m: rep[re.escape(m.group(0))], q)
    print(""Processed query: {}\n"".format(replaced))
    print(""---------------------------\n"")
</code></pre>

<p>You can read more about how <a href=""https://docs.python.org/3/library/re.html#re.sub"" rel=""nofollow""><code>re.sub()</code></a> works.</p>
"
39853569,2063361.0,2016-10-04T13:24:45Z,39853321,1,"<p>Simpler way to achieve this without taking care of <code>datetime</code> formatting identifiers will be the usage of <a href=""http://dateutil.readthedocs.io/en/latest/parser.html"" rel=""nofollow""><code>dateutil.parser()</code></a>. For example:</p>

<pre><code>&gt;&gt;&gt; import dateutil.parser 
&gt;&gt;&gt; date_string = 'Tue Oct 04 2016 12:13:00 GMT+0200 (CEST)'
&gt;&gt;&gt; dateutil.parser.parse(date_string)
datetime.datetime(2016, 10, 4, 12, 13, tzinfo=tzoffset(u'CEST', -7200))
</code></pre>
"
39853886,103206.0,2016-10-04T13:40:28Z,39843488,1,"<p>It seems to me that this would be fundamentally impossible. Let's say you write a function that returns this RBI (""really big int""). If the computer is capable of storing it, then someone else could write a function that returns the same value. Is your RBI greater than itself?</p>

<p>Perhaps you can achieve the desired result with something like @wim's answer: Create an object that overrides the comparison operators to make ""&lt;"" always return false and "">"" always return true. (I haven't written a lot of Python. In most object-oriented languages, this would only work if the comparison puts your value first, IF RBI>x. If someone writes the comparison the other way, IF x>RBI, it will fail because the compiler doesn't know how to compare integers to a user-defined class.)</p>
"
39853938,6753042.0,2016-10-04T13:43:17Z,39658717,1,"<p>Here's an alternative, possibly simpler solution:</p>

<pre><code>%matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt

m = 100
n = 100
matrix = np.random.normal(0,1,m*n).reshape(m,n)

fig = plt.figure()
ax = fig.add_subplot(111)
plt.ion()

fig.show()
fig.canvas.draw()

for i in range(0,100):
    ax.clear()
    ax.plot(matrix[i,:])
    fig.canvas.draw()
</code></pre>
"
39854068,6863004.0,2016-10-04T13:48:46Z,39848164,1,"<p>There is one pitfall regarding python descriptors.</p>

<p>Using your code, you will reference the same value, stored in StringVector.x.prop and StringVector.y.prop respectively:</p>

<pre><code>v1 = StringVector(1, 2)
print('current StringVector ""x"": ', StringVector.__dict__['x'].prop)
v2 = StringVector(3, 4)
print('current StringVector ""x"": ', StringVector.__dict__['x'].prop)

print(v1.x)
print(v2.x)
</code></pre>

<p>will have the following output:</p>

<pre><code>Write
Write
current StringVector ""x"":  1
Write
Write
current StringVector ""x"":  3
Read
3.0
Read
3.0
</code></pre>

<p>I suppose this is not what you want=). To store unique value per object inside object, make the following changes:</p>

<pre><code>class MyNewStringMemory(object):
    def __init__(self, convert, name):
        self.convert = convert
        self.name = '_' + name

    def __get__(self, obj, objtype):
        print('Read')
        return self.convert(getattr(obj, self.name))

    def __set__(self, obj, val):
        print('Write')
        setattr(obj, self.name, str(val))

    def __delete__(self, obj):
        print('Delete')


class StringVector(Vector):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    x = MyNewStringMemory(float, 'x')
    y = MyNewStringMemory(float, 'y')


v1 = StringVector(1, 2)
v2 = StringVector(3, 4)

print(v1.x, type(v1.x))
print(v1._x, type(v1._x))
print(v2.x, type(v2.x))
print(v2._x, type(v2._x))
</code></pre>

<p>Output:</p>

<pre><code>Write
Write
Write
Write
Read
Read
1.0 &lt;class 'float'&gt;
1 &lt;class 'str'&gt;
Read
Read
3.0 &lt;class 'float'&gt;
3 &lt;class 'str'&gt;
</code></pre>

<p>Also, you definitely could save data inside centralized store, using descriptor's <code>__set__</code> method.</p>

<p>Refer to this document: <a href=""https://docs.python.org/3/howto/descriptor.html"" rel=""nofollow"">https://docs.python.org/3/howto/descriptor.html</a></p>
"
39854093,2466407.0,2016-10-04T13:50:00Z,39849497,1,"<p>Try to get the Page Source after detecting the required ID/CSS_SELECTOR/CLASS or LINK.</p>

<p>You can always use explicit wait of Selenium WebDriver.</p>

<pre><code>from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
driver = webdriver.Remote('http://0.0.0.0:xxxx/wd/hub', desired_capabilities)
driver.get(url)
f = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID,idName) 
# here 10 is time for which script will try to find given id
# provide the id name
dom = BeautifulSoup(driver.page_source, parser)

f = dom.find('iframe', id='dsq-app1')
driver.switch_to_frame('dsq-app1')
s = driver.page_source
f.replace_with(BeautifulSoup(s, 'html.parser'))

with open('out.html', 'w') as fe:
    fe.write(dom.encode('utf-8'))
</code></pre>

<p>Correct me if this not work</p>
"
39854349,4068678.0,2016-10-04T14:01:39Z,39849497,1,"<p>To add to Or Duan's answer I provide what I ended up doing. The problem of finding whether a page or parts of a page have loaded completely is an intricate one. I tried to use implicit and explicit waits but again I ended up receiving half-loaded frames. My workaround is to check the <code>readyState</code> of the original document and the readyState of iframes.</p>

<p>Here is a sample function</p>

<pre><code>def _check_if_load_complete(driver, timeout=10):
    elapsed_time = 1
    while True:
        if (driver.execute_script('return document.readyState') == 'complete' or
                elapsed_time == timeout):
            break
        else:
            sleep(0.0001)
        elapsed_time += 1
</code></pre>

<p>then I used that function right after I changed the focus of the driver to the iframe</p>

<pre><code>driver.switch_to_frame('dsq-app1')
_check_if_load_complete(driver, timeout=10)
</code></pre>
"
39855576,6779307.0,2016-10-04T14:56:37Z,39855525,6,"<p>Use <code>enumerate</code> instead of <code>index</code> in case of duplicate values</p>

<pre><code>[s for i, s in enumerate(subj) if 'author' not in department[i]]
</code></pre>
"
39855581,6786975.0,2016-10-04T14:56:45Z,39855410,0,"<p>How it works :</p>

<pre><code>a, a = 2, 1
--&gt; a does not exist, create variable a and set value to 2
--&gt; a already exists, value of a changed to 1
</code></pre>

<p>When you have different variables, it works exactly the same way :</p>

<pre><code>a, b, a = 1, 2, 3
--&gt; a does not exist, create variable a and set value to 1
--&gt; b does not exist, create variable b and set value to 2
--&gt; a already exists, value of a changed to 3
</code></pre>
"
39855605,567292.0,2016-10-04T14:57:45Z,39843488,23,"<p>Konsta Vesterinen's <a href=""https://github.com/kvesteri/infinity""><code>infinity.Infinity</code></a> would work (<a href=""https://pypi.python.org/pypi/infinity/"">pypi</a>), except that it doesn't inherit from <code>int</code>, but you can subclass it:</p>

<pre><code>from infinity import Infinity
class IntInfinity(Infinity, int):
    pass
assert isinstance(IntInfinity(), int)
assert IntInfinity() &gt; 1e100
</code></pre>

<p>Another package that implements ""infinity"" values is <a href=""https://pypi.python.org/pypi/Extremes"">Extremes</a>, which was salvaged from the rejected <a href=""https://www.python.org/dev/peps/pep-0326/"">PEP 326</a>; again, you'd need to subclass from <code>extremes.Max</code> and <code>int</code>.</p>
"
39855609,1431750.0,2016-10-04T14:57:52Z,39855525,0,"<p>If <code>department</code> and <code>subj</code> are definitely in the same order ie. corresponding elements of each match up, then use <a href=""https://docs.python.org/2/library/functions.html#zip"" rel=""nofollow""><code>zip</code></a> to iterate over both lists simultaneously:</p>

<pre><code>[(d, s) for d, s in zip(department, subject) if 'author' not in d]
</code></pre>

<p>(using <code>d</code> for department and <code>s</code> for subject).</p>

<p>And this way you don't need to reference each element by index. Just standard iteration.</p>

<p>Edit: If you want keep lists separate then you can do the first step for <code>dept</code> as you already have and then modify the second loop for subject like so, still filtering against 'author' while looping over both:</p>

<pre><code>[s for d, s in zip(department, subject) if 'author' not in d]
</code></pre>

<p>(so the value of <code>d</code> is ignored in the 2nd loop)</p>

<p>Example of output:</p>

<pre><code>&gt;&gt;&gt; department = ['something', 'else', 'author', 'okay']
&gt;&gt;&gt; subject = ['some_subj', 'else_subj', 'author_subj', 'okay_subj']
&gt;&gt;&gt; [(d, s) for d, s in zip(department, subject) if 'author' not in d]
[('something', 'some_subj'), ('else', 'else_subj'), ('okay', 'okay_subj')]
&gt;&gt;&gt;
&gt;&gt;&gt; # and if you MUST do them seprately:
... [s for s in department if 'author' not in s]  # that's your `deepfilt`
['something', 'else', 'okay']
&gt;&gt;&gt; [s for d, s in zip(department, subject) if 'author' not in d]  # this is for `subfilt`
['some_subj', 'else_subj', 'okay_subj']
</code></pre>
"
39855637,2979576.0,2016-10-04T14:58:56Z,39855410,7,"<p>Yes, it is part of the python language reference that tuple assignment must take place left to right. </p>

<p><a href=""https://docs.python.org/2.3/ref/assignment.html"">https://docs.python.org/2.3/ref/assignment.html</a></p>

<blockquote>
  <p>An assignment statement evaluates the expression list (remember that
  this can be a single expression or a comma-separated list, the latter
  yielding a tuple) and assigns the single resulting object to each of
  the target lists, from left to right.</p>
</blockquote>

<p>So all Python implementations should follow this rule (as confirmed by the experiments in the other answer). </p>

<p>Personally, I would still be hesitant to use this as it seems unclear to a future reader of the code. </p>
"
39855733,6607524.0,2016-10-04T15:03:22Z,39855697,-1,"<p>When you've already coded the __len__ method, u dont need to call it by <code>carpet.__len__()</code>. Instead of use <code>len(carpet)</code>.</p>
"
39855755,565635.0,2016-10-04T15:04:22Z,39855732,3,"<p>Use <code>global</code> to modify a variable outside of the function:</p>

<pre><code>def UpdateText():
    global currentMovie
    currentMovie = random.randint(0, 100)
    print(currentMovie)
</code></pre>

<p>However, don't use <code>global</code>. It's generally a <a href=""https://en.wikipedia.org/wiki/Code_smell"" rel=""nofollow"">code smell</a>.</p>
"
39855760,4038957.0,2016-10-04T15:04:37Z,39855697,0,"<pre><code>for d in range(0, len(carpet) - 1, +1):
nr_total = sum(carpet[d].packages[i].nr_buys for i, _ in enumerate(carpet[d].packages))
</code></pre>

<p>You want to get the <code>indexes</code> of a <code>dict</code>, so use <code>enumerate</code>, which returns a <code>tuple</code>: <code>(index, item)</code>.
Also, why are you using <code>dict.__len__</code>?</p>
"
39855917,1780611.0,2016-10-04T15:11:46Z,39855697,0,"<p>Another method of doing this, which I feel is more Pythonic, is:</p>

<pre><code>nr_total = sum(package.nr_buys for package in carpet[d].packages)
</code></pre>

<p>The difference, as stated above, is that calling <code>for x in list</code> in python gives you the items of the list, and not their indices. That means that the <code>i</code> in your answer is not the integer index in <code>packages</code> of your item, but your item from <code>packages</code> itself, which you can use directly.</p>

<p>Now this can be done from the start:</p>

<pre><code>nr_total = sum(package.nr_buys for mini_carpet in carpet for package in mini_carpet)
</code></pre>

<p>(don't ask me why Python uses that order, it confuses me as well, but that's how list comprehensions work.)</p>
"
39855991,3125566.0,2016-10-04T15:15:15Z,39855697,0,"<blockquote>
  <p>sum the <em>various</em> <code>nr_buys</code> that are within a mini_carpet, within a carpet</p>
</blockquote>

<p>Asides what others have said about trying to index your list with a non integer object, you're also throwing away the result of previous iterations. </p>

<p>You can use a mapping/dictionary to keep these results using the carpet index as the key, which can later be used to access the result of the sums of <code>nr_buys</code> values of the packages in that carpet:</p>

<pre><code>nr_total = {}

for i, c in enumerate(carpet):
    nr_total[i] = sum(package.nr_buys for package in c.packages)
</code></pre>

<p>To get the total <code>nr_buys</code> for all the carpets, you'll simply do:</p>

<pre><code>total = sum(nr_total.values())
</code></pre>
"
39856029,4014959.0,2016-10-04T15:17:20Z,39855697,2,"<p>In Python, it's generally better to loop directly over the items in a list, rather than looping indirectly using indices. It's easier to read, and more efficient not to muck around with indices that you don't really need.</p>

<p>To get a total for each <code>mini_carpet</code> you can do this:</p>

<pre><code>for mini in carpet:
    nr_total = sum(package.nr_buys for package in mini)
    # Do something with nr_total
</code></pre>

<p>To get a single grand total, do a double <code>for</code> loop in the generator expression:</p>

<pre><code>nr_total = sum(package.nr_buys for mini in carpet for package in mini)
</code></pre>
"
39856310,3019689.0,2016-10-04T15:30:13Z,39611045,6,"<p>The first thing to say is: if it's about multiple cores on the same processor, <code>numpy</code> is already capable of parallelizing the operation better than we could ever do by hand (see the discussion at <a href=""https://stackoverflow.com/questions/38000663/multiplication-of-large-arrays-in-python"">multiplication of large arrays in python</a> )</p>

<p>In this case the key would be simply to ensure that the multiplication is all done in a wholesale array operation rather than a Python <code>for</code>-loop:</p>

<pre><code>test2 = x[n.newaxis, :] * y[:, n.newaxis]

n.abs( test - test2 ).max()  # verify equivalence to mult(): output should be 0.0, or very small reflecting floating-point precision limitations
</code></pre>

<p>[If you actually wanted to spread this across multiple separate CPUs, that's a different matter, but the question seems to suggest a single (multi-core) CPU.]</p>

<hr>

<p>OK, bearing the above in mind: let's suppose you want to parallelize an operation more complicated than just <code>mult()</code>.  Let's assume you've tried hard to optimize your operation into wholesale array operations that <code>numpy</code> can parallelize itself, but your operation just isn't susceptible to this.  In that case, you can use a shared-memory <code>multiprocessing.Array</code> created with <code>lock=False</code>, and <code>multiprocessing.Pool</code> to assign processes to address non-overlapping chunks of it, divided up over the <code>y</code> dimension (and also simultaneously over <code>x</code> if you want). An example listing is provided below. Note that this approach does not explicitly do exactly what you specify (club the results together and append them into a single array). Rather, it does something more efficient: multiple processes simultaneously assemble their portions of the answer in non-overlapping portions of shared memory. Once done, no collation/appending is necessary: we just read out the result.</p>

<pre><code>import os, numpy, multiprocessing, itertools

SHARED_VARS = {} # the best way to get multiprocessing.Pool to send shared multiprocessing.Array objects between processes is to attach them to something global - see http://stackoverflow.com/questions/1675766/

def operate( slices ):
    # grok the inputs
    yslice, xslice = slices
    y, x, r = get_shared_arrays('y', 'x', 'r')
    # create views of the appropriate chunks/slices of the arrays:
    y = y[yslice]
    x = x[xslice]
    r = r[yslice, xslice]
    # do the actual business
    for i in range(len(r)):
        r[i] = y[i] * x  # If this is truly all operate() does, it can be parallelized far more efficiently by numpy itself.
                         # But let's assume this is a placeholder for something more complicated.

    return 'Process %d operated on y[%s] and x[%s] (%d x %d chunk)' % (os.getpid(), slicestr(yslice), slicestr(xslice), y.size, x.size)

def check(y, x, r):
    r2 = x[numpy.newaxis, :] * y[:, numpy.newaxis]  # obviously this check will only be valid if operate() literally does only multiplication (in which case this whole business is unncessary)
    print( 'max. abs. diff. = %g' % numpy.abs(r - r2).max() )
    return y, x, r

def slicestr(s):
    return ':'.join( '' if x is None else str(x) for x in [s.start, s.stop, s.step] )

def m2n(buf, shape, typecode, ismatrix=False):
    """"""
    Return a numpy.array VIEW of a multiprocessing.Array given a
    handle to the array, the shape, the data typecode, and a boolean
    flag indicating whether the result should be cast as a matrix.
    """"""
    a = numpy.frombuffer(buf, dtype=typecode).reshape(shape)
    if ismatrix: a = numpy.asmatrix(a)
    return a

def n2m(a):
    """"""
    Return a multiprocessing.Array COPY of a numpy.array, together
    with shape, typecode and matrix flag.
    """"""
    if not isinstance(a, numpy.ndarray): a = numpy.array(a)
    return multiprocessing.Array(a.dtype.char, a.flat, lock=False), tuple(a.shape), a.dtype.char, isinstance(a, numpy.matrix)

def new_shared_array(shape, typecode='d', ismatrix=False):
    """"""
    Allocate a new shared array and return all the details required
    to reinterpret it as a numpy array or matrix (same order of
    output arguments as n2m)
    """"""
    typecode = numpy.dtype(typecode).char
    return multiprocessing.Array(typecode, int(numpy.prod(shape)), lock=False), tuple(shape), typecode, ismatrix

def get_shared_arrays(*names):
    return [m2n(*SHARED_VARS[name]) for name in names]

def init(*pargs, **kwargs):
    SHARED_VARS.update(pargs, **kwargs)

if __name__ == '__main__':

    ylen = 1000
    xlen = 2000

    init( y=n2m(range(ylen)) )
    init( x=n2m(numpy.random.rand(xlen)) )
    init( r=new_shared_array([ylen, xlen], float) )

    print('Master process ID is %s' % os.getpid())

    #print( operate([slice(None), slice(None)]) ); check(*get_shared_arrays('y', 'x', 'r'))  # local test

    pool = multiprocessing.Pool(initializer=init, initargs=SHARED_VARS.items())
    yslices = [slice(0,333), slice(333,666), slice(666,None)]
    xslices = [slice(0,1000), slice(1000,None)]
    #xslices = [slice(None)]  # uncomment this if you only want to divide things up in the y dimension
    reports = pool.map(operate, itertools.product(yslices, xslices))
    print('\n'.join(reports))
    y, x, r = check(*get_shared_arrays('y', 'x', 'r'))
</code></pre>
"
39856605,94977.0,2016-10-04T15:45:43Z,39843488,15,"<blockquote>
  <p>Use case: library function expects an integer, and the only easy way to force a certain behavior is to pass a very large integer.</p>
</blockquote>

<p>This sounds like a flaw in the library that should be fixed in its interface. Then all its users would benefit. What library is it?</p>

<p>Creating a magical int subclass with overridden comparison operators might work for you. It's brittle, though; you never know what the library is going to do with that object. Suppose it converts it to a string. What should happen? And data is naturally used in different ways as a library evolves; you may update the library one day to find that your trick doesn't work anymore.</p>
"
39857366,6517848.0,2016-10-04T16:24:21Z,39658717,0,"<p>Here is a library that deals with real-time plotting/logging data (<a href=""https://pypi.python.org/pypi/joystick/"" rel=""nofollow"">joystick</a>), although I am not sure it is working with jupyter. You can install it using the usual <code>pip install joystick</code>.</p>

<p>Hard to make a working solution without more details on your data. Here is an option:</p>

<pre><code>import joystick as jk
import numpy as np

class test(jk.Joystick):
   # initialize the infinite loop decorator
    _infinite_loop = jk.deco_infinite_loop()

    def _init(self, *args, **kwargs):
        """"""
        Function called at initialization, see the docs
        """"""
        # INIT DATA HERE
        self.shape = (10, 4) # M, N
        self.data = np.random.random(self.shape)
        self.xaxis = range(self.shape[1])
        ############
        # create a graph frame
        self.mygraph = self.add_frame(
                   jk.Graph(name=""TheName"", size=(500, 500), pos=(50, 50),
                            fmt=""go-"", xnpts=self.shape[1], freq_up=5, bgcol=""w"",
                            xylim=(0, self.shape[1]-1, None, None)))

    @_infinite_loop(wait_time=0.5)
    def _generate_fake_data(self):  # function looped every 0.5 second
        """"""
        Loop starting with the simulation start, getting data and
        pushing it to the graph every 0.5 seconds
        """"""
        # NEW (RANDOM) DATA
        new_data = np.random.random(self.shape[1])
        # concatenate data
        self.data = np.vstack((self.data, new_data))
        # push new data to the graph
        self.mygraph.set_xydata(self.xaxis, self.data[-1])

t = test()
t.start()

t.stop()
t.exit()
</code></pre>

<p>This code will create a graph that is auto-updating 5 times a second (freq_up=5), while new data is (randomly) generated every 0.5 seconds (wait_time=0.5) and pushed to the graph for display.</p>

<p>If you don't want the Y-axis to wiggle around, type <code>t.mygraph.xylim = (0, t.shape[1]-1, 0, 1)</code>.</p>
"
39857729,4211135.0,2016-10-04T16:47:07Z,39857691,1,"<p>You can use a list comprehension along with the <code>dict</code> constructor, as follows:</p>

<pre><code>&gt;&gt;&gt; obj = [ 'a', 'c', 'b' ]
&gt;&gt;&gt; dict((x, i + 1) for i, x in enumerate(obj))
{'a': 1, 'c': 2, 'b': 3}
</code></pre>

<p>As noted in the comments, you can also use a simple dict comprehension:</p>

<pre><code>&gt;&gt;&gt; { x: i + 1 for i, x in enumerate(obj) }
{'a': 1, 'c': 2, 'b': 3}
</code></pre>
"
39857811,2437514.0,2016-10-04T16:52:24Z,39843488,-4,"<p>Another way to do this (very much inspired by wim's answer) might be an object that isn't infinite, but increases on the fly as needed. </p>

<p>Here's what I have in mind: </p>

<pre><code>from functools import wraps

class AlwaysBiggerDesc():
    '''A data descriptor that always returns a value bigger than instance._compare'''
    def __get__(self, instance, owner):
        try:
            return instance._compare + 1
        except AttributeError:
            return instance._val
    def __set__(self, instance, value):
        try:
            del instance._compare
        except AttributeError:
            pass
        instance._val = value

class BiggerThanYou(int):
    '''A class that behaves like an integer but that increases as needed so as to be 
    bigger than ""other"" values. Defaults to 1 so that instances are considered
    to be ""truthy"" for boolean comparisons.'''
    val = AlwaysBiggerDesc()
    def __getattribute__(self, name):
        f = super().__getattribute__(name)
        try:
            intf = getattr(int,name)
        except AttributeError:
            intf = None
        if f is intf:
            @wraps(f)
            def wrapper(*args):
                try:
                    self._compare = args[1]
                except IndexError:
                    self._compare = 0 # Note: 1 will be returned by val descriptor
                new_bigger = BiggerThanYou()
                try:
                    new_bigger.val = f(self.val, *args[1:])
                except IndexError:
                    new_bigger.val =  f(self.val)
                return new_bigger
            return wrapper
        else:
            return f            
    def __repr__(self):
        return 'BiggerThanYou()'
    def __str__(self):
        return '1000...'
</code></pre>

<p>Something like this might avoid a lot of weird behavior that one might not expect. Note that with this kind of approach, if two <code>BiggerThanYou</code> instances are involved in an operation, the LHS would be considered bigger than the RHS. </p>

<p>EDIT: currently this is not working- I'll fix it later. it seems I am being bitten by the <a href=""http://stackoverflow.com/a/13063764/2437514"">special method lookup functionality</a>. </p>
"
39858556,4669431.0,2016-10-04T17:39:51Z,39804034,0,"<p>tested with PyQt4 and normally opened pages with HTTPS</p>

<pre><code>import sys
from PyQt4.QtGui import QApplication
from PyQt4.QtCore import QUrl
from PyQt4.QtWebKit import QWebView

class Browser(QWebView):
    def __init__(self):
        QWebView.__init__(self)
        self.loadFinished.connect(self._result_available)

    def _result_available(self, ok):
        frame = self.page().mainFrame()
        print(frame.toHtml())

if __name__ == '__main__':
    app = QApplication(sys.argv)
    view = Browser()
    view.load(QUrl('https://www.google.com'))
    app.exec_()
</code></pre>
"
39859024,4211135.0,2016-10-04T18:08:30Z,39858364,1,"<p>Using a simple string equality check of two hashed and salted tokens will not work. The <a href=""https://docs.djangoproject.com/en/1.10/topics/auth/passwords/#django.contrib.auth.hashers.check_password"" rel=""nofollow"">Django docs for password management</a> offer a very simple method in the <code>django.contrib.auth.hashers</code> namespace that handles all of this for you:</p>

<pre><code>&gt;&gt;&gt; token = 'test'
&gt;&gt;&gt; enc_token1 = make_password(token)
&gt;&gt;&gt; check_password('test', enc_token1)
True
</code></pre>

<p>The <code>check_password</code> method does a few things behind the hood, like check if the hashing algorithm has changed. It returns the result of the <code>verify</code> method of an algorithm that implements the <code>BasePasswordHasher</code> baseclass. Here's an example of an implementation from the <a href=""https://docs.djangoproject.com/en/1.10/_modules/django/contrib/auth/hashers/#check_password"" rel=""nofollow"">source of the <code>PBKDF2PasswordHasher</code></a>:</p>

<pre><code>def verify(self, password, encoded):
    algorithm, iterations, salt, hash = encoded.split('$', 3)
    assert algorithm == self.algorithm
    encoded_2 = self.encode(password, salt, int(iterations))
    return constant_time_compare(encoded, encoded_2)
</code></pre>

<p>Note how the salt is found by splitting the encoded_string on '$', since the <a href=""https://docs.djangoproject.com/en/1.10/topics/auth/passwords/#how-django-stores-passwords"" rel=""nofollow"">Django docs note</a> that</p>

<blockquote>
  <p>The password attribute of a User object is a string in this format:</p>

<pre><code>&lt;algorithm&gt;$&lt;iterations&gt;$&lt;salt&gt;$&lt;hash&gt;
</code></pre>
</blockquote>
"
39859358,6207849.0,2016-10-04T18:30:38Z,39857148,1,"<p>Create a <code>DF</code> mapping the column names to it's levels of the multi-index <code>DF</code>:</p>

<pre><code>level_df = pd.DataFrame(df.columns.values.tolist(), columns=np.array(df.columns.names))
level_df
</code></pre>

<p><a href=""http://i.stack.imgur.com/55E5e.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/55E5e.png"" alt=""Image""></a></p>

<p>Create another <code>DF</code> which keeps our required mapping containing list of dicts:</p>

<pre><code>keep = pd.DataFrame(to_keep)
keep
</code></pre>

<p><a href=""http://i.stack.imgur.com/rUQrD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rUQrD.png"" alt=""Image""></a></p>

<p>Concatenate both <code>DFs</code> row-wise:</p>

<pre><code>df_concat = pd.concat([level_df, keep], ignore_index=True)
df_concat
</code></pre>

<p><a href=""http://i.stack.imgur.com/0gFSj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0gFSj.png"" alt=""Image""></a></p>

<p>Remove all duplicates from concatenated <code>DF</code>. Drop off the level after checking if the remaining values in the concatenated <code>DF</code> is present in <code>level_df</code>:</p>

<pre><code>cond = df_concat[~df_concat.duplicated(keep=False)]
df.drop([tuple(x) for x in cond[cond.isin(level_df).all(axis=1)].values], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/i3pfE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i3pfE.png"" alt=""Image""></a></p>

<p>Note: Here it is assumed that the keys of the dictionaries match the multi-index column names.</p>
"
39859509,3470611.0,2016-10-04T18:40:42Z,39858238,0,"<p>There are only 2 ways to do this: </p>

<p>One is use <code>""coalesce(1)""</code>
This will make sure that all the data is saved into 1 file rather than multiple files (200 is the spark default no of partitions) use <code>dataframe.write.save(""/this/is/path"")</code>. </p>

<p>The other option is write the output to a hive table and then use <code>hive -e ""select * from table"" &gt; data.tsv</code> which will be tab separated.</p>
"
39859949,5050917.0,2016-10-04T19:08:17Z,39857796,1,"<p>The main problem is that you forget the statement <code>global userfeed</code> in both <code>data_updater</code> and <code>web_handle</code> functions. So, according to <a href=""http://stackoverflow.com/a/292502/5050917"">how python resolves scopes</a>, in <code>web_handle</code> it was referring to the global variable you defined and in <code>data_updater</code> to a local one, created by the statement <code>userfeed = [x for x ...</code>. Using global variables in this context is explicitly <a href=""http://aiohttp.readthedocs.io/en/stable/web.html#data-sharing-aka-no-singletons-please"" rel=""nofollow"">discouraged</a> so there is an example using the <code>dict</code> interface of the <code>aiohttp.web.Application</code> object to safely refer to your variable between the functions.</p>

<pre><code>import asyncio
import random
from aiohttp import web


async def data_updater(app):
    while True:
        await asyncio.sleep(3)
        app[""userfeed""] = [x for x in range(random.randint(1, 20))]

async def web_handle(request):
    userfeed = request.app[""userfeed""]
    return web.Response(text=str(userfeed))

async def init(loop, port=8000):
    app = web.Application(loop=loop)
    app.router.add_route('GET', '/', web_handle)
    handler = app.make_handler()
    srv = await loop.create_server(
        handler, '127.0.0.1', port=port)
    return srv, app, handler

if __name__ == ""__main__"":
    loop = asyncio.get_event_loop()
    srv, app, handler = loop.run_until_complete(init(loop, 8000))
    app['userfeed'] = []
    asyncio.ensure_future(data_updater(app))
    try:
        loop.run_forever()
    except KeyboardInterrupt:
        pass
    finally:
        srv.close()
        loop.run_until_complete(srv.wait_closed())
        loop.run_until_complete(app.shutdown())
        loop.run_until_complete(handler.finish_connections(60.0))
        loop.run_until_complete(app.cleanup())
    loop.close()
</code></pre>

<p>When you refresh the page on <code>127.0.0.1:8000</code> you should have some new random numbers as they are updated every 3 seconds server-side (you can put back the <code>print</code> statement in <code>data_updater</code> to verify it).</p>
"
39860234,1735406.0,2016-10-04T19:25:32Z,39860158,2,"<p>You can use <code>replaceAll</code>, and use <code>$n</code>, where ""n"" is the group you want to match. For example:</p>

<pre><code>yourString.replaceAll(yourRegex, ""$1"")
</code></pre>

<p>Replaces the matched parts with the first group.</p>
"
39860394,3524613.0,2016-10-04T19:35:36Z,39857515,0,"<p>Try adding the <code>deadline</code> parameter:</p>

<p><code>my_result = urlfetch.fetch(my_url, deadline=15)</code></p>
"
39860720,1949548.0,2016-10-04T19:55:36Z,39735676,0,"<p>Found an alternative way using Pandas Indexing.</p>

<p>This can be simply done by </p>

<pre><code>df[df&gt;0] = 1
</code></pre>

<p>simple as that!</p>
"
39861016,6779307.0,2016-10-04T20:16:22Z,39860985,3,"<pre><code>'{:.5}'.format(zip_)
</code></pre>

<p>where <code>zip_</code> is the string containing the zip code.  More on <code>format</code> here: <a href=""https://docs.python.org/2/library/string.html#format-string-syntax"" rel=""nofollow"">https://docs.python.org/2/library/string.html#format-string-syntax</a></p>
"
39861056,1612432.0,2016-10-04T20:18:52Z,39860158,1,"<p>Maybe other way to do this is having a <code>regex</code>, for example:</p>

<pre><code>val regExtractor = """"""a(b+)(c+)(d*)"""""".r
</code></pre>

<p>And then match the <code>String:</code></p>

<pre><code>val string = ""abbbbbbbbbccdd""

val newString = string match {
  case regExtractor(g1, g2, g3) =&gt;
    s""""""String Replaced: ${g1.replace(g1, ""XXXXX"")},
        | ${g2.replace(g2, ""YYYYY"")}"""""".stripMargin
}
</code></pre>

<p><code>newString</code> will be:</p>

<pre><code>""String Replaced: XXXXX, YYYYY""
</code></pre>
"
39861071,6451573.0,2016-10-04T20:19:49Z,39860985,2,"<p>Process title line separately, then read row by row like you do, just modify second <code>line</code> column by truncating to 5 characters.</p>

<pre><code>import csv

my_file_name = ""NVG.txt""
cleaned_file = ""cleanNVG.csv""
remove_words = ['INAC-EIM','-INAC','TO-INAC','TO_INAC','SHIP_TO-inac','SHIP_TOINAC']


with open(my_file_name, 'r', newline='') as infile, open(cleaned_file, 'w',newline='') as outfile:
    writer = csv.writer(outfile)
    cr = csv.reader(infile, delimiter='|')
    # iterate over title line and write it as-is
    writer.writerow(next(cr))
    for line in cr:
        if not any(remove_word in element for element in line for remove_word in remove_words):
            line[1] = line[1][:5]   # truncate
            writer.writerow(line)
</code></pre>

<p>alternately, you could use <code>line[1] = line[1].split(""-"")[0]</code> which would keep everything on the left of the dash character.</p>

<p>Note the special processing for the title line: <code>cr</code> is an iterator. I just consume it manually before the <code>for</code> loop to perform a pass-through processing.</p>
"
39861270,214686.0,2016-10-04T20:33:54Z,39852896,4,"<p>This type of dynamic programming is available in scikit-image as <code>route_through_array</code> and <code>shortest_path</code>: <a href=""http://scikit-image.org/docs/dev/api/skimage.graph.html"" rel=""nofollow"">http://scikit-image.org/docs/dev/api/skimage.graph.html</a></p>
"
39861278,6909442.0,2016-10-04T20:34:17Z,39860985,1,"<p>to get first 5 <em>characters</em> in a string use <code>str[:6]</code></p>

<p>in your case:</p>

<pre><code>with open(my_file_name, 'r', newline='') as infile, open(cleaned_file, 'w',newline='') as outfile:
    writer = csv.writer(outfile)
    for line in csv.reader(infile, delimiter='|'):
        if not any(remove_word in element for element in line for remove_word in remove_words):
            line[1] = line[1][:6]
            writer.writerow(line)
</code></pre>

<p><code>line[1] = line[1][:6]</code> will set 2nd column in your file to first 5 characters in itself.</p>
"
39861748,100297.0,2016-10-04T21:06:29Z,39861662,1,"<p>The GitHub pull request names the source: </p>

<blockquote>
  <p>mvantellingen wants to merge 12 commits into <code>master</code> from <code>multiple-msg-parts</code></p>
</blockquote>

<p><code>multiple-msg-parts</code> is just another branch in the same repository. Just clone that repository and check out <a href=""https://github.com/mvantellingen/python-zeep/tree/multiple-msg-parts"" rel=""nofollow"">that specific branch</a>.</p>

<p>Other pull requests may have been created from a branch in a different repository; the source repository will then have a <code>&lt;username&gt;:&lt;branch&gt;</code> form, at which point you'd clone the project from that specific user to get that branch. For example, <a href=""https://github.com/mvantellingen/python-zeep/pull/185"" rel=""nofollow"">this pull request</a> is sourced from <code>andrewserong:check-node-get-children</code>, so you'd clone <a href=""https://github.com/andrewserong/python-zeep"" rel=""nofollow""><code>andrewserong/python-zeep</code></a> and switch to the <code>check-node-get-children</code> branch instead.</p>

<p>PyCharm lets you <a href=""https://www.jetbrains.com/help/pycharm/2016.2/cloning-a-repository-from-github.html"" rel=""nofollow"">clone directly from GitHub</a>; once cloned use the <em>VCS</em> menu item to switch branches.</p>
"
39861833,674064.0,2016-10-04T21:12:27Z,39861662,1,"<blockquote>
  <p>The only way I see now is that I fork the repository, download and apply the pull request as a patch [...]</p>
</blockquote>

<p>As <a href=""http://stackoverflow.com/questions/39861662/testing-pull-request-on-other-persons-package#comment67011346_39861662"">Martijn Pieters commented</a>: clone the repository and check out the mentioned branch. You can do that without forking it on GitHub and without manually applying the pull request:</p>

<pre><code>git clone git@github.com:mvantellingen/python-zeep.git
git checkout multiple-msg-parts
</code></pre>

<p>or even in a single command:</p>

<pre><code>git clone git@github.com:mvantellingen/python-zeep.git --branch multiple-msg-parts
</code></pre>

<blockquote>
  <p>[...] and then import the function from that project</p>
</blockquote>

<p>I guess you won't get around that part if you want to test the pull request's change.</p>

<p>Though, the author didn't ask you <em>to test</em> the pull request (even though they might have meant to do so); they asked you <em>whether</em> you <em>can</em> test it:</p>

<blockquote>
  <p>I'm working on a fix in #205. It required some refactoring, are you able to give it a try for me?</p>
</blockquote>

<p>To which ""No, because I don't know how."" would have been an acceptable answer. ;-)</p>
"
39861907,100297.0,2016-10-04T21:17:33Z,39861740,5,"<p>A custom class without an <a href=""https://docs.python.org/3/reference/datamodel.html#object.__eq__"" rel=""nofollow""><code>__eq__</code> method</a> defaults to testing for <em>identity</em>. That is to say, two references to an instance of such a class are only equal if the reference they exact same object.</p>

<p>You'll need to define a custom <code>__eq__</code> method that returns <code>True</code> when two instances contain the same time:</p>

<pre><code>def __eq__(self, other):
    if not isinstance(other, Clock):
        return NotImplemented
    return (self.h, self.m, self.adl) == (other.h, other.m, other.adl)
</code></pre>

<p>By returning the <code>NotImplemented</code> singleton for something that is not a <code>Clock</code> instance (or a subclass), you let Python know that the <code>other</code> object could also be asked to test for equality.</p>

<p>However, your code accepts values greater than the normal hour and minute ranges; rather than store hours and minutes, store seconds and normalise that value:</p>

<pre><code>class Clock:
    def __init__(self, h, m):
        # store seconds, but only within the range of a day
        self.seconds = (h * 3600 + m * 60) % 86400
        self.adl = 0

    def make_time(self):
        s = self.esconds
        if self.adl: s += self.adl
        s %= 86400
        if s == 0:
            return '00:00'

        s, h = s % 3600, s // 3600
        m = s // 60
        return '{:02d}:{:02d}'.format(h, m)

    def __eq__(self, other):
        if not isinstance(other, Clock):
            return NotImplemented
        return (self.seconds, self.adl) == (other.seconds, other.adl)
</code></pre>

<p>Now your two clock instances will test equal because internally they store the exact same time in a day. Note that I used the <code>%</code> modulus operator rather than a <code>while</code> loop and subtracting.</p>
"
39861932,616616.0,2016-10-04T21:19:04Z,39861685,1,"<p>You are correct.  </p>

<p>Each <a href=""http://distributed.readthedocs.io/en/latest/worker.html"" rel=""nofollow"">distributed.Worker</a> object contains a <a href=""https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor"" rel=""nofollow"">concurrent.futures.ThreadPoolExecutor</a> with multiple threads.  Tasks are run on this <code>ThreadPoolExecutor</code> for parallel performance.  All communication and coordination tasks are managed by the Tornado IOLoop.</p>

<p>Generally this solution allows computation to happen separately from communication and administration.  This allows parallel computing within a worker and allows workers to respond to server requests even while computing tasks.</p>

<h3>Command line options</h3>

<p>When you make the following call:</p>

<pre><code>dask-worker --nprocs N --nthreads T
</code></pre>

<p>It starts <code>N</code> separate <code>distributed.Worker</code> objects in separate Python processes.  Each of these workers has a ThreadPoolExecutor with <code>T</code> threads.</p>
"
39861935,6758673.0,2016-10-04T21:19:12Z,39860431,1,"<p>You're looking for <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.reduceat.html"" rel=""nofollow""><code>numpy.add.reduceat</code></a>:</p>

<pre><code>N = np.random.poisson(1, SIM_NUM)
losses = np.random.lognormal(0,1, np.sum(N))

x = np.zeros(SIM_NUM)
offsets = np.r_[0, np.cumsum(N[N&gt;0])]
x[N&gt;0] = np.add.reduceat(losses, offsets[:-1])
</code></pre>

<p>The case where <code>n == 0</code> is handled separately, because of how <code>reduceat</code> works. Also, be sure to use <code>numpy.sum</code> on arrays instead of the much slower Python <code>sum</code>.</p>

<p>If this is faster than the other answer depends on the mean of your Poisson distribution.</p>
"
39861970,46914.0,2016-10-04T21:21:42Z,39861911,1,"<p>Rhe em dash in your string is a unicode character, which will be interpreted as multiple characters <a href=""http://www.fileformat.info/info/unicode/char/2014/index.htm"" rel=""nofollow"">(3 in your case)</a>. Your version of python is not unicode-aware so you'll either need to match 3 characters to capture <code>.{3}</code> the dash, match the character exactly in your expression, or use a different version of python.</p>

<p>A few notes regarding your expression; You should always prefix your regular expression strings with <code>r'...'</code> so that your <code>\</code> escapes will be interpreted correctly.</p>

<p>A <code>.</code> in a regular expression has a special meaning, it will match any single character. If you need a period/decimal point, you need to escape the dot <code>\.</code>.</p>

<pre><code>pattern = re.compile(r'[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]{3} .')
</code></pre>
"
39862114,2302572.0,2016-10-04T21:33:07Z,39861960,1,"<p>Make sure you have executable permission for <code>python_script</code>.
You can make <code>python_script</code> executable by </p>

<p><code>chmod +x python_script</code></p>

<p>Also check if you are giving correct path for <code>python_script</code></p>
"
39862127,3293881.0,2016-10-04T21:33:37Z,39860431,3,"<p>We can use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html"" rel=""nofollow""><code>np.bincount</code></a> , which is quite efficient for such interval/ID based summing operations specially when working with <code>1D</code> arrays. The implementation would look something like this -</p>

<pre><code># Generate all poisson distribution values in one go
pv = np.random.poisson(1,SIM_NUM)

# Use poisson values to get count of total for random lognormal needed.
# Generate all those random numbers again in vectorized way 
rand_arr = np.random.lognormal(0, 1, pv.sum())

# Finally create IDs using pv as extents for use with bincount to do
# ID based and thus effectively interval-based summing
out = np.bincount(np.arange(pv.size).repeat(pv),rand_arr,minlength=SIM_NUM)
</code></pre>

<p>Runtime test -</p>

<p>Function definitions :</p>

<pre><code>def original_app1(SIM_NUM):
    X = []
    for _i in range(SIM_NUM):
        nr_claims = np.random.poisson(1)
        temp = []
        for _j in range(nr_claims):
             temp.append(np.random.lognormal(0, 1))
        X.append(sum(temp))
    return X

def original_app2(SIM_NUM):
    N = np.random.poisson(1, SIM_NUM)
    X = []
    for n in N:
        X.append(sum(np.random.lognormal(0, 1, n)))
    return X

def vectorized_app1(SIM_NUM):
    pv = np.random.poisson(1,SIM_NUM)
    r = np.random.lognormal(0, 1,pv.sum())
    return np.bincount(np.arange(pv.size).repeat(pv),r,minlength=SIM_NUM)
</code></pre>

<p>Timings on large datasets :</p>

<pre><code>In [199]: SIM_NUM = 1000

In [200]: %timeit original_app1(SIM_NUM)
100 loops, best of 3: 2.6 ms per loop

In [201]: %timeit original_app2(SIM_NUM)
100 loops, best of 3: 6.65 ms per loop

In [202]: %timeit vectorized_app1(SIM_NUM)
1000 loops, best of 3: 252 Âµs per loop

In [203]: SIM_NUM = 10000

In [204]: %timeit original_app1(SIM_NUM)
10 loops, best of 3: 26.1 ms per loop

In [205]: %timeit original_app2(SIM_NUM)
10 loops, best of 3: 77.5 ms per loop

In [206]: %timeit vectorized_app1(SIM_NUM)
100 loops, best of 3: 2.46 ms per loop
</code></pre>

<p>So, we are looking at some <strong><code>10x+</code></strong> speedup there.</p>
"
39862264,1461210.0,2016-10-04T21:44:27Z,39836953,2,"<p><strong>@francis's</strong> solution can be vectorized using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.accumulate.html"" rel=""nofollow""><code>np.maximum.accumulate</code></a>.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

recall = np.linspace(0.0, 1.0, num=42)
precision = np.random.rand(42)*(1.-recall)

# take a running maximum over the reversed vector of precision values, reverse the
# result to match the order of the recall vector
decreasing_max_precision = np.maximum.accumulate(precision[::-1])[::-1]
</code></pre>

<p>You can also use <a href=""http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.step"" rel=""nofollow""><code>plt.step</code></a> to get rid of the <code>for</code> loop used for plotting:</p>

<pre><code>fig, ax = plt.subplots(1, 1)
ax.hold(True)
ax.plot(recall, precision, '--b')
ax.step(recall, decreasing_max_precision, '-r')
</code></pre>

<p><a href=""http://i.stack.imgur.com/sCnKo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sCnKo.png"" alt=""enter image description here""></a></p>
"
39862274,1172428.0,2016-10-04T21:45:30Z,39861911,3,"<p>The issue is that your â is a unicode character. When in a <code>str</code>, it actually behaves more like several characters:</p>

<pre><code>&gt;&gt;&gt; print len('â')
3
</code></pre>

<p>But, if you use a <code>unicode</code> instead of a <code>str</code>:</p>

<pre><code>&gt;&gt;&gt; print len(u'â')
1
</code></pre>

<p>And so, the following will print <code>True</code>:</p>

<pre><code>def learn_re(s):
    pattern=re.compile(""[0-9]{2}:[0-9]{2}:[0-9]{2}.[0-9]{3} . C"")
    if pattern.match(s):
        return True
    return False

print learn_re(u""01:01:01.123 â C"")
</code></pre>

<p>Note that this behavior is specific to python 2. In python 3, <code>str</code> and <code>unicode</code> are merged into a single <code>str</code> type, and so this distinction is not required.</p>
"
39862503,1296806.0,2016-10-04T22:07:16Z,39860158,3,"<p>The scaladoc has one example. Provide a function from <code>Match</code> instead of a string.</p>

<pre><code>scala&gt; val r = ""a(b)(c)+"".r
r: scala.util.matching.Regex = a(b)(c)+

scala&gt; val s = ""123 abcccc and abcc""
s: String = 123 abcccc and abcc

scala&gt; r.replaceAllIn(s, m =&gt; s""a${m.group(1).toUpperCase}${m.group(2)*3}"")
res0: String = 123 aBccc and aBccc
</code></pre>

<p>The resulting string also does group substitution.</p>

<pre><code>scala&gt; val r = ""a(b)(c+)"".r
r: scala.util.matching.Regex = a(b)(c+)

scala&gt; r.replaceAllIn(s, m =&gt; if (m.group(2).length &gt; 3) ""$1"" else ""$2"")
res3: String = 123 b and cc

scala&gt; r.replaceAllIn(s, m =&gt; s""$$${ if (m.group(2).length &gt; 3) 1 else 2 }"")
res4: String = 123 b and cc
</code></pre>
"
39862644,5566217.0,2016-10-04T22:20:45Z,39862573,0,"<p>When creating your Popen object you have the stderr set. When you go to open the stdout subprocess can't open it because you have stderr set instead. You can fix this by either adding <code>stdout=subprocess.PIPE</code> to the Popen object args, or by doing <code>with proc.stderr</code> instead of <code>with proc.stderr</code>. Also, you want to change your with statement a little, like this.</p>

<pre><code>with proc.stdout as stdout:
    for line in stdout:
        sys.stdout.write(line)
</code></pre>
"
39863089,364696.0,2016-10-04T23:06:09Z,39862662,3,"<p>The definition of <code>__weakref__</code> and <code>__dict__</code> is so the appropriate descriptor gets invoked to access the ""real"" location of the weak reference list and the instance dictionary of instances of the class when Python level code looks for it. The base class <code>object</code> is bare bones, and does not reserve space for <code>__weakref__</code> or <code>__dict__</code> on instances (<code>weakref.ref(object())</code> will fail, as will <code>setattr(object(), 'foo', 0)</code>).</p>

<p>For a user-defined class, it needs to define descriptors that can find those values (in CPython, these accessors are usually bypassed because there are direct pointers at the C layer, but if you explicitly reference <code>instance.__dict__</code>, it needs to know how to find it through the instance's class). The first child of <code>object</code> (without <code>__slots__</code>) needs to define these descriptors, so they can be found. The subclasses don't need to, because the location of <code>__dict__</code> and <code>__weakref__</code> doesn't change; they can continue to use the accessor for the base class, since their instances have those attributes at the same relative offset.</p>

<p>Basically, the first non-<code>__slot__</code>-ed user defined class is creating an idea of an instance as a C struct of the form:</p>

<pre><code>struct InstanceObject {
    ... common object stuff ...
    ... anything defined by __slots__ in super class(es), if anything ...
    PyObject *location_of___dict__;
    PyObject *location_of___weakref__;
}
</code></pre>

<p>and the accessors are implemented to check the type to determine the offset of those <code>location*</code> struct members, then retrieve them. The <code>struct</code> definition of a child class doesn't change the offset of those <code>location*</code> members (if new <code>__slots__</code> are added, they're just appended), so it can reuse the same accessor from the parent.</p>
"
39863094,642070.0,2016-10-04T23:07:07Z,39862573,0,"<p>Once you make <code>stdout</code> a <code>PIPE</code> you can just create a loop that reads the pipe and writes it to as many places as you want. Dealing with <code>stderr</code> makes the situation a little trickier because you need a background reader for it also. So, create a thread and a utility that writes to multiple files and you are done.</p>

<pre><code>import sys
import threading
from StringIO import StringIO
import subprocess as subp
import shlex

def _file_copy_many(fp, *write_to):
    """"""Copy from one file object to one or more file objects""""""
    while True:
        buf = fp.read(65536)
        if not buf:
            return
        for fp in write_to:
            fp.write(buf)

# for test
cmd = ""ls -l""

# will hold stdout and stderr
outbuf = StringIO()
errbuf = StringIO()

# run the command and close its stdin
proc = subp.Popen(shlex.split(cmd), stdin=subp.PIPE, stdout=subp.PIPE, 
    stderr=subp.PIPE)
proc.stdin.close()

# background thread to write stderr to buffer
stderr_thread = threading.Thread(target=_file_copy_many, 
    args=(proc.stderr, errbuf))
stderr_thread.start()

# write stdout to screen and buffer then wait for program completion
_file_copy_many(proc.stdout, sys.stdout, outbuf)
return_code = proc.wait()

# wait for err thread
stderr_thread.join()

# use buffers
print outbuf.tell(), errbuf.tell()
outbuf.seek(0)
errbuf.seek(0)
</code></pre>

<p>This is similar to what <code>subprocess.Popen.communicate</code> does but writes to more destinations.</p>
"
39863493,2912349.0,2016-10-05T00:00:50Z,39801880,6,"<p>Networkx has decent plotting facilities for exploratory data
analysis, it is not the tool to make publication quality figures,
for various reason that I don't want to go into here.  I hence
rewrote that part of the code base from scratch, and made a
stand-alone drawing module called netgraph that can be found
<a href=""https://github.com/paulbrodersen/netgraph"">here</a> (like the original purely based on matplotlib). The API is
very, very similar and well documented, so it should not be too
hard to mold to your purposes.</p>

<p>Building on that I get the following result:</p>

<p><a href=""http://i.stack.imgur.com/kjwqP.png""><img src=""http://i.stack.imgur.com/kjwqP.png"" alt=""enter image description here""></a></p>

<p>I chose colour to denote the edge strength as you can<br>
1) indicate negative values, and<br>
2) distinguish small values better.<br>
However, you can also pass an edge width to netgraph instead (see <code>netgraph.draw_edges()</code>). </p>

<p>The different order of the branches is a result of your data structure (a dict), which indicates no inherent order. You would have to amend your data structure and the function <code>_parse_input()</code> below to fix that issue.</p>

<p>Code:</p>

<pre><code>import itertools
import numpy as np
import matplotlib.pyplot as plt
import netgraph; reload(netgraph)

def plot_layered_network(weight_matrices,
                         distance_between_layers=2,
                         distance_between_nodes=1,
                         layer_labels=None,
                         **kwargs):
    """"""
    Convenience function to plot layered network.

    Arguments:
    ----------
        weight_matrices: [w1, w2, ..., wn]
            list of weight matrices defining the connectivity between layers;
            each weight matrix is a 2-D ndarray with rows indexing source and columns indexing targets;
            the number of sources has to match the number of targets in the last layer

        distance_between_layers: int

        distance_between_nodes: int

        layer_labels: [str1, str2, ..., strn+1]
            labels of layers

        **kwargs: passed to netgraph.draw()

    Returns:
    --------
        ax: matplotlib axis instance

    """"""
    nodes_per_layer = _get_nodes_per_layer(weight_matrices)

    node_positions = _get_node_positions(nodes_per_layer,
                                         distance_between_layers,
                                         distance_between_nodes)

    w = _combine_weight_matrices(weight_matrices, nodes_per_layer)

    ax = netgraph.draw(w, node_positions, **kwargs)

    if not layer_labels is None:
        ax.set_xticks(distance_between_layers*np.arange(len(weight_matrices)+1))
        ax.set_xticklabels(layer_labels)
        ax.xaxis.set_ticks_position('bottom')

    return ax

def _get_nodes_per_layer(weight_matrices):
    nodes_per_layer = []
    for w in weight_matrices:
        sources, targets = w.shape
        nodes_per_layer.append(sources)
    nodes_per_layer.append(targets)
    return nodes_per_layer

def _get_node_positions(nodes_per_layer,
                        distance_between_layers,
                        distance_between_nodes):
    x = []
    y = []
    for ii, n in enumerate(nodes_per_layer):
        x.append(distance_between_nodes * np.arange(0., n))
        y.append(ii * distance_between_layers * np.ones((n)))
    x = np.concatenate(x)
    y = np.concatenate(y)
    return np.c_[y,x]

def _combine_weight_matrices(weight_matrices, nodes_per_layer):
    total_nodes = np.sum(nodes_per_layer)
    w = np.full((total_nodes, total_nodes), np.nan, np.float)

    a = 0
    b = nodes_per_layer[0]
    for ii, ww in enumerate(weight_matrices):
        w[a:a+ww.shape[0], b:b+ww.shape[1]] = ww
        a += nodes_per_layer[ii]
        b += nodes_per_layer[ii+1]

    return w

def test():
    w1 = np.random.rand(4,5) #&lt; 0.50
    w2 = np.random.rand(5,6) #&lt; 0.25
    w3 = np.random.rand(6,3) #&lt; 0.75

    import string
    node_labels = dict(zip(range(18), list(string.ascii_lowercase)))

    fig, ax = plt.subplots(1,1)
    plot_layered_network([w1,w2,w3],
                         layer_labels=['start', 'step 1', 'step 2', 'finish'],
                         ax=ax,
                         node_size=20,
                         node_edge_width=2,
                         node_labels=node_labels,
                         edge_width=5,
    )
    plt.show()
    return

def test_example(input_dict):
    weight_matrices, node_labels = _parse_input(input_dict)
    fig, ax = plt.subplots(1,1)
    plot_layered_network(weight_matrices,
                         layer_labels=['', '1', '2', '3', '4'],
                         distance_between_layers=10,
                         distance_between_nodes=8,
                         ax=ax,
                         node_size=300,
                         node_edge_width=10,
                         node_labels=node_labels,
                         edge_width=50,
    )
    plt.show()
    return

def _parse_input(input_dict):
    weight_matrices = []
    node_labels = []

    # initialise sources
    sources = set()
    for v in input_dict[1].values():
        for s in v.keys():
            sources.add(s)
    sources = list(sources)

    for ii in range(len(input_dict)):
        inner_dict = input_dict[ii+1]
        targets = inner_dict.keys()

        w = np.full((len(sources), len(targets)), np.nan, np.float)
        for ii, s in enumerate(sources):
            for jj, t in enumerate(targets):
                try:
                    w[ii,jj] = inner_dict[t][s]
                except KeyError:
                    pass

        weight_matrices.append(w)
        node_labels.append(sources)
        sources = targets

    node_labels.append(targets)
    node_labels = list(itertools.chain.from_iterable(node_labels))
    node_labels = dict(enumerate(node_labels))

    return weight_matrices, node_labels

# --------------------------------------------------------------------------------
# script
# --------------------------------------------------------------------------------

if __name__ == ""__main__"":

    # test()

    input_dict =   {
        1: {
            ""Group 1"":{""sample_0"":0.5, ""sample_1"":0.5, ""sample_2"":0, ""sample_3"":0, ""sample_4"":0},
            ""Group 2"":{""sample_0"":0, ""sample_1"":0, ""sample_2"":1, ""sample_3"":0, ""sample_4"":0},
            ""Group 3"":{""sample_0"":0, ""sample_1"":0, ""sample_2"":0, ""sample_3"":0.5, ""sample_4"":0.5}
            },
        2: {
            ""Group 1"":{""Group 1"":1, ""Group 2"":0, ""Group 3"":0},
            ""Group 2"":{""Group 1"":0, ""Group 2"":1, ""Group 3"":0},
            ""Group 3"":{""Group 1"":0, ""Group 2"":0, ""Group 3"":1}
            },
        3: {
            ""Group 1"":{""Group 1"":0.25, ""Group 2"":0, ""Group 3"":0.75},
            ""Group 2"":{""Group 1"":0.25, ""Group 2"":0.75, ""Group 3"":0}
            },
        4: {
            ""Group 1"":{""Group 1"":1, ""Group 2"":0},
            ""Group 2"":{""Group 1"":0.25, ""Group 2"":0.75}
            }
        }

    test_example(input_dict)

    pass
</code></pre>
"
39863502,5050917.0,2016-10-05T00:01:45Z,39862022,1,"<p>I guess there is different ways to do this :</p>

<p>-you can, for example, use the <strong><code>threading</code></strong> module to launch all your requests from your single client, with something like:</p>

<pre><code>result_list = []  # Add the result to a list for the example 
rlock = threading.RLock()

def client_thread(client_url, request, i):
    context = zmq.Context.instance()
    socket = context.socket(zmq.REQ)

    socket.setsockopt_string(zmq.IDENTITY, '{}'.format(i))
    socket.connect(client_url)

    socket.send(request.encode())
    reply = socket.recv()

    with rlock:
        result_list.append((i, reply))
    return

def client_task():
    # tasks = list with all your tasks
    url_client = ""ipc://frontend.ipc""
    threads = []
    for i in range(len(tasks)):
        thread = threading.Thread(target=client_thread,
                                    args=(url_client, tasks[i], i,))
        thread.start()
        threads.append(thread)
</code></pre>

<p>-you can take benefit of an evented library like <strong><code>asyncio</code></strong> (there is a submodule <a href=""http://pyzmq.readthedocs.io/en/latest/api/zmq.asyncio.html"" rel=""nofollow"">zmq.asyncio</a> and an other library <a href=""https://github.com/aio-libs/aiozmq"" rel=""nofollow"">aiozmq</a>, the last one offers a higher level of abstraction). In this case you will send your requests to the workers, sequentially too, but without blocking for each response (and so not keeping the main loop busy) and get the results when they came back to the main loop. This could look like this:</p>

<pre><code>import asyncio
import zmq.asyncio

async def client_async(request, context, i, client_url):
    """"""Basic client sending a request (REQ) to a ROUTER (the broker)""""""
    socket = context.socket(zmq.REQ)
    socket.setsockopt_string(zmq.IDENTITY, '{}'.format(i))
    socket.connect(client_url)
    await socket.send(request.encode())
    reply = await socket.recv()
    socket.close()
    return reply


async def run(loop):
    # tasks = list full of tasks
    url_client = ""ipc://frontend.ipc""
    asyncio_tasks = []
    ctx = zmq.asyncio.Context()
    for i in range(len(tasks)):
        task = asyncio.ensure_future(client_async(tasks[i], ctx, i, url_client))
        asyncio_tasks.append(task)

    responses = await asyncio.gather(*asyncio_tasks)
    return responses

zmq.asyncio.install()
loop = asyncio.get_event_loop()
results = loop.run_until_complete(run(loop))
</code></pre>

<p>I didn't tested theses two snippets but they are both coming (with modifications to fit the question) from code i have using zmq in a similar configuration than your question.</p>
"
39863901,3244542.0,2016-10-05T00:58:15Z,39863718,2,"<p>Even though this is a possible duplicate I want to write out a tiny bit of python logging knowledge. </p>

<p>DON'T pass loggers around. You can always access any given logger by <code>logging.getLogger(&lt;log name as string&gt;)</code>. By default it looks like* flask uses the name you provide to the <code>Flask</code> class. </p>

<p>So if your main module is called 'my_tool', you would want to do <code>logger = logging.getLogger('my_tool')</code>in the <code>Service</code> module. </p>

<p>To add onto that, I like to be explicit about naming my loggers and packages, so I would do <code>Flask('my_tool')</code>** and in other modules, have sub level loggers like. <code>logger = logging.getLogger('my_tool.services')</code> that all use the same root logger (and handlers).</p>

<p>* No experience, based off other answer. </p>

<p>** Again, don't use flask, dk if that is good practice</p>

<p><strong>Edit: Super simple stupid example</strong></p>

<p><strong>Main Flask app</strong></p>

<pre><code>import sys
import logging

import flask

from module2 import hi

app = flask.Flask('tester')

handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
app.logger.addHandler(handler)
app.logger.setLevel(logging.DEBUG)

@app.route(""/index"")
def index():
    app.logger.debug(""TESTING!"")
    hi()
    return ""hi""

if __name__ == '__main__':
    app.run()
</code></pre>

<p><strong>module2</strong></p>

<pre><code>import logging

log = logging.getLogger('tester.sub')


def hi():
    log.warning('warning test')
</code></pre>

<p><strong>Outputs</strong></p>

<pre><code>127.0.0.1 - - [04/Oct/2016 20:08:29] ""GET /index HTTP/1.1"" 200 -
2016-10-04 20:08:29,098 - tester - DEBUG - TESTING!
2016-10-04 20:08:29,098 - tester.sub - WARNING - warning test
</code></pre>

<p><strong>Edit 2: Messing with subloggers</strong>  </p>

<p>Totally unneeded, just for general knowledge. </p>

<p>By defining a child logger, done by adding a <code>.something</code> after the root logger name in <code>logging.getLogger('root.something')</code> it gives you basiclly a different namespace to work with. </p>

<p>I personally like using it to group functionality in logging. So have some <code>.tool</code> or <code>.db</code> to know what type of code is logging. But it also allows so that those child loggers can have their own handlers. So if you only want some of your code to print to <code>stderr</code>, or to a log you can do so. Here is an example with a modified <code>module2</code>.</p>

<p><strong>module2</strong> </p>

<pre><code>import logging
import sys

log = logging.getLogger('tester.sub')
handler = logging.StreamHandler(sys.stderr)
handler.setFormatter(logging.Formatter('%(name)s - %(levelname)s - %(message)s'))
log.addHandler(handler)
log.setLevel(logging.INFO)


def hi():
    log.warning(""test"")
</code></pre>

<p>Output</p>

<pre><code>127.0.0.1 - - [04/Oct/2016 20:23:18] ""GET /index HTTP/1.1"" 200 -
2016-10-04 20:23:18,354 - tester - DEBUG - TESTING!
tester.sub - WARNING - test
2016-10-04 20:23:18,354 - tester.sub - WARNING - test
</code></pre>
"
39864197,4983450.0,2016-10-05T01:41:45Z,39863989,2,"<p>Here is an option with a <code>for</code> loop:</p>

<pre><code>zeros = 0
result = [0]

for i in lst:
    if i == 0:
        zeros += 1

    elif zeros &gt;= 2:                # if there are more than two zeros, append the new 
                                    # element to the result
        result.append(i)
        zeros = 0

    else:                           # Otherwise add it to the last element
        result[-1] += i
        zeros = 0

result
# [-7, -8, -4, -65, 20, -3]
</code></pre>

<p>To get the corresponding index, you can use <code>enumerate</code>:</p>

<pre><code>zeros = 0
reSum = [0]
reInd = [[]]
for i, v in enumerate(lst):
    if v == 0:
        zeros += 1
    elif zeros &gt;= 2:
        zeros = 0
        reSum.append(v)
        reInd.append([i])
â
    else:
        zeros = 0
        reSum[-1] += v
        reInd[-1] += [i]

reSum
# [-7, -8, -4, -65, 20, -3]

reInd
# [[1],
#  [4],
#  [7],
#  [11, 13, 14, 15, 17],
#  [20, 21, 23, 24, 25],
#  [32, 33, 34, 36, 37, 38, 39]]
</code></pre>
"
39864221,722427.0,2016-10-05T01:43:53Z,39864184,1,"<p>You'll need to run <code>python3</code> instead of <code>python</code> if that's not obvious. This is definitely, as you described, a ""quick fix"" </p>

<p>My suggested fix is to use <a href=""https://virtualenv.pypa.io/en/stable/"" rel=""nofollow"">virtualenv</a> and pass in the Python executable you would like to use as so:</p>

<p><code>virtualenv -p /usr/bin/python3.5 /my/virtualenv/&gt;</code></p>
"
39864232,4664044.0,2016-10-05T01:45:19Z,39864184,4,"<p>You don't need to rename anything for co-existence of different versions of Python.</p>

<p>The different versions of python are installed on different folders automatically.</p>

<p>When use the command prompt you can use the commands <code>py2</code> or <code>py3</code> to refer to the different versions of python. The next works too:</p>

<pre><code>C:\Users\user1&gt;py -2
</code></pre>

<p>and</p>

<pre><code>C:\Users\user1&gt;py -3
</code></pre>

<p>This also works with <code>pip2</code> and <code>pip3</code> for install new packages.</p>

<p>For more details, you can read this article: <a href=""https://docs.python.org/3/using/windows.html?#python-launcher-for-windows"" rel=""nofollow"" title=""Python Launcher for Windows"">Python Launcher for Windows</a>.</p>
"
39864265,1672429.0,2016-10-05T01:48:47Z,39863989,6,"<p>I group pairs of adjacent numbers by whether they hold any truth. Then take the truthy groups and sum them. Might be a bit too complicated, but I like using the <code>any</code> key.</p>

<pre><code>&gt;&gt;&gt; from itertools import groupby
&gt;&gt;&gt; [sum(a for a, _ in g) for k, g in groupby(zip(lst, lst[1:] + [0]), any) if k]
[-7, -8, -4, -65, 20, -3]
</code></pre>

<p>(Thanks to blhsing and ShadowRanger for improvements.)</p>

<p>Bit shorter way to turn the pairs back into singles (first is Python 2, second is Python 3):</p>

<pre><code>&gt;&gt;&gt; [sum(zip(*g)[0]) for k, g in groupby(zip(lst, lst[1:] + [0]), any) if k]
&gt;&gt;&gt; [sum(next(zip(*g))) for k, g in groupby(zip(lst, lst[1:] + [0]), any) if k]
</code></pre>
"
39864268,364696.0,2016-10-05T01:49:30Z,39863989,0,"<p>Cheesy solution if your values all occur in <code>range(-128, 128)</code> is to use the <code>split</code> and <code>replace</code> methods of <code>bytearray</code> (which conveniently converts to and from <code>int</code>) to do the splitting:</p>

<pre><code>lst = [0, -7, 0, 0, -8, 0, 0, -4, 0, 0, 0, -6, 0, -4, -29, -10, 0, -16, 0, 0, 2, 3, 0, 18, -1, -2, 0, 0, 0, 0, 0, 0, 21, 10, -10, 0, -12, 3, -5, -10]

# Adjust all  values up by 0x80 so they fall in bytearray's expected 0-255 range
bts = bytearray(x + 0x80 for x in lst)

# Split on double \x80 (the value of 0 after adjustment)
# then remove single \x80 from each part
parts = [b.replace(b'\x80', b'') for b in bts.split(b'\x80\x80')]

# Undo adjustment and unpack (and filter empty sublists out completely)
newlst = [b[0] - 0x80 if len(b) == 1 else [x - 0x80 for x in b] for b in parts if b]

# Or to just get the sums, no need to create newlst, just filter and sum en masse:
sums = [sum(b) - 0x80 * len(b) for b in parts if b]
</code></pre>
"
39864396,6896151.0,2016-10-05T02:08:59Z,39863989,0,"<pre><code>list = lst = [0, -7, 0, 0, -8, 0, 0, -4, 0, 0, 0, -6, 0, -4, -29, -10, 0, -16, 0, 0, 2, 3, 0, 18, -1, -2, 0, 0, 0, 0, 0, 0, 21, 10, -10, 0, -12, 3, -5, -10]
lastElement = None
new_list=[]
sub_list=[]
for element in list:
    if element == 0 == lastElement:
        if sub_list == []:
            continue
        new_list.append(sub_list)
        sub_list = []
    elif element == 0:
        pass
    else:
        sub_list.append(element)
    lastElement = element
new_list.append(sub_list)
print(new_list)
[[-7], [-8], [-4], [-6, -4, -29, -10, -16], [2, 3, 18, -1, -2], [21, 10, -10, -12, 3, -5, -10]]
</code></pre>
"
39865217,3244542.0,2016-10-05T04:00:21Z,39865185,1,"<p>You're list assignment is off, <code>l[i]</code> is not saying 'the value that equals <code>i</code>' but 'position <code>i</code> in list <code>l</code>'. You also don't want to modify a list as you iterate over it. </p>

<p>I think this is more what you want. Creates a new list of mod3 items of the incoming list. </p>

<pre><code>def reduceMod3(l):
    return [i % 3 for i in l]
</code></pre>
"
39865221,376371.0,2016-10-05T04:00:46Z,39865185,4,"<p>When you write <code>for i in l</code>, you are accessing each element of the list, not the index. Instead, you should write </p>

<pre><code>for i in range(len(l)):
</code></pre>

<p>You can also solve this with a list comprehension:</p>

<pre><code>return [item % 3 for item in l]
</code></pre>
"
39865233,268977.0,2016-10-05T04:01:53Z,39857515,1,"<p>So, <code>urlfetch.set_default_fetch_deadline()</code> did eventually work for me. The problem was my underlying http client (oauth2client / httplib2) was essentially stored in a global. Once I created it in the task handler thread the <code>set_default_fetch_deadline</code> worked.</p>
"
39865301,104458.0,2016-10-05T04:09:56Z,39864096,1,"<p>Hash table.  Or in Python terms, just use a <code>set</code>.</p>

<p>Put each item from the <strong><em>smaller</em></strong> file into the set.  200K items is perfectly fine.  Enumerate each item in the larger file to see if it exists in the smaller file.  If there is a match, remove the item from the the hash table.</p>

<p>When you are done, any item remaining in the set represents an item not found in the larger file.</p>

<p>My Python is a little rusty, but it would go something like this:</p>

<pre><code>s = set()

with open(""small_file.txt"") as f:
     content = f.readlines()

for line in content:
    line = line.strip()
    s.add(line)

with open(""large_file.txt"") as f:
    for line in f:
         if line in s:
            s.discard(line.strip())

for i in s:
    print(i)
</code></pre>
"
39865306,3244542.0,2016-10-05T04:10:21Z,39864096,0,"<p>Haven't tested, but I think this would be non memory intensive (no idea on speed): </p>

<pre><code>unique = []

with open('large_file.txt') as lf, open('small_file.txt') as sf:
    for small_line in sf:
        for large_line in lf:
            if small_line == large_line:
                break
        else:
            unique.append(small_line)
        lf.seek(0)
</code></pre>
"
39866299,3581217.0,2016-10-05T05:50:01Z,39854373,0,"<p>One method (based on this: <a href=""http://stackoverflow.com/a/34985243/3581217"">http://stackoverflow.com/a/34985243/3581217</a> answer) which seems to work is to create a <code>Dataframe</code> where the observations from the different sites have different columns, then a <code>dropna()</code> with <code>subset</code> set to either all columns, or the two sites I want to compare, which drops all rows where data is missing. </p>

<pre><code>import pandas as pd
import numpy as np
from io import StringIO

data1 = StringIO(""""""\
  1,  2001-01-01, 00:00, 1.0
  1,  2001-01-01, 01:00, 1.1
  1,  2001-01-01, 02:00, 1.2
  1,  2001-01-01, 03:00, 1.3
"""""")

data2 = StringIO(""""""\
  2,  2001-01-01, 00:00, 2.0
  2,  2001-01-01, 01:00, -99
  2,  2001-01-01, 02:00, 2.2
  2,  2001-01-01, 03:00, 2.3
"""""")

data3 = StringIO(""""""\
  3,  2001-01-01, 00:00, 3.0
  3,  2001-01-01, 01:00, 3.1
  3,  2001-01-01, 02:00, -99
  3,  2001-01-01, 03:00, 3.3
  3,  2001-01-01, 04:00, 3.4
"""""")

columns = ['id','date','time','T1']
df1 = pd.read_table(data1, header=None, names=columns, delimiter=',', na_values=-99, parse_dates=[['date','time']])
columns = ['id','date','time','T2']
df2 = pd.read_table(data2, header=None, names=columns, delimiter=',', na_values=-99, parse_dates=[['date','time']])
columns = ['id','date','time','T3']
df3 = pd.read_table(data3, header=None, names=columns, delimiter=',', na_values=-99, parse_dates=[['date','time']])

df = pd.concat([df1,df2,df3]).groupby('date_time').max()
df = df.dropna(subset=['T1','T2','T3'])
</code></pre>

<p>The resulting <code>Dataframe</code> looks like:</p>

<pre><code>In [232]: df
Out[232]: 
                      T1   T2   T3  id
date_time                             
2001-01-01 00:00:00  1.0  2.0  3.0   3
2001-01-01 03:00:00  1.3  2.3  3.3   3
</code></pre>

<p>And if I want to compare only two sites, ignoring in this case <code>T3</code>, a <code>df.dropna(subset=['T1','T2'])</code> results in:</p>

<pre><code>In [234]: df
Out[234]: 
                      T1   T2   T3  id
date_time                             
2001-01-01 00:00:00  1.0  2.0  3.0   3
2001-01-01 02:00:00  1.2  2.2  NaN   3
2001-01-01 03:00:00  1.3  2.3  3.3   3
</code></pre>

<p>Is this the way to go? Still feels a bit non-Panda-ish..?</p>
"
39866932,1832058.0,2016-10-05T06:34:06Z,39866051,4,"<p>I don't know why but <code>rational=False</code> helps</p>

<pre><code>sympy.solve([sympy.Eq(b - a**2.552 - c), sympy.Eq(b, 2)], rational=False)
</code></pre>

<p>see: <a href=""http://stackoverflow.com/questions/17087629/sympy-hangs-when-trying-to-solve-a-simple-algebraic-equation"">sympy hangs when trying to solve a simple algebraic equation</a></p>
"
39867338,2901002.0,2016-10-05T06:59:19Z,39867061,1,"<p>IIUC you can use:</p>

<pre><code>from itertools import chain 

#flat nested lists
a = list(chain.from_iterable(df['tfidf_sorted']))
#sorting
a.sort(key=lambda x: x[1], reverse=True)
#get 10 top
print (a[:10])
</code></pre>

<p>Or if need top 10 per row add <code>[:10]</code>:</p>

<pre><code>df['tfidf_sorted'] = df['tfidf'].apply(lambda y: (sorted(y.items(), key=lambda x: x[1], reverse=True))[:10])
</code></pre>
"
39867407,6899440.0,2016-10-05T07:02:54Z,39863989,0,"<pre><code>map(lambda x:eval(re.sub('^,','',x,1).replace(',','+')) if(len(x))&gt;3 else eval(x.replace(',','+')),filter(lambda x:x!=',0',re.split(r',0,0,|\|',""|"".join(re.split(r'(,0){3,}',l)))))
</code></pre>
"
39868750,3991400.0,2016-10-05T08:16:44Z,39868209,3,"<p>When you open a shell a whole of stuff is silently initialized for you, and most important for your issue, environment variables are set. What you most likely miss is the definition of <code>LIBRARY_PATH</code>, which is the variable used by the linker to look for libraries matching the ones you instruct it to link using the <code>-lNAME</code> flags. </p>

<p>What the linker needs is a list of directories where it will search for files matching <code>libNAME.{a,so}</code>. You can also pass these directories directly using the <code>-L</code> flag, but in general, you should probably try to use a program like CMake, Make or any other build tool.</p>

<p>This will give you access to commands like <code>find_package</code> and <code>target_link_libraries</code> (CMake), to find, respectively add libraries to your build targets, instead of having to maintain your python to compile your stuff.</p>
"
39868807,3510736.0,2016-10-05T08:20:05Z,39868762,3,"<p>Any triple <em>(x, y, z)</em>, with <em>z = (14 - 2x - 5y) / 8</em>, satisfies your constraint. </p>

<p>Note that <em>x + y + (14 - 2x - 5y) / 8</em> is unbounded from below. This function decreases when each of <em>x</em> and <em>y</em> decrease, with no finite minimum.</p>
"
39869034,986961.0,2016-10-05T08:33:03Z,39868762,0,"<p>From your first equation:</p>

<p>x = (14 - 5y - 8x) / 2</p>

<p>so, you now only need to minimize</p>

<p>(14 - 5y - 8z) / 2 + y + z</p>

<p>which is</p>

<p>(14 - 3y - 6z) / 2</p>

<p>But we can ignore the ' / 2' part for minimization purposes.</p>

<p>Presumably, there must be some other constraints on your problem, since as described the solution is that both y and z may grow without bound.</p>
"
39869214,6207849.0,2016-10-05T08:41:40Z,39863487,1,"<p>Sort the column names containing date strings and later use it as a subset to return the columns in that particular order:</p>

<pre><code>from datetime import datetime
df[sorted(df.columns, key=lambda x: datetime.strptime(x, '%m/%y'))]
</code></pre>

<p><a href=""http://i.stack.imgur.com/iYDPP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iYDPP.png"" alt=""Image""></a></p>

<hr>

<p><strong><em>Toy Data:</em></strong></p>

<pre><code>from datetime import datetime
np.random.seed(42)

cols = [['STYLE', 'COLOR', 'SIZE', 'FOR', 'FOR', 'FOR', 'FOR'],
        ['', '', '', '01/17', '10/16', '11/16', '12/16']]
tups = list(zip(*cols))
index = pd.MultiIndex.from_tuples(tups, names=[None, 'MONTH'])
clean_table_grouped = pd.DataFrame(np.random.randint(0, 100, (100, 7)), 
                                   index=np.arange(100), columns=index)
clean_table_grouped = clean_table_grouped.head()
clean_table_grouped
</code></pre>

<p><a href=""http://i.stack.imgur.com/57842.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/57842.png"" alt=""Image""></a></p>

<p>Split the multi-index <code>DF</code> into two with the one containing the forecast values and the other the remaining <code>DF</code>.</p>

<pre><code>for_df = clean_table_grouped[['FOR']]
clean_table_grouped = clean_table_grouped.drop(['FOR'], axis=1, level=0)
</code></pre>

<p>Forecast <code>DF</code>:</p>

<pre><code>for_df
</code></pre>

<p><a href=""http://i.stack.imgur.com/H2hbF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/H2hbF.png"" alt=""Image""></a></p>

<p>Remaining <code>DF</code>:</p>

<pre><code>clean_table_grouped
</code></pre>

<p><a href=""http://i.stack.imgur.com/Usr7L.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Usr7L.png"" alt=""Image""></a></p>

<p>Sorting the columns in the forecast <code>DF</code> by applying the same procedure as done in the pre-edited post.</p>

<pre><code>order = sorted(for_df['FOR'].columns.tolist(), key=lambda x: datetime.strptime(x, '%m/%y'))
</code></pre>

<p>Making the <code>DF</code> in the same order by subsetting the sorted <code>list</code> of columns.</p>

<pre><code>for_df = for_df['FOR'][order]
</code></pre>

<p>Concatenate the forecast <code>DF</code> with itself to create a multi-index like column.</p>

<pre><code>for_df = pd.concat([for_df, for_df], axis=1, keys=['FOR'])
</code></pre>

<p>Finally, join them on the common index.</p>

<pre><code>clean_table_grouped.join(for_df)
</code></pre>

<p><a href=""http://i.stack.imgur.com/4cHZk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4cHZk.png"" alt=""Image""></a></p>
"
39869597,2185825.0,2016-10-05T09:00:03Z,39868762,0,"<p>I do not know any general fast solution for n variables, or not using hit &amp; trail loops. But for the given specific equation  <code>2x + 5y + 8z = 14</code>, there maybe some shortcut based on observation.</p>

<p>Notice that the range is very small for any possible solutions:  </p>

<p><code>0&lt;=x&lt;=7</code>,  <code>0&lt;=y&lt;=2</code>,  <code>0&lt;=z&lt;=1</code></p>

<p>Also other than x = 7, you have at least to use 2 variables.
<strong>(x+y+z = 7 for this case)</strong></p>

<hr>

<p>Let's find what we got if using only 2 variables:</p>

<p>If you choose to use (x,z) or (y,z), as <code>z</code> can only be 1, <code>x</code> or <code>y</code> is trivial. </p>

<p><strong>(x+y+z = 4 for (x,z),  no solution for (y,z))</strong></p>

<p>If you choose to use (x,y), as <code>x</code>'s coefficient is even and <code>y</code>'s coefficient is odd,  you must choose even number of <code>y</code> to achieve an even R.H.S. (14). Which means <code>y</code> must be 2, <code>x</code> is then trivial.</p>

<p><strong>(x+y+z = 4 for this case)</strong></p>

<hr>

<p>Let's find what we got if using all 3 variables:</p>

<p>Similarly, <code>z</code> must be 1, so basically it's using 2 variables (x,y) to achieve 14-8 = 6 which is even.</p>

<p>Again we use similar argument, so we must choose even number of <code>y</code> which is 2, however at this point 2y + 1z > 14 already, which means there is <strong>no solution using all 3 variables</strong>.</p>

<hr>

<p>Therefore simply by logic, reduce the equation by using 1 or 2 variables, we can find that <strong>minimum x+y+z is 4</strong> to achieve 14 <strong>(x=3,y=0,z=1  or x=2,y=2,z=0)</strong></p>
"
39869632,1339507.0,2016-10-05T09:01:52Z,39868990,5,"<p>I get the feeling that this question is designed with the expectation that you would initially instinctively do it the way you have. However, I believe there's a slightly different approach that would be faster.</p>

<p>You can produce all the numbers that contain the number 13 yourself, without having to loop through all the numbers in between. For example:</p>

<p>2 digits:
13</p>

<p>3 digits position 1:
113
213
313 etc.</p>

<p>3 digits position 2: 131
132
133 etc.</p>

<p>Therefore, you don't have to check all the number from 0 to n*9. You simply count all the numbers with 13 in them until the length is larger than N.</p>

<p>This may not be the fastest solution (in fact I'd be surprised if this couldn't be solved efficiently by using some mathematics trickery) but I believe it will be more efficient than the approach you have currently taken.</p>
"
39869693,1925388.0,2016-10-05T09:04:17Z,39868990,4,"<p>I think this can be solved via recursion:</p>

<pre><code>ans(n) = { ans([n/2])^2 - ans([n/2]-1)^2 }, if n is even
ans(n) = { ans([n/2]+1)*ans([n/2]) - ans([n/2])*ans([n/2]-1) }, if n is odd
</code></pre>

<p>Base Cases:</p>

<ul>
<li><code>ans(0)</code> = 1</li>
<li><code>ans(1)</code> = 10</li>
</ul>

<p>It's implementation is running quite fast even for larger inputs like <code>10^9</code> ( which is expected as its complexity is <code>O(log[n])</code> instead of <code>O(n)</code> like the other answers ):  </p>

<pre><code>cache = {}

mod = 1000000009

def ans(n):
    if cache.has_key(n):
        return cache[n]

    if n == 0:
        cache[n] = 1
        return cache[n]
    if n == 1:
        cache[n] = 10
        return cache[n]

    temp1 = ans(n/2)
    temp2 = ans(n/2-1)

    if (n &amp; 1) == 0:
        cache[n] = (temp1*temp1 - temp2*temp2) % mod
    else:
        temp3 = ans(n/2 + 1)
        cache[n] = (temp1 * (temp3 - temp2)) % mod

    return cache[n]

print ans(1000000000)
</code></pre>

<p><a href=""http://ideone.com/TNqxJw"" rel=""nofollow"">Online Demo</a></p>

<p><strong>Explanation:</strong></p>

<p>Let a string <code>s</code> have even number of digits 'n'.<br>
Let <code>ans(n)</code> be the answer for the input <code>n</code>, i.e. the number of strings without the substring <code>13</code> in them.<br>
Therefore, the answer for string <code>s</code> having length <code>n</code> can be written as the multiplication of the answer for the first half of the string (<code>ans([n/2])</code>) and the answer for the second half of the string (<code>ans([n/2])</code>), minus the number of cases where the string <code>13</code> appears in the middle of the number <code>n</code>, i.e. when the last digit of the first half is <code>1</code> and the first digit of the second half is <code>3</code>.</p>

<p>This can expressed mathematically as:  </p>

<pre><code>ans(n) = ans([n/2])^2 - ans([n/2]-1)*2
</code></pre>

<p>Similarly for the cases where the input number <code>n</code> is odd, we can derive the following equation:  </p>

<pre><code>ans(n) = ans([n/2]+1)*ans([n/2]) - ans([n/2])*ans([n/2]-1)
</code></pre>
"
39869878,1220089.0,2016-10-05T09:12:20Z,39868990,3,"<p>This a P&amp;C problem. I'm going to assume 0 is valid string and so is 00, 000 and so on, each being treated distinct from the other.</p>

<p>The total number of strings not containing 13, of length N, is unsurprisingly given by:</p>

<pre><code>(Total Number of strings of length N) - (Total number of strings of length N that have 13 in them)
</code></pre>

<p>Now, the Total number of strings of length N is easy, you have 10 digits and N slots to put them in: <code>10^N</code>.</p>

<p>The number of strings of length N with 13 in them is a little trickier.
You'd think you can do something like this:</p>

<pre><code>=&gt; (N-1)C1 * 10^(N-2)
=&gt; (N-1) * 10^(N-2)
</code></pre>

<p>But you'd be wrong, or more accurately, you'd be over counting certain strings. For example, you'd be over counting the set of string that have two or more 13s in them.</p>

<p>What you really need to do is apply the <a href=""https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle"" rel=""nofollow"">inclusion-exclusion principle</a> to count the number of strings with 13 in them, so that they're all included once.</p>

<p>If you look at this problem as a set counting problem, you have quite a few sets:</p>

<pre><code>S(0,N): Set of all strings of Length N.
S(1,N): Set of all strings of Length N, with at least one '13' in it.
S(2,N): Set of all strings of Length N, with at least two '13's in it.
...
S(N/2,N): Set of all strings of Length N, with at least floor(N/2) '13's in it.
</code></pre>

<p>You want the set of all strings with 13 in them, but counted at most once. You can use the inclusion-exclusion principle for computing that set.</p>
"
39870385,2842758.0,2016-10-05T09:34:14Z,39868990,2,"<p>In fact this question is more about math than about python.
For N figures there is 10^N possible unique strings. To get the answer to the problem we need to subtract the number of string containing ""13"". 
If string starts from ""13"" we have 10^(N-2) possible unique strings. If we have 13 at the second possition (e.i. a string like x13...), we again have 10^(N-2) possibilities. But we can't continue this logic further as this will lead us to double calculation of string which have 13 at different possitions. For example for N=4 there will be a string ""1313"" which we will calculate twice. To avoid this we should calculate only those strings which we haven't calculated before. So for ""13"" on possition <code>p</code> (counting from 0) we should find the number of unique string which don't have ""13"" on the left side from <code>p</code>, that is for each p 
number_of_strings_for_13_at_p = total_number_of_strings_without_13(N=p-1) * 10^(N-p-2)
So we recursevily define the total_number_of_strings_without_13 function.</p>

<p>Here is the idea in the code:</p>

<pre><code>def number_of_strings_without_13(N):
    sum_numbers_with_13 = 0
    for p in range(N-1):
        if p &lt; 2:
            sum_numbers_with_13 += 10**(N-2)
        else:
            sum_numbers_with_13 += number_of_strings_without_13(p) * 10**(N-p-2)

    return 10**N - sum_numbers_with_13
</code></pre>

<p>I should say that <code>10**N</code> means 10 in the power of N. All the other is described above. The functions also has a surprisingly pleasent ability to give correct answers for N=1 and N=2.</p>

<p>To test this works correct I've rewritten your code into function and refactored a little bit:</p>

<pre><code>def number_of_strings_without_13_bruteforce(N):
    without_13 = 0
    for i in range(10**N):
        if str(i).count(""13""):
            continue
        without_13 += 1
    return without_13

for N in range(1, 7):
    print(number_of_strings_without_13(N),
          number_of_strings_without_13_bruteforce(N))
</code></pre>

<p>They gave the same answers. With bigger N bruteforce is very slow. But for very large N recursive function also gets mush slower. There is a well known solution for that: as we use the value of <code>number_of_strings_without_13</code> with parameters smaller than N multiple times, we should remember the answers and not recalculate them each time. It's quite simple to do like this:</p>

<pre><code>def number_of_strings_without_13(N, answers=dict()):
    if N in answers:
        return answers[N]

    sum_numbers_with_13 = 0
    for p in range(N-1):
        if p &lt; 2:
            sum_numbers_with_13 += 10**(N-2)
        else:
            sum_numbers_with_13 += number_of_strings_without_13(p) * 10**(N-p-2)

    result = 10**N - sum_numbers_with_13
    answers[N] = result
    return result
</code></pre>
"
39871459,240950.0,2016-10-05T10:25:20Z,39871227,4,"<p>No, the syntax is not valid. It is easy to prove by checking the documentation. In Python 2, an identifier is constructed by the following <a href=""https://docs.python.org/2/reference/lexical_analysis.html#identifiers"" rel=""nofollow"">rules</a>:</p>

<pre><code>identifier ::=  (letter|""_"") (letter | digit | ""_"")*
letter     ::=  lowercase | uppercase
lowercase  ::=  ""a""...""z""
uppercase  ::=  ""A""...""Z""
digit      ::=  ""0""...""9""
</code></pre>

<p>In Py3 the rules are more or less the same, beside being expanded up to the range of Unicode characters.</p>

<p>It seems that the author probably meant something like</p>

<pre><code>class Node:
    ...
    def _get_child_candidates(self, ...):
        ...
</code></pre>
"
39871509,2141635.0,2016-10-05T10:27:50Z,39871227,2,"<p>As in my comment you cannot, the valid identifiers for python3 are in the <a href=""https://docs.python.org/3/reference/lexical_analysis.html#identifiers"" rel=""nofollow"">docs</a>:</p>

<p><em>Identifiers (also referred to as names) are described by the following lexical definitions.</em></p>

<p><em>The syntax of identifiers in Python is based on the Unicode standard annex UAX-31, with elaboration and changes as defined below; see also PEP 3131 for further details.</em></p>

<p><em>Within the ASCII range (U+0001..U+007F), the valid characters for identifiers are the same as in Python 2.x: the uppercase and lowercase letters A through Z, the underscore _ and, except for the first character, the digits 0 through 9.</em></p>

<p><em>Python 3.0 introduces additional characters from outside the ASCII range (see PEP 3131). For these characters, the classification uses the version of the Unicode Character Database as included in the unicodedata module.</em></p>

<p>If you examine the code you can see it is a typo in the original question:</p>

<pre><code>def node._get_child_candidates(self, distance, min_dist, max_dist):
    if self._leftchild and distance - max_dist &lt; self._median:
        yield self._leftchild
    if self._rightchild and distance + max_dist &gt;= self._median:
        yield self._rightchild  
</code></pre>

<p>And this is the caller:</p>

<pre><code>result, candidates = list(), [self]
while candidates:
    node = candidates.pop() # creates an instance
    distance = node._get_dist(obj)
    if distance &lt;= max_dist and distance &gt;= min_dist:
        result.extend(node._values)
    # the _get_child_candidates node is called 
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))
return result
</code></pre>

<p>So the method <code>_get_child_candidates</code> is called on the instance. So really the actual code looks like:</p>

<pre><code>def _get_child_candidates(self, distance, min_dist, max_dist):
    if self._leftchild and distance - max_dist &lt; self._median:
        yield self._leftchild
    if self._rightchild and distance + max_dist &gt;= self._median:
        yield self._rightchild  
</code></pre>

<p>And this is the caller:</p>

<pre><code>result, candidates = list(), [self]
while candidates:
    node = candidates.pop() # creates an instance
    distance = node._get_dist(obj)
    if distance &lt;= max_dist and distance &gt;= min_dist:
        result.extend(node._values)
    # the _get_child_candidates node is called 
    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))
return result
</code></pre>
"
39871672,6108661.0,2016-10-05T10:36:26Z,39845025,0,"<p>What about this?
I am making use of in-built python functions to eliminate loops and improve efficiency.</p>

<pre><code>test_str = 'abbcccdddd'

remaining_letters = [1]   # dummy initialisation
# sort alphabetically
unique_letters = sorted(set(test_str))
frequencies = [test_str.count(letter) for letter in unique_letters]

out = []

while(remaining_letters):

    # in case of ties, index takes the first occurence, so the alphabetical order is preserved  
    max_idx = frequencies.index(max(frequencies))    
    out.append(unique_letters[max_idx])

    #directly update frequencies instead of calculating them again
    frequencies[max_idx] -= 1  
    remaining_letters = [idx for idx, freq in enumerate(frequencies) if freq&gt;0]

print''.join(out)   #dcdbcdabcd
</code></pre>
"
39871874,1016216.0,2016-10-05T10:45:03Z,39868990,2,"<p>Let <code>f(n)</code> be the number of sequences of length n that have no ""13"" in them, and <code>g(n)</code> be the number of sequences of length n that have ""13"" in them.</p>

<p>Then <code>f(n) = 10^n - g(n)</code> (in mathematical notation), because it's the number of possible sequences (<code>10^n</code>) minus the ones that contain ""13"".</p>

<p>Base cases:</p>

<pre><code>f(0) = 1
g(0) = 0
f(1) = 10
g(1) = 0
</code></pre>

<p>When looking for the sequences <em>with</em> ""13"", a sequence can have a ""13"" at the beginning. That will account for <code>10^(n-2)</code> possible sequences with ""13"" in them. It could also have a ""13"" in the second position, again accounting for <code>10^(n-2)</code> possible sequences. But if it has a ""13"" in the <em>third</em> position, and we'd assume there would also be <code>10^(n-2)</code> possible sequences, we could those twice that already had a ""13"" in the first position. So we have to substract them. Instead, we count <code>10^(n-4)</code> times <code>f(2)</code> (because those are exactly the combinations in the first two positions that don't have ""13"" in them).</p>

<p>E.g. for g(5):</p>

<pre><code>g(5) = 10^(n-2) + 10^(n-2) + f(2)*10^(n-4) + f(3)*10^(n-5)
</code></pre>

<p>We can rewrite that to look the same everywhere:</p>

<pre><code>g(5) = f(0)*10^(n-2) + f(1)*10^(n-3) + f(2)*10^(n-4) + f(3)*10^(n-5)
</code></pre>

<p>Or simply the sum of <code>f(i)*10^(n-(i+2))</code> with <code>i</code> ranging from <code>0</code> to <code>n-2</code>.</p>

<p>In Python:</p>

<pre><code>from functools import lru_cache

@lru_cache(maxsize=1024)
def f(n):
    return 10**n - g(n)

@lru_cache(maxsize=1024)
def g(n):
    return sum(f(i)*10**(n-(i+2)) for i in range(n-1))  # range is exclusive
</code></pre>

<p>The <code>lru_cache</code> is optional, but often a good idea when working with recursion.</p>

<hr>

<pre><code>&gt;&gt;&gt; [f(n) for n in range(10)]
[1, 10, 99, 980, 9701, 96030, 950599, 9409960, 93149001, 922080050]
</code></pre>

<p>The results are instant and <s>it works for very large numbers</s>.</p>
"
39872732,2776376.0,2016-10-05T11:27:22Z,39847995,0,"<p>Looking at the code I guess they expect to call the converter with something like:</p>

<pre><code>python json_to_tsv.py plate/sample1/sample1.json
</code></pre>

<p>Try copying your JSON file to a directory called <code>sample1</code> inside a directory called <code>plate</code> and see if you get the same error when you call it like in the example above.</p>

<hr>

<p><strong>Update</strong></p>

<p>The problem is indeed as described above.</p>

<p><strong>Doesn't work:</strong></p>

<pre><code>python json_to_tsv.py result_TB.json 
</code></pre>

<blockquote>
  <p>mykrobe_version   file    plate_name  sample  drug    phylo_group species lineage phylo_group_per_covg    species_per_covg    lineage_per_covg    phylo_group_depth   species_depth   lineage_depth   susceptibility  variants
  (gene:alt_depth:wt_depth:conf)    genes
  (prot_mut-ref_mut:percent_covg:depth) </p>

<pre><code>Traceback (most recent call last):   File ""json_to_tsv.py"", line 157, in &lt;module&gt;
    sample_name = get_sample_name(f)   File ""json_to_tsv.py"", line 78, in get_sample_name
    return f.split('/')[-2] IndexError: list index out of range
</code></pre>
</blockquote>

<p><strong>Works:</strong></p>

<pre><code>python json_to_tsv.py plate/sample/result_TB.json 
</code></pre>

<blockquote>
  <p>mykrobe_version   file    plate_name  sample  drug    phylo_group species lineage phylo_group_per_covg    species_per_covg    lineage_per_covg    phylo_group_depth   species_depth   lineage_depth   susceptibility  variants (gene:alt_depth:wt_depth:conf) genes (prot_mut-ref_mut:percent_covg:depth)</p>
  
  <p>-1    result_TB   plate   sample  NA</p>
</blockquote>
"
39872935,5693776.0,2016-10-05T11:37:04Z,39656433,2,"<pre><code>import email
import imaplib
import os

class FetchEmail():

connection = None
error = None
mail_server=""host_name""
username=""outlook_username""
password=""password""
self.save_attachment(self,msg,download_folder)
def __init__(self, mail_server, username, password):
    self.connection = imaplib.IMAP4_SSL(mail_server)
    self.connection.login(username, password)
    self.connection.select(readonly=False) # so we can mark mails as read

def close_connection(self):
    """"""
    Close the connection to the IMAP server
    """"""
    self.connection.close()

def save_attachment(self, msg, download_folder=""/tmp""):
    """"""
    Given a message, save its attachments to the specified
    download folder (default is /tmp)

    return: file path to attachment
    """"""
    att_path = ""No attachment found.""
    for part in msg.walk():
        if part.get_content_maintype() == 'multipart':
            continue
        if part.get('Content-Disposition') is None:
            continue

        filename = part.get_filename()
        att_path = os.path.join(download_folder, filename)

        if not os.path.isfile(att_path):
            fp = open(att_path, 'wb')
            fp.write(part.get_payload(decode=True))
            fp.close()
    return att_path

def fetch_unread_messages(self):
    """"""
    Retrieve unread messages
    """"""
    emails = []
    (result, messages) = self.connection.search(None, 'UnSeen')
    if result == ""OK"":
        for message in messages[0].split(' '):
            try: 
                ret, data = self.connection.fetch(message,'(RFC822)')
            except:
                print ""No new emails to read.""
                self.close_connection()
                exit()

            msg = email.message_from_string(data[0][1])
            if isinstance(msg, str) == False:
                emails.append(msg)
            response, data = self.connection.store(message, '+FLAGS','\\Seen')

        return emails

    self.error = ""Failed to retreive emails.""
    return emails
</code></pre>

<p>Above code works for me to download attachment.Hope this really helpful for any one.</p>
"
39873115,1219006.0,2016-10-05T11:46:12Z,39845025,1,"<p>Since the alphabet will always be a constant 26 characters,
this will work in O(N) and only takes a constant amount of space of 26</p>

<pre><code>from collections import Counter
from string import ascii_lowercase

def sorted_alphabet(text):
    freq = Counter(text)
    alphabet = filter(freq.get, ascii_lowercase) # alphabet filtered with freq &gt;= 1
    top_freq = max(freq.values()) if text else 0 # handle empty text eg. ''
    for top_freq in range(top_freq, 0, -1): # from top_freq to 1
        for letter in alphabet:
            if freq[letter] &gt;= top_freq:
                yield letter

print ''.join(sorted_alphabet('abbcccdddd'))
print ''.join(sorted_alphabet('dbdd'))
print ''.join(sorted_alphabet(''))
print ''.join(sorted_alphabet('xxxxaaax'))
</code></pre>

<hr>

<pre><code>dcdbcdabcd
ddbd

xxaxaxax
</code></pre>
"
39873264,364696.0,2016-10-05T11:54:47Z,39845025,1,"<p>Your solution involves 26 linear scans of the string and a bunch of unnecessary 
conversions to count the frequencies. You can save some work by replacing all those linear scans with a linear count step, another linear repetition generation, then a sort to order your letters and a final linear pass to strip counts: </p>

<pre><code>from collections import Counter      # For unsorted input
from itertools import groupby        # For already sorted input
from operator import itemgetter

def makenewstring(inp):
    # When inp not guaranteed to be sorted:
    counts = Counter(inp).iteritems()

    # Alternative if inp is guaranteed to be sorted:
    counts = ((let, len(list(g))) for let, g in groupby(inp))

    # Create appropriate number of repetitions of each letter tagged with a count
    # and sort to put each repetition of a letter in correct order
    # Use negative n's so much more common letters appear repeatedly at start, not end
    repeats = sorted((n, let) for let, cnt in counts for n in range(0, -cnt, -1))

    # Remove counts and join letters
    return ''.join(map(itemgetter(1), repeats))
</code></pre>

<hr>

<p><strong>Updated:</strong> It occurred to me that my original solution could be made much more concise, a one-liner actually (excluding required imports), that minimizes temporaries, in favor of a single sort-by-key operation that uses a trick to sort each letter by the count of that letter seen so far:</p>

<pre><code>from collections import defaultdict
from itertools import count

def makenewstring(inp):
    return ''.join(sorted(inp, key=lambda c, d=defaultdict(count): (-next(d[c]), c)))
</code></pre>

<p>This is actually the same basic logic as the original answer, it just accomplishes it by having <code>sorted</code> perform the decoration and undecoration of the values implicitly instead of doing it ourselves explicitly (implicit decorate/undecorate is the whole point of <code>sorted</code>'s <code>key</code> argument; it's doing the <a href=""https://en.wikipedia.org/wiki/Schwartzian_transform"" rel=""nofollow"">Schwartzian transform</a> for you).</p>

<p>Performance-wise, both approaches are similar; they both (in practice) scale linearly for smaller inputs (the one-liner up to inputs around 150 characters long, the longer code, using <code>Counter</code>, up to inputs in the <code>len</code> 2000 range), and while the growth is super-linear above that point, it's always below the theoretical <code>O(n log_2 n)</code> (likely due to the data being not entirely random thanks to the counts and limited alphabet, ensuring Python's TimSort has some existing ordering to take advantage of). The one-liner is somewhat faster for smaller strings (<code>len</code> 100 or less), the longer code is somewhat faster for larger strings (I'm guessing it has something to do with the longer code creating some ordering by grouping runs of counts for each letter). Really though, it hardly matters unless the input strings are expected to be huge.</p>
"
39873410,2902280.0,2016-10-05T12:01:54Z,39872981,3,"<p>You can use <code>array.argsort()</code> and ignore the values corresponding to change in ids:</p>

<pre><code>&gt;&gt;&gt; id_ind = ids.argsort(kind='mergesort')
&gt;&gt;&gt; times_diffs = np.diff(times[id_ind])
array([ 0.2, -0.2,  0.3,  0.6, -1.1,  1.2])
</code></pre>

<p>To see which values you need to discard, you could use a Counter to count the number of times per id (<code>from collections import Counter</code>)</p>

<p>or just sort ids, and see where its diff is nonzero: these are the indices where id change, and where you time diffs are irrelevant:</p>

<pre><code>times_diffs[np.diff(ids[id_ind]) == 0] # ids[id_ind] being the sorted indices sequence
</code></pre>

<p>and finally you can split this array with np.split and np.where:</p>

<pre><code>np.split(times_diffs, np.where(np.diff(ids[id_ind]) != 0)[0])
</code></pre>

<p>As you mentionned in your comment, <code>argsort()</code> default algorithm (quicksort) might not preserve order between equals times, so the <code>argsort(kind='mergesort')</code> option must be used.</p>
"
39873434,3510736.0,2016-10-05T12:02:58Z,39872981,2,"<p>Say you <code>np.argsort</code> by <code>ids</code>:</p>

<pre><code>inds = np.argsort(ids, kind='mergesort')
&gt;&gt;&gt; array([1, 3, 2, 4, 5, 0, 6])
</code></pre>

<p>Now sort <code>times</code> by this, <code>np.diff</code>, and prepend a <code>nan</code>:</p>

<pre><code>diffs = np.concatenate(([np.nan], np.diff(times[inds])))
&gt;&gt;&gt; diffs 
array([ nan,  0.2, -0.2,  0.3,  0.6, -1.1,  1.2])
</code></pre>

<p>These differences are correct except for the boundaries. Let's calculate those</p>

<pre><code>boundaries = np.concatenate(([False], ids[inds][1: ] == ids[inds][: -1]))
&gt;&gt;&gt; boundaries
array([False,  True, False,  True,  True, False,  True], dtype=bool)
</code></pre>

<p>Now we can just do</p>

<pre><code>diffs[~boundaries] = np.nan
</code></pre>

<hr>

<p>Let's see what we got:</p>

<pre><code>&gt;&gt;&gt; ids[inds]
array([0, 0, 1, 1, 1, 2, 2])

&gt;&gt;&gt; times[inds]
array([ 0.3,  0.5,  0.3,  0.6,  1.2,  0.1,  1.3])

&gt;&gt;&gt; diffs
array([ nan,  0.2,  nan,  0.3,  0.6,  nan,  1.2])
</code></pre>
"
39874178,3510736.0,2016-10-05T12:37:11Z,39872981,0,"<p>I'm adding another answer, since, even though these things are possible in <code>numpy</code>, I think that the higher-level <a href=""http://pandas.pydata.org/"" rel=""nofollow""><code>pandas</code></a> is much more natural for them.</p>

<p>In <code>pandas</code>, you could do this in one step, after creating a DataFrame:</p>

<pre><code>df = pd.DataFrame({'ids': ids, 'times': times})

df['diffs'] = df.groupby(df.ids).transform(pd.Series.diff)
</code></pre>

<p>This gives:  </p>

<pre><code>&gt;&gt;&gt; df
   ids  times  diffs
0    2    0.1    NaN
1    0    0.3    NaN
2    1    0.3    NaN
3    0    0.5    0.2
4    1    0.6    0.3
5    1    1.2    0.6
6    2    1.3    1.2
</code></pre>
"
39874828,613246.0,2016-10-05T13:06:53Z,39872981,1,"<p>The <a href=""https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP"" rel=""nofollow"">numpy_indexed</a> package (disclaimer: I am its author) contains efficient and flexible functionality for these kind of grouping operations:</p>

<pre><code>import numpy_indexed as npi
unique_ids, diffed_time_groups = npi.group_by(keys=ids, values=times, reduction=np.diff)
</code></pre>

<p>Unlike pandas, it does not require a specialized datastructure just to perform this kind of rather elementary operation.</p>
"
39876069,284795.0,2016-10-05T14:00:41Z,39844473,1,"<p>Here's a solution in one loop:</p>

<pre><code>def max_product(A):
    """"""Calculate maximal product of elements of A""""""
    product = 1
    greatest_negative = float(""-inf"") # greatest negative multiplicand so far

    for x in A:
        product = max(product, product*x, key=abs)
        if x &lt;= -1:
            greatest_negative = max(x, greatest_negative)

    return max(product, product // greatest_negative)

assert max_product([2,3]) == 6
assert max_product([-2,-3]) == 6
assert max_product([-1, -2, -3, 0, 2]) == 12
assert max_product([]) == 1
assert max_product([-5]) == 1
</code></pre>

<p>Extra credit: what if the integer constraint were relaxed? What extra information do you need to collect during the loop?</p>
"
39876160,4244609.0,2016-10-05T14:05:33Z,39858238,1,"<p>I suggest this:</p>

<pre><code>dataframes = []
#creating index
map_res = map_res.zipWithIndex()
# setting index as key
map_res = map_res.map(lambda x: (x[1],x[0]))
# creating one spark df per element
for i in range(0, map_res.count()):
    partial_dataframe_pd  = map_res.lookup(i)
    partial_dataframe = sqlContext.createDataFrame(partial_dataframe_pd)
    dataframes.append(partial_dataframe)
# concatination
result_df = dataframes.pop()
for df in dataframes:
    result_df.union(df)   
#saving
result_df.write.parquet(""..."")
</code></pre>

<p>If you have small number of partitions (2-100) then it should work rather fast. </p>
"
39876241,1881999.0,2016-10-05T14:08:59Z,39875272,0,"<p>The timeout argument to the <code>s.get()</code> function is tricky. <a href=""http://docs.python-requests.org/en/latest/user/quickstart/#timeouts"" rel=""nofollow"">Here</a> I found a good explanation for its unusual behavior. The <code>timeout</code> will stop the process if the requested url does not respond but it wouldn't stop it if it respond infinitely.</p>

<p>In your case, the connection is established nut the requested page is just sending responses in an infinite loop.</p>

<p>You can set a timeout for the whole function call: <a href=""http://stackoverflow.com/questions/2281850/timeout-function-if-it-takes-too-long-to-finish"">Timeout function if it takes too long to finish</a></p>
"
39876573,2386026.0,2016-10-05T14:22:15Z,39830598,1,"<p>I have solved this using CMake and this translates directly to using <code>autoconf</code> and <code>automake</code> and thereby makefiles.</p>

<p>The idea is to introduce the following variable</p>

<pre><code>DEPENDENCIES = `swig -M -python -c++ -I. example.i | sed 's/\//g'`
</code></pre>

<p>and make your target depend on this. The above generates a list of dependencies of all headers and <code>.i</code> files your SWIG interface file may include.</p>
"
39876686,4420124.0,2016-10-05T14:27:02Z,39876608,2,"<p>You can do veeeery simple trick:</p>

<pre><code>from os import system
while True:
    system('clear')  # or 'cls' if you are running windows
    user_input = input('say something:')
    print('result: ' + user_input)
    input()
</code></pre>
"
39877121,648265.0,2016-10-05T14:44:42Z,39830598,3,"<p>Producing any kind of target from any kind of source, that's the essence of a makefile:</p>

<pre><code>.i.cpp:
    swig -python -c++ $&lt;
</code></pre>

<p>This elegance will, however, break with <code>nmake</code> (<a href=""https://www.gnu.org/software/make/manual/html_node/Chained-Rules.html"" rel=""nofollow"">as opposed to GNU <code>make</code></a>) if the <code>.cpp</code> file is missing because <a href=""http://stackoverflow.com/questions/4808674/nmake-inference-rules-limited-to-depth-of-1""><code>nmake</code> doesn't try to chain inference rules through a missing link</a>.<br>
Moreover, it will break silently and ""build"" from stale versions of the files that are later in the build chain (which includes the resulting executable) if they are present.</p>

<p>Possible <s>kludges</s> workarounds here (save for ditching <code>nmake</code>, of course) are:</p>

<ul>
<li><p>invoke <code>nmake</code> multiple times, first, to generate all files that are an intermediate steps between two <a href=""https://msdn.microsoft.com/en-us/library/968fkazs.aspx"" rel=""nofollow"">inference rules</a> (which can in turn require multiple invocations if they are generated from one another), and then for the final targets</p>

<ul>
<li><p>This requires an external script which can very well be another makefile. E.g.:
move the current <code>Makefile</code> to <code>main_makefile</code> and create a new <code>Makefile</code> with commands for the main target like this:</p>

<pre><code>python -c ""import os,os.path,subprocess;
           subprocess.check_call(['nmake', '/F', 'main_makefile']
               +[os.path.splitext(f)[0]+'.cpp'
                 for f in os.listdir('.') if os.path.isfile(f)
                                             and f.endswith('.i')])""
nmake /F main_makefile
</code></pre></li>
</ul></li>
<li><p>do not rely solely on inference rules but have an explicit rule for each <code>.cpp</code> to be produced (that's what CMake does btw)</p>

<ul>
<li><p>this asks for the relevant part of Makefile to be autogenerated. That part can be <code>!INCLUDE</code>'d, but still, external code is needed to do the generation before <code>nmake</code> gets to work on the result. Example code (again, in Python):</p>

<pre class=""lang-python prettyprint-override""><code>import os,os.path,subprocess
for f in os.listdir('.') if os.path.isfile(f) and f.endswith('.i'):
    print '""%s"": ""%s""'%(os.path.splitext(f)[0]+'.cxx',f)
    #quotes are to allow for special characters,
    # see https://msdn.microsoft.com/en-us/library/956d3677.aspx
    #command is not needed, it will be added from the inferred rule I gave
    # in the beginning, see http://www.darkblue.ch/programming/Namke.pdf, p.41 (567)
</code></pre></li>
</ul></li>
</ul>
"
39877519,1577947.0,2016-10-05T15:01:23Z,39866976,0,"<p>Line 3 below is the solution. I needed to import the <code>tkinter.messagebox</code> module explicitly.</p>

<pre><code>import tkinter as tk
from tkinter import ttk
import tkinter.messagebox

''' pyinstaller.exe --onefile --windowed run.py '''

def someotherfunction():
  '''
  This is some message that I
  want to appear but it currently doesn\'t
  seem to work

  when I put try to launch a messagebox
  showinfo window...
  '''
  pass

def showhelpwindow():
  return tk.messagebox.showinfo(title='How to use this tool',
                         message=someotherfunction.__doc__)

root = tk.Tk()
helpbutton = ttk.Button(root, text='?', command=showhelpwindow, width=2)
helpbutton.grid(row=0, column=3, sticky='e')
root.mainloop()
</code></pre>
"
39879026,4727135.0,2016-10-05T16:12:40Z,39779538,2,"<p>Indeed, the information you need is not stored in the <code>ast</code>. I don't know the details of what you need, but it looks like you could use the <code>tokenize</code> module from the standard library. The idea is that every logical Python statement is ended by a <code>NEWLINE</code> token (also it could be a semicolon, but as I understand it is not your case). I tested this approach with such file:</p>

<pre><code># first comment
class SomethingRecord:
    description = ('line 1'
                   'line 2'
                   'line 3')

class SomethingRecord2:
    description = ('line 1',
                   'line 2',
                   # comment in the middle

                   'line 3')

class SomethingRecord3:
    description = 'line 1' \
                  'line 2' \
                  'line 3'
    whatever = 'line'

class SomethingRecord3:
    description = 'line 1', \
                  'line 2', \
                  'line 3'
                  # last comment
</code></pre>

<p>And here is what I propose to do:</p>

<pre><code>import tokenize
from io import BytesIO
from collections import defaultdict

with tokenize.open('testmod.py') as f:
    code = f.read()
    enc = f.encoding

rl = BytesIO(code.encode(enc)).readline
tokens = list(tokenize.tokenize(rl))

token_table = defaultdict(list)  # mapping line numbers to token numbers
for i, tok in enumerate(tokens):
    token_table[tok.start[0]].append(i)

def find_end(start):
    i = token_table[start][-1]  # last token number on the start line
    while tokens[i].exact_type != tokenize.NEWLINE:
        i += 1
    return tokens[i].start[0]

print(find_end(3))
print(find_end(8))
print(find_end(15))
print(find_end(21))
</code></pre>

<p>This prints out:</p>

<pre><code>5
12
17
23
</code></pre>

<p>This seems to be correct, you could tune this approach depending on what exactly you need. <code>tokenize</code> is more verbose than <code>ast</code> but also more flexible. Of course the best approach is to use them both for different parts of your task.</p>

<hr>

<p><strong>EDIT:</strong> I tried this in Python 3.4, but I think it should also work in other versions.</p>
"
39879187,901925.0,2016-10-05T16:22:30Z,39876136,1,"<p>So for a smaller <code>m</code>:</p>

<pre><code>In [513]: m = np.mgrid[:3,:4]
In [514]: m.shape
Out[514]: (2, 3, 4)
In [515]: m
Out[515]: 
array([[[0, 0, 0, 0],
        [1, 1, 1, 1],
        [2, 2, 2, 2]],

       [[0, 1, 2, 3],
        [0, 1, 2, 3],
        [0, 1, 2, 3]]])
In [516]: ll = list(zip(*(v.ravel() for v in m)))
In [517]: ll
Out[517]: 
[(0, 0),
 (0, 1),
 (0, 2),
 ...
 (2, 3)]
In [518]: a2=np.empty(m.shape[1:], dtype=object)
In [519]: a2.ravel()[:] = ll
In [520]: a2
Out[520]: 
array([[(0, 0), (0, 1), (0, 2), (0, 3)],
       [(1, 0), (1, 1), (1, 2), (1, 3)],
       [(2, 0), (2, 1), (2, 2), (2, 3)]], dtype=object)
</code></pre>

<p>Making an empty of the right shape, and filling it via <code>[:]=</code> is the best way of controlling the <code>object</code> depth of such an array.  <code>np.array(...)</code> defaults to the highest possible dimension, which in this case would 3d. </p>

<p>So the main question is - is there a better way of constructing that <code>ll</code> list of tuples. </p>

<pre><code> a2.ravel()[:] = np.array(ll)
</code></pre>

<p>does not work, complaining <code>(12,2) into shape (12)</code>.</p>

<p>Working backwards, if I start with an array like <code>ll</code>, turn it into a nested list, the assignment works, except elements of <code>a2</code> are lists, not tuples:</p>

<pre><code>In [533]: a2.ravel()[:] = np.array(ll).tolist()
In [534]: a2
Out[534]: 
array([[[0, 0], [0, 1], [0, 2], [0, 3]],
       [[1, 0], [1, 1], [1, 2], [1, 3]],
       [[2, 0], [2, 1], [2, 2], [2, 3]]], dtype=object)
</code></pre>

<p><code>m</code> shape is (2,3,4)<code>and</code>np.array(ll)<code>shape is (12,2), then</code>m.reshape(2,-1).T` produces the same thing.</p>

<pre><code>a2.ravel()[:] = m.reshape(2,-1).T.tolist()
</code></pre>

<p>I could have transposed first, and then reshaped, <code>m.transpose(1,2,0).reshape(-1,2)</code>.</p>

<p>To get tuples I need to pass the reshaped array through a comprehension:</p>

<pre><code>a2.ravel()[:] = [tuple(l) for l in m.reshape(2,-1).T]
</code></pre>

<p>===============</p>

<p><code>m.transpose(1,2,0).astype(object)</code> is still 3d; it's just changed the integers with pointers to integers.  There's a 'wall' between the array dimensions and the dtype.  Things like reshape and transpose only operate on the dimensions, and don't penetrate that wall, or move it.  Lists are pointers all the way down. Object arrays use pointers only at the <code>dtype</code> level.</p>

<p>Don't be afraid of the <code>a2.ravel()[:]=</code> expression.  <code>ravel</code> is a cheap reshape, and assignment to a flatten version of an array may actually be faster than assignment to 2d version.  After all, the data (in this case pointers) is stored in a flat data buffer.</p>

<p>But (after playing around a bit) I can do the assignment without the ravel or reshape (still need the <code>tolist</code> to move the <code>object</code> boundary).  The list nesting has to match the <code>a2</code> shape down to 'object' level.</p>

<pre><code>a2[...] = m.transpose(1,2,0).tolist()   # even a2[:] works
</code></pre>

<p>(This brings to mind a discussion about giving <code>np.array</code> a <code>maxdim</code> parameter - <a href=""http://stackoverflow.com/questions/38774922/prevent-numpy-from-creating-a-multidimensional-array"">Prevent numpy from creating a multidimensional array</a>).</p>

<p>The use of <code>tolist</code> seems like an inefficiency.  But if the elements of <code>a2</code> are tuples (or rather pointers to tuples), those tuples have to be created some how.  The <code>c</code> databuffer of the <code>m</code> cannot be viewed as a set of tuples.  <code>tolist</code> (with the <code>[tuple...]</code> comprehension) might well be the most efficient way of creating such objects.</p>

<p>==============</p>

<p>Did I note that the transpose can be indexed, producing 2 element arrays with the right numbers?</p>

<pre><code>In [592]: m.transpose(1,2,0)[1,2]
Out[592]: array([1, 2])
In [593]: m.transpose(1,2,0)[0,1]
Out[593]: array([0, 1])
</code></pre>

<p>==================</p>

<p>Since the <code>tolist</code> for a structured array uses tuples, I could do:</p>

<pre><code>In [598]: a2[:]=m.transpose(1,2,0).copy().view('i,i').reshape(a2.shape).tolist()

In [599]: a2
Out[599]: 
array([[(0, 0), (0, 1), (0, 2), (0, 3)],
       [(1, 0), (1, 1), (1, 2), (1, 3)],
       [(2, 0), (2, 1), (2, 2), (2, 3)]], dtype=object)
</code></pre>

<p>and thus avoid the list comprehension.  It's not necessarily simpler or faster.</p>
"
39879196,2069380.0,2016-10-05T16:22:55Z,39779538,6,"<p>As a workaround you can change:</p>

<pre><code>    description = 'line 1' \
              'line 2' \
              'line 3'
</code></pre>

<p>to:</p>

<pre><code>    description = 'new value'; tmp = 'line 1' \
              'line 2' \
              'line 3'
</code></pre>

<p>etc. </p>

<p>It is a simple change but indeed ugly code produced.</p>
"
39879256,2842758.0,2016-10-05T16:26:24Z,39868990,2,"<p>Thanks to L3viathan's comment now it is clear. The logic is beautiful.</p>

<p>Let's assume <code>a(n)</code> is a number of strings of <code>n</code> digits without ""13"" in it. If we know all the good strings for <code>n-1</code>, we can add one more digit to the left of each string and calculate <code>a(n)</code>. As we can combine previous digits with any of 10 new, we will get <code>10*a(n-1)</code> different strings. But we must subtract the number of strings, which now starts with ""13"" which we wrongly summed like OK at previous step. There is <code>a(n-2)</code> of such wrongly adde strings. So <code>a(n+1) = 10*a(n-1) - a(n-2)</code>. That is it. Such simple.</p>

<p>What is even more interesting is that this sequence can be calculated without iterations with a formula <a href=""https://oeis.org/A004189"" rel=""nofollow"">https://oeis.org/A004189</a> But practically that doesn't helps much, as the formula requires floating point calculations which will lead to rounding and would not work for big n (will give answer with some mistake).</p>

<p>Nevertheless the original sequence is quite easy to calculate and it doesn't need to store all the previous values, just the last two. So here is the code</p>

<pre><code>def number_of_strings(n):
    result = 0
    result1 = 99
    result2 = 10
    if n == 1:
        return result2
    if n == 2:
        return result1
    for i in range(3, n+1):
        result = 10*result1 - result2
        result2 = result1
        result1 = result
    return result 
</code></pre>

<p>This one is several orders faster than my previous suggestion. And memory consumption is now just O(n)</p>

<p>P.S. If you run this with Python2, you'd better change <code>range</code> to <code>xrange</code></p>
"
39879949,584846.0,2016-10-05T17:07:32Z,39675844,1,"<p>The simple answer is that they are different languages.  Yes, Python coroutines are stackful, first-class and asymmetric.  See this answer: <a href=""http://stackoverflow.com/q/715758/584846"">Coroutine vs Continuation vs Generator</a></p>

<p>From the Lua <a href=""https://www.lua.org/pil/9.1.html"" rel=""nofollow"">documentation</a>:</p>

<blockquote>
  <p>Some people call asymmetric coroutine semi-coroutines (because they
  are not symmetrical, they are not really co). However, other people
  use the same term semi-coroutine to denote a restricted implementation
  of coroutines, where a coroutine can only suspend its execution when
  it is not inside any auxiliary function, that is, when it has no
  pending calls in its control stack. In other words, only the main body
  of such semi-coroutines can yield. A generator in Python is an example
  of this meaning of semi-coroutines.</p>
  
  <p>Unlike the difference between symmetric and asymmetric coroutines, the
  difference between coroutines and generators (as presented in Python)
  is a deep one; generators are simply not powerful enough to implement
  several interesting constructions that we can write with true
  coroutines. Lua offers true, asymmetric coroutines. Those that prefer
  symmetric coroutines can implement them on top of the asymmetric
  facilities of Lua. It is an easy task. (Basically, each transfer does
  a yield followed by a resume.)</p>
</blockquote>

<p>Also, see this discussion on Python's developer mail list: <a href=""https://mail.python.org/pipermail/python-dev/2015-April/139695.html"" rel=""nofollow"">PEP 492: What is the real goal?</a></p>
"
39880566,510937.0,2016-10-05T17:46:30Z,39880411,2,"<p>You can <code>import __main__</code> to access the notebook scope:</p>

<pre><code>import unittest
from IPython.display import Markdown, display

import __main__


def printmd(string):
    display(Markdown(string))

class Tests(unittest.TestCase):

    def check_add_2(self, add_2):
        val = 5
        self.assertAlmostEqual(add_2(val), 7)

    def check_add_n(self, add_n):
        __main__.n = 6
        val = 5
        self.assertAlmostEqual(add_n(val), 11)


check = Tests()
def run_check(check_name, func, hint=False):
    try:
        getattr(check, check_name)(func)
    except check.failureException as e:
        printmd('**&lt;span style=""color: red;""&gt;FAILED&lt;/span&gt;**')
        if hint:
            print('Hint:',  e)
        return
    printmd('**&lt;span style=""color: green;""&gt;PASSED&lt;/span&gt;**')
</code></pre>

<p>This gives me a <code>PASSED</code> output.</p>

<hr>

<p>This works because when you execute a python file that file is stored in <code>sys.modules</code> as the <code>__main__</code> module. This is precisely why the <code>if __name__ == '__main__':</code> idiom is used. It is possible to import such module and since it is already in the module cache it will not re-execute it or anything.</p>
"
39881020,4983450.0,2016-10-05T18:13:10Z,39880209,2,"<p>Normally you will use the <code>diff()</code> function to calculate the adjacent difference and you can convert the index to a normal series and then use the <code>diff()</code> function which gives a series of <code>time delta</code> data type:</p>

<pre><code>df.index.to_series().diff()

# 2016-08-10      NaT
# 2016-08-12   2 days
# 2016-08-14   2 days
# 2016-08-17   3 days
# 2016-08-18   1 days
# dtype: timedelta64[ns]
</code></pre>

<p>To convert the time delta data type to numeric types:</p>

<pre><code>import numpy as np
df['Delta_Days'] = (df.index.to_series().diff() / np.timedelta64(1, 'D')).astype(float)

df

#              Delta  Delta_Days
#2016-08-10 0.006619         NaN
#2016-08-12 0.006595         2.0
#2016-08-14 0.006595         2.0
#2016-08-17 0.006595         3.0
#2016-08-18 0.006595         1.0
</code></pre>
"
39882959,3642398.0,2016-10-05T20:17:39Z,39882645,2,"<p>Unfortunately, you <strong>cannot</strong> get this information from the Selenium webdriver, nor will you be able to any time in the near future it seems. An excerpt from <a href=""https://github.com/seleniumhq/selenium-google-code-issue-archive/issues/141"" rel=""nofollow"">a very long conversation on the subject</a>:</p>

<blockquote>
  <p>This feature isn't going to happen.</p>
</blockquote>

<p>The gist of the main reason being, from what I gather from the discussion, that the webdriver is meant for ""driving the browser"", and extending the API beyond that primary goal will, in the opinion of the developers, cause the overall quality and reliability of the API to suffer. </p>

<p>One potential workaround that I have seen suggested in a number of places, including the conversation linked above, is to use <a href=""https://github.com/lightbody/browsermob-proxy"" rel=""nofollow"">BrowserMob Proxy</a>, which can be used to capture HTTP content, and <a href=""https://github.com/lightbody/browsermob-proxy#using-with-selenium"" rel=""nofollow"">can be used with selenium</a> - though the linked example does not use the Python selenium API. It does seem that there is <a href=""https://github.com/AutomatedTester/browsermob-proxy-py"" rel=""nofollow"">a Python wrapper for BrowserMob Proxy</a>, but I cannot vouch for it's efficacy since I have never used it. </p>
"
39883010,4629534.0,2016-10-05T20:21:49Z,39882645,-1,"<p>You are meaning HTTP header data, right? This is not really the scope of Selenium: <a href=""http://www.seleniumhq.org/"" rel=""nofollow"">Selenium automates browsers. That's it!</a> So if you cannot do it with your browser (and I don't know of any way), Selenium is the wrong tool to use. However, if you can do it with JavaScript you could use <code>driver.execute_script(script, *args)</code> as explained <a href=""http://selenium-python.readthedocs.io/api.html"" rel=""nofollow"">here</a>.</p>
"
39883694,2901002.0,2016-10-05T21:07:49Z,39883656,1,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html"" rel=""nofollow""><code>combine_first</code></a>:</p>

<pre><code>print (df1.combine_first(df2))
       A      B      C      D
0   22.0   23.0   24.0   25.0
1    2.0    4.0    6.0    8.0
2   56.0   58.0   59.0   60.0
3  100.0  101.0  102.0  103.0
</code></pre>

<p>Or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""nofollow""><code>fillna</code></a>:</p>

<pre><code>print (df1.fillna(df2))
       A      B      C      D
0   22.0   23.0   24.0   25.0
1    2.0    4.0    6.0    8.0
2   56.0   58.0   59.0   60.0
3  100.0  101.0  102.0  103.0
</code></pre>

<p>Or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.update.html"" rel=""nofollow""><code>update</code></a>:</p>

<pre><code>df1.update(df2)
print (df1)
       A      B      C      D
0   22.0   23.0   24.0   25.0
1    2.0    4.0    6.0    8.0
2   56.0   58.0   59.0   60.0
3  100.0  101.0  102.0  103.0
</code></pre>
"
39883697,624829.0,2016-10-05T21:07:57Z,39883656,1,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html"" rel=""nofollow"">combine_first</a></p>

<pre><code>df1.combine_first(df2)
</code></pre>
"
39884043,4099593.0,2016-10-05T21:31:59Z,39883994,4,"<p>You can use a <a href=""https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"" rel=""nofollow"">list comprehension</a> to solve your issue. </p>

<pre><code>&gt;&gt;&gt; input_elements = [""a"", ""b"", ""c"", ""d""]
&gt;&gt;&gt; my_array = [""1"", ""2"", ""3"", ""4""]
&gt;&gt;&gt; [my_array+[i] for i in input_elements]
</code></pre>

<p>The result looks like</p>

<pre><code>&gt;&gt;&gt; from pprint import pprint
&gt;&gt;&gt; pprint([my_array+[i] for i in input_elements])
[['1', '2', '3', '4', 'a'],
 ['1', '2', '3', '4', 'b'],
 ['1', '2', '3', '4', 'c'],
 ['1', '2', '3', '4', 'd']]
</code></pre>

<p>See <a href=""http://stackoverflow.com/questions/34835951/what-does-list-comprehension-in-python-mean-how-does-it-work-and-how-can-i-us"">What does &quot;list comprehension&quot; in Python mean? How does it work and how can I use it?</a> for more details about them. </p>
"
39884088,1952500.0,2016-10-05T21:34:46Z,39883994,1,"<p>You need to make a copy of the old list in the loop:</p>

<pre><code>input_elements = [""a"", ""b"", ""c"", ""d""]
my_array = [""1"", ""2"", ""3"", ""4""]
new_list = []
for e in input_elements:
  tmp_list = list(my_array)
  tmp_list.append(e)
  new_list.append(tmp_list)

print(new_list)
</code></pre>

<p>Output:</p>

<pre><code>[['1', '2', '3', '4', 'a'], ['1', '2', '3', '4', 'b'], ['1', '2', '3', '4', 'c'], ['1', '2', '3', '4', 'd']]
</code></pre>

<p>Note that <code>tmp_list = list(my_array)</code> makes a new copy of <code>my_array</code>.</p>
"
39884191,2060137.0,2016-10-05T21:42:48Z,39883994,1,"<p>I'm assuming the output you're getting is:</p>

<pre><code>['1', '2', '3', '4', 'a', 'b', 'c', 'd']
</code></pre>

<p>...because, that's what I'm getting.</p>

<p>The problem is, in your loop, you're simply adding a new element to the existing array, then printing the ""grand total.""  So, you add a, then you add b, then you add c, then d... all to the same array, then printing out the whole shebang.</p>

<p>The easiest solution for your particular problem is, in your for loop, print the array as it is, with the <code>e</code> selection concatenated.  Like so:</p>

<pre><code>input_elements = [""a"", ""b"", ""c"", ""d""]
my_array = [""1"", ""2"", ""3"", ""4""]

for e in input_elements:
  print my_array + [e]
</code></pre>

<p>That way, you're printing the array with the extra element, without actually affecting the original array... keeping it ""clean"" to loop back through and add the next element.</p>

<p>This method allows you to achieve the desired result without having to result to extra memory allocation or unnecessary variables.</p>

<p>If you have other things to do during the <code>for</code> loop, you could always add the element, then remove it after processing using the <code>pop</code> function, like so:</p>

<pre><code>for e in input_elements:
  my_array.append(e)
  print my_array
  # Do some other nifty stuff
  my_array.pop()
</code></pre>

<p>Another option is to use List Comprehension, which allows you to iterate through an array as more of an inherent statement:</p>

<p><code>print [my_array+[e] for e in input_elements]</code></p>
"
39886191,6580543.0,2016-10-06T01:39:39Z,39885770,3,"<p>Cool problem. I brute forced this w/out using pandas or numpy, but I got your answer (thanks for working it out). I have not tested it on anything else. I also don't know how fast it is since it only goes through each dataframe once, but does not do any vectorization.</p>

<pre><code>import pandas as pd
#############################################################################
#Preparing the dataframes
times_1 = [""2016-10-05 11:50:02.000734"",""2016-10-05 11:50:03.000033"",
           ""2016-10-05 11:50:10.000479"",""2016-10-05 11:50:15.000234"",
           ""2016-10-05 11:50:37.000199"",""2016-10-05 11:50:49.000401"",
           ""2016-10-05 11:50:51.000362"",""2016-10-05 11:50:53.000424"",
           ""2016-10-05 11:50:53.000982"",""2016-10-05 11:50:58.000606""]
times_1 = [pd.Timestamp(t) for t in times_1]
vals_1 = [0.50,0.25,0.50,0.25,0.50,0.50,0.25,0.75,0.25,0.75]

times_2 = [""2016-10-05 11:50:07.000537"",""2016-10-05 11:50:11.000994"",
           ""2016-10-05 11:50:19.000181"",""2016-10-05 11:50:35.000578"",
           ""2016-10-05 11:50:46.000761"",""2016-10-05 11:50:49.000295"",
           ""2016-10-05 11:50:51.000835"",""2016-10-05 11:50:55.000792"",
           ""2016-10-05 11:50:55.000904"",""2016-10-05 11:50:57.000444""]
times_2 = [pd.Timestamp(t) for t in times_2]
vals_2 = [0.50,0.50,0.50,0.50,0.50,0.75,0.75,0.25,0.75,0.75]

data_1 = pd.DataFrame({""time"":times_1,""vals"":vals_1})
data_2 = pd.DataFrame({""time"":times_2,""vals"":vals_2})
#############################################################################

shared_time = 0      #Keep running tally of shared time
t1_ind = 0           #Pointer to row in data_1 dataframe
t2_ind = 0           #Pointer to row in data_2 dataframe

#Loop through both dataframes once, incrementing either the t1 or t2 index
#Stop one before the end of both since do +1 indexing in loop
while t1_ind &lt; len(data_1.time)-1 and t2_ind &lt; len(data_2.time)-1:
    #Get val1 and val2
    val1,val2 = data_1.vals[t1_ind], data_2.vals[t2_ind]

    #Get the start and stop of the current time window
    t1_start,t1_stop = data_1.time[t1_ind], data_1.time[t1_ind+1]
    t2_start,t2_stop = data_2.time[t2_ind], data_2.time[t2_ind+1]

    #If the start of time window 2 is in time window 1
    if val1 == val2 and (t1_start &lt;= t2_start &lt;= t1_stop):
        shared_time += (min(t1_stop,t2_stop)-t2_start).total_seconds()
        t1_ind += 1
    #If the start of time window 1 is in time window 2
    elif val1 == val2 and t2_start &lt;= t1_start &lt;= t2_stop:
        shared_time += (min(t1_stop,t2_stop)-t1_start).total_seconds()
        t2_ind += 1
    #If there is no time window overlap and time window 2 is larger
    elif t1_start &lt; t2_start:
        t1_ind += 1
    #If there is no time window overlap and time window 1 is larger
    else:
        t2_ind += 1

#How I calculated the maximum possible shared time (not pretty)
shared_start = max(data_1.time[0],data_2.time[0])
shared_stop = min(data_1.time.iloc[-1],data_2.time.iloc[-1])
max_possible_shared = (shared_stop-shared_start).total_seconds()

#Print output
print ""Shared time:"",shared_time
print ""Total possible shared:"",max_possible_shared
print ""Percent shared:"",shared_time*100/max_possible_shared,""%""
</code></pre>

<p>Output:</p>

<pre><code>Shared time: 17.000521
Total possible shared: 49.999907
Percent shared: 34.0011052421 %
</code></pre>
"
39886705,2701362.0,2016-10-06T02:45:36Z,39785577,9,"<p>I was able to reproduce your problem on my Raspberry Pi 1, Model B by running your script and connecting a jumper cable between ground and GPIO27 to simulate red button presses.  (Those are pins 25 and 13 on my particular Pi model.)  </p>

<p>The python interpreter is crashing with a Segmentation Fault in the thread dedicated to polling GPIO events after <code>red</code> returns from handling a button press.  After looking at the implementation of the Python <code>GPIO</code> module, it is clear to me that it is unsafe to call <code>remove_event_detect</code> from within an event handler callback, and this is causing the crash.  In particular, removing an event handler while that event handler is currently running can lead to memory corruption, which will result in crashes (as you have seen) or other strange behaviors. </p>

<p>I suspect you are removing and re-adding the event handlers because you are concerned about getting a callback during the time when you are handing a button press.  There is no need to do this.  The GPIO module spins up a single polling thread to monitor GPIO events, and will wait for one callback to return before calling another, regardless of the number of GPIO events you are watching.</p>

<p>I suggest you simply make your calls to <code>add_event_detect</code> as your script starts, and never remove the callbacks.  Simply removing <code>add_events</code> and <code>remove_events</code> (and their invocations) from your script will correct the problem.</p>

<p>If you are interested in the details of the problem in the <code>GPIO</code> module, you can take a look at the <a href=""https://pypi.python.org/packages/c1/a8/de92cf6d04376f541ce250de420f4fe7cbb2b32a7128929a600bc89aede5/RPi.GPIO-0.6.2.tar.gz"">C source code for that module</a>.  Take a look at <code>run_callbacks</code> and <code>remove_callbacks</code> in the file <code>RPi.GPIO-0.6.2/source/event_gpio.c</code>.  Notice that both of these functions use a global chain of <code>struct callback</code> nodes.  <code>run_callbacks</code> walks the callback chain by grabbing one node, invoking the callback, and then following that node's link to the next callback in the chain.  <code>remove_callbacks</code> will walk the same callback chain, and free the memory associated with the callbacks on a particular GPIO pin.  If <code>remove_callbacks</code> is called in the middle of <code>run_callbacks</code>, the node currently held by <code>run_callbacks</code> can be freed (and have its memory potentially reused and overwritten) before the pointer to the next node is followed.</p>

<p>The reason you see this problem only for the red button is likely due to the order of calls to <code>add_event_detect</code> and <code>remove_event_detect</code> causes the memory previously used by the callback node for the red button to be reclaimed for some other purpose and overwritten earlier than the memory used from the green button callback node is similarly reclaimed.  However, be assured that the problem exists for both buttons -- it is just luck that that the memory associated with the green button callback isn't changed before the pointer to the next callback node is followed.</p>

<p>More generally, there is a concerning lack of thread synchronization around the callback chain use in the GPIO module in general, and I suspect similar problems could occur if <code>remove_event_detect</code> or <code>add_event_detect</code> are called while an event handler is running, even if events are removed from another thread!  I would suggest that the author of the <code>RPi.GPIO</code> module should use some synchronization to ensure that the callback chain can't be modified while callbacks are being made.  (Perhaps, in addition to checking whether the chain is being modified on the polling thread itself, <code>pthread_mutex_lock</code> and <code>pthread_mutex_unlock</code> could be used to prevent other threads from modifying the callback chain while it is in use by the polling thread.)</p>

<p>Unfortunately, that is not currently the case, and for this reason I suggest you avoid calling <code>remove_event_detect</code> entirely if you can avoid it.</p>
"
39888055,5766416.0,2016-10-06T05:19:17Z,39887880,0,"<p>Addon to my comment:</p>

<pre><code>def f(lst=False):
    return True if lst else False

f() #False
[f(True) for _ in range(3)] # [True True True]
</code></pre>

<p>This would work, but is this the real problem that you're trying to solve? It seems a really unintuitive use-case which can be solved better by other means.</p>
"
39888195,5249307.0,2016-10-06T05:30:50Z,39887880,8,"<p>You can determine this by inspecting the stack frame in the following sort of way:</p>

<pre><code>def f():
    try:
        raise ValueError
    except Exception as e:
        if e.__traceback__.tb_frame.f_back.f_code.co_name == '&lt;listcomp&gt;':
            return True
</code></pre>

<p>Then: </p>

<pre><code>&gt;&gt;&gt; print(f())
None
&gt;&gt;&gt; print([f() for x in range(10)])
[True, True, True, True, True, True, True, True, True, True]
</code></pre>

<p>Its not to be recommended though.  Really, its not.</p>

<h3>NOTE</h3>

<p>As it stands this only detects list comprehensions as requested.  It will not detect the use of a generator.  For example:</p>

<pre><code>&gt;&gt;&gt; print(list(f() for x in range(10)))
[None, None, None, None, None, None, None, None, None, None]
</code></pre>
"
39888287,673271.0,2016-10-06T05:38:45Z,39868762,0,"<p>Another tool to solve this type of problems is <a href=""http://scip.zib.de"" rel=""nofollow"">SCIP</a>. There is also an easy to use Python interface available on GitHub: <a href=""https://github.com/SCIP-Interfaces/PySCIPOpt"" rel=""nofollow"">PySCIPOpt</a>.</p>

<p>In general (mixed) integer programming problems are very hard to solve (NP complexity) and often even simple looking instances with only a few variables and constraints can take hours to prove the optimal solution.</p>
"
39888724,5669946.0,2016-10-06T06:12:07Z,39888013,2,"<p>You should look into the mock module (I think it's part of the unittest module now in Python 3).</p>

<p>It enables you to run tests without the need to depened in any external resources while giving you control over how the mocks interact with your code.</p>

<p>I would start from the docs in <a href=""http://www.voidspace.org.uk/python/mock/"" rel=""nofollow"">Voidspace</a></p>

<p>Here's an example:</p>

<pre><code>import unittest2 as unittest
import mock

class GetDriveSizeTestSuite(unittest.TestCase):

  @mock.patch('path/to/original/file.subprocess.Popen')
  def test_a_scenario_with_mock_subprocess(self, mock_popen):
    mock_popen.return_value.communicate.return_value = ('Expected_value', '')
    mock_popen.return_value.returncode = '0'
    self.assertEqual('expected_value', get_drive_size('some device'))
</code></pre>
"
39890190,447599.0,2016-10-06T07:36:41Z,39890147,1,"<p>It is a very common mistake to forget that the activations also take vram, not just the parameters. This takes the required vram multiple times higher than your calculation (at the very least by a <code>minibatch_size</code> factor.</p>

<p>So, in the beginning when the network is created, only the parameters are allocated. However, when the training starts, the activations (times each minibatch) get allocated, giving the behavior you observe.</p>
"
39890983,3764814.0,2016-10-06T08:19:28Z,39796852,11,"

<p><a href=""http://stackoverflow.com/a/39796953/3764814"">Sebastian's answer</a> already explains pretty well why your current attempt doesn't work.</p>

<h3>.NET</h3>

<p>Since <s>you're</s> <a href=""http://stackoverflow.com/users/1020526/revo"">revo</a> is interested in a .NET flavor workaround, the solution becomes trivial:</p>

<pre class=""lang-none prettyprint-override""><code>(?&lt;letter&gt;.)(?!.*?\k&lt;letter&gt;)(?&lt;!\k&lt;letter&gt;.+?)
</code></pre>

<p><a href=""http://regexstorm.net/tester?p=(%3F%3Cletter%3E.)(%3F!.*%3F%5Ck%3Cletter%3E)(%3F%3C!%5Ck%3Cletter%3E.%2B%3F)&amp;i=tooth%0D%0Aaardvark%0D%0Aaah%0D%0Aheh%0D%0Ahehe%0D%0Aheho%0D%0Ahahah%0D%0Ahahxyz%0D%0Ahahxyza"" rel=""nofollow"">Demo link</a></p>

<p>This works because .NET supports <strong>variable-length lookbehinds</strong>. You can also get that result with Python (see below).</p>

<p>So for each letter <code>(?&lt;letter&gt;.)</code> we check:</p>

<ul>
<li>if it's repeated further in the input <code>(?!.*?\k&lt;letter&gt;)</code> </li>
<li>if it was already encountered before <code>(?&lt;!\k&lt;letter&gt;.+?)</code><br>
(we have to skip the letter we're testing when going backwards, hence the <code>+</code>).</li>
</ul>

<hr>

<h3>Python</h3>

<p>The Python <a href=""https://pypi.python.org/pypi/regex"" rel=""nofollow"">regex module</a> also supports variable-length lookbehinds, so the regex above will work with a small syntactical change: you need to replace <code>\k</code> with <code>\g</code> (which is quite unfortunate as with this module <code>\g</code> is a group backreference, whereas with PCRE it's a recursion).</p>

<p>The regex is:</p>

<pre class=""lang-none prettyprint-override""><code>(?&lt;letter&gt;.)(?!.*?\g&lt;letter&gt;)(?&lt;!\g&lt;letter&gt;.+?)
</code></pre>

<p>And here's an example:</p>

<pre class=""lang-none prettyprint-override""><code>$ python
Python 2.7.10 (default, Jun  1 2015, 18:05:38)
[GCC 4.9.2] on cygwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import regex
&gt;&gt;&gt; regex.search(r'(?&lt;letter&gt;.)(?!.*?\g&lt;letter&gt;)(?&lt;!\g&lt;letter&gt;.+?)', 'tooth')
&lt;regex.Match object; span=(4, 5), match='h'&gt;
</code></pre>

<hr>

<h3>PCRE</h3>

<p>Ok, now things start to get dirty: since PCRE doesn't support variable-length lookbehinds, we need to <em>somehow</em> remember whether a given letter was already encountered in the input or not.</p>

<p>Unfortunately, the regex engine doesn't provide random access memory support. The best we can get in terms of generic memory is a <em>stack</em> - but that's not sufficient for this purpose, as a stack only lets us access its topmost element.</p>

<p>If we accept to restrain ourselves to a given alphabet, we can abuse capturing groups for the purpose of storing flags. Let's see this on a limited alphabet of the three letters <code>abc</code>:</p>

<pre class=""lang-none prettyprint-override""><code># Anchor the pattern
\A

# For each letter, test to see if it's duplicated in the input string
(?(?=[^a]*+a[^a]*a)(?&lt;da&gt;))
(?(?=[^b]*+b[^b]*b)(?&lt;db&gt;))
(?(?=[^c]*+c[^c]*c)(?&lt;dc&gt;))

# Skip any duplicated letter and throw it away
[a-c]*?\K

# Check if the next letter is a duplicate
(?:
  (?(da)(*FAIL)|a)
| (?(db)(*FAIL)|b)
| (?(dc)(*FAIL)|c)
)
</code></pre>

<p>Here's how that works:</p>

<ul>
<li>First, the <code>\A</code> anchor ensures we'll process the input string only once</li>
<li>Then, for each letter <code>X</code> of our alphabet, we'll set up a <em>is duplicate</em> flag <code>dX</code>:

<ul>
<li>The conditional pattern <code>(?(cond)then|else)</code> is used there:

<ul>
<li>The condition is <code>(?=[^X]*+X[^X]*X)</code> which is true if the input string contains the letter <code>X</code> twice.</li>
<li>If the condition is true, the <em>then</em> clause is <code>(?&lt;dX&gt;)</code>, which is an empty capturing group that will match the empty string.</li>
<li>If the condition is false, the <code>dX</code> group won't be matched</li>
</ul></li>
<li>Next, we lazily skip valid letters from our alphabet: <code>[a-c]*?</code></li>
<li>And we throw them out in the final match with <code>\K</code></li>
<li>Now, we're trying to match <em>one</em> letter whose <code>dX</code> flag is <em>not</em> set. For this purpose, we'll do a conditional branch: <code>(?(dX)(*FAIL)|X)</code>

<ul>
<li>If <code>dX</code> was matched (meaning that <code>X</code> is a duplicated character), we <code>(*FAIL)</code>, forcing the engine to backtrack and try a different letter.</li>
<li>If <code>dX</code> was <em>not</em> matched, we try to match <code>X</code>. At this point, if this succeeds, we know that <code>X</code> is the first non-duplicated letter.</li>
</ul></li>
</ul></li>
</ul>

<p>That last part of the pattern could also be replaced with:</p>

<pre class=""lang-none prettyprint-override""><code>(?:
  a (*THEN) (?(da)(*FAIL))
| b (*THEN) (?(db)(*FAIL))
| c (*THEN) (?(dc)(*FAIL))
)
</code></pre>

<p>Which is <em>somewhat</em> more optimized. It matches the current letter <em>first</em> and only <em>then</em> checks if it's a duplicate.</p>

<p>The full pattern for the lowercase letters <code>a-z</code> looks like this:</p>

<pre class=""lang-none prettyprint-override""><code># Anchor the pattern
\A

# For each letter, test to see if it's duplicated in the input string
(?(?=[^a]*+a[^a]*a)(?&lt;da&gt;))
(?(?=[^b]*+b[^b]*b)(?&lt;db&gt;))
(?(?=[^c]*+c[^c]*c)(?&lt;dc&gt;))
(?(?=[^d]*+d[^d]*d)(?&lt;dd&gt;))
(?(?=[^e]*+e[^e]*e)(?&lt;de&gt;))
(?(?=[^f]*+f[^f]*f)(?&lt;df&gt;))
(?(?=[^g]*+g[^g]*g)(?&lt;dg&gt;))
(?(?=[^h]*+h[^h]*h)(?&lt;dh&gt;))
(?(?=[^i]*+i[^i]*i)(?&lt;di&gt;))
(?(?=[^j]*+j[^j]*j)(?&lt;dj&gt;))
(?(?=[^k]*+k[^k]*k)(?&lt;dk&gt;))
(?(?=[^l]*+l[^l]*l)(?&lt;dl&gt;))
(?(?=[^m]*+m[^m]*m)(?&lt;dm&gt;))
(?(?=[^n]*+n[^n]*n)(?&lt;dn&gt;))
(?(?=[^o]*+o[^o]*o)(?&lt;do&gt;))
(?(?=[^p]*+p[^p]*p)(?&lt;dp&gt;))
(?(?=[^q]*+q[^q]*q)(?&lt;dq&gt;))
(?(?=[^r]*+r[^r]*r)(?&lt;dr&gt;))
(?(?=[^s]*+s[^s]*s)(?&lt;ds&gt;))
(?(?=[^t]*+t[^t]*t)(?&lt;dt&gt;))
(?(?=[^u]*+u[^u]*u)(?&lt;du&gt;))
(?(?=[^v]*+v[^v]*v)(?&lt;dv&gt;))
(?(?=[^w]*+w[^w]*w)(?&lt;dw&gt;))
(?(?=[^x]*+x[^x]*x)(?&lt;dx&gt;))
(?(?=[^y]*+y[^y]*y)(?&lt;dy&gt;))
(?(?=[^z]*+z[^z]*z)(?&lt;dz&gt;))

# Skip any duplicated letter and throw it away
[a-z]*?\K

# Check if the next letter is a duplicate
(?:
  a (*THEN) (?(da)(*FAIL))
| b (*THEN) (?(db)(*FAIL))
| c (*THEN) (?(dc)(*FAIL))
| d (*THEN) (?(dd)(*FAIL))
| e (*THEN) (?(de)(*FAIL))
| f (*THEN) (?(df)(*FAIL))
| g (*THEN) (?(dg)(*FAIL))
| h (*THEN) (?(dh)(*FAIL))
| i (*THEN) (?(di)(*FAIL))
| j (*THEN) (?(dj)(*FAIL))
| k (*THEN) (?(dk)(*FAIL))
| l (*THEN) (?(dl)(*FAIL))
| m (*THEN) (?(dm)(*FAIL))
| n (*THEN) (?(dn)(*FAIL))
| o (*THEN) (?(do)(*FAIL))
| p (*THEN) (?(dp)(*FAIL))
| q (*THEN) (?(dq)(*FAIL))
| r (*THEN) (?(dr)(*FAIL))
| s (*THEN) (?(ds)(*FAIL))
| t (*THEN) (?(dt)(*FAIL))
| u (*THEN) (?(du)(*FAIL))
| v (*THEN) (?(dv)(*FAIL))
| w (*THEN) (?(dw)(*FAIL))
| x (*THEN) (?(dx)(*FAIL))
| y (*THEN) (?(dy)(*FAIL))
| z (*THEN) (?(dz)(*FAIL))
)
</code></pre>

<p>And here's the <a href=""https://regex101.com/r/Mhwxog/1"" rel=""nofollow"">demo on regex101</a>, complete with unit tests.</p>

<p>You can expand on this pattern if you need a larger alphabet, but obviously this is <em>not</em> a general-purpose solution. It's primarily of educational interest and should <em>not</em> be used for any serious application.</p>

<hr>

<p>For other flavors, you may try to tweak the pattern to replace PCRE features with simpler equivalents:</p>

<ul>
<li><code>\A</code> becomes <code>^</code></li>
<li><code>X (*THEN) (?(dX)(*FAIL))</code> can be replaced with <code>(?(dX)(?!)|X)</code></li>
<li>You may throw away the <code>\K</code> and replace the last noncapturnig group <code>(?:</code>...<code>)</code> with a named group like <code>(?&lt;letter&gt;</code>...<code>)</code> and treat its content as the result.</li>
</ul>

<p>The only required but somewhat unusual construct is the conditional group <code>(?(cond)then|else)</code>.</p>
"
39892096,6523882.0,2016-10-06T09:14:55Z,39765738,1,"<p>I would suggest that you choose the number of clusters (K) to be much smaller than the number of training examples you have in your data set. It is not right to run the K-Means algorithm when the number of clusters you desire is greater than or equal to the number of training examples.
The error occurs when you try to pass the blaze object with an undesirable shape, to the KMeans function.
Please check : 
<a href=""https://blaze.readthedocs.io/en/latest/csv.html"" rel=""nofollow"">https://blaze.readthedocs.io/en/latest/csv.html</a></p>
"
39892264,6551368.0,2016-10-06T09:24:27Z,39888949,-1,"<p>You can try to test your function with <code>timeit</code>. This <a href=""https://docs.python.org/2/library/timeit.html"" rel=""nofollow"">doc</a>
 could be helpful.</p>

<p>Or the same magic function <code>%%timeit</code> in Jupyter notebook. You just need to write <code>%%timeit func(data)</code> and you will get a response with the assessment of your function. This <a href=""https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/"" rel=""nofollow"">paper</a> could help you with it.</p>
"
39892785,6931140.0,2016-10-06T09:49:49Z,39750879,-3,"<pre><code>def find_shortest_path(graph, start, end, path=[]):
    path = path + [start]
    if start == end:
        return path
    if start not in graph:
        return None
    shortest = None
    for node in graph[start]:
        if node not in path:
            newpath = find_shortest_path(graph, node, end, path)
            if newpath:
                if not shortest or len(newpath) &lt; len(shortest):
                    shortest = newpath
    return shortest


def get_it_done(choices, number):
    mapping = {}
    graph = {} 

    for choice in choices:
        if choice in number:
            _from = number.index(choice)
            _to = _from + len(choice)
            mapping.setdefault((_from, _to), choice)

    items = sorted(mapping.items(), key=lambda x: x[0])
    for _range, value in items:
        _from, _to = _range
        graph.setdefault(_from, []).append(_to)
    start = 0
    end = _range[1] #this is hack, works only in python 2.7
    path = find_shortest_path(graph, start, end) 
    ranges = [tuple(path[i:i+2]) for i in range(len(path) - 1)]
    if len(ranges) == 1:
        return [mapping[(start, graph[start][-1])]]
    return [mapping[_range] for _range in ranges]


if __name__ == ""__main__"":
    examples = [
        # Example1 -&gt;
        # Solution ['012345678910203040506070', '80', '90', '100', '200', '300', '400', '500', '600', '700', '800', '900']
        (
            [
                ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"",
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""100"", ""200"", ""300"", ""400"", ""500"", ""600"", ""700"", ""800"", ""900"",
                ""012345678910203040506070""
            ],
            ""0123456789102030405060708090100200300400500600700800900""
        ),
        ## Example2
        ## Solution ['100']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""100""
        ),
        ## Example3
        ## Solution ['101234567891020304050', '6070809010020030040050', '0600700800900']
        (
            [
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""012345678910203040506070"",
                ""101234567891020304050"",
                ""6070809010020030040050"",
                ""0600700800900""
            ],
            ""10123456789102030405060708090100200300400500600700800900""
        ),
        ### Example4
        ### Solution ['12', '34', '56', '78', '90']
        (
            [
                ""12"", ""34"", ""56"", ""78"", ""90"",
                ""890"",
            ],
            ""1234567890""
        ),
        ## Example5
        ## Solution ['12', '34']
        (
            [
                ""1"", ""2"", ""3"",
                ""12"", ""23"", ""34""
            ],
            ""1234""
        )
    ]

    for (choices, large_number) in examples:
        res = get_it_done(choices, large_number)
        print(""{0}\n{1}\n{2} --&gt; {3}"".format(
            large_number, """".join(res), res, """".join(res) == large_number))
        print('-' * 80)
</code></pre>
"
39893259,5309300.0,2016-10-06T10:12:17Z,39892460,1,"<p>Using your code as a base, you could do the following:</p>

<pre><code>old = set((line.strip() for line in open('1.txt')))
new = set((line.strip() for line in open('2.txt')))

with open('diff.txt', 'w') as diff:
    for line in new:
        if line not in old:
            diff.write('[-] {}\n'.format(line))

    for line in old:
        if line not in new:
            diff.write('[+] {}\n'.format(line))
</code></pre>

<p>There's a couple of tweaks in here:</p>

<ol>
<li>We want to read the individual lines of both the old and new
files to compare.</li>
<li>We don't have to <code>strip</code> each individual line as we have done that while reading the file.</li>
<li>We use <code>{}</code> and <code>.format()</code> to build text strings.</li>
<li>Using <code>\n</code> ensures we put each entry on a new line of our output file.</li>
<li>Using <code>with</code> for the file we are writing to lets us open it without having to call <code>close</code> and (if my knowledge is correct) allows for better handling of any program crashes once the file has been opened.</li>
</ol>
"
39893267,5811078.0,2016-10-06T10:12:46Z,39892460,0,"<p>You can try this one:</p>

<pre><code>old_f = open('1.txt')
new_f = open('2.txt')
diff = open('diff.txt', 'w')

old = [line.strip() for line in old_f]
new = [line.strip() for line in new_f]

for line in old:
    if line not in new:
        print '[-] ' + str(line)
        diff.write('[-] ' + str(line) + '\n'


for line in new:
    if line not in old:
        print '[+]' + str(line)
        diff.write('[+] ' + str(line) + '\n'

old_f.close()
new_f.close()
diff.close()
</code></pre>
"
39893322,3569627.0,2016-10-06T10:14:50Z,39892460,3,"<p>In the following solution I've used sets, so the order doesn't matter and we can do direct subtraction with the old and new to see what has changed.</p>

<p>I've also used the <code>with</code> context manager pattern for opening files, which is a neat way of ensuring they are closed again.</p>

<pre><code>def read_items(filename):
    with open(filename) as fh:
        return {line.strip() for line in fh}

def diff_string(old, new):
    return ""\n"".join(
        ['[-] %s' % gone for gone in old - new] +
        ['[+] %s' % added for added in new - old]
    )

with open('diff.txt', 'w') as fh:
    fh.write(diff_string(read_items('1.txt'), read_items('2.txt')))
</code></pre>

<p>Obviously you could print out the diff string if you wanted to.</p>
"
39893540,279858.0,2016-10-06T10:25:48Z,39892920,0,"<p>Why not compute the statistics on a subset of the original data? For example, here we compute the mean and std for just 100 points:</p>

<pre><code>sample_size = 100
data_train = np.random.rand(1000, 20, 10, 10)

# Take subset of training data
idxs = np.random.choice(data_train.shape[0], sample_size)
data_train_subset = data_train[idxs]

# Compute stats
mean = np.mean(data_train_subset, axis=(0,2,3))
std = np.std(data_train_subset, axis=(0,2,3))
</code></pre>

<p>If your data is 1.7Gb, it is highly unlikely that you need all the data to get an accurate estimation of the mean and std.</p>

<p>In addition, could you get away with fewer bits in your datatype? I'm not sure what datatype <code>caffe.io.datum_to_array</code> returns, but you could do:</p>

<pre><code>data = caffe.io.datum_to_array(datum).astype(np.float32)
</code></pre>

<p>to ensure the data is <code>float32</code> format. (If the data is currently <code>float64</code>, then this will save you half the space).</p>
"
39894511,3065657.0,2016-10-06T11:14:51Z,39894363,1,"<p>You can sort them first:</p>

<pre><code>conset = set(map(tuple, map(sorted, consarray)))
print (conset)
</code></pre>

<p>gives:</p>

<pre><code>{('10.125.255.133', '104.244.42.130')}
</code></pre>
"
39894522,3125566.0,2016-10-06T11:15:20Z,39894363,3,"<p>You could either apply <em>sorting</em> before making the <em>tuples</em>:</p>

<pre><code>conset = set(map(lambda x: tuple(sorted(x)), consarray))
</code></pre>

<p>Or use <em>fronzensets</em> instead of <em>tuples</em>:</p>

<pre><code>conset = set(map(frozenset, consarray))
</code></pre>

<p>To guarantee that the first item will be retained and the second not inserted, you could use a <em>regular</em> <code>for</code> loop:</p>

<pre><code>conset = set()
for x in consarray:
    x = frozenset(x) 
    if x in conset:
        continue
    conset.add(x)
</code></pre>
"
39894555,41316.0,2016-10-06T11:16:49Z,39855732,3,"<p>Here's a simple (python 2.x) example of how to 1 <em>not</em> use globals and 2 use a (simplistic) domain model class. </p>

<p>The point is: you should first design your domain model independently from your user interface, then write the user interface code calling on your domain model. In this case your UI is a Tkinter GUI, but the same domain model should be able to work with a command line UI, a web UI or whatever.</p>

<p>NB : for python 3.x, replace <code>Tkinter</code> with <code>tkinter</code> (lowercase) and you can't get rid of the <code>object</code> base class for <code>Model</code>.</p>

<pre><code>import random
from Tkinter import *


class Model(object):
    def __init__(self):
        self.currentMovie = 0

    def UpdateCurrentMovie(self):
        self.currentMovie = random.randint(0, 100)
        print(self.currentMovie)

    def UpdateWatched(self):
        print(self.currentMovie)

    def ExampleWithArgs(self, arg):
        print(""ExampleWithArg({})"".format(arg))


def main():
    model = Model()
    root = Tk()
    root.title(""MovieSelector9000"")
    root.geometry(""900x600"")
    app = Frame(root)
    app.grid()
    canvas = Canvas(app, width = 300, height = 75)
    canvas.pack(side = ""left"")
    button1 = Button(canvas, text = ""SetRandomMovie"", command=model.UpdateCurrentMovie)
    button2 = Button(canvas, text = ""GetRandomMovie"", command=model.UpdateWatched)
    button3 = Button(canvas, text = ""ExampleWithArg"", command=lambda: model.ExampleWithArgs(""foo""))
    button1.pack(anchor = NW, side = ""left"")
    button2.pack(anchor = NW, side = ""left"")
    button3.pack(anchor = NW, side = ""left"")
    root.mainloop()

if __name__ == ""__main__"":
    main()
</code></pre>
"
39894721,1252759.0,2016-10-06T11:25:03Z,39894363,1,"<p>Since you're using <code>numpy</code>, you can use <code>numpy.unique</code>, eg:</p>

<pre><code>a = np.array([('10.125.255.133', '104.244.42.130'), ('104.244.42.130', ' 10.125.255.133')])
</code></pre>

<p>Then <code>np.unique(a)</code> gives you:</p>

<pre><code>array(['10.125.255.133', '104.244.42.130'], dtype='&lt;U14')
</code></pre>
"
39895669,2877364.0,2016-10-06T12:12:43Z,39895330,1,"<p>First of all, is the point at ~(50, 37) <code>p</code> or <code>s+p</code>?  If <code>p</code>, that might be your problem right there!  If the Y component of your <code>p</code> variable is positive, you won't get the results you expect when you do the dot product.</p>

<p>Assuming that point is <code>s+p</code>, if a bit of Post-It scribbling is correct,</p>

<pre><code>p_len = np.linalg.norm(p)
p_hat = p / p_len
red_len = p_hat.dot(b_hat) * p_len   # red_len = |x-s|
    # because p_hat . b_hat = 1 * 1 * cos(angle) = |x-s| / |p|
red_point = s + red_len * b_hat
</code></pre>

<p>Not tested!  YMMV.  Hope this helps.</p>
"
39896204,2912349.0,2016-10-06T12:36:31Z,39894896,2,"<p>There is no equivalent syntactic sugar in matplotlib. You will have to preprocess your data, e.g.: </p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

ymin, ymax = 0, 0.9
x, y = np.random.rand(2,1000)
y[y&gt;ymax] = ymax
fig, ax = plt.subplots(1,1)
ax.plot(x, y, 'o', ms=10)
ax.set_ylim(ymin, ymax)
plt.show()
</code></pre>
"
39896349,588071.0,2016-10-06T12:43:45Z,39894896,1,"<p>you can just use <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.minimum.html"" rel=""nofollow""><code>np.minimum</code></a> on the <code>y</code> data to set anything above your upper limit to that limit. <code>np.minimum</code> calculates the minima element-wise, so only those values greater than <code>ymax</code> will be set to <code>ymax</code>.</p>

<p>For example: </p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0., np.pi*2, 30)
y = 10. * np.sin(x)

ymax = 5

fig, ax = plt.subplots(1)
ax.scatter(x, np.minimum(y, ymax))

plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/cEMSo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cEMSo.png"" alt=""enter image description here""></a></p>
"
39897500,1453822.0,2016-10-06T13:34:03Z,39897333,5,"<p>I think I would create a list of functions (I assume that all the <code>do(stuff)</code> in your example are actually different functions). Then you can use a <code>for</code> loop:</p>

<pre><code>list_of_funcs = [func1, func2, func3]
for func in list_of_funcs:
    func(stuff)
    if not condition:
        break
return (...)
</code></pre>

<p>If the conditions are different then you can also create a list of conditions (which will be a list of functions that return <code>True</code> or <code>False</code>), then you can use <code>zip</code> in the following manner:</p>

<pre><code>list_of_funcs = [func1, func2, func3]
list_of_conditions = [cond1, cond2, cond3]
for func, cond in zip(list_of_funcs, list_of_conditions):
    func(stuff)
    if not cond():
        break
return (...)
</code></pre>

<p>This way your actual code stays the same length and in the same indentation level, no matter how many functions and conditions you may have.</p>
"
39897551,1126841.0,2016-10-06T13:36:36Z,39897333,0,"<p>Refactoring your code is a <em>much</em> better idea than what I am about to suggest, but this is an option.</p>

<pre><code>class GotoEnd(Exception):
    pass

def foo(bar):

  try:
    do(stuff)
    if not condition: raise GotoEnd
    do(stuff)
    if not condition2: raise GotoEnd

    do(stuff)
    if not condition3: raise GotoEnd

    ...
  except GotoEnd:
    pass

  return (...)
</code></pre>
"
39897622,5669946.0,2016-10-06T13:40:01Z,39779538,1,"<p>My solution takes a different path: When I had to change code in another file I opened the file, found the line and got all the next lines which had a deeper indent than the first and return the line number for the first line which isn't deeper. 
I return None, None if I couldn't find the text I was looking for.
This is of course incomplete, but I think it's enough to get you through :)</p>

<pre><code>def get_all_indented(text_lines, text_in_first_line):
    first_line = None
    indent = None
    for line_num in range(len(text_lines)):
        if indent is not None and first_line is not None:
            if not text_lines[line_num].startswith(indent):
                return first_line, line_num     # First and last lines
        if text_in_first_line in text_lines[line_num]:
            first_line = line_num
            indent = text_lines[line_num][:text_lines[line_num].index(text_in_first_line)] + ' '  # At least 1 more space.
    return None, None
</code></pre>
"
39897690,5251107.0,2016-10-06T13:43:39Z,39732842,2,"<p>The css styling of Bokeh widgets for Jupyter notebooks is in <a href=""http://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css"" rel=""nofollow"">http://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css</a>, where <code>height:16px</code> for elements <code>.bk-root .bk-slick-header-column.bk-ui-state-default</code> is hardcoded. So it cannot be changed without changing the css. </p>

<p>It can be styled adhock by <code>HTML</code> function</p>

<pre><code>from IPython.core.display import HTML
HTML(""""""
&lt;style&gt;
.bk-root .bk-slick-header-column.bk-ui-state-default {
height: 25px!important;
}
&lt;/style&gt;
"""""")
</code></pre>

<p>For the persistent change css can be added to <code>custom</code> directory in Jupyter config. You can figure out where it is by calling </p>

<pre><code>jupyter --config-dir
</code></pre>

<p>By default it is <code>~/.jupyter</code>
The new css need to be in <code>~/.jupyter/custom/custom.css</code> then.</p>

<p>Before<a href=""http://i.stack.imgur.com/2vyXx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2vyXx.png"" alt=""enter image description here""></a></p>

<p>After<a href=""http://i.stack.imgur.com/a7EAU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/a7EAU.png"" alt=""enter image description here""></a></p>
"
39898113,1217358.0,2016-10-06T14:02:54Z,39895330,2,"<p>If you are using the plot to visually determine if the solution looks correct, you must plot the data using the same scale on each axis, i.e. use <code>plt.axis('equal')</code>. If the axes do not have equal scales, the angles between lines are distorted in the plot.</p>
"
39898547,3726604.0,2016-10-06T14:22:05Z,39898514,4,"<p>The list builtin will accept any iterator: </p>

<pre><code>l = list(gen_items())
</code></pre>
"
39898551,2296458.0,2016-10-06T14:22:14Z,39898514,3,"<p>You can just directly create it using <code>list</code> which will handle iterating for you</p>

<pre><code>&gt;&gt;&gt; list(gen_items())
[2, 3, 4, 5, 6, 7, 8, 9]
</code></pre>
"
39898561,5466926.0,2016-10-06T14:22:31Z,39885723,1,"<p>One way of smoothing could be to use <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"" rel=""nofollow"">convolve2d</a>:</p>

<pre><code>import numpy as np
from scipy import signal

B = np.array([[34, 100, 15],
              [62,  17, 87],
              [17,  34, 60]])
kernel = np.full((2, 2), .25)
smoothed = signal.convolve2d(B, kernel)
# [[  8.5   33.5   28.75   3.75]
#  [ 24.    53.25  54.75  25.5 ]
#  [ 19.75  32.5   49.5   36.75]
#  [  4.25  12.75  23.5   15.  ]]
</code></pre>

<p>The above pads the matrix with zeros from all sides and then calculates the mean of each 2x2 window placing the value at the center of the window.</p>

<p>If the matrices were actually larger, then using a 3x3 kernel (such as <code>np.full((3, 3), 1/9)</code>) and passing <code>mode='same'</code> to <code>convolve2d</code> would give a smoothed <code>B</code> with its shape preserved and elements ""matching"" the original. Otherwise you may need to decide what to do with the boundary values to make the shapes the same again.</p>

<p>To move <code>A</code> towards the smoothed <code>B</code>, it can be set to a chosen affine combination of the matrices using standard arithmetic operations, for instance: <code>A = .2 * A + .8 * smoothed</code>.</p>
"
39898727,2230844.0,2016-10-06T14:30:47Z,39820443,0,"<p>You can also try pythonnet:</p>

<p><a href=""http://www.python4.net"" rel=""nofollow"">python4.net</a></p>
"
39899081,3334049.0,2016-10-06T14:46:14Z,39898927,4,"<p>You need to have defined <code>camera</code> outside the scope of your methods as well. What the <code>global</code> keyword does is tell Python that you will modify that variable which you defined externally. If you haven't, you get this 
error.</p>

<p><strong>EDIT</strong></p>

<p>I didn't notice that you had already declared <code>camera</code> externally. However, you delete the variable inside the <code>Camera()</code> method, which has pretty much the same effect when you try to modify the variable again.</p>

<p><strong>EDIT 2</strong></p>

<p>Now that I can see what your code really does and what you intend to do, I don't think you should be working with a global <code>camera</code> at all, but pass it as parameter instead. This should work:</p>

<pre><code>camera_port = 0
ramp_frames = 400

def get_image(camera):
    retval, im = camera.read()
    return im

def Camera(camera):
    for i in xrange(ramp_frames):
        temp = get_image(camera)
    print(""Taking image..."")
    camera_capture = get_image(camera)
    file = ""opencv.png""
    cv2.imwrite(file, camera_capture)

def Sendmail():
    loop_value = 1
    while loop_value==1:
        try:
            urllib2.urlopen(""https://google.com"")
        except urllib2.URLError, e:
            print ""Network currently down."" 
            sleep(20)
        else:
            print ""Up and running."" 
            loop_value = 0

def Email():
    loop_value = 2
    while loop_value==2:
        try:
            camera = cv2.VideoCapture(camera_port) 
            Camera(camera)
            Sendmail()
            yag = yagmail.SMTP('email',   'pass')
            yag.send('amitaagarwal565@gmail.com', subject = ""This is    opencv.png"", contents = 'opencv.png')
            print ""done""
        except smtplib.SMTPAuthenticationError:
            print 'Retrying in 30 seconds'
            sleep(30)
        else:
            print 'Sent!'
            sleep(20)
            loop_value = 2
</code></pre>
"
39899817,2063361.0,2016-10-06T15:18:10Z,39899580,4,"<p>In order to get the same result in Python 2.6, you have to explicitly do:</p>

<pre><code>'%.12g' % float_variable
</code></pre>

<p>Better to create a custom function to do this as:</p>

<pre><code>def convert_to_my_float(float_value):
    return float('%.12g' % float_value)
</code></pre>

<hr>

<p>As per <a href=""https://docs.python.org/2/library/decimal.html#decimal-objects"" rel=""nofollow"">Python's Decimal Objects</a> Document:</p>

<blockquote>
  <p>Changed in version 2.6: leading and trailing whitespace characters are
  permitted when creating a Decimal instance from a string.</p>
  
  <p>Changed in version 2.7: The argument to the constructor is now
  permitted to be a float instance.</p>
</blockquote>

<p>The answer to <em>Why they are behaving differently?</em> is, because <code>float.__repr__()</code> and <code>float.__str__()</code> methods in Python 2.7 changed.</p>
"
39899896,2336654.0,2016-10-06T15:21:36Z,39899005,2,"<p>create a custom function to flatten <code>columnB</code> then use <code>pd.concat</code></p>

<pre><code>def flatten(js):
    return pd.DataFrame(js).set_index('pos').squeeze()

pd.concat([df.drop(['columnA', 'columnB'], axis=1),
           df.columnA.apply(pd.Series),
           df.columnB.apply(flatten)], axis=1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/FVzRP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FVzRP.png"" alt=""enter image description here""></a></p>
"
39900295,2336654.0,2016-10-06T15:41:14Z,39900061,3,"<p>list are unhashable.  however, tuples are hashable</p>

<p>use</p>

<pre><code>df.groupby([df.a.apply(tuple)])
</code></pre>

<p><strong><em>setup</em></strong><br>
<code>df = pd.DataFrame(dict(a=[list('ab'), list('ba'), list('ac'), list('ca')]))</code><br>
<strong><em>results</em></strong><br>
<code>df.groupby([df.a.apply(tuple)]).size()</code></p>

<pre><code>a
(a, b)    1
(a, c)    1
(b, a)    1
(c, a)    1
dtype: int64
</code></pre>
"
39901297,2414194.0,2016-10-06T16:32:35Z,39900208,-1,"<p>For small arrays:</p>

<pre><code>    from collections import defaultdict
    indices = defaultdict(list)
    for index, column in enumerate(a.transpose()):
        indices[tuple(column)].append(index)
    unique = [kk for kk, vv in indices.items() if len(vv) == 1]
    non_unique = {kk:vv for kk, vv in indices.items() if len(vv) != 1}
</code></pre>
"
39901407,3293881.0,2016-10-06T16:38:48Z,39900208,0,"<p>Here's a vectorized approach to give us a list of arrays as output -</p>

<pre><code>ids = np.ravel_multi_index(a.astype(int),a.max(1).astype(int)+1)
sidx = ids.argsort()
sorted_ids = ids[sidx]
out = np.split(sidx,np.nonzero(sorted_ids[1:] &gt; sorted_ids[:-1])[0]+1)
</code></pre>

<p>Sample run -</p>

<pre><code>In [62]: a
Out[62]: 
array([[ 1.,  0.,  0.,  0.,  0.],
       [ 2.,  0.,  4.,  3.,  0.]])

In [63]: out
Out[63]: [array([1, 4]), array([3]), array([2]), array([0])]
</code></pre>
"
39901889,6005062.0,2016-10-06T17:07:07Z,39900061,2,"<p>You can also sort values by column.</p>

<p>Example:</p>

<pre><code>x = [['a', 'b'], ['b', 'a'], ['a', 'c'], ['c', 'a']]
df = pandas.DataFrame({'a': Series(x)})
df.a.sort_values()

     a
0   [a, b]
2   [a, c]
1   [b, a]
3   [c, a]
</code></pre>

<p>However, for what I understand, you want to sort <code>[b, a]</code> to <code>[a, b]</code>, and <code>[c, a]</code> to <code>[a, c]</code> and then <code>set</code> values in order to get only <code>[a, b][a, c]</code>.</p>

<p>i'd recommend use <code>lambda</code></p>

<p>Try:</p>

<pre><code>result = df.a.sort_values().apply(lambda x: sorted(x))
result = DataFrame(result).reset_index(drop=True)
</code></pre>

<p>It returns:</p>

<pre><code>0    [a, b]
1    [a, c]
2    [a, b]
3    [a, c]
</code></pre>

<p>Then get unique values:</p>

<pre><code>newdf = pandas.DataFrame({'a': Series(list(set(result['a'].apply(tuple))))})
newdf.sort_values(by='a')

     a
0   (a, b)
1   (a, c)
</code></pre>
"
39901950,4211135.0,2016-10-06T17:10:35Z,39901833,3,"<p>The value is already that way in the Series:</p>

<pre><code>&gt;&gt;&gt; x = pd.Series([16,14,12,10,3.1])
&gt;&gt;&gt; x
0    16.0
1    14.0
2    12.0
3    10.0
4     3.1
dtype: float64
&gt;&gt;&gt; x.iloc[4]
3.1000000000000001
</code></pre>

<p>This has to do with floating point precision:</p>

<pre><code>&gt;&gt;&gt; np.float64(3.1)
3.1000000000000001
</code></pre>

<p>See <a href=""http://stackoverflow.com/questions/5160339/floating-point-precision-in-python-array"">Floating point precision in Python array</a> for more information about this.</p>

<p>Concerning the <code>KeyError</code> in your edit, I was not able to reproduce. See the below:</p>

<pre><code>&gt;&gt;&gt; d = {x[i]:i for i in x.index}
&gt;&gt;&gt; d
{16.0: 0, 10.0: 3, 12.0: 2, 14.0: 1, 3.1000000000000001: 4}
&gt;&gt;&gt; x[4]
3.1000000000000001
&gt;&gt;&gt; d[x[4]]
4
</code></pre>

<p>My suspicion is that the <code>KeyError</code> is coming from the <code>Series</code>: what is <code>mySeries[26]</code> returning?</p>
"
39901954,6912707.0,2016-10-06T17:10:56Z,39901833,6,"<p>The dictionary isn't changing the floating point representation of 3.1, but it is actually displaying the full precision. Your print of mySeries[26] is truncating the precision and showing an approximation.</p>

<p>You can prove this:</p>

<pre><code>pd.set_option('precision', 20)
</code></pre>

<p>Then view mySeries.</p>

<pre><code>0    16.00000000000000000000
1    14.00000000000000000000
2    12.00000000000000000000
3    10.00000000000000000000
4     3.10000000000000008882
dtype: float64
</code></pre>

<p><strong>EDIT</strong>:</p>

<p><a href=""https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html"" rel=""nofollow"">What every  computer programmer should know about floating point arithmetic</a> is always a good read.</p>

<p><strong>EDIT</strong>:</p>

<p>Regarding the KeyError, I was not able to replicate the problem.</p>

<pre><code>&gt;&gt; x = pd.Series([16,14,12,10,3.1])
&gt;&gt; a = {x[i]: i for i in x.index}
&gt;&gt; a[x[4]]
4
&gt;&gt; a.keys()
[16.0, 10.0, 3.1000000000000001, 12.0, 14.0]
&gt;&gt; hash(x[4])
2093862195
&gt;&gt; hash(a.keys()[2])
2093862195
</code></pre>
"
39901991,5130927.0,2016-10-06T17:13:11Z,39901833,0,"<p>You can round it to the accuracy you can accept:</p>

<pre><code>&gt;&gt;&gt; hash(round(3.1, 2))
2093862195
&gt;&gt;&gt; hash(round(3.1000000000000001, 2))
2093862195
</code></pre>
"
39902281,584846.0,2016-10-06T17:31:31Z,39899451,2,"<p>Not sure about your first question, but you might be able to simulate it with a sleep statement in your transaction.</p>

<p>For your second question, there is another architecture that you could use.  If the waiting queue duration is relatively short (minutes instead of hours), you might want to use memcache.  It will be a lot faster than writing to disk and you can avoid dealing with consistency issues.</p>
"
39903058,2074981.0,2016-10-06T18:19:13Z,39900493,1,"<p>According to the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html"" rel=""nofollow"">pandas documentation</a>, <code>values</code> should take the name of a single column, not an iterable.</p>

<blockquote>
  <p>values : column to aggregate, optional</p>
</blockquote>
"
39903229,5066140.0,2016-10-06T18:29:09Z,39712602,2,"<p>You can do this without having to loop or iterate through your dataframe.  Per <a href=""http://stackoverflow.com/a/10964938/5066140"">Wes McKinney</a> you can use <code>.apply()</code> with a groupBy object and define a function to apply to the groupby object.  If you use this with <code>.shift()</code> (<a href=""http://stackoverflow.com/a/22082596/5066140"">like here</a>) you can get your result without using any loops.</p>

<p><strong>Terse example:</strong></p>

<pre><code># Group by Employee ID
grouped = df.groupby(""Employee ID"")
# Define function 
def get_unique_events(group):
    # Convert to date and sort by date, like @Khris did
    group[""Effective Date""] = pd.to_datetime(group[""Effective Date""])
    group = group.sort_values(""Effective Date"")
    event_series = (group[""Effective Date""] - group[""Effective Date""].shift(1) &gt; pd.Timedelta('365 days')).apply(lambda x: int(x)).cumsum()+1
    return event_series

event_df = pd.DataFrame(grouped.apply(get_unique_events).rename(""Unique Event"")).reset_index(level=0)
df = pd.merge(df, event_df[['Unique Event']], left_index=True, right_index=True)
df['Output'] = df['Unique Event'].apply(lambda x: ""Unique Leave Event "" + str(x))
df['Match'] = df['Desired Output'] == df['Output']

print(df)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>  Employee ID Effective Date        Desired Output  Unique Event  \
3         100     2013-01-01  Unique Leave Event 1             1
2         100     2014-07-01  Unique Leave Event 2             2
1         100     2015-06-05  Unique Leave Event 2             2
0         100     2016-01-01  Unique Leave Event 2             2
6         200     2013-01-01  Unique Leave Event 1             1
5         200     2015-01-01  Unique Leave Event 2             2
4         200     2016-01-01  Unique Leave Event 2             2
7         300        2014-01  Unique Leave Event 1             1

                 Output Match
3  Unique Leave Event 1  True
2  Unique Leave Event 2  True
1  Unique Leave Event 2  True
0  Unique Leave Event 2  True
6  Unique Leave Event 1  True
5  Unique Leave Event 2  True
4  Unique Leave Event 2  True
7  Unique Leave Event 1  True
</code></pre>

<p><strong>More verbose example for clarity:</strong></p>

<pre><code>import pandas as pd

data = {'Employee ID': [""100"", ""100"", ""100"",""100"",""200"",""200"",""200"",""300""],
        'Effective Date': [""2016-01-01"",""2015-06-05"",""2014-07-01"",""2013-01-01"",""2016-01-01"",""2015-01-01"",""2013-01-01"",""2014-01""],
        'Desired Output': [""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 2"",""Unique Leave Event 2"",""Unique Leave Event 1"",""Unique Leave Event 1""]}
df = pd.DataFrame(data, columns=['Employee ID','Effective Date','Desired Output'])

# Group by Employee ID
grouped = df.groupby(""Employee ID"")

# Define a function to get the unique events
def get_unique_events(group):
     # Convert to date and sort by date, like @Khris did
    group[""Effective Date""] = pd.to_datetime(group[""Effective Date""])
    group = group.sort_values(""Effective Date"")
    # Define a series of booleans to determine whether the time between dates is over 365 days
    # Use .shift(1) to look back one row
    is_year = group[""Effective Date""] - group[""Effective Date""].shift(1) &gt; pd.Timedelta('365 days')
    # Convert booleans to integers (0 for False, 1 for True)
    is_year_int = is_year.apply(lambda x: int(x))    
    # Use the cumulative sum function in pandas to get the cumulative adjustment from the first date.
    # Add one to start the first event as 1 instead of 0
    event_series = is_year_int.cumsum() + 1
    return event_series

# Run function on df and put results into a new dataframe
# Convert Employee ID back from an index to a column with .reset_index(level=0)
event_df = pd.DataFrame(grouped.apply(get_unique_events).rename(""Unique Event"")).reset_index(level=0)

# Merge the dataframes
df = pd.merge(df, event_df[['Unique Event']], left_index=True, right_index=True)

# Add string to match desired format
df['Output'] = df['Unique Event'].apply(lambda x: ""Unique Leave Event "" + str(x))

# Check to see if output matches desired output
df['Match'] = df['Desired Output'] == df['Output']

print(df)
</code></pre>

<p><strong>You get the same output:</strong></p>

<pre><code>  Employee ID Effective Date        Desired Output  Unique Event  \
3         100     2013-01-01  Unique Leave Event 1             1
2         100     2014-07-01  Unique Leave Event 2             2
1         100     2015-06-05  Unique Leave Event 2             2
0         100     2016-01-01  Unique Leave Event 2             2
6         200     2013-01-01  Unique Leave Event 1             1
5         200     2015-01-01  Unique Leave Event 2             2
4         200     2016-01-01  Unique Leave Event 2             2
7         300        2014-01  Unique Leave Event 1             1

                 Output Match
3  Unique Leave Event 1  True
2  Unique Leave Event 2  True
1  Unique Leave Event 2  True
0  Unique Leave Event 2  True
6  Unique Leave Event 1  True
5  Unique Leave Event 2  True
4  Unique Leave Event 2  True
7  Unique Leave Event 1  True
</code></pre>
"
39903338,3642398.0,2016-10-06T18:34:43Z,39903242,9,"<p>Use <code>.get()</code> with a default argument of <code>""N/A""</code> which will be used if the key does not exist:</p>

<pre><code>nObject.TextString = self.var.jobDetails.get(""Overall Weight"", ""N/A"")
</code></pre>

<h1>Update</h1>

<p>If empty strings need to be handled, simply modify as follows:</p>

<pre><code>nObject.TextString = self.var.jobDetails.get(""Overall Weight"") or ""N/A""
</code></pre>

<p>This will set <code>nObject.TextString</code> to ""N/A"" if a <code>KeyError</code> is raised, or if the value is retrieved is empty: <code>''</code>, <code>[]</code>, etc.</p>
"
39903350,2172464.0,2016-10-06T18:35:11Z,39903242,16,"<p>Use <code>get()</code> function for dictionaries. It will return <code>None</code> if the key doesn't exist or if you specify a second value, it will set that as the default. Then your syntax will look like:</p>

<pre><code>nObject.TextString = self.var.jobDetails.get('Overall Weight', 'N/A')
</code></pre>
"
39903427,771848.0,2016-10-06T18:39:56Z,39902832,2,"<p>You can handle both cases using <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#get-text"" rel=""nofollow""><code>get_text()</code></a> with ""strip"" and ""separator"":</p>

<pre><code>from bs4 import BeautifulSoup

dat=""""""
&lt;table&gt;
    &lt;tr&gt;
        &lt;td class=""xyz""&gt;
            text 1
            &lt;br&gt;
            text 2
        &lt;/td&gt;

        &lt;td class=""xyz""&gt;
            text 1
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;
""""""

soup = BeautifulSoup(dat, 'html.parser')
for td in soup.select(""table &gt; tr &gt; td.xyz""):
    print(td.get_text(separator="" "", strip=True))
</code></pre>

<p>Prints:</p>

<pre><code>text 1 text 2
text 1
</code></pre>
"
39903442,642070.0,2016-10-06T18:40:43Z,39903242,3,"<p>I think this is a good case for setting the default value in advance</p>

<pre><code>if nObject.TextString == ""overall_weight"":
    nObject.TextString = ""N/A""
    try:
        if self.var.jobDetails[""Overall Weight""]:
            nObject.TextString = self.var.jobDetails[""Overall Weight""]
    except KeyError:
        pass
</code></pre>

<p><strong>RADICAL RETHINK</strong></p>

<p>Ditch that first answer (just keeping it because it got an upvote). If you really want to go pythonic, (and you always want to set a value on TextString) replace the whole thing with</p>

<pre><code>nObject.TextString = (nObject.TextString == ""overall_weight""
    and self.var.jobDetails.get(""Overall Weight"")
    or ""N/A"")
</code></pre>

<p>Python <code>and</code> and <code>or</code> operations return their last calculated value, not True/False and you can use that to walk through the combinations.</p>
"
39903519,3155933.0,2016-10-06T18:45:36Z,39903242,63,"<p>Use <code>dict.get()</code> which will return the value associated with the given key if it exists otherwise <code>None</code>. (Note that <code>''</code> and <code>None</code> are both falsey values.) If <code>s</code> is true then assign it to <code>nObject.TextString</code> otherwise give it a value of <code>""N/A""</code>.</p>

<pre><code>if nObject.TextString == ""overall_weight"":
    nObject.TextString = self.var.jobDetails.get(""Overall Weight"") or ""N/A""
</code></pre>
"
39903835,613246.0,2016-10-06T19:05:50Z,39900208,0,"<p>The <a href=""https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP"" rel=""nofollow"">numpy_indexed</a> package (disclaimer: I am its author) contains efficient functionality for computing these kind of things:</p>

<pre><code>import numpy_indexed as npi
unique_columns = npi.unique(a, axis=1)
non_unique_column_idx = npi.multiplicity(a, axis=1) &gt; 1
</code></pre>

<p>Or alternatively:</p>

<pre><code>unique_columns, column_count = npi.count(a, axis=1)
duplicate_columns = unique_columns[:, column_count &gt; 1]
</code></pre>
"
39903915,1984065.0,2016-10-06T19:10:04Z,39756807,0,"<p>You can use <code>grequests</code>. It allows other greenlets to run while the request is made. It is compatible with the <code>requests</code> library and returns a <code>requests.Response</code> object. The usage is as follows:</p>

<pre><code>import grequests

@app.route('/do', methods = ['POST'])
def do():
    result = grequests.map([grequests.get('slow api')])
    return result[0].content
</code></pre>

<p>Edit: I've added a test and saw that the time didn't improve with grequests since gunicorn's gevent worker already performs monkey-patching when it is initialized: <a href=""https://github.com/benoitc/gunicorn/blob/master/gunicorn/workers/ggevent.py#L65"" rel=""nofollow"">https://github.com/benoitc/gunicorn/blob/master/gunicorn/workers/ggevent.py#L65</a></p>
"
39904054,6508896.0,2016-10-06T19:20:05Z,39750879,4,"<p>Sorry, the implementation is a bit hacky. But I think it always returns the optimal answer. (Did not proove, though.) It is a fast and complete implementation in python and returns the correct answers for all proposed use cases.</p>

<p>The algorithm is recursive and works as follows:</p>

<ol>
<li>start at the beginning of the text.</li>
<li>find matching chunks that can be used as the first chunk.</li>
<li>for each matching chunk, recursively start at step 1. with the rest of the text (i.e. the chunk removed from the start) and collect the solutions</li>
<li>return the shortest of the collected solutions</li>
</ol>

<p>When the algorithm is done, all possible paths (and the not possible ones, i.e. no match at the end) should have been traversed exactly once.</p>

<p>To perform step 2 efficiently, I build a patricia tree for the choices so the possible chunks matching the beginning of the text can be looked up quickly.</p>

<pre><code>def get_seq_in_tree(tree, choice):
    if type(tree)!=dict:
        if choice == tree:
            return [choice]
        return []
    for i in range(1, len(choice)+1):
        if choice[:i] in tree:
            return [choice[:i]] + get_seq_in_tree(tree[choice[:i]], choice[i:])
    return []

def seq_can_end_here(tree, seq):
    res = []
    last = tree
    for e, c in enumerate(seq):
        if '' in last[c]:
            res.append(e+1)
        last = last[c]
    return res

def build_tree(choices):
    tree = {}
    choices = sorted(choices)
    for choice in choices:
        last = tree
        for c in choice:
            if c not in last:
                last[c] = {}
            last = last[c]
        last['']=None
    return tree

solution_cache = {}
ncalls = 0

def solve(tree, number):
    global solution_cache
    global ncalls
    ncalls +=1

    # take every path only once
    if number in solution_cache: 
        return solution_cache[number]

    solutions = []
    seq =  get_seq_in_tree(tree, number)
    endings = seq_can_end_here(tree, seq)
    for i in reversed(endings):
        current_solution = []
        current_solution.append(number[:i])
        if i == len(number):
            solutions.append(current_solution)
        else:
            next_solution = solve(tree, number[i:])
            if next_solution:
                solutions.append(current_solution + next_solution)
    if not solutions:
        return None

    shortest_solution = sorted([(len(solution), solution) for solution in solutions])[0][1]

    solution_cache[number] = shortest_solution
    return shortest_solution

def get_it_done(choices, number):
    tree = build_tree(choices)
    solution = solve(tree, number)
    return solution


if __name__ == ""__main__"":

    examples = [
        # Example1 -&gt;
        # Solution ['012345678910203040506070', '80', '90', '100', '200', '300', '400', '500', '600', '700', '800', '900']
        (
            [
                ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"",
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""100"", ""200"", ""300"", ""400"", ""500"", ""600"", ""700"", ""800"", ""900"",
                ""012345678910203040506070""
            ],
            ""0123456789102030405060708090100200300400500600700800900""
        ),
        ## Example2
        ## Solution ['100']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""100""
        ),
        ## Example3
        ## Solution ['101234567891020304050', '6070809010020030040050', '0600700800900']
        (
            [
                ""10"", ""20"", ""30"", ""40"", ""50"", ""60"", ""70"", ""80"", ""90"",
                ""012345678910203040506070"",
                ""101234567891020304050"",
                ""6070809010020030040050"",
                ""0600700800900""
            ],
            ""10123456789102030405060708090100200300400500600700800900""
        ),
        ### Example4
        ### Solution ['12', '34', '56', '78', '90']
        (
            [
                ""12"", ""34"", ""56"", ""78"", ""90"",
                ""890"",
            ],
            ""1234567890""
        ),
        ## Example5
        ## Solution ['12', '34']
        (
            [
                ""1"", ""2"", ""3"",
                ""12"", ""23"", ""34""
            ],
            ""1234""
        ),
        # Example6
        ## Solution ['100', '10']
        (
            [""0"", ""1"", ""10"", ""100""],
            ""10010""
        )
    ]

    score = 0
    for (choices, large_number) in examples:
        res = get_it_done(choices, large_number)
        flag = """".join(res) == large_number
        print(""{0}\n{1}\n{2} --&gt; {3}"".format(
            large_number, """".join(res), res, flag))
        print('-' * 80)
        score += flag

    print(""Score: {0}/{1} = {2:.2f}%"".format(score, len(examples), score / len(examples) * 100))
</code></pre>

<p>I guess the complexity is something like O(L * N * log(C)) where L is the length of the text, N is the size of the vocabulary and C is the number of choices.</p>

<p><strong>EDIT:</strong> Included the missing test case.</p>
"
39904193,1103558.0,2016-10-06T19:29:13Z,39903242,-2,"<p>How about <a href=""http://effbot.org/zone/python-with-statement.htm"" rel=""nofollow""><code>with</code></a>:</p>

<pre><code>key = 'Overall Weight'

with n_object.text_string = self.var.job_details[key]:
    if self.var.job_details[key] is None \
    or if self.var.job_details[key] is '' \
    or if KeyError:
        n_object.text_string = 'N/A'
</code></pre>
"
39904389,3510736.0,2016-10-06T19:41:27Z,39904161,6,"<p>If you need to take into account things like skovorodkin's example in the comment, </p>

<pre><code>[(1, 4), (4, 8), (8, 10)]
</code></pre>

<p>(or even more complex examples), then one way to do efficiently would be using graphs. </p>

<p>Say you create a digraph (possibly using <a href=""https://networkx.github.io/"" rel=""nofollow""><code>networkx</code></a>), where each pair is a node, and there is an edge from <em>(a, b)</em> to node <em>(c, d)</em> if <em>b == c</em>. Now run <a href=""https://networkx.github.io/documentation/networkx-1.9/reference/generated/networkx.algorithms.dag.topological_sort.html"" rel=""nofollow"">topological sort</a>, iterate according to the order, and merge accordingly. You should take care to handle nodes with two (or more) outgoing edges properly.</p>

<hr>

<p>I realize your question states you'd like to avoid loops on account of the long list size. Conversely, for long lists, I doubt you'll find even an efficient linear time solution using list comprehension (or something like that). Note that you cannot sort the list in linear time, for example.</p>

<hr>

<p>Here is a possible implementation:</p>

<p>Say we start with</p>

<pre><code>l = [(1,4), (8,10), (19,25), (10,13), (14,16), (25,30)]
</code></pre>

<p>It simplifies the following to remove duplicates, so let's do:</p>

<pre><code>l = list(set(l))
</code></pre>

<p>Now to build the digraph:</p>

<pre><code>import networkx as nx
import collections

g = nx.DiGraph()
</code></pre>

<p>The vertices are simply the pairs:</p>

<pre><code>g.add_nodes_from(l)
</code></pre>

<p>To build the edges, we need a dictionary:</p>

<pre><code>froms = collections.defaultdict(list)
for p in l:
    froms[p[0]].append(p)
</code></pre>

<p>Now we can add the edges:</p>

<pre><code>for p in l:
    for from_p in froms[p[1]]:
        g.add_edge(p, from_p)
</code></pre>

<p>Next two lines are unneeded - they're just here to show what the graph looks like at this point:</p>

<pre><code>&gt;&gt;&gt; g.nodes()
[(25, 30), (14, 16), (10, 13), (8, 10), (1, 4), (19, 25)]

&gt;&gt;&gt; g.edges()
[((8, 10), (10, 13)), ((19, 25), (25, 30))]
</code></pre>

<p>Now, let's sort the pairs by topological sort:</p>

<pre><code>l = nx.topological_sort(g)
</code></pre>

<p>Finally, here's the tricky part. The result will be a DAG. We have to to traverse things recursively, but remember what we visited already.</p>

<p>Let's create a dict of what we visited:</p>

<pre><code>visited = {p: False for p in l}
</code></pre>

<p>Now a recursive function, that given a node, returns the maximum range edge from any node reachable from it:</p>

<pre><code>def visit(p):
    neighbs = g.neighbors(p)
    if visited[p] or not neighbs:
        visited[p] = True
        return p[1]
    mx = max([visit(neighb_p) for neighb_p in neighbs])
    visited[p] = True
    return mx
</code></pre>

<p>We're all ready. Let's create a list for the final pairs:</p>

<pre><code>final_l = []
</code></pre>

<p>and visit all nodes:</p>

<pre><code>for p in l:
    if visited[p]:
        continue
    final_l.append((p[0], visit(p)))
</code></pre>

<p>Here's the final result:</p>

<pre><code>&gt;&gt;&gt; final_l
[(1, 4), (8, 13), (14, 16)]
</code></pre>
"
39904486,2867928.0,2016-10-06T19:47:03Z,39904161,2,"<p>Here is one optimized recursion approach:</p>

<pre><code>In [44]: def find_intersection(m_list):
             for i, (v1, v2) in enumerate(m_list):
                 for j, (k1, k2) in enumerate(m_list[i + 1:], i + 1):
                     if v2 == k1:
                         m_list[i] = (v1, m_list.pop(j)[1])
                         return find_intersection(m_list)
             return m_list
</code></pre>

<p>Demo:</p>

<pre><code>In [45]: lst = [(1,4), (8,10), (19,25), (10,13), (14,16), (25,30)]

In [46]: find_intersection(lst)
Out[46]: [(1, 4), (8, 13), (19, 30), (14, 16)]
</code></pre>
"
39904568,799163.0,2016-10-06T19:52:43Z,39904161,5,"<p>If they don't overlap, then you can sort them, and then just combine adjacent ones.</p>

<p>Here's a generator that yields the new tuples:</p>

<pre><code>def combine_ranges(L):
    L = sorted(L)  # Make a copy as we're going to remove items!
    while L:
        start, end = L.pop(0)  # Get the first item
        while L and L[0][0] == end:
            # While the first of the rest connects to it, adjust
            # the end and remove the first of the rest
            _, end = L.pop(0)
        yield (start, end)

print(list(combine_ranges(List)))
</code></pre>

<p>If speed is important, use a <code>collections.deque</code> instead of a list, so that the <code>.pop(0)</code> operations can be in constant speed.</p>
"
39904580,6451573.0,2016-10-06T19:53:09Z,39904161,2,"<p>non-recursive approach, using sorting (I've added more nodes to handle complex case):</p>

<pre><code>l = [(1,4), (8,10), (19,25), (10,13), (14,16), (25,30), (30,34), (38,40)]
l = sorted(l)

r=[]
idx=0

while idx&lt;len(l):
    local=idx+1
    previous_value = l[idx][1]
    # search longest string
    while local&lt;len(l):
        if l[local][0]!=previous_value:
            break
        previous_value = l[local][1]
        local+=1
    # store tuple
    r.append((l[idx][0],l[local-1][1]))
    idx = local


print(r)
</code></pre>

<p>result:</p>

<pre><code>[(1, 4), (8, 13), (14, 16), (19, 34), (38, 40)]
</code></pre>

<p>The only drawback is that original sort order is not preserved. I don't know if it's a problem.</p>
"
39904588,5378816.0,2016-10-06T19:54:05Z,39904161,1,"<p>The list is first sorted and adjacent pairs of (min1, max1), (min2, max2) are merged together if they overlap.</p>

<pre><code>MIN=0
MAX=1

def normalize(intervals):
    isort = sorted(intervals)
    for i in range(len(isort) - 1): 
        if isort[i][MAX] &gt;= isort[i + 1][MIN]:
            vmin = isort[i][MIN]
            vmax = max(isort[i][MAX], isort[i + 1][MAX])
            isort[i] = None
            isort[i + 1] = (vmin, vmax)
    return [r for r in isort if r is not None]

List1 = [(1,4), (8,10), (19,25), (10,13), (14,16), (25,30)]
List2 = [(1, 4), (4, 8), (8, 10)]
print(normalize(List1))
print(normalize(List2))

#[(1, 4), (8, 13), (14, 16), (19, 30)]
#[(1, 10)]
</code></pre>
"
39904621,1639625.0,2016-10-06T19:56:07Z,39904161,2,"<p>You can use a dictionary to map the different end indices to the range ending at that index; then just iterate the list sorted by start index and merge the segments accordingly:</p>

<pre><code>def join_lists(lst):
    ending = {}  # will map end position to range
    for start, end in sorted(lst):  # iterate in sorted order
        if start in ending:
            ending[end] = (ending[start][0], end)  # merge
            del ending[start]  # remove old value
        else:
            ending[end] = (start, end)
    return list(ending.values())  # return remaining values from dict
</code></pre>

<p>Alternatively, as pointed out by <a href=""http://stackoverflow.com/questions/39904161/organizing-list-of-tuples/39904621#comment67105721_39904621"">Tomer W in comments</a>, you can do without the sorting, by iterating the list twice, making this solution take only linear time (<em>O(n)</em>) w.r.t. the length of the list.</p>

<pre><code>def join_lists(lst):
    ending = {}  # will map end position to range
    # first pass: add to dictionary
    for start, end in lst:
        ending[end] = (start, end)
    # second pass: lookup and merge
    for start, end in lst:
        if start in ending:
            ending[end] = (ending[start][0], end)
            del ending[start]
    # return remaining values from dict
    return list(ending.values())
</code></pre>

<p>Examples output, for both cases:</p>

<pre><code>&gt;&gt;&gt; join_lists([(1,4), (8,10), (19,25), (10,13), (14,16), (25,30)])
[(1, 4), (8, 13), (14, 16), (19, 30)]
&gt;&gt;&gt; join_lists(lst = [(1, 4), (4, 8), (8, 10)])
[(1, 10)]
</code></pre>
"
39904740,636626.0,2016-10-06T20:03:32Z,39904590,6,"<p>Python scripts are interpreted as you go. So when the interpreter enters <code>__init__()</code> the class variable <code>A</code> isn't defined yet (you are inside it), same with <code>self</code> (that is a different parameter and only available in function body).</p>

<p>However anything in that class is interpreted top to bottom, so <code>class_var</code> is defined so you can simply use that one.</p>

<pre><code>class A(object):
    class_var = 'Hi'
    def __init__(self, var=class_var):
        self.var = var
</code></pre>

<p>but I am not super certain that this will be stable across different interpreters...</p>
"
39904762,1324967.0,2016-10-06T20:05:00Z,39904161,1,"<p>The following should work.  It breaks tuples into individual numbers, then finds the tuple bound on each cluster.  This should work even with difficult overlaps, like <code>[(4, 10), (9, 12)]</code></p>

<p>It's a very simple fix.</p>

<pre><code># First turn your list of tuples into a list of numbers:
my_list = []
for item in List: my_list = my_list + [i for i in range(item[0], item[1]+1)]

# Then create tuple pairs:
output = []
a = False
for x in range(max(my_list)+1):
    if (not a) and (x in my_list): a = x
    if (a) and (x+1 not in my_list):
        output.append((a, x))
        a = False

print output
</code></pre>
"
39905034,3293881.0,2016-10-06T20:23:38Z,39904506,3,"<p>Let's look at a NumPy based solution and thus let's assume that the input data is in an array named <code>a</code>. Now, the number of pairwise combinations for 4 such variables would be <code>4*3/2 = 6</code>. We can generate the IDs corresponding to such combinations with <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.triu_indices.html#numpy.triu_indices"" rel=""nofollow""><code>np.triu_indices()</code></a>. Then, we index into the columns of <code>a</code> with those indices. We perform the subtractions and divisions and simply add the columns ignoring the NaN affected results with <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.nansum.html"" rel=""nofollow""><code>np.nansum()</code></a> for the desired output.</p>

<p>Thus, we would have an implementation like so -</p>

<pre><code>R,C = np.triu_indices(a.shape[1],1)
out = 100*np.nansum((a[:,R] - a[:,C])/a[:,C],0)
</code></pre>

<p>Sample run -</p>

<pre><code>In [121]: a
Out[121]: 
array([[ 0.1,  0.3,  0.8,  1. ],
       [ 1. ,  0.2,  nan,  nan],
       [ 0.7,  nan,  2. ,  0.5],
       [ nan,  4. ,  0.6,  0.8]])

In [122]: out
Out[122]: 
array([ 333.33333333, -152.5       ,  -50.        ,  504.16666667,
        330.        ,  255.        ])

In [123]: 100 * ((0.1 - 0.3)/0.3 + (1 - 0.2)/0.2) # Sample's first o/p elem
Out[123]: 333.33333333333337
</code></pre>

<p>If you need the output as <code>(4,4)</code> array, we can use <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.squareform.html"" rel=""nofollow""><code>Scipy's squareform</code></a> -</p>

<pre><code>In [124]: from scipy.spatial.distance import squareform

In [125]: out2D = squareform(out)
</code></pre>

<p>Let's convert to a pandas dataframe for a good visual feedback -</p>

<pre><code>In [126]: pd.DataFrame(out2D,index=list('ABCD'),columns=list('ABCD'))
Out[126]: 
            A           B           C    D
A    0.000000  333.333333 -152.500000  -50
B  333.333333    0.000000  504.166667  330
C -152.500000  504.166667    0.000000  255
D  -50.000000  330.000000  255.000000    0
</code></pre>

<p>Let's compute <code>[B,C]</code> manually and check back -</p>

<pre><code>In [127]: 100 * ((0.3 - 0.8)/0.8 + (4 - 0.6)/0.6)
Out[127]: 504.1666666666667
</code></pre>
"
39906056,6934327.0,2016-10-06T21:43:49Z,39905702,0,"<p>So I just did the same thing, but got closed contours (see images). Did you check for any updates on the package?</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

plt.gca().set_aspect('equal')
x,y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))
r = x*x + y*y
plt.contour(np.log(r))
plt.show()
</code></pre>

<p><a href=""http://i.stack.imgur.com/QVrXD.png"" rel=""nofollow"">Zoomed Out</a></p>

<p><a href=""http://i.stack.imgur.com/y5P6n.png"" rel=""nofollow"">Zoomed In</a></p>
"
39906235,6207849.0,2016-10-06T21:57:45Z,39899005,3,"<p>Here's a solution using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html"" rel=""nofollow""><code>json.normalize</code></a> again by using a custom function to get the data in the correct format understood by <code>json.normalize</code> function.</p>

<pre><code>import ast
from pandas.io.json import json_normalize

def only_dict(d):
    '''
    Convert json string representation of dictionary to a python dict
    '''
    return ast.literal_eval(d)

def list_of_dicts(ld):
    '''
    Create a mapping of the tuples formed after 
    converting json strings of list to a python list   
    '''
    return dict([(list(d.values())[1], list(d.values())[0]) for d in ast.literal_eval(ld)])

A = json_normalize(df['columnA'].apply(only_dict).tolist()).add_prefix('columnA.')
B = json_normalize(df['columnB'].apply(list_of_dicts).tolist()).add_prefix('columnB.pos.') 
</code></pre>

<p>Finally, join the <code>DFs</code> on the common index to get:</p>

<pre><code>df[['id', 'name']].join([A, B])
</code></pre>

<p><a href=""http://i.stack.imgur.com/SpBIg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SpBIg.png"" alt=""Image""></a></p>
"
39906460,5696027.0,2016-10-06T22:16:56Z,39729710,0,"<p>The problem concerns environment variables: cron is started by the system and knows nothing about user environments.</p>

<p>So, the solution to a problem is to make cron run a shell script that sets the needed environment variables first and then runs the script. In my case I needed to set the <code>PATH</code> varibable like this: <code>PATH=/root/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code>
Also, it may be useful to set <code>HOME</code> or <code>DISPLAY</code> varibles in some cases.</p>
"
39907637,839957.0,2016-10-07T00:32:49Z,39907315,4,"<p>You can use the <code>duplicated</code> method to return a boolean indexer of whether elements are duplicates or not:</p>

<pre><code>In [214]: pd.Series(['M', '0', 'M', '0']).duplicated()
Out[214]:
0    False
1    False
2     True
3     True
dtype: bool
</code></pre>

<p>Then you could create a mask by mapping this across the rows of your dataframe, and using <code>where</code> to perform your substitution:</p>

<pre><code>is_duplicate = df.apply(pd.Series.duplicated, axis=1)
df.where(~is_duplicate, 0)

  col1 col2 col3 col4
0    A    B    C    0
1    M    0    0    0
2    B    0    0    0
3    X    0    Y    0
</code></pre>
"
39907658,953863.0,2016-10-07T00:35:17Z,39903242,2,"<p>The Zen of Python says ""Explicit is better than implicit."" I have found this to be very true in my own experience. When I write a piece of code I think to my self, ""Will I understand what this means a year from now?"" If the answer is ""no"" then it needs to be re-written or documented. The accepted answer relies on remembering the implementation of dict.get to know how it will handle the corner cases. Since the OP has 3 clear criteria, I would instead document them clearly in an if statement.</p>

<pre><code>if nObject.TextString == ""overall_weight"" and \
    ""Overall Weight"" in self.var.jobDetails and \
    self.var.jobDetails[""Overall Weight""] != """":
    nObject.TextString = self.var.jobDetails[""Overall Weight""]
else:
    nObject.TextString = ""N/A""
</code></pre>

<p>It's certainly more verbose... but that's a good thing. There is no question when reading this what the behavior will be.</p>
"
39907887,24998.0,2016-10-07T01:09:25Z,39907806,6,"<p>Something like this is pretty short:</p>

<pre><code>def isWordGuessed(secretWord, lettersGuessed):
  return all([c in lettersGuessed for c in secretWord])
</code></pre>

<p>For every character in the <code>secretWord</code> ensure it's in the <code>lettersGuessed</code>. This basically creates a list of booleans and the built-in <a href=""https://docs.python.org/3/library/functions.html#all"" rel=""nofollow"">all</a> returns <code>True</code> if every element in the array is <code>True</code>.</p>

<p>Also, FWIW: Idiomatic python would use underscores and not camel case.</p>
"
39909245,2617068.0,2016-10-07T04:10:02Z,39909214,5,"<p>You can specify a maximum number of times to split with the second argument to <a href=""https://docs.python.org/3.5/library/stdtypes.html#str.split"" rel=""nofollow""><code>split</code></a>.</p>

<pre><code>list1 = ['EW:G:B&lt;&lt;LADHFSSFAFFF', 'CB:E:OWTOWTW', 'PP:E:A,A&lt;F&lt;AF', 'GR:A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7', 'SX:F:-111', 'DS:f:115.5', 'MW:AA:0', 'MA:A:0XT:i:0', 'EW:EE:KJERWEWERKJWE']
d = dict(item.split(':', 1) for item in list1)
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; import pprint
&gt;&gt;&gt; pprint.pprint(d)
{'CB': 'E:OWTOWTW',
 'DS': 'f:115.5',
 'EW': 'EE:KJERWEWERKJWE',
 'GR': 'A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7',
 'MA': 'A:0XT:i:0',
 'MW': 'AA:0',
 'PP': 'E:A,A&lt;F&lt;AF',
 'SX': 'F:-111'}
</code></pre>

<p>If you'd like to keep track of values for non-unique keys, like <code>'EW:G:B&lt;&lt;LADHFSSFAFFF'</code> and <code>'EW:EE:KJERWEWERKJWE'</code>, you could add keys to a <code>collections.defaultdict</code>:</p>

<pre><code>import collections
d = collections.defaultdict(list)
for item in list1:
    k,v = item.split(':', 1)
    d[k].append(v)
</code></pre>

<p>Result:</p>

<pre><code>&gt;&gt;&gt; pprint.pprint(d)
{'CB': ['E:OWTOWTW'],
 'DS': ['f:115.5'],
 'EW': ['G:B&lt;&lt;LADHFSSFAFFF', 'EE:KJERWEWERKJWE'],
 'GR': ['A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7'],
 'MA': ['A:0XT:i:0'],
 'MW': ['AA:0'],
 'PP': ['E:A,A&lt;F&lt;AF'],
 'SX': ['F:-111']}
</code></pre>
"
39909368,3375713.0,2016-10-07T04:26:06Z,39909214,2,"<p>You can also use <a href=""https://docs.python.org/3/library/stdtypes.html#str.partition"" rel=""nofollow""><code>str.partition</code></a></p>

<pre><code>list1 = ['EW:G:B&lt;&lt;LADHFSSFAFFF', 'CB:E:OWTOWTW', 'PP:E:A,A&lt;F&lt;AF', 'GR:A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7', 'SX:F:-111', 'DS:f:115.5', 'MW:AA:0', 'MA:A:0XT:i:0', 'EW:EE:KJERWEWERKJWE']

d = dict([t for t in x.partition(':') if t!=':'] for x in list1)

# or more simply as TigerhawkT3 mentioned in the comment
d = dict(x.partition(':')[::2] for x in list1)

for k, v in d.items():
    print('{}: {}'.format(k, v))
</code></pre>

<p>Output:</p>

<pre><code>MW: AA:0
CB: E:OWTOWTW
GR: A:OUO-1-XXX-EGD:forthyFive:1:HMJeCXX:7
PP: E:A,A&lt;F&lt;AF
EW: EE:KJERWEWERKJWE
SX: F:-111
DS: f:115.5
MA: A:0XT:i:0
</code></pre>
"
39910139,2320035.0,2016-10-07T05:36:53Z,39910050,5,"<p>numpy's sources are <a href=""https://github.com/numpy/numpy"" rel=""nofollow"">at github</a> so you can use github's <a href=""https://github.com/numpy/numpy/search?utf8=%E2%9C%93&amp;q=exponential"" rel=""nofollow"">source-search</a>.</p>

<p>As often, these parts of the library are not implemented in pure python.</p>

<p>The python-parts (in regards to your question) are <a href=""https://github.com/numpy/numpy/blob/7ccf0e08917d27bc0eba34013c1822b00a66ca6d/numpy/random/mtrand/mtrand.pyx"" rel=""nofollow"">here</a>:</p>

<p>The more relevant code-part is from <a href=""https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c"" rel=""nofollow"">distributions.c</a>:</p>

<pre><code>double rk_standard_exponential(rk_state *state)
{
    /* We use -log(1-U) since U is [0, 1) */
    return -log(1.0 - rk_double(state));
}

double rk_exponential(rk_state *state, double scale)
{
    return scale * rk_standard_exponential(state);
}
</code></pre>
"
39910425,2225682.0,2016-10-07T06:01:55Z,39909927,3,"<p>There is <code>parser._option_string_actions</code> which is mapping between option strings (<code>-d</code> or <code>--do_x</code>) and <a href=""https://docs.python.org/3/library/argparse.html#action-classes"" rel=""nofollow""><code>Action</code> objects</a>. <code>Action.help</code> attribute holds the help string.</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument(""-d"", ""--do_x"", action='store_true',
                    help='the program will do X')
args = parser.parse_args()
if args.do_x:
    print(parser._option_string_actions['--do_x'].help)
    # OR  print(parser._option_string_actions['-d'].help)
</code></pre>
"
39910688,901925.0,2016-10-07T06:20:34Z,39909927,2,"<p><code>parser._actions</code> is a list of the <code>Action</code> objects.  You can also grab object when creating the parser.</p>

<pre><code>a=parser.add_argument(...)
...

If args.do_x:
      print a.help
</code></pre>

<p>Play with <code>argparse</code> in an interactive session.  Look at <code>a</code> from such an assignment.</p>
"
39911201,267540.0,2016-10-07T06:56:50Z,39756807,1,"<p>First a bit of background, A blocking socket is the default kind of socket, once you start reading your app or thread does not regain control until data is actually read, or you are disconnected. This is how <code>python-requests</code>, operates by default. There is a spin off called <code>grequests</code> which provides non blocking reads.</p>

<blockquote>
  <p>The major mechanical difference is that send, recv, connect and accept
  can return without having done anything. You have (of course) a number
  of choices. You can check return code and error codes and generally
  drive yourself crazy. If you donât believe me, try it sometime</p>
</blockquote>

<p>Source: <a href=""https://docs.python.org/2/howto/sockets.html"" rel=""nofollow"">https://docs.python.org/2/howto/sockets.html</a></p>

<p>It also goes on to say:</p>

<blockquote>
  <p>Thereâs no question that the fastest sockets code uses non-blocking
  sockets and select to multiplex them. You can put together something
  that will saturate a LAN connection without putting any strain on the
  CPU. The trouble is that an app written this way canât do much of
  anything else - it needs to be ready to shuffle bytes around at all
  times.</p>
  
  <p>Assuming that your app is actually supposed to do something more than
  that, threading is the optimal solution</p>
</blockquote>

<p>But do you want to add a whole lot of complexity to your view by having it spawn it's own threads. Particularly when gunicorn as <a href=""http://docs.gunicorn.org/en/stable/design.html#async-workers"" rel=""nofollow"">async workers</a>?</p>

<blockquote>
  <p>The asynchronous workers available are based on Greenlets (via
  Eventlet and Gevent). Greenlets are an implementation of cooperative
  multi-threading for Python. In general, an application should be able
  to make use of these worker classes with no changes.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Some examples of behavior requiring asynchronous workers: Applications
  making long blocking calls (Ie, external web services)</p>
</blockquote>

<p>So to cut a long story short, don't change anything! Just let it be. If you are making any changes at all, let it be to introduce caching. Consider using <a href=""https://cachecontrol.readthedocs.io/en/latest/"" rel=""nofollow"">Cache-control</a> an extension recommended by python-requests developers.</p>
"
39911346,820410.0,2016-10-07T07:04:32Z,39910941,3,"<p>There is a concept of <a href=""https://wiki.python.org/moin/PythonDecorators"" rel=""nofollow"">decorators in python</a>. They help you abstract out the common code you want to inject on any function on your will. Make a decorator named <code>log</code> as follows:</p>

<pre><code>import logging
from functools import wraps

def log(func):
    @wraps(func)
    def wrapper(self, *args, **kwargs):
            logging.debug(""args - %s"", str(args))
            logging.debug(""kwargs - %s"", str(kwargs))
            response = func(self, *args, **kwargs)
            logging.debug(""response - %s"", str(response))
            return response
    return wrapper
</code></pre>

<p>Now, you can use this decorator for any function as follows:</p>

<pre><code>@log
def funt1(self,weight=None):
    return weight
</code></pre>

<p>The above code will log all the arguments &amp; their returned values with just 1 line of code.</p>

<p><a href=""http://thecodeship.com/patterns/guide-to-python-function-decorators/"" rel=""nofollow"">Here</a> is a good blog on understanding decorators.</p>
"
39912344,779516.0,2016-10-07T08:01:22Z,39905702,3,"<p>Disclaimer: this is more an <strong>explanation + hack</strong> than a real answer.</p>

<p>I believe that there is a fundamental problem the way matplotlib makes contour plots.  Essentially, all contours are collections of lines (<code>LineCollection</code>), while they should be collection of possibly closed lines (<code>PolyCollection</code>).  There might be good reasons why things are done this way, but in the simple example I made this choice clearly produces artifacts.  A not-very-nice solution is to convert a posteriori all <code>LineCollection</code>'s into <code>PolyCollection</code>'s.  This is what is done in the following code</p>

<pre><code>from matplotlib.collections import PolyCollection

eps = 1e-5
plt.gca().set_aspect('equal')
x,y = np.meshgrid(np.linspace(-1,1,100), np.linspace(-1,1,100))
r = x*x + y*y
plt.contour(np.log(r/1.2))
ca = plt.gca()
N = len(ca.collections)
for n in range(N):
    c = ca.collections.pop(0)
    for s in c.get_segments():
        closed = (abs(s[0,0] - s[-1,0]) &lt; eps) and (abs(s[0,1] - s[-1,1]) &lt; eps)            
        p = PolyCollection([s], edgecolors=c.get_edgecolors(), 
                           linewidths=c.get_linewidths(), linestyles=c.get_linestyles(),
                           facecolors=c.get_facecolors(), closed=closed)
        ca.add_collection(p)
plt.savefig(""test.pdf"")
</code></pre>

<p>A zoom of the result obtained shows that everything is OK now:</p>

<p><a href=""http://i.stack.imgur.com/rEP4i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rEP4i.png"" alt=""enter image description here""></a></p>

<p>Some care is taken to check if a contour is closed: in the present code, this is done with an approximate equality check for the first and last point: I am wandering if there is a better way to do this (perhaps matplotlib returns some data to check closed contours?).  In any case, again, this is hack: I would be happy to hear if anyone has a better solution (or has a way to fix this within matplotlib).</p>
"
39913456,4657412.0,2016-10-07T09:03:59Z,39900630,1,"<p>If you add the line <a href=""https://docs.python.org/2/c-api/exceptions.html"" rel=""nofollow""><code>PyErr_PrintEx(1)</code></a> it helpfully tells you:</p>

<blockquote>
  <p>TypeError: argument list must be a tuple</p>
</blockquote>

<p>This is confirmed by <a href=""https://docs.python.org/2/c-api/object.html"" rel=""nofollow"">the documentation for <code>PyObject_CallObject</code></a>:</p>

<blockquote>
  <p>Call a callable Python object callable_object, with arguments given by
  the tuple args.</p>
</blockquote>

<p>There's a whole bunch of ways of calling functions from the C-api. I picked one that doesn't require a tuple and it works for me (but pick the one you like):</p>

<pre><code>PyObject* shouldbetrue = PyObject_CallFunctionObjArgs(arrayEndsWith, oneByteArray,NULL);
</code></pre>
"
39914589,267540.0,2016-10-07T10:02:31Z,39914524,4,"<p>json.dumps and dump have a parameter called indent</p>

<pre><code>If ``indent`` is a non-negative integer, then JSON array elements and
    object members will be pretty-printed with that indent level. An indent
    level of 0 will only insert newlines. ``None`` is the most compact
    representation.  Since the default item separator is ``', '``,  the
    output might include trailing whitespace when ``indent`` is specified.
    You can use ``separators=(',', ': ')`` to avoid this
</code></pre>

<p>Something like this would do:</p>

<pre><code>json.dump(jsonDict,f,indent=4)
</code></pre>
"
39916236,6730448.0,2016-10-07T11:33:47Z,39752235,0,"<p>I would rather write this as a comment but unfortunately I can't. You should change the minLineLength and minLineGap. Or what if its just sqaures that you have to find, I would get all the lines and check the angles between them to get lines only along squares. I have worked with HoughLineP before and it is pretty much based on the above two arguments. Additionally, try using Bilateral filtering. I really helps when the sharpening using median filter doesn't help. </p>

<p><a href=""http://docs.opencv.org/2.4/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.html"" rel=""nofollow"">Bilateral Filter</a></p>
"
39916238,3581181.0,2016-10-07T11:33:51Z,39899451,1,"<p>1.- If you do the entity get and the post inside a transaction, then the same entity can not be matched for a game and therefore no error and it remains consistent.</p>

<p>2.- The 1 write per second is sthe limit for transactions inside the same entity group. If you need more, you can shard the queue entity.</p>

<p>You can use a dedicated memcache or a redis instance to avoid contention. This are much faster than the datastore.</p>

<p>See how these guys use tree nodes to do the match making:
<a href=""https://www.youtube.com/watch?v=9nWyWwY2Onc"" rel=""nofollow"">https://www.youtube.com/watch?v=9nWyWwY2Onc</a></p>
"
39917248,6734406.0,2016-10-07T12:24:15Z,39778435,2,"<p>Yes, you can use following python libraries:</p>

<ul>
<li><a href=""https://pypi.python.org/pypi/dill"" rel=""nofollow"">dill</a> (required)</li>
<li><a href=""https://pypi.python.org/pypi/python-memcached"" rel=""nofollow"">python-memcached</a> (optional)</li>
</ul>

<p>Let's follow the example. You have two files:</p>

<pre><code># save.py - it puts deserialized file handler object to memcached
import dill
import memcache            


mc = memcache.Client(['127.0.0.1:11211'], debug=0)
file_handler = open('data.txt', 'r')
mc.set(""file_handler"", dill.dumps(file_handler))
print 'saved!'   
</code></pre>

<p>and </p>

<pre><code># read_from_file.py - it gets deserialized file handler object from memcached, 
#                     then serializes it and read lines from it
import dill
import memcache


mc = memcache.Client(['127.0.0.1:11211'], debug=0)
file_handler = dill.loads(mc.get(""file_handler""))
print file_handler.readlines() 
</code></pre>

<p>Now if you run:</p>

<pre><code>python save.py
python read_from_file.py
</code></pre>

<p>you can get what you want. </p>

<p><strong>Why it works?</strong></p>

<p>Because you didn't close the file (<code>file_handler.close()</code>), so object still exist in memory (has not been garbage collected, because of <a href=""https://docs.python.org/2/library/weakref.html"" rel=""nofollow"">weakref</a>) and you can use it. Even in different process.</p>

<p><strong>Solution</strong></p>

<pre><code>import dill
import memcache


mc = memcache.Client(['127.0.0.1:11211'], debug=0)
serialized = mc.get(""file_handler"")
if serialized:
    file_handler = dill.loads(serialized)
else:
    file_handler = open('data.txt', 'r')
    mc.set(""file_handler"", dill.dumps(file_handler))
print file_handler.readlines() 
</code></pre>
"
39917409,6187156.0,2016-10-07T12:32:47Z,39876608,2,"<p>I believe this is what you want:</p>

<pre><code>import colorama
colorama.init()
no = 0
while True:
    user_input = str(raw_input('\033[2A'*no + '\033[KSay something: '))
    print '\033[KResult: ' + user_input
    no = 1
</code></pre>

<p>This how it looks after entering the string:</p>

<p><a href=""http://i.stack.imgur.com/2sk8G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2sk8G.png"" alt=""Working solution""></a></p>

<p>This implementation works on windows, however, if you use Linux, if I am not mistaken these are not necessary:</p>

<pre><code>import colorama
colorama.init() 
</code></pre>

<p>EDIT: Modified my code a bit so it does not overwrite the text that was printed before the execution of the code. Also added an image of working implementation.</p>
"
39918038,1577341.0,2016-10-07T13:06:02Z,39917988,7,"<p>In Python all values are objects with built-in type info. Variables are references to these values. So their type is 'dynamic', just equal to the type of what they happen to refer to (point to) at a particular moment.</p>

<p>Whenever memory is allocated for the contents of a variable, a value is available. Since it has a type, the amount of memory needed is known.</p>

<p>The references (variables) themselves always occupy the same amount of memory, no matter what they point to, since they just contain a conceptual address.</p>

<p>This indeed means that in</p>

<pre><code>def f (x):
    print (x)
</code></pre>

<p>x doesn't have a type, since it doesn't have a particular value yet.
The upside is that this is very flexible.
The downside is that the compiler has only limited means to discover errors.
For this reason Python was recently enriched with <a href=""http://stackoverflow.com/questions/32557920/what-are-type-hints-in-python-3-5"">type hints</a>.
Tools like <a href=""http://mypy-lang.org/"" rel=""nofollow"">mypy</a> allow static typechecking, even though the interpreter doesn't need it.
But the programmer sometimes does, especially at module boundaries (API's) when she's working in a team.</p>
"
39918089,6671342.0,2016-10-07T13:08:05Z,39917988,2,"<p>Python is dynamically typed language which means that the type of variables are decided in running time. As a result python interpreter will distinguish  the variable's types (in running time) and give  the exact space in memory needed. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) . </p>

<p>On the other hand C and C++ are statically typed languages which means that the types of variables are known in compilation time.</p>

<p>Using dynamic typing in programming languages has the advantage that gives more potential to language, for example we can have lists with different types (for example a list that contains chars and integers). This wouldn't be possible with static typing since the type of the list should be known from the compilation time...).<br>
One disadvantage of dynamic typing is that the compiler-interpreter in many cases must keeps a record of types in order to extract the types of variables, which makes it more slow in comparison with C or C++.</p>

<p>A dynamic typed language like python can be also strongly typed. Python is strongly typed as the interpreter keeps track of all variables types and is restrictive about how types can be intermingled.</p>
"
39918342,3145469.0,2016-10-07T13:23:45Z,39917988,6,"<p>Dynamically typed languages typically use boxed representation, which includes runtime type information. E.g. instead of storing direct pointers to a value, the system uses a box struct that contains the value (or pointer to it) as well as some additional metainformation.  You can see how he standard Python implementation does it here: <a href=""https://github.com/python/cpython/blob/master/Include/object.h"">https://github.com/python/cpython/blob/master/Include/object.h</a></p>

<p>There are some interesting tricks that can be employed here. For instance, one technique is value tagging, where the type description is stored as part of the value itself, utilising unused bytes. For instance, pointers on current x86-64 CPUs can't utilise the full address space, which gives you some bits to play with. Another variant of this technique is NaN-tagging (I believe this was first used by Mike Pall, author of LuaJIT) - where all values are stored as doubles, and various NaN states of the value signal that it is actually a pointer or some other type of data. </p>
"
39918391,2336654.0,2016-10-07T13:25:56Z,39918053,2,"<p>consider your dataframe <code>df</code></p>

<pre><code>df = pd.DataFrame([
        [0, 1, 2, 'T'],
        [1, 2, 3, 'F'],
        [2, 1, 3, 'F'],
        [1, 0, 2, 'T'],
    ], [1, 2, 3, 4], list('ABCD'))
</code></pre>

<p><a href=""http://i.stack.imgur.com/XtqBK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XtqBK.png"" alt=""enter image description here""></a></p>

<p><strong><em>solution</em></strong></p>

<pre><code>df.set_index('D', append=True) \
    .rename_axis(['col'], 1) \
    .rename_axis([None, 'val2']) \
    .stack().to_frame('val') \
    .reset_index(['col', 'val2']) \
    [['col', 'val', 'val2']]
</code></pre>

<p><a href=""http://i.stack.imgur.com/RRNtS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RRNtS.png"" alt=""enter image description here""></a></p>
"
39918511,5626112.0,2016-10-07T13:31:29Z,39918053,3,"<p>I would use melt, and you can sort it how ever you like</p>

<pre><code>pd.melt(df.reset_index(),id_vars=['index','D'], value_vars=['A','B','C']).sort_values(by='index')
Out[40]: 
    index  D variable  value
0       1  T        A      0
4       1  T        B      1
8       1  T        C      2
1       2  F        A      1
5       2  F        B      2
9       2  F        C      3
2       3  F        A      2
6       3  F        B      1
10      3  F        C      0
3       4  T        A      1
7       4  T        B      0
11      4  T        C      2
</code></pre>

<p>then obviously you can name column as you like</p>

<pre><code>df.set_index('index').rename(columns={'D': 'col', 'variable': 'val2', 'value': 'val'})
</code></pre>
"
39918601,6380626.0,2016-10-07T13:35:49Z,39917988,1,"<p>The Python interpreter analyzes each variable when the program runs. Before running, it doesn't know whether you've got an integer, a float, or a string in any of your variables.</p>

<p>When you have a statically typed language background (Java in my case), it's a bit unusual. Dynamic typing saves you a lot of time and lines of code in large scripts. It prevents you from having errors because you have forgotten to define some variable. However, static typing lets you have more control on how data is stored in a computer's memory.</p>
"
39920128,1832539.0,2016-10-07T14:51:34Z,39919699,2,"<p>Assuming your exceptions are being raised from <code>foo.authenticate()</code>, what you want to realize here is that it does not necessarily matter whether the data is in fact <em>really</em> valid in your tests. What you are trying to say really is this: </p>

<p>When this external method raises with something, my code should behave accordingly based on that something. </p>

<p>So, with that in mind, what you want to do is have different test methods where you pass what <em>should</em> be valid data, and have your code react accordingly. The data itself does not matter, but it provides a documented way of showing how the code should behave with data that is passed in that way. </p>

<p>Ultimately, you should not care how the nova client handles the data you give it (nova client is tested, and you should not care about it). What you care about is what it spits back at you and how you want to handle it, regardless of what you gave it. </p>

<p>In other words, for the sake of your tests, you can actually pass a dummy url as: </p>

<pre><code>""this_is_a_dummy_url_that_works""
</code></pre>

<p>For the sake of your tests, you can let that pass, because in your <code>mock</code>, you will raise accordingly. </p>

<p>For example. What you should be doing here is actually mocking out <code>Client</code> from <code>novaclient</code>. With that mock in hand, you can now manipulate whatever call within novaclient so you can properly test your code.</p>

<p>This actually brings us to the root of your problem. Your first exception is catching the following:</p>

<pre><code>except (OSError, NotFound, ClientException)
</code></pre>

<p>The problem here, is that you are now catching <code>ClientException</code>. Almost every exception in <code>novaclient</code> inherits from <code>ClientException</code>, so no matter what you try to test beyond that exception line, you will never reach those exceptions. You have two options here. Catch <code>ClientException</code>, and just raise a custom exception, or, remote <code>ClientException</code>, and be more explicit (like you already are).</p>

<p>So, let us go with removing <code>ClientException</code> and set up our example accordingly. </p>

<p>So, in your <em>real</em> code, you should be now setting your first exception line as:</p>

<pre><code>except (OSError, NotFound) as e:
</code></pre>

<p>Furthermore, the next problem you have is that you are not mocking properly. You are supposed to mock with respect to where you are testing. So, if your <code>setup_nova</code> method is in a module called <code>your_nova_module</code>. It is with respect to that, that you are supposed to mock. The example below illustrates all this. </p>

<pre><code>@patch(""your_nova_module.Client"", return_value=Mock())
def test_setup_nova_failing_unauthorized_user(self, mock_client):
    dummy_creds = {
        'AUTH_URL': 'this_url_is_valid',
        'USERNAME': 'my_bad_user. this should fail',
        'PASSWORD': 'bad_pass_but_it_does_not_matter_what_this_is',
        'VERSION': '2.1',
        'PROJECT_ID': 'does_not_matter'
    }

    mock_nova_client = mock_client.return_value
    mock_nova_client.authenticate.side_effect = Unauthorized(401)

    with self.assertRaises(UnauthorizedUser):
        setup_nova(dummy_creds)
</code></pre>

<p>So, the main idea with the example above, is that it does not matter what data you are passing. What really matters is that you are wanting to know how your code will react when an external method raises.</p>

<p>So, our goal here is to actually raise something that will get your second exception handler to be tested: <code>Unauthorized</code></p>

<p>This code was tested against the code you posted in your question. The only modifications were made were with module names to reflect my environment.</p>
"
39920176,6107054.0,2016-10-07T14:53:30Z,39765738,5,"<p>I think you need to convert your pandas dataframe into an numpy array before you fit. </p>

<pre><code>from blaze import *
import numpy

from sklearn.cluster import KMeans
data_numeric = numpy.array(data('data.csv'))
data_cluster = KMeans(n_clusters=5)
data_cluster.fit(data_numeric)
</code></pre>
"
39920186,6933794.0,2016-10-07T14:53:50Z,39919699,0,"<p>If you wish to mock out http servers from bogus urls, I suggest you check out <a href=""https://github.com/gabrielfalcao/HTTPretty"" rel=""nofollow"">HTTPretty</a>. It mocks out urls at a socket level so it can trick most Python HTTP libraries that it's a valid url.</p>

<p>I suggest the following setup for your unittest:</p>

<pre><code>class FooTest(unittest.TestCase):
    def setUp(self):
        httpretty.register_uri(httpretty.GET, ""http://bogus.example.com/v2.0"",
                       body='[{""response"": ""Valid""}]',
                       content_type=""application/json"")
    @httpretty.activate
    def test_test_case(self):
        resp = requests.get(""http://bogus.example.com/v2.0"")
        self.assertEquals(resp.status_code, 200)
</code></pre>

<p>Note that the mock will only apply to stacks that are decorated with <code>http.activate</code> decorator, so it won't leak to other places in your code that you don't want to mock. Hope that makes sense.</p>
"
39920776,6936382.0,2016-10-07T15:23:22Z,39910050,1,"<p>A lot of numpy functions are written with C/C++ and Fortran. <code>numpy.source()</code> returns the source code only for objects written in Python. It is written on <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.source.html"" rel=""nofollow"">NumPy website</a>.</p>

<p>You can find all of the NumPy functions on their GitHub page. One that you need is written in C. Here is the link to the <a href=""https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.c"" rel=""nofollow"">file</a>.</p>
"
39920991,6917600.0,2016-10-07T15:34:52Z,39877725,3,"<p>Yes it does make sense. This is commonly done in matlab. Here is a link to a similar application:</p>

<p><a href=""http://www.mathworks.com/help/signal/ug/cross-correlation-of-delayed-signal-in-noise.html"" rel=""nofollow"">http://www.mathworks.com/help/signal/ug/cross-correlation-of-delayed-signal-in-noise.html</a></p>

<p>Several considerations</p>

<p>Cross-correlation is commonly used for times when the signal in question has too much noise. If you don't have any noise to worry about I would use a different method however. </p>
"
39921537,1062499.0,2016-10-07T16:04:28Z,39804774,4,"<p>OK, here's a suggestion.  In the vector case, if you have <em>x</em> as a vector of length <code>n</code>, then <code>g(x)</code> is also a vector of length <code>n</code>.  However, <code>g'(x)</code> is not a vector, it's the <a href=""https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"" rel=""nofollow"">Jacobian matrix</a>, and will be of size <code>n X n</code>.  Similarly, in the minibatch case, where <em>X</em> is a matrix of size <code>m X n</code>, <code>g(X)</code> is <code>m X n</code> but <code>g'(X)</code> is <code>n X n</code>.  Try:</p>

<pre><code>def gGradient(x): #gradient of sigmoid
    return np.dot(g(x).T, 1 - g(x))
</code></pre>

<p>@Paul is right that the bias terms should be vectors, not matrices.  You should have:</p>

<pre><code>b1 = np.random.rand(k) #bias term from input layer to hidden layer (k,)
b2 = np.random.rand(n) #bias term from hidden layer to output layer of autoencoder, shape (n,)
b3 = np.random.rand(n) #bias term from output layer of autoencoder to entire output of the machine, shape (n,)
</code></pre>

<p>Numpy's broadcasting means that you don't have to change your calculation of <code>xhat</code>.</p>

<p>Then (I think!) you can compute the derivatives like this:</p>

<pre><code>dSdxhat = (1/float(m)) * (xhat-x)
dSdw3 = np.dot(h2.T,dSdxhat)
dSdb3 = dSdxhat.mean(axis=0)
dSdh2 = np.dot(dSdxhat, w3.T)
dSdz2 = np.dot(dSdh2, gGradient(z2))
dSdb2 = dSdz2.mean(axis=0)
dSdw2 = np.dot(h1.T,dSdz2)
dSdh1 = np.dot(dSdz2, w2.T)
dSdz1 = np.dot(dSdh1, gGradient(z1))
dSdb1 = dSdz1.mean(axis=0)
dSdw1 = np.dot(x.T,dSdz1)
</code></pre>

<p>Does this work for you?</p>

<p><strong>Edit</strong></p>

<p>I've decided that I'm not at all sure that <code>gGradient</code> is supposed to be a matrix.  How about:</p>

<pre><code>dSdxhat = (xhat-x) / m
dSdw3 = np.dot(h2.T,dSdxhat)
dSdb3 = dSdxhat.sum(axis=0)
dSdh2 = np.dot(dSdxhat, w3.T)
dSdz2 = h2 * (1-h2) * dSdh2
dSdb2 = dSdz2.sum(axis=0)
dSdw2 = np.dot(h1.T,dSdz2)
dSdh1 = np.dot(dSdz2, w2.T)
dSdz1 = h1 * (1-h1) * dSdh1
dSdb1 = dSdz1.sum(axis=0)
dSdw1 = np.dot(x.T,dSdz1)
</code></pre>
"
39921608,3990607.0,2016-10-07T16:08:19Z,39921607,4,"<p><strong>Yes There is!</strong></p>

<p><strong>Credit:</strong>
It was hard to find the information and get it working but here is an example copying from the principles and code found <a href=""https://github.com/tensorflow/tensorflow/issues/1095"" rel=""nofollow"">here</a> and <a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""nofollow"">here</a>.</p>

<p><strong>Requirements:</strong>
Before we start, there are two requirement for this to be able to succeed. First you need to be able to write your activation as a function on numpy arrays. Second you have to be able to write the derivative of that function either as a function in Tensorflow (easier) or in the worst case scenario as a function on numpy arrays. </p>

<p><strong>Writing Activation function:</strong></p>

<p>So let's take for example this function which we would want to use an activation function:</p>

<pre><code>def spiky(x):
    r = x % 1
    if r &lt;= 0.5:
        return r
    else:
        return 0
</code></pre>

<p>Which look as follows:
<a href=""http://i.stack.imgur.com/gTUBr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gTUBr.png"" alt=""Spiky Activation""></a></p>

<p>The first step is making it into a numpy function, this is easy:</p>

<pre><code>import numpy as np
np_spiky = np.vectorize(spiky)
</code></pre>

<p>Now we should write its derivative.</p>

<p><strong>Gradient of Activation:</strong>
In our case it is easy, it is 1 if x mod 1 &lt; 0.5 and 0 otherwise. So:</p>

<pre><code>def d_spiky(x):
    r = x % 1
    if r &lt;= 0.5:
        return 1
    else:
        return 0
np_d_spiky = np.vectorize(d_spiky)
</code></pre>

<p>Now for the hard part of making a TensorFlow function out of it.</p>

<p><strong>Making a numpy fct to a tensorflow fct:</strong>
We will start by making np_d_spiky into a tensorflow function. There is a function in tensorflow <code>tf.py_func(func, inp, Tout, stateful=stateful, name=name)</code> <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html"" rel=""nofollow"">[doc]</a> which transforms any numpy function to a tensorflow function, so we can use it:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.framework import ops

np_d_spiky_32 = lambda x: np_d_spiky(x).astype(np.float32)


def tf_d_spiky(x,name=None):
    with ops.op_scope([x], name, ""d_spiky"") as name:
        y = tf.py_func(np_d_spiky_32,
                        [x],
                        [tf.float32],
                        name=name,
                        stateful=False)
        return y[0]
</code></pre>

<p><code>tf.py_func</code> acts on lists of tensors (and returns a list of tensors), that is why we have <code>[x]</code> (and return <code>y[0]</code>). The <code>stateful</code> option is to tell tensorflow whether the function always gives the same output for the same input (stateful = False) in which case tensorflow can simply the tensorflow graph, this is our case and will probably be the case in most situations. One thing to be careful of at this point is that numpy used <code>float64</code> but tensorflow uses <code>float32</code> so you need to convert your function to use <code>float32</code> before you can convert it to a tensorflow function otherwise tensorflow will complain. This is why we need to make <code>np_d_spiky_32</code> first. </p>

<p><strong>What about the Gradients?</strong> The problem with only doing the above is that even though we now have <code>tf_d_spiky</code> which is the tensorflow version of <code>np_d_spiky</code>, we couldn't use it as an activation function if we wanted to because tensorflow doesn't know how to calculate the gradients of that function. </p>

<p><strong>Hack to get Gradients:</strong> As explained in the sources mentioned above, there is a hack to define gradients of a function using <code>tf.RegisterGradient</code> <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/framework.html#RegisterGradient"" rel=""nofollow"">[doc]</a> and <code>tf.Graph.gradient_override_map</code> <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/framework.html"" rel=""nofollow"">[doc]</a>. Copying the code from <a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""nofollow"">harpone</a> we can modify the <code>tf.py_func</code> function to make it define the gradient at the same time:</p>

<pre><code>def py_func(func, inp, Tout, stateful=True, name=None, grad=None):

    # Need to generate a unique name to avoid duplicates:
    rnd_name = 'PyFuncGrad' + str(np.random.randint(0, 1E+8))

    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example
    g = tf.get_default_graph()
    with g.gradient_override_map({""PyFunc"": rnd_name}):
        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)
</code></pre>

<p>Now we are almost done, the only thing is that the grad function we need to pass to the above py_func function needs to take a special form. It needs to take in an operation, and the previous gradients before the operation and propagate the gradients backward after the operation.</p>

<p><strong>Gradient Function:</strong> So for our spiky activation function that is how we would do it:</p>

<pre><code>def spikygrad(op, grad):
    x = op.inputs[0]

    n_gr = tf_d_spiky(x)
    return grad * n_gr  
</code></pre>

<p>The activation function has only one input, that is why <code>x = op.inputs[0]</code>. If the operation had many inputs, we would need to return a tuple, one gradient for each input. For example if the operation was <code>a-b</code>the gradient with respect to <code>a</code> is <code>+1</code> and with respect to <code>b</code> is <code>-1</code> so we would have <code>return +1*grad,-1*grad</code>. Notice that we need to return tensorflow functions of the input, that is why need <code>tf_d_spiky</code>, <code>np_d_spiky</code> would not have worked because it cannot act on tensorflow tensors. Alternatively we could have written the derivative using tensorflow functions:</p>

<pre><code>def spikygrad2(op, grad):
    x = op.inputs[0]
    r = tf.mod(x,1)
    n_gr = tf.to_float(tf.less_equal(r, 0.5))
    return grad * n_gr  
</code></pre>

<p><strong>Combining it all together:</strong> Now that we have all the pieces, we can combine them all together:</p>

<pre><code>np_spiky_32 = lambda x: np_spiky(x).astype(np.float32)

def tf_spiky(x, name=None):

    with ops.op_scope([x], name, ""spiky"") as name:
        y = py_func(np_spiky_32,
                        [x],
                        [tf.float32],
                        name=name,
                        grad=spikygrad)  # &lt;-- here's the call to the gradient
        return y[0]
</code></pre>

<p>And now we are done. And we can test it.</p>

<p><strong>Test:</strong></p>

<pre><code>with tf.Session() as sess:

    x = tf.constant([0.2,0.7,1.2,1.7])
    y = tf_spiky(x)
    tf.initialize_all_variables().run()

    print(x.eval(), y.eval(), tf.gradients(y, [x])[0].eval())
</code></pre>

<blockquote>
  <p>[ 0.2 0.69999999  1.20000005  1.70000005] [ 0.2 0. 0.20000005  0.] [ 1.  0.  1.  0.]</p>
</blockquote>

<p><strong>Success!</strong></p>
"
39921621,2728397.0,2016-10-07T16:08:53Z,39843488,1,"<blockquote>
  <p><strong>In Python 3.5, you can do:</strong></p>
  
  <p><code>import math
  test = math.inf</code></p>
  
  <p>And then:</p>
  
  <p><code>test &gt; 1
  test &gt; 10000
  test &gt; x</code></p>
  
  <p>Will always be true. Unless of course, as pointed out, x is also infinity or ""nan"" (""not a number"").</p>
</blockquote>

<p><a href=""https://stackoverflow.com/questions/7781260/how-can-i-represent-an-infinite-number-in-python"">How can I represent an infinite number in Python?</a></p>

<p>Answered by @WilHall</p>
"
39922569,5066140.0,2016-10-07T17:05:16Z,39674713,1,"<p><strong>Tensor shape</strong></p>

<p>You're right that Keras is expecting a 3D tensor for an LSTM neural network, but I think the piece you are missing is that Keras expects that <em>each observation can have multiple dimensions</em>.  </p>

<p>For example, in Keras I have used word vectors to represent documents for natural language processing.  Each word in the document is represented by an n-dimensional numerical vector (so if <code>n = 2</code> the word 'cat' would be represented by something like <code>[0.31, 0.65]</code>).  To represent a single document, the word vectors are lined up in sequence (e.g. 'The cat sat.' = <code>[[0.12, 0.99], [0.31, 0.65], [0.94, 0.04]]</code>).  A document would be a single sample in a Keras LSTM.</p>

<p>This is analogous to your time series observations.  A document is like a time series, and a word is like a single observation in your time series, but in your case it's just that the representation of your observation is just <code>n = 1</code> dimensions.</p>

<p>Because of that, I think your tensor should be something like <code>[[[a1], [a2], ... , [aT]], [[b1], [b2], ..., [bT]], ..., [[x1], [x2], ..., [xT]]]</code>, where <code>x</code> corresponds to <code>nb_samples</code>, <code>timesteps = T</code>, and <code>input_dim = 1</code>, because each of your observations is only one number.</p>

<p><strong>Batch size</strong></p>

<p>Batch size should be set to maximize throughput without exceeding the memory capacity on your machine, per this <a href=""http://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network"">Cross Validated post</a>. As far as I know your input does not need to be a multiple of your batch size, neither when training the model and making predictions from it.</p>

<p><strong>Examples</strong></p>

<p>If you're looking for sample code, on the <a href=""https://github.com/fchollet/keras/tree/master/examples"" rel=""nofollow"">Keras Github</a> there are a number of examples using LSTM and other network types that have sequenced input.</p>
"
39922697,6912707.0,2016-10-07T17:14:38Z,39922504,0,"<p>You are missing the 75 and 150 contours because those values are never crossed in the array. The value 150 exists, and the value 75(.000000000000014) exist, but those are the min and max values. Contours describe a line/surface <strong>boundary</strong>.</p>

<pre><code>#levels modified
levels=[76,95,115,135,149]
cs=plt.contour(x,y,vel,levels)
plt.clabel(cs,inline=1,fontsize=9)
</code></pre>

<p><a href=""http://i.stack.imgur.com/APVfF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/APVfF.png"" alt=""with modified levels""></a></p>
"
39923012,5626112.0,2016-10-07T17:37:45Z,39922986,3,"<p>use the sum() method</p>

<pre><code>df.groupby(['Name','Fruit']).sum()

Out[31]: 
               Number
Fruit   Name         
Apples  Bob        16
        Mike        9
        Steve      10
Grapes  Bob        35
        Tom        87
        Tony       15
Oranges Bob        67
        Mike       57
        Tom        15
        Tony        1
</code></pre>
"
39923111,1029516.0,2016-10-07T17:44:57Z,39922986,1,"<p>You can use <code>groupby</code> and <code>sum</code>:</p>

<pre><code>df.groupby(['Name', 'Fruit']).sum()

               Number
Name  Fruit          
Bob   Apples       16
      Grapes       35
      Oranges      67
Mike  Apples        9
      Oranges      57
Steve Apples       10
Tom   Grapes       87
      Oranges      15
Tony  Grapes       15
      Oranges       1
</code></pre>
"
39923156,6293600.0,2016-10-07T17:47:40Z,39863487,0,"<p>My own solution was based upon the below post's second answer:
<a href=""http://stackoverflow.com/questions/11194610/how-can-i-reorder-multi-indexed-dataframe-columns-at-a-specific-level"">How can I reorder multi-indexed dataframe columns at a specific level</a></p>

<p>Pretty much... just create a new dataframe with the multiindex you want.
Trying to insert values using .ix,.loc,.iloc isn't well supported with multiindexed dataframes. If you're looking to completely change the values of the subset of columns (not just swap), Nickil's solution of separating and re-joining the tables is definitely the way to go. However, if you're only looking to swap the columns, the below works perfectly fine. I selected this as the answer over Nickil's solution because this solution worked better for me as I had other data besides 'FOR' grouped by month and it gave me <em>more flexibility in reordering the columns</em>.</p>

<p>First, store the lists IN THE ORDER YOU WANT IT:</p>

<pre><code>&gt;&gt;reindex_list = ['STYLE','COLOR','SIZE','FOR'] #desired order
&gt;&gt;month_list = clean_table_grouped.ix[0:,""FOR""].columns.tolist()
&gt;&gt;month_list.sort(key = lambda x: x[0:2]) #sort by month ascending
&gt;&gt;month_list.sort(key = lambda x: x[-2:]) #sort by year ascending
</code></pre>

<p>Then create a zipped listed where style, color, size get zipped with '', and 'FOR' gets zipped with each month. Like so:</p>

<pre><code>[('STYLE',''),('COLOR',''),..., ('FOR','10/16'), ('FOR','11/16'), ...]
</code></pre>

<p>Here is an algorithm that does it automagically:</p>

<pre><code>&gt;&gt;zip_list = []
&gt;&gt;
for i in reindex_list:
if i in ['FOR']:
    for j in month_list:
        if j != '':
            zip_list.append(zip([i],[j])[0])
else:
    zip_list.append(zip([i],[''])[0])
</code></pre>

<p>Then create a multi index from the tuple list you just zipped:</p>

<pre><code>&gt;&gt;multi_cols = pd.MultiIndex.from_tuples(zip_list, names=['','MONTH'])
</code></pre>

<p>And finally, create a new dataframe from the old with the new multiindex:</p>

<pre><code>&gt;&gt;clean_table_grouped_ordered = pd.DataFrame(clean_table_grouped, columns=multi_cols)
&gt;&gt;clean_table_grouped_ordered[0:5]
       STYLE COLOR SIZE FOR
 MONTH                  10/16   11/16   12/16  01/17
       ####  ####  ###  15.0    15.0    15.0    0.0
       ####  ####  ###  15.0    15.0    15.0    0.0
       ####  ####  ###  15.0    15.0    15.0    0.0
       ####  ####  ###  15.0    15.0    15.0    0.0
       ####  ####  ###  15.0    15.0    15.0    0.0
       ####  ####  ###  15.0    15.0    15.0    0.0
</code></pre>
"
39923815,2817602.0,2016-10-07T18:35:14Z,39922986,1,"<p>Both the other answers accomplish what you want.  </p>

<p>You can use the <code>pivot</code> functionality to arrange the data in a nice table</p>

<pre><code>df.groupby(['Fruit','Name'],as_index = False).sum().pivot('Fruit','Name').fillna(0)



Name    Bob     Mike    Steve   Tom    Tony
Fruit                   
Apples  16.0    9.0     10.0    0.0     0.0
Grapes  35.0    0.0     0.0     87.0    15.0
Oranges 67.0    57.0    0.0     15.0    1.0
</code></pre>
"
39924210,99989.0,2016-10-07T19:04:35Z,39843488,0,"<p>You should not be inheriting from <code>int</code> unless you want both its <em>interface</em> and its <em>implementation</em>.  (Its implementation is an automatically-widening set of bits representing a finite number.  You clearly dont' want that.)  Since you only want the <em>interface</em>, then inherit from the ABC <code>Integral</code>.  Thanks to @ecatmur's answer, we can use <code>infinity</code> to deal with the nitty-gritty of infinity (including negation).  Here is how we could combine <code>infinity</code> with the ABC <code>Integral</code>:</p>

<pre><code>import pytest
from infinity import Infinity
from numbers import Integral


class IntegerInfinity(Infinity, Integral):

    def __and__(self, other):
        raise NotImplementedError

    def __ceil__(self):
        raise NotImplementedError

    def __floor__(self):
        raise NotImplementedError

    def __int__(self):
        raise NotImplementedError

    def __invert__(self, other):
        raise NotImplementedError

    def __lshift__(self, other):
        raise NotImplementedError

    def __mod__(self, other):
        raise NotImplementedError

    def __or__(self, other):
        raise NotImplementedError

    def __rand__(self, other):
        raise NotImplementedError

    def __rlshift__(self, other):
        raise NotImplementedError

    def __rmod__(self, other):
        raise NotImplementedError

    def __ror__(self, other):
        raise NotImplementedError

    def __round__(self):
        raise NotImplementedError

    def __rrshift__(self, other):
        raise NotImplementedError

    def __rshift__(self, other):
        raise NotImplementedError

    def __rxor__(self, other):
        raise NotImplementedError

    def __trunc__(self):
        raise NotImplementedError

    def __xor__(self, other):
        raise NotImplementedError

def test():
    x = IntegerInfinity()
    assert x &gt; 2
    assert not x &lt; 3
    assert x &gt;= 5
    assert not x &lt;= -10
    assert x == x
    assert not x &gt; x
    assert not x &lt; x
    assert x &gt;= x
    assert x &lt;= x
    assert -x == -x
    assert -x &lt;= -x
    assert -x &lt;= x
    assert -x &lt; x
    assert -x &lt; -1000
    assert not -x &lt; -x
    with pytest.raises(Exception):
        int(x)
    with pytest.raises(Exception):
        x | x
    with pytest.raises(Exception):
        ceil(x)
</code></pre>

<p>This can be run with <code>pytest</code> to verify the required invariants.</p>
"
39924574,4651101.0,2016-10-07T19:31:18Z,39695700,0,"<p>Answer was simple as my_token was coming as string and i was converting it to a number. Adding this before converting to a number did the trick:</p>

<p><code>my_token.lstrip(""0"") #removes leading characters</code></p>
"
39925027,4882880.0,2016-10-07T20:08:12Z,39752235,0,"<p>in images processing they are some roles you have to go through such as filters before you go for edges detection, in your condition the dust is just a noise that you have to remove by filter, use gausse or blure after that use thresholding and then use canny for edges and in opencv they are cornere detection you can use, or you can just go for key point after threshholding if i'm not wrong.. try to do those steps and see the resulte</p>
"
39925278,1874627.0,2016-10-07T20:25:58Z,39752235,3,"<p>You are using too small value for rho.</p>

<p><strong>Try the below code:-</strong></p>

<pre><code>import numpy as np
import cv2

gray = cv2.imread('lines.jpg')
edges = cv2.Canny(gray,50,150,apertureSize = 3)
cv2.imwrite('edges-50-150.jpg',edges)
minLineLength=100
lines = cv2.HoughLinesP(image=edges,rho=1,theta=np.pi/180, threshold=100,lines=np.array([]), minLineLength=minLineLength,maxLineGap=80)

a,b,c = lines.shape
for i in range(a):
    cv2.line(gray, (lines[i][0][0], lines[i][0][1]), (lines[i][0][2], lines[i][0][3]), (0, 0, 255), 3, cv2.LINE_AA)
    cv2.imwrite('houghlines5.jpg',gray)
</code></pre>

<p>Note, the change in <strong>rho value, pi value and maxLineGap</strong> to reduce outliers.</p>

<p><strong>Input Image</strong>
<a href=""http://i.stack.imgur.com/4wQjD.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4wQjD.jpg"" alt=""Input Image""></a></p>

<p><strong>Edges Image</strong>
<a href=""http://i.stack.imgur.com/Z1LHg.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Z1LHg.jpg"" alt=""Edges Image""></a></p>

<p><strong>Output Image</strong>
<a href=""http://i.stack.imgur.com/TGbc8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TGbc8.jpg"" alt=""Output Image""></a></p>
"
39926039,5276520.0,2016-10-07T21:26:43Z,39925931,3,"<p>If I understood you right, you want that <code>counter == start</code> for the initial call without specifying the <code>counter</code> manually in the first call.</p>

<p>For this, you can set <code>counter</code> to a default value of <code>None</code> and check for this at the beginning of the function, setting <code>counter</code> to the appropriate value if this is the case:</p>

<pre><code>def Euler5(start, end, counter=None):
    if counter is None:
        counter = start

    x = counter
    while start &lt;= end:
        if x % counter == x % start:
            return Euler5(start+1, end, x)
        else:
            x += counter
    return x
</code></pre>
"
39926675,2357112.0,2016-10-07T22:36:20Z,39926628,5,"<p>You added together a sparse matrix object and a normal ndarray:</p>

<pre><code>X = I+A
</code></pre>

<p>The result is a dense <em>matrix</em> object, an instance of <code>np.matrix</code>, not a normal ndarray.</p>

<p>This:</p>

<pre><code>np.reshape(X, -1)
</code></pre>

<p>ends up returning a matrix, which can't be less than 2D.</p>

<p>This:</p>

<pre><code>np.reshape(np.copy(X), -1)
</code></pre>

<p>makes a normal ndarray in <code>np.copy(X)</code>, so you get a 1D output from <code>np.reshape</code>.</p>

<p>Be very careful at all times about whether you're dealing with sparse matrices, dense matrices, or standard ndarrays. Avoid <code>np.matrix</code> whenever possible.</p>
"
39927261,1388292.0,2016-10-07T23:53:51Z,39836725,10,"<p>A <code>Gtk.RecentManager</code> needs to emit the <code>changed</code> signal for the update to be written in a private attribute of the C++ class. To use a <code>RecentManager</code> object in an application, you need to start the event loop by calling <code>Gtk.main</code>:</p>

<pre><code>from gi.repository import Gtk

recent_mgr = Gtk.RecentManager.get_default()
uri = r'file:/path/to/my/file'
recent_mgr.add_item(uri)
Gtk.main()
</code></pre>

<p>If you don't call <code>Gtk.main()</code>, the <code>changed</code> signal is not emitted and nothing happens.</p>

<p>To answer @andlabs query, the reason why <code>RecentManager.add_item</code> returns a boolean is because the <code>g_file_query_info_async</code> function is called. The callback function <code>gtk_recent_manager_add_item_query_info</code> then gathers the mimetype, application name and command into a <code>GtkRecentData</code> struct and finally calls <code>gtk_recent_manager_add_full</code>. The source is <a href=""https://github.com/GNOME/gtk/blob/master/gtk/gtkrecentmanager.c"">here</a>.</p>

<p>If anything goes wrong, it is well after <code>add_item</code> has finished, so the method just returns <code>True</code> if the object it is called from is a <code>RecentManager</code> and if the uri is not <code>NULL</code>; and <code>False</code> otherwise. </p>

<p>The documentation is inaccurate in saying:</p>

<blockquote>
  <p>Returns</p>
  
  <p>TRUE if the new item was successfully added to the recently used resources list</p>
</blockquote>

<p>as returning <code>TRUE</code> only means that an asynchronous function was called to deal with the addition of a new item.</p>
"
39927339,4677930.0,2016-10-08T00:06:21Z,39927318,1,"<p>I think that what you want is a dic of lists.</p>

<pre><code>result_dict = {'1':[], '2':[], '3':[], '4':[], '5':[], '6' :[]}
for word in text.split():
    if str(len(word)) in result_dict:
        result_dict[str(len(word))].append(word)
return result_dict
</code></pre>
"
39927356,2063361.0,2016-10-08T00:08:44Z,39927318,1,"<p>Instead of defining the default value as <code>0</code>, assign it as <code>set()</code> and within <code>if</code> condition do, <code>result_dict[str(len(word))].add(word)</code>. </p>

<p><em>Also, instead of preassigning <code>result_dict</code>, you should use <a href=""https://docs.python.org/2/library/collections.html#collections.defaultdict"" rel=""nofollow""><code>collections.defaultdict</code></a>.</em> </p>

<p><em>Since you need non-repetitive words, I am using <a href=""https://docs.python.org/2/library/functions.html#func-set"" rel=""nofollow""><code>set</code></a> as value instead of <code>list</code>.</em> </p>

<p>Hence, your final code should be:</p>

<pre><code>from collections import defaultdict
def get_word_len_dict(text):
    result_dict = defaultdict(set)
    for word in text.split():
        result_dict[str(len(word))].add(word)
    return result_dict
</code></pre>

<p>In case it is must that you want <code>list</code> as values (I think <code>set</code> should suffice your requirement), you need to further iterate it as:</p>

<pre><code>for key, value in result_dict.items():
    result_dict[key] = list(value)
</code></pre>
"
39927365,1029516.0,2016-10-08T00:10:05Z,39927318,1,"<p>the problem here is you are <em>counting</em> the word by length, instead you want to <em>group</em> them. You can achieve this by storing a list instead of a int:</p>

<pre><code>def get_word_len_dict(text):
    result_dict = {}
    for word in text.split():
        if len(word) in result_dict:
            result_dict[len(word)].add(word)
        else:
            result_dict[len(word)] = {word} #using a set instead of list to avoid duplicates
    return result_dict
</code></pre>

<p>Other improvements:</p>

<ul>
<li>don't hardcode the key in the initialized <code>dict</code> but let it empty instead. Let the code add the new keys dynamically when necessary</li>
<li>you can use <code>int</code> as keys instead of strings, it will save you the conversion</li>
<li>use <code>set</code>s to avoid repetitions</li>
</ul>

<hr>

<h2>Using <code>groupby</code></h2>

<p>Well, I'll try to propose something different: you can group by length using <a href=""https://docs.python.org/2/library/itertools.html#itertools.groupby"" rel=""nofollow""><code>groupby</code></a> from the python standard library</p>

<pre><code>import itertools
def get_word_len_dict(text):
    # split and group by length (you get a list if tuple(key, list of values)
    groups = itertools.groupby(sorted(text.split(), key=lambda x: len(x)), lambda x: len(x))
    # convert to a dictionary with sets 
    return {l: set(words) for l, words in groups}
</code></pre>
"
39927367,5276520.0,2016-10-08T00:10:10Z,39927318,1,"<p>You say you want the keys to be integers but then you convert them to strings before storing them as a key. There is no need to do this in Python; integers can be dictionary keys.</p>

<p>Regarding your question, simply initialize the values of the keys to empty lists instead of the number 0. Then, in the loop, append the word to the list stored under the appropriate key (the length of the word), like this:</p>

<pre><code>string = ""the faith that he had had had had an affect on his life""

def get_word_len_dict(text):
    result_dict = {i : [] for i in range(1, 7)}
    for word in text.split():
        length = len(word)
        if length in result_dict:
            result_dict[length].append(word)
    return result_dict      
</code></pre>

<p>This results in the following:</p>

<pre><code>&gt;&gt;&gt; get_word_len_dict(string)
{1: [], 2: ['he', 'an', 'on'], 3: ['the', 'had', 'had', 'had', 'had', 'his'], 4: ['that', 'life'], 5: ['faith'], 6: ['affect']}
</code></pre>

<p>If you, as you mentioned, wish to remove the duplicate words when collecting your input string, it seems elegant to use a set and convert to a list as a final processing step, if this is needed. Also note the use of <code>defaultdict</code> so you don't have to manually initialize the dictionary keys and values as a default value <code>set()</code> (i.e. the empty set) gets inserted for each key that we try to access but not others:</p>

<pre><code>from collections import defaultdict

string = ""the faith that he had had had had an affect on his life""

def get_word_len_dict(text):
    result_dict = defaultdict(set)
    for word in text.split():
        length = len(word)
        result_dict[length].add(word)
    return {k : list(v) for k, v in result_dict.items()}
</code></pre>

<p>This gives the following output:</p>

<pre><code>&gt;&gt;&gt; get_word_len_dict(string)
{2: ['he', 'on', 'an'], 3: ['his', 'had', 'the'], 4: ['life', 'that'], 5: ['faith'], 6: ['affect']}
</code></pre>
"
39927394,2442613.0,2016-10-08T00:15:36Z,39927318,1,"<p>Fixing Sabian's answer so that duplicates aren't added to the list:</p>

<pre><code>def get_word_len_dict(text):
    result_dict = {1:[], 2:[], 3:[], 4:[], 5:[], 6 :[]}
    for word in text.split():
        n = len(word)
        if n in result_dict and word not in result_dict[n]:
            result_dict[n].append(word)
    return result_dict
</code></pre>
"
39927424,4913922.0,2016-10-08T00:22:36Z,39927318,1,"<p>Check out <a href=""https://docs.python.org/3/tutorial/datastructures.html"" rel=""nofollow"">list comprehensions</a></p>

<p>Integers are legal dictionaries keys so there is no need to make the numbers strings unless you want it that way for some other reason.
<code>if statement</code> in the <code>for loop</code> controls flow to add word only once. You could get this effect more automatically if you use <code>set()</code> type instead of <code>list()</code> as your value data structure. See more in the docs. I believe the following does the job:</p>

<pre><code>def get_word_len_dict(text):
    result_dict = {len(word) : [] for word in text.split()}
    for word in text.split():
        if word not in result_dict[len(word)]:
            result_dict[len(word)].append(word) 
    return result_dict
</code></pre>

<p>try to make it better ;)</p>
"
39927438,6939121.0,2016-10-08T00:24:30Z,39927318,1,"<p>What you need is a map to list-construct (if not many words, otherwise a 'Counter' would be fine):
Each list stands for a word class (number of characters). Map is checked whether word class ('3') found before. List is checked whether word ('had') found before.</p>

<pre><code>def get_word_len_dict(text):
    result_dict = {}
    for word in text.split():
        if not result_dict.get(str(len(word))): # add list to map?
            result_dict[str(len(word))] = []

        if not word in result_dict[str(len(word))]: # add word to list?
            result_dict[str(len(word))].append(word)

    return result_dict
</code></pre>

<p>--></p>

<pre><code>3 ['the', 'had', 'his']
2 ['he', 'an', 'on']
5 ['faith']
4 ['that', 'life']
6 ['affect']
</code></pre>
"
39927669,847344.0,2016-10-08T01:07:22Z,39927318,1,"<p>Your code is counting the occurrence of each word length - but not storing the words themselves.</p>

<p>In addition to capturing each word into a list of words with the same size, you also appear to want:</p>

<ol>
<li>If a word length is not represented, do not return an empty list for that length - just don't have a key for that length.</li>
<li>No duplicates in each word list</li>
<li>Each word list is sorted</li>
</ol>

<p>A set container is ideal for accumulating the words - sets naturally eliminate any duplicates added to them. </p>

<p>Using defaultdict(sets) will setup an empty dictionary of sets -- a dictionary key will only be created if it is referenced in our loop that examines each word.</p>

<pre><code>from collections import defaultdict 

def get_word_len_dict(text):

    #create empty dictionary of sets 
    d = defaultdict(set)

    # the key is the length of each word
    # The value is a growing set of words
    # sets automatically eliminate duplicates
    for word in text.split():
        d[len(word)].add(word)

    # the sets in the dictionary are unordered
    # so sort them into a new dictionary, which is returned
    # as a dictionary of lists

    return {i:sorted(d[i]) for i in d.keys()}
</code></pre>

<p>In your example string of </p>

<pre><code>a=""the faith that he had had had had an affect on his life""
</code></pre>

<p>Calling the function like this:</p>

<pre><code>z=get_word_len_dict(a)
</code></pre>

<p>Returns the following list:</p>

<pre><code>print(z)
{2: ['an', 'he', 'on'], 3: ['had', 'his', 'the'], 4: ['life', 'that'], 5: ['faith'], 6: ['affect']}
</code></pre>

<p>The type of each value in the dictionary is ""list"".</p>

<pre><code>print(type(z[2]))
&lt;class 'list'&gt;
</code></pre>
"
39929490,1640404.0,2016-10-08T06:38:45Z,39846735,0,"<p>I tried implementing this in python. It isn't quite fast enough to pass the test, but it runs 50x faster then uoyilmaz's solution ported to python. The code for that is below:</p>

<pre><code>#!/usr/bin/env python2.7

from bisect import insort_left
from itertools import combinations


def answer_1(l):
    """"""My own solution.""""""
    indices = {}
    setdefault_ = indices.setdefault
    for i, x in enumerate(l):
        setdefault_(x, []).append(i)

    out = 0
    highest_value = max(l)
    for i, x in enumerate(l):
        multiples = []
        for m in xrange(1, int(highest_value / x) + 1):
            if x * m in indices:
                for j in indices[x * m]:
                    if i &lt; j:
                        insort_left(multiples, (j, x * m))

        if multiples:
            multiples = [m[1] for m in multiples]
            for pair in combinations(multiples, 2):
                out += pair[1] % pair[0] == 0

    return out


def answer_2(l):
    """"""@uoyilmaz's solution ported from Java.""""""
    out = 0
    pair_counts = [0] * len(l)
    for i in xrange(1, len(l) - 1):
        for j in xrange(i):
            if l[i] % l[j] == 0:
                pair_counts[i] += 1

    for i in xrange(2, len(l)):
        for j in xrange(1, i):
            if l[i] % l[j] == 0:
                out += pair_counts[j]

    return out


answer = answer_1

# -----------------------------------------------------------------------------

_SEED = 1.23


def benchmark(sample_count):
    from random import seed, randint
    import timeit
    clock = timeit.default_timer

    seed(_SEED)
    samples = [[randint(1, 999999) for _ in xrange(randint(2, 2000))]
                for _ in xrange(sample_count)]

    start = clock()
    for sample in samples:
        answer(sample)

    end = clock()
    print(""%.4f s elapsed for %d samples."" % (end - start, sample_count))


def test():
    # Provided test cases.
    assert(answer([1, 1, 1]) == 1)
    assert(answer([1, 2, 3, 4, 5, 6]) == 3)

    # Custom test cases.
    assert(answer([1]) == 0)
    assert(answer([1, 2]) == 0)
    assert(answer([2, 4]) == 0)
    assert(answer([1, 1, 1, 1]) == 4)
    assert(answer([1, 1, 1, 1, 1]) == 10)
    assert(answer([1, 1, 1, 1, 1, 1]) == 20)
    assert(answer([1, 1, 1, 1, 1, 1, 1]) == 35)
    assert(answer([1, 1, 2]) == 1)
    assert(answer([1, 1, 2, 2]) == 4)
    assert(answer([1, 1, 2, 2, 2]) == 10)
    assert(answer([1, 1, 2, 2, 2, 3]) == 11)
    assert(answer([1, 2, 4, 8, 16]) == 10)
    assert(answer([2, 4, 5, 9, 12, 34, 45]) == 1)
    assert(answer([2, 2, 2, 2, 4, 4, 5, 6, 8, 8, 8]) == 90)
    assert(answer([2, 4, 8]) == 1)
    assert(answer([2, 4, 8, 16]) == 4)
    assert(answer([3, 4, 2, 7]) == 0)
    assert(answer([6, 5, 4, 3, 2, 1]) == 0)
    assert(answer([4, 7, 14]) == 0)
    assert(answer([4, 21, 7, 14, 8, 56, 56, 42]) == 9)
    assert(answer([4, 21, 7, 14, 56, 8, 56, 4, 42]) == 7)
    assert(answer([4, 7, 14, 8, 21, 56, 42]) == 4)
    assert(answer([4, 8, 4, 16]) == 2)


def main():
    test()
    benchmark(100)


if __name__ == '__main__':
    main()
</code></pre>

<p>Now if anyone has an idea on how to speed this up further, I'm open for suggestions.</p>
"
39930596,2138800.0,2016-10-08T09:04:20Z,39613476,0,"<p>An approach that has served me really well is to use a webserver to handle and scale the process pool. flask-sqlalchemy even in its default state will keep a connection pool and not close each connection on each request response cycle. </p>

<p>The asyncio executor can just call url end points to execute your functions. The added benefit is that because all processes doing the work are behind a url you can trivially scale your worker pool across mutliple machines, adding more processes via gunicorn or one of the other many methods to scale a simple wsgi server. Plus you get all the fault tolerant goodness. </p>

<p>The downside is that you might be passing more information across the network. However as you say the problem is CPU bound and you will probably be passing much more data to and from the database. </p>
"
39930787,2153965.0,2016-10-08T09:28:13Z,39928716,0,"<p>If you want to remove duplication, you can define a function called <code>logout_user</code></p>

<p>In that function, you can remove the session record from your database as well as <code>session.clear()</code>.</p>

<p>Call this function when <code>\logout</code> or wherever suitable.</p>
"
39931909,3142222.0,2016-10-08T11:40:26Z,39922986,0,"<p>Also you can use agg function,</p>

<pre><code>df.groupby(['Name', 'Fruit'])['Number'].agg('sum')
</code></pre>
"
39934209,4099593.0,2016-10-08T15:36:21Z,39934187,6,"<p>It's holding the value of the previous comprehsension. Try inverting them, you'll get an error</p>

<pre><code>&gt;&gt;&gt; data = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]
&gt;&gt;&gt; [y for y in x for x in data]
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
</code></pre>

<p>To further test it out, print <code>x</code> and see </p>

<pre><code>&gt;&gt;&gt; [y for x in data for y in x]
[0, 1, 2, 3, 4, 5, 6, 7, 8]
&gt;&gt;&gt; x
[6, 7, 8]
&gt;&gt;&gt; [y for y in x for x in data]
[6, 6, 6, 7, 7, 7, 8, 8, 8]
</code></pre>
"
39934236,2063361.0,2016-10-08T15:38:41Z,39934187,2,"<p>The list comprehensions are as:</p>

<pre><code>[y for x in data for y in x]
[y for y in x for x in data]
</code></pre>

<p>A <code>for</code> loop conversion of <code>[y for y in x for x in data]</code> is:</p>

<pre><code>for y in x:
    for x in data:
        y
</code></pre>

<p>Here <code>x</code> is holding the last updated value of <code>x</code> of your previous list comprehension which is:</p>

<pre><code>[6, 7, 8]
</code></pre>
"
39936333,2867928.0,2016-10-08T19:11:21Z,39936220,1,"<p>Basically it's always better to keep track of your exceptions and handle them properly. But regarding the difference between the <code>while</code> and <code>for</code> loops in this case when you are calling a <code>next()</code> function within a <code>while</code> loop it's always possible to raise an StopIteration exception, but when you are using a <code>for</code> loop based on the number of <code>next()</code> calls and your iteration it may be different.</p>

<p>For example in this case for even number of iteration it doesn't raise an exception but for odds it does. And the reason is that in even iteration numbers your next is always one item in front of the for loop, while for odds it's not like so.</p>

<pre><code>In [1]: it = iter(range(4))

In [2]: for x in it:
   ...:     print(x)
   ...:     next(it)
   ...:     
0
2

In [3]: it = iter(range(3))

In [4]: for x in it:
            print(x)
            next(it)
   ...:     
0
2
---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
&lt;ipython-input-4-1b3db7079e29&gt; in &lt;module&gt;()
      1 for x in it:
      2             print(x)
----&gt; 3             next(it)
      4 

StopIteration: 
</code></pre>
"
39936670,1324967.0,2016-10-08T19:46:48Z,39936344,0,"<p>If all you need is the count, this can be generalized to <a href=""https://en.wikipedia.org/wiki/Binomial_coefficient"" rel=""nofollow""><code>n</code> choose <code>k</code></a>.  Your total size is <code>n</code> and the number of elements of <code>""A""</code> is <code>k</code>.  So, your answer would be:</p>

<p>(n choose k) = (40 choose 20) = <a href=""http://www.wolframalpha.com/input/?i=40+choose+20"" rel=""nofollow"">137846528820</a></p>
"
39937284,90511.0,2016-10-08T20:56:33Z,39937160,3,"<p>Right now your algorithm has O(N^3) running time, meaning that every time you double the length of the initial list the running time goes up by 8 times.</p>

<p>In the worst case, you cannot improve this. For example, if your numbers are all successive powers of 2, meaning that every number divides every number grater than it, then every triple of numbers is a valid solution so just to print out all the solutions is going to be just as slow as what you are doing now.</p>

<p>If you have a lower ""density"" of numbers that divide other numbers, one thing you can do to speed things up is to search for pairs of numbers instead of triples. This will take time that is only O(N^2), meaning the running time goes up by 4 times when you double the length of the input list. Once you have a list of pairs of numbers you can use it to build a list of triples.</p>

<pre><code># For simplicity, I assume that a number can't occur more than once in the list.
# You will need to tweak this algorithm to be able to deal with duplicates.

# this dictionary will map each number `n` to the list of other numbers
# that appear on the list that are multiples of `n`.
multiples = {}
for n in numbers:
   multiples[n] = []

# Going through each combination takes time O(N^2)
for x in numbers:
   for y in numbers:
     if x != y and y % x == 0:
         multiples[x].append(y)

# The speed on this last step will depend on how many numbers
# are multiples of other numbers. In the worst case this will
# be just as slow as your current algoritm. In the fastest case
# (when no numbers divide other numbers) then it will be just a
# O(N) scan for the outermost loop.
for x in numbers:
    for y in multiples[x]:
        for z in multiples[y]:
            print(x,y,z)
</code></pre>

<p>There might be even faster algorithms, that also take advantage of algebraic properties of division but in your case I think a O(N^2) is probably going to be fast enough.</p>
"
39937498,632088.0,2016-10-08T21:19:31Z,39937160,3,"<p>the key insight is:</p>

<p>if <strong>a</strong> divides <strong>b</strong>, it means <strong>a</strong> ""fits into <strong>b</strong>"".
if <strong>a</strong> doesn't divide <strong>c</strong>, then it means ""<strong>a</strong> doesn't fit into <strong>c</strong>"".
And if <strong>a</strong> can't fit into <strong>c</strong>, then <strong>b</strong> cannot fit into <strong>c</strong> (imagine if <strong>b</strong> fitted into <strong>c</strong>, since <strong>a</strong> fits into <strong>b</strong>, then <strong>a</strong> would fit into all the <strong>b</strong>'s that fit into <strong>c</strong> and so <strong>a</strong> would have to fit into <strong>c</strong> too.. (think of prime factorisation etc))</p>

<p>this means that we can optimise. If we sort the numbers smallest to largest and start with the smaller numbers first. First iteration, start with the smallest number as <strong>a</strong>
If we partition the numbers into two groups, group <strong>1</strong>, the numbers which <strong>a</strong> divides, and group <strong>2</strong> the group which <strong>a</strong> doesn't divide, then we know that no numbers in group <strong>1</strong> can divide numbers in group <strong>2</strong> because no numbers in group <strong>2</strong> have <strong>a</strong> as a factor.</p>

<p>so if we had [2,3,4,5,6,7], we would start with 2 and get:
[2,4,6] and [3,5,7]
we can repeat the process on each group, splitting into smaller groups. This suggests an algorithm that could count the triples more efficiently. The groups will get really small really quickly, which means its efficiency should be fairly close to the size of the output.</p>
"
39938882,6246044.0,2016-10-09T00:50:11Z,39938449,4,"<p>You make a mistake in storing all the values returned from <code>getPoints()</code>. You should store only the possible totals for the points returned so far. You can store all those in a set, and update them with the all the possible values returned from <code>getPoints()</code>. A set will automatically remove duplicate scores, such as 1+11 and 11+1. You can change the set to a sorted list at the end. Here is my code:</p>

<pre><code>def getPointTotal(aList):
    totals = {0}
    for i in aList:
        totals = {p + t for p in getPoints(i) for t in totals}
    return sorted(list(totals))
</code></pre>

<p>I get these results:</p>

<pre><code>&gt;&gt;&gt; print(getPointTotal([1,26, 12]))
[21, 31]
&gt;&gt;&gt; print(getPointTotal([1,14]))
[2, 12, 22]
</code></pre>
"
39938941,1730261.0,2016-10-09T01:02:10Z,39938890,2,"<p>Your issue lies here:</p>

<pre><code>result.append(random.random)
</code></pre>

<p>You are appending the method <code>random.random</code> onto the list âÂ which has the type <code>builtin_function_or_method</code> (thus resulting in the error you are receiving â how would you compare functions?).</p>

<p>Instead, you want to call the method:</p>

<pre><code>result.append(random.random())
</code></pre>
"
39938949,1832058.0,2016-10-09T01:03:56Z,39938862,0,"<p>You use <code>next(file)</code> which reads next line so you skip some lines.</p>
"
39939032,2063361.0,2016-10-09T01:21:31Z,39938890,0,"<p>In <code>generate_random_list()</code> function, you are doing <code>random.random</code>. Since it is a function, you should write it as <code>random.random()</code>. Hence, the code of your <code>generate_random_list()</code> function should be:</p>

<pre><code>def generate_random_list():
    result = []
    for i in range(10):
        result.append(random.random())
    return result
</code></pre>
"
39939038,1405065.0,2016-10-09T01:23:00Z,39938449,1,"<p>Rory Daulton's answer is a good one, and it efficiently gives you the different totals that are possible. I want to offer another approach which is not necessarily better than that one, just a bit different. The benefit to my approach is that you can see the sequence of scores that lead to a given total, not only the totals at the end.</p>

<pre><code>def getPointTotal(cards):
    card_scores = [getPoints(card) for card in cards] # will be a list of lists
    combos = {sorted(scores) for scores in itertools.product(*card_scores)}
    return [(scores, sum(scores)) for scores in combos]
</code></pre>

<p>The key piece of this code is the call to <code>itertools.product(*card_scores)</code>. This takes the lists you got from <code>getPoints</code> for each of the cards in the input list and gets all the combinations. So  <code>product([1, 11], [1, 11], [10])</code> will give <code>(1, 1, 10)</code>, <code>(1, 11, 10)</code>, <code>(11, 1, 10)</code>, and <code>(11, 11, 10)</code>.</p>

<p>This is probably a bit overkill for blackjack scoring where there are not going to be many variations on the scores for a given set of cards. But for a different problem (i.e. a different implementation of the <code>getPoints</code> function), it could be very interesting.</p>
"
39939861,4323.0,2016-10-09T03:53:47Z,39939773,1,"<p>A simple solution is to explicitly ignore the lines you don't need:</p>

<pre><code>with open(path) as infile:
    lines = infile.readlines()
np.loadtxt(lines[2:-2])
del lines # if you want to immediately release the memory
</code></pre>

<p>This directly gives you what you want, assuming the header and footer are always two lines each.</p>
"
39940117,6043170.0,2016-10-09T04:46:35Z,39939741,1,"<p>If all you want to do is print the response that you get back, you can do that in <code>SendRequest</code>, but I suspect tha tyour real problem is that you are self-serializing your post data when <code>requests</code> does that for you.  In any case, since your question is about printing:</p>

<pre><code>    if response.status_code == 200:
        print('Yay, my response was: %s' % response.content)
        self.LastResponse = response
        self.LastJson = json.loads(response.text)
        return True
    else:
        print (""Request return "" + str(response.status_code) + "" error!"")
        # for debugging
        try:
            self.LastResponse = response
            self.LastJson = json.loads(response.text)
        except:
            pass
        return False
</code></pre>
"
39940289,1217358.0,2016-10-09T05:19:37Z,39939773,1,"<p>To avoid the error that might occur because of the text at the end, you can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html"" rel=""nofollow""><code>numpy.genfromtxt</code></a> with the <code>max_rows</code> argument.  For example,</p>

<pre><code>In [26]: with open(filename, 'rb') as f:
    ...:     f.readline()  # skip the header
    ...:     nrows, ncols = [int(field) for field in f.readline().split()]
    ...:     data = np.genfromtxt(f, dtype=int, max_rows=nrows)
    ...:     

In [27]: data
Out[27]: 
array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1],
       [1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1],
       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],
       [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1],
       [1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]])
</code></pre>

<p>(I opened the file in binary mode to avoid a bytes/str problem that <code>genfromtxt</code> has in Python 3.)</p>
"
39941098,5747944.0,2016-10-09T07:27:39Z,39756807,1,"<p>If you're deploying your Flask application with gunicorn, it is already non-blocking. If a client is waiting on a response from one of your views, another client can make a request to the same view without a problem. There will be multiple workers to process multiple requests concurrently. No need to change your code for this to work. This also goes for pretty much every Flask deployment option.</p>
"
39941425,5399734.0,2016-10-09T08:11:30Z,39941393,1,"<p>Referencing <a href=""https://docs.python.org/3/reference/compound_stmts.html#the-for-statement"" rel=""nofollow"">the document</a>: </p>

<blockquote>
  <p>The for-loop makes assignments to the variables(s) in the target list.
  This overwrites all previous assignments to those variables including
  those made in the suite of the for-loop:</p>

<pre><code>for i in range(10):
    print(i)
    i = 5             # this will not affect the for-loop
                      # because i will be overwritten with the next
                      # index in the range
</code></pre>
</blockquote>

<p>So your second example is identical to:</p>

<pre><code>alist = [[1,2], [3,4], [5,6]]
for item in alist:
    item.append(10)    # this statement modifies each item in alist
                       # , which is what item the variable ""points to""
print(alist)
</code></pre>

<p>That is to say, <code>item</code> is a new variable <strong>in every iteration</strong>. As a result, assigning values to it in any specific iteration is pointless, because its value will be overridden by the reference to the next item in <code>alist</code> at the very beginning of the next iteration.</p>
"
39941443,1971805.0,2016-10-09T08:13:29Z,39941393,2,"<p>Are you forgetting that <code>list.append()</code> does not return the list itself but actually modifies the list in place?</p>

<p><code>item = 1</code> does as expected. For the rest of the for-loop, item is now <code>1</code>, and not the list it originally was. It won't reassign what <code>item</code> is, that's not what for-loops do.</p>

<p>However, in your second loop, you're now assigning <code>item = None</code>, because the append function does not return anything but it appends the item to the list in place:</p>

<pre><code>&gt;&gt;&gt; L = [1, 2, 3]
&gt;&gt;&gt; L.append(4)
&gt;&gt;&gt; L
[1, 2, 3, 4]
</code></pre>

<p>Thus, your code is basically saying ""go through each sublist in my main list and append <code>10</code> to it"".</p>
"
39941451,1735406.0,2016-10-09T08:14:22Z,39941393,0,"<p>When you write</p>

<pre><code>for item in alist:
</code></pre>

<p>You are actually creating a copy of each item in <code>liast</code> in the <code>item</code> variable, and you are <em>not</em> getting a reference to <code>item</code>.</p>

<p>However, <code>append</code> changes the list in-place and doesn't return a value, and that's why you're getting the values changed to <code>None</code> (due to the assignment - if you remove it you'll get the appending working fine).</p>
"
39941454,2133144.0,2016-10-09T08:14:37Z,39941393,2,"<p>The <code>=</code> operator does not make any change in the second code, using <code>.append</code> causes changes in <code>alist</code>. Use following line as the third line in the second code. You will see the same result:</p>

<pre><code>item.append(10)
</code></pre>

<p>In the first code <code>item</code> point to another object by <code>item=1</code>, so <code>alist</code> does not change. In the second code you make change on <code>alist</code> by calling <code>append</code> method of it.</p>
"
39941472,21945.0,2016-10-09T08:16:19Z,39941393,4,"<p>In the first example <code>item</code> is bound to each element in list <code>alist</code>, and then <code>item</code> is <em>rebound</em> to the integer <code>1</code>. This does not change the element of the list - it merely <em>rebinds</em> the name <code>item</code> to the <code>int</code> object <code>1</code>.</p>

<p>In the second example the list element (itself a list) is mutated by <code>append()</code>. <code>item</code> is still bound to the sub-list so <code>item.append()</code> mutates the sub-list.</p>
"
39941492,5624358.0,2016-10-09T08:20:06Z,39941393,2,"<p>It's pointer variable in python.</p>

<p>The first example:</p>

<pre><code>for item in alist:
   item = 1
</code></pre>

<p>each item is pointing to each _item in alist but suddenly you change the item value, not the of the value of the _item in alist, as a result nothing change to alist</p>

<p>The second example: </p>

<pre><code>for item in alist:
    item = item.append(10)
</code></pre>

<p>each item is pointing to each _item in alist and then you append something to the item sharing the same memory location of _item, as a result value of _item in alist are changed and alist is changed too.</p>
"
39941795,4118756.0,2016-10-09T09:01:10Z,39941128,3,"<p>There are two ways to get rid of the dark blue corners:</p>

<p>You can flag the data with zero values:</p>

<pre><code>data[data == 0] = np.nan
plt.imshow(data, interpolation = 'none', vmin = 0)
</code></pre>

<p>Or you can create a masked array for <code>imshow</code>:</p>

<pre><code>data_masked = np.ma.masked_where(data == 0, data)
plt.imshow(data_masked, interpolation = 'none', vmin = 0)
</code></pre>

<p>The two solutions above both solve your problem, although the use of masks is a bit more general.</p>

<p>If you want to retain the exact color configuration you need to manually set the <code>vmin</code>/<code>vmax</code> arguments for plotting the image. Passing <code>vmin = 0</code> to <code>plt.imshow</code> above makes sure that the discarded zeros still show up on the color bar.
<a href=""http://i.stack.imgur.com/iIjpF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iIjpF.png"" alt=""masked-imshow""></a></p>
"
39942033,287976.0,2016-10-09T09:28:59Z,39941393,5,"<p>This is a somewhat unintuitive behavor of variables. It happens because, in Python, variables are always references to values.</p>

<h1>Boxes and tags</h1>

<p>In some languages, we tend to think about variables as ""boxes"" where we put values; in Python, however, they are references, and behave more like tags or ""nicknames"" to values. So, when you attribute 1 to <code>item</code>, you are changing only the variable reference, not the list it is pointing to.</p>

<p>A graphic representation can help. The image below represents the list created by <code>alist = [[1,2], [3,4], [5,6]]</code></p>

<p><a href=""http://i.stack.imgur.com/KpcjH.png""><img src=""http://i.stack.imgur.com/KpcjH.png"" alt=""A list with three sublists""></a></p>

<p>Given that, let's see what happens when we execute your first loop.</p>

<h1>The first loop</h1>

<p>When you execute <code>for item in alist</code>, you are asking the interpreter to take each value from the list, one per time, put it on the variable <code>item</code> and do some operation in it. In the first operation, for example, we have this new schema:</p>

<p><a href=""http://i.stack.imgur.com/wPXKX.png""><img src=""http://i.stack.imgur.com/wPXKX.png"" alt=""Now a variable points to a sublist""></a></p>

<p>Note that we do not copy the sublist to <code>item</code>; instead, we <em>point</em> to it through <code>item</code>. Then, we execute <code>item = 1</code> â but what does it mean? It mean that we are making <code>item</code> point to the value <code>1</code>, instead of pointing to the sublist:</p>

<p><a href=""http://i.stack.imgur.com/snmRy.png""><img src=""http://i.stack.imgur.com/snmRy.png"" alt=""item points to other value now""></a></p>

<p>Note that the old reference is lost (it is the red arrow) and now we have a new. But we just changed a variable pointing to a list â we did not alter the list itself.</p>

<p>Then, we enter to the second iteration of the loop, and now <code>item</code> points to the second sublist:</p>

<p><a href=""http://i.stack.imgur.com/ERT2X.png""><img src=""http://i.stack.imgur.com/ERT2X.png"" alt=""item pointing to the second sublist""></a></p>

<p>When we execute <code>item = 1</code>, again, we just make the variable point to aonther value, without changing the list:</p>

<p><a href=""http://i.stack.imgur.com/7jxHt.png""><img src=""http://i.stack.imgur.com/7jxHt.png"" alt=""enter image description here""></a></p>

<p>Now, what happens when we execute the second loop?</p>

<h1>The second loop</h1>

<p>The second loop starts as the first one: we make <code>item</code> refer to the first sublist:</p>

<p><a href=""http://i.stack.imgur.com/wPXKX.png""><img src=""http://i.stack.imgur.com/wPXKX.png"" alt=""Now a variable points to a sublist""></a></p>

<p>The first difference, however, is that we call <code>item.append()</code>. <code>append()</code> is a <em>method</em>, so it can change the value of the object it is calling. As we use to say, we are sending a <em>message</em> to the object pointed by <code>item</code> to append the value 10. In this case, the operation is <em>not</em> being made in the variable <code>item</code>, but directly in the object it refers! So here is the result of calling <code>item.append()</code>:</p>

<p><a href=""http://i.stack.imgur.com/XXcfr.png""><img src=""http://i.stack.imgur.com/XXcfr.png"" alt=""A list has grown""></a></p>

<p>However, we do not only append a value to the list! We also assign the value returned by <code>item.append()</code>. This will sever the <code>item</code> reference to the sublist, but here is a catch: <code>append()</code> returns <code>None</code>.</p>

<p><a href=""http://i.stack.imgur.com/7T77S.png""><img src=""http://i.stack.imgur.com/7T77S.png"" alt=""enter image description here""></a></p>

<h1>The <code>None</code> value</h1>

<p><code>None</code> is a value that represents, basically, the unavailability of a relevant value. When a function returns <code>None</code>, it is saying, most of the time, ""I have nothing relevant to give you back."" <code>append()</code> does change its list directly, so there is nothing it needs to return.</p>

<p>It is important because you probably believed <code>item</code> would point to the appended list <code>[1, 2, 10]</code>, right? No, now it points to <code>None</code>. So, you would expect the code below...</p>

<pre><code>alist = [[1,2], [3,4], [5,6]]
for item in alist:
    item = item.append(10)
    print(item)
</code></pre>

<p>To print something like this:</p>

<pre><code>[1, 2, 10]
[3, 4, 10]
[5, 6, 10]
</code></pre>

<p>But this <em>does not</em> happen. <em>This</em> is what happens:</p>

<pre><code>&gt;&gt;&gt; alist = [[1,2], [3,4], [5,6]]
&gt;&gt;&gt; for item in alist:
...     item = item.append(10)
...     print(item)
...
None
None
None
</code></pre>

<p>Yet, as we commented, the <code>append()</code> method changed the lists themselves. So, while the <code>item</code> variable was useless after the assignment, the final list is modified!</p>

<pre><code>&gt;&gt;&gt; print alist
[[1, 2, 10], [3, 4, 10], [5, 6, 10]]
</code></pre>

<p>If you want to use the appended list inside the loop, just do not assign the method returned value to <code>item</code>. Do this:</p>

<pre><code>&gt;&gt;&gt; alist = [[1,2], [3,4], [5,6]]
&gt;&gt;&gt; for item in alist:
...     item.append(10)
...     print item
... 
[1, 2, 10]
[3, 4, 10]
[5, 6, 10]
</code></pre>

<p>This works because <code>item</code> will still point to the list.</p>

<h1>Conclusion</h1>

<p>References are somewhat complex to understand at first, let alone master. Yet, they are really powerful and can be learned if you follow examples etc. Your example is a bit complicated because there is more happening here.</p>

<p>The <a href=""http://www.pythontutor.com/"">Python Tutor</a> can help you understand what is going on, because it executes each step graphically. <a href=""http://www.pythontutor.com/visualize.html#code=alist%20%3D%20%5B%5B1,2%5D,%20%5B3,4%5D,%20%5B5,6%5D%5D%0Afor%20item%20in%20alist%3A%0A%20%20%20%20item%20%3D%201%0Aprint(alist%29%0A%0Aalist%20%3D%20%5B%5B1,2%5D,%20%5B3,4%5D,%20%5B5,6%5D%5D%0Afor%20item%20in%20alist%3A%0A%20%20%20%20item%20%3D%20item.append(10%29%0Aprint(alist%29&amp;cumulative=false&amp;curInstr=0&amp;heapPrimitives=false&amp;mode=display&amp;origin=opt-frontend.js&amp;py=2&amp;rawInputLstJSON=%5B%5D&amp;textReferences=false"">Check your own code running there!</a></p>
"
39942230,4099593.0,2016-10-09T09:53:36Z,39942189,3,"<p>Dictionaries overwrite the previous key, Use a <a href=""https://docs.python.org/3/library/collections.html#collections.defaultdict"" rel=""nofollow"">defaultdict here</a> </p>

<pre><code>&gt;&gt;&gt; import collections 
&gt;&gt;&gt; answer = collections.defaultdict(set)
&gt;&gt;&gt; for line in f: 
...     k, v = line.split("";"")
...     answer[k].add(v)
... 
&gt;&gt;&gt; answer
defaultdict(&lt;class 'set'&gt;, {'b': {'d'}, 'd': {'g'}, 'f': {'d', 'g'}, 'e': {'d'}, 'a': {'c', 'b'}, 'c': {'f', 'e'}})
</code></pre>

<p>If you prefer the traditional approach, then you can add a <code>if</code> condition </p>

<pre><code>&gt;&gt;&gt; answer = {}
&gt;&gt;&gt; for line in f:
...     k,v = line.split("";"")
...     if k in answer:
...         answer[k].add(v)
...     else:
...         answer[k] = {v}
... 
&gt;&gt;&gt; answer
{'b': {'d'}, 'd': {'g'}, 'f': {'d', 'g'}, 'e': {'d'}, 'a': {'c', 'b'}, 'c': {'f', 'e'}}
</code></pre>
"
39943161,847552.0,2016-10-09T11:39:12Z,39941174,0,"<p>Just use <code>socket</code> functions. The first line of code in your question is almost 10 times faster than your string manipulations:</p>

<pre><code>from socket import inet_ntop, inet_pton, AF_INET6
def compact1(addr, inet_ntop=inet_ntop, inet_pton=inet_pton, AF_INET6=AF_INET6):
    return inet_ntop(AF_INET6, inet_pton(AF_INET6, addr))

from ipaddress import IPv6Address
def compact2(addr, IPv6Address=IPv6Address):
    return IPv6Address(addr)

import re
def compact3(addr, sub=re.sub):
    address = "":"".join('' if i=='0000' else i.lstrip('0') for i in addr.split(':'))
    return sub(r'(:)\1+', r'\1\1', address).lower()
</code></pre>

<p>And now let's <code>%timeit</code>:</p>

<pre><code>In[9]: ips = [':'.join('{:x}'.format(random.randint(0, 2**16 - 1)) for i in range(8)) for _ in range(65565)]

In[10]: %timeit for ip in ips: compact1(ip)
10 loops, best of 3: 52.9 ms per loop

In[11]: %timeit for ip in ips: compact2(ip)
1 loop, best of 3: 715 ms per loop

In[12]: %timeit for ip in ips: compact3(ip)
1 loop, best of 3: 411 ms per loop
</code></pre>
"
39943787,2141635.0,2016-10-09T12:46:02Z,39943765,3,"<p>You can <a href=""https://docs.python.org/2/library/ast.html#ast.literal_eval"" rel=""nofollow""><em>literal_eval</em></a> each key and reassign:</p>

<pre><code>from ast import literal_eval

bigram_dict = {""('key1', 'key2')"": 'meaning', ""('key22', 'key13')"": 'mean2'}


for k,v in bigram_dict.items():
    bigram_dict[literal_eval(k)] = v
</code></pre>

<p>Or to create a new dict, just use the same logic with a <em>dict comprehension</em>:</p>

<pre><code>{literal_eval(k):v for k,v in bigram_dict.items()}
</code></pre>

<p>Both will give you:</p>

<pre><code>{('key1', 'key2'): 'meaning', ('key22', 'key13'): 'mean2'}
</code></pre>
"
39943789,1832539.0,2016-10-09T12:46:20Z,39943765,6,"<p>This can be done using a dictionary comprehension, where you call <a href=""https://docs.python.org/3/library/ast.html#ast.literal_eval"">literal_eval</a> on the key:</p>

<pre><code>from ast import literal_eval
bigram_dict = {""('key1', 'key2')"": 'meaning', ""('key22', 'key13')"": 'mean2'}

res = {literal_eval(k): v for k,v in bigram_dict.items()}
</code></pre>

<p>Result:</p>

<pre><code>{('key22', 'key13'): 'mean2', ('key1', 'key2'): 'meaning'}
</code></pre>
"
39945278,4964651.0,2016-10-09T15:29:42Z,39945178,2,"<p>You should set the name attribute of your index to <code>None</code>:</p>

<pre><code>df.index.names = [None]
df.head()
#       Boys    Girls
#1996   333490  315995
#1997   329577  313518
#1998   325903  309998
</code></pre>

<p>As for retrieving the data for <code>1998</code>, simply lose the quotes:</p>

<pre><code>df.loc[1998]
#Boys     325903
#Girls    309998
#Name: 1998, dtype: int64
</code></pre>
"
39945689,3760132.0,2016-10-09T16:10:20Z,39945615,1,"<p>In Python 2.7, <code>input()</code> returns an integer. To read input as a string use the function <code>raw_input()</code> instead. Alternatively, you can switch to Python 3 and where <code>input()</code> always returns a string.</p>

<p>Also your solution isn't very neat in case the user is providing numbers with more than 1 digits. For example the string ""123"" can be interpreted as [1, 2, 3], [12, 3] and so on. </p>

<p>A neat solution is to ask the user to provide the input separated by spaces as follows x_1, x_2, ... x_n</p>

<p>Then your code in Python 3.0 will look like</p>

<pre><code>lst = [int(x) for x in input().split()]
</code></pre>

<p>And for Python 2.7</p>

<pre><code>lst = [int(x) for x in raw_input().split()]
</code></pre>
"
39945704,389289.0,2016-10-09T16:11:54Z,39945615,1,"<p>Function <code>input</code> behaves quite differently in Python 2 and in Python 3.</p>

<p>This seems to be Python 2. In Python 2, <code>input</code> evaluates entered data as Python code. If only digits are entered, <code>input</code> will return one integer. Converting that to a list is not possible, hence the error.</p>

<p><code>input</code> is unsafe and causes many problems, so it is best to avoid it. Use <code>raw_input</code> instead.</p>

<pre><code>user_input = raw_input(""Please enter an 8 digit number: "")
</code></pre>

<p>This will return a string, e.g. <code>'12345678'</code>.</p>

<p>This can be converted to a list. The list will iterate through the string character by character.</p>

<pre><code>digits = list(user_input)   # e.g. ['1', '2', '3', '4', '5', '6', '7', '8']
</code></pre>

<p>But that is not even needed, you can directly do as you did:</p>

<pre><code>numbers = [int(i) for i in user_input]   # e.g. [1, 2, 3, 4, 5, 6, 7, 8]
</code></pre>

<hr>

<p>BTW, the Python 3 version of <code>input</code> is the same as Python 2 <code>raw_input</code>.</p>
"
39945762,120163.0,2016-10-09T16:17:34Z,39779538,7,"<p>I looked at the other answers; it appears people are doing backflips to get around the problems of computing line numbers, when your real problem is one of modifying the code.   That suggests the baseline machinery is not helping you the way you really need.</p>

<p>If you use a <a href=""https://en.wikipedia.org/wiki/Program_transformation"">program transformation system (PTS)</a>, you could avoid a lot of this nonsense.</p>

<p>A good PTS will parse your source code to an AST, and then let you apply source-level rewrite rules to modify the AST, and will finally convert the modified AST back into source text.   Generically PTSes accept transformation rules of essentially this form:</p>

<pre><code>   if you see *this*, replace it by *that*
</code></pre>

<p>[A parser that builds an AST is NOT a PTS.  They don't allow rules like this; you can write ad hoc code to hack at the tree, but that's usually pretty awkward.   Not do they do the AST to source text regeneration.]</p>

<p>(My PTS, see bio, called) DMS is a PTS that could accomplish this.  OP's specific example would be accomplished easily by using the following rewrite rule:</p>

<pre><code> source domain Python; -- tell DMS the syntax of pattern left hand sides
 target domain Python; -- tell DMS the syntax of pattern right hand sides

 rule replace_description(e: expression): statement -&gt; statement =
     "" description = \e ""
  -&gt;
     "" description = ('line 1'
                      'line 2'
                      'line 3')"";
</code></pre>

<p>The one transformation rule is given an name <em>replace_description</em> to distinguish it from all the other rule we might define.   The rule parameters (e: expression) indicate the pattern will allow an arbitrary expression as defined by the source language.   <em>statement->statement</em> means the rule maps a statement in the source language, to a statement in the target language; we could use any other syntax category from the Python grammar provided to DMS.  The <strong>""</strong> used here is a <em>metaquote</em>, used to distinguish the syntax of the rule language form the syntax of the subject language.  The second <strong>-></strong>  separates the source pattern <em>this</em> from the target pattern <em>that</em>.</p>

<p>You'll notice that there is no need to mention line numbers.  The PTS converts the rule surface syntax into corresponding ASTs by actually parsing the patterns with the same parser used to parse the source file.  The ASTs produced for the patterns are used to effect the pattern match/replacement.   Because this is driven from ASTs, the actual layout of the orginal code (spacing, linebreaks, comments) don't affect DMS's ability to match or replace.  Comments aren't a problem for matching because they are attached to tree nodes rather than being tree nodes; they are preserved in the transformed program.  DMS does capture line and precise column information for all tree elements; just not needed to implement transformations. Code layout is also preserved in the output by DMS, using that line/column information. </p>

<p>Other PTSes offer generally similar capabilities.</p>
"
39946300,1391444.0,2016-10-09T17:13:39Z,39946092,5,"<p>Totally silly of course, but for fun:</p>

<pre><code>s = '34 3 542 11'

n = """"; total = 0
for c in s:
    if c == "" "":
        total = total + int(n)
        n = """"
    else:
        n = n + c
# add the last number
total = total + int(n)

print(total)
&gt; 590
</code></pre>

<p>This assumes all characters (apart from whitespaces) are figures.</p>
"
39946311,3377150.0,2016-10-09T17:14:56Z,39946092,2,"<p>You've definitely put some effort in here, but one part of your approach definitely won't work as-is: you're iterating over the <em>characters</em> in the string, but you keep trying to treat each character as its own number. I've written a (very commented) method that accomplishes what you want without using any lists or list methods:</p>

<pre><code>def sum_numbers(s):
    """"""
    Convert a string of numbers into a sum of those numbers.

    :param s: A string of numbers, e.g. '1 -2 3.3 4e10'.
    :return: The floating-point sum of the numbers in the string.
    """"""
    def convert_s_to_val(s):
        """"""
        Convert a string into a number. Will handle anything that
        Python could convert to a float.

        :param s: A number as a string, e.g. '123' or '8.3e-18'.
        :return: The float value of the string.
        """"""
        if s:
            return float(s)
        else:
            return 0
    # These will serve as placeholders.
    sum = 0
    current = ''
    # Iterate over the string character by character.
    for c in s:
        # If the character is a space, we convert the current `current`
        # into its numeric representation.
        if c.isspace():
            sum += convert_s_to_val(current)
            current = ''
        # For anything else, we accumulate into `current`.
        else:
            current = current + c
    # Add `current`'s last value to the sum and return.
    sum += convert_s_to_val(current)
    return sum
</code></pre>

<hr>

<p>Personally, I would use this one-liner, but it uses <code>str.split()</code>:</p>

<pre><code>def sum_numbers(s):
    return sum(map(float, s.split()))
</code></pre>
"
39946356,6054783.0,2016-10-09T17:18:55Z,39946092,0,"<p>Try this:</p>

<pre><code>def sum_numbers(s):
    sum = 0
    #This string will represent each number
    number_str = ''
    for i in s:
        if i == ' ':
            #if it is a whitespace it means
            #that we have a number so we incease the sum
            sum += int(number_str)
            number_str = ''
            continue
        number_str += i
    else:
        #add the last number
        sum += int(number_str)
    return sum
</code></pre>
"
39946415,5771269.0,2016-10-09T17:25:24Z,39946092,1,"<p>No lists were used (nor harmed) in the production of this answer:</p>

<pre><code>def sum_string(string):
    total = 0

    if len(string):
        j = string.find("" "") % len(string) + 1
        total += int(string[:j]) + sum_string(string[j:])

    return total
</code></pre>

<p>If the string is noisier than the OP indicates, then this should be more robust:</p>

<pre><code>import re

def sum_string(string):
    pattern = re.compile(r""[-+]?\d+"")

    total = 0

    match = pattern.search(string)

    while match:

        total += int(match.group())

        match = pattern.search(string, match.end())

    return total
</code></pre>

<p><strong>EXAMPLES</strong></p>

<pre><code>&gt;&gt;&gt; sum_string('34 3 542 11')
590
&gt;&gt;&gt; sum_string('   34    4   ')
38
&gt;&gt;&gt; sum_string('lksdjfa34adslkfja4adklfja')
38
&gt;&gt;&gt; # and I threw in signs for fun
... 
&gt;&gt;&gt; sum_string('34 -2 45 -8 13')
82
&gt;&gt;&gt; 
</code></pre>
"
39946519,298607.0,2016-10-09T17:36:33Z,39946092,0,"<p>You could write a generator:</p>

<pre><code>def nums(s):
    idx=0
    while idx&lt;len(s):
        ns=''
        while idx&lt;len(s) and s[idx].isdigit():
            ns+=s[idx]
            idx+=1
        yield int(ns)
        while idx&lt;len(s) and not s[idx].isdigit():
            idx+=1

&gt;&gt;&gt; list(nums('34 3 542 11'))
[34, 3, 542, 11]
</code></pre>

<p>Then just sum that:</p>

<pre><code>&gt;&gt;&gt; sum(nums('34 3 542 11')) 
590
</code></pre>

<p>or, you could use <code>re.finditer</code> with a regular expression and a generator construction:</p>

<pre><code>&gt;&gt;&gt; sum(int(m.group(1)) for m in re.finditer(r'(\d+)', '34 3 542 11'))
590
</code></pre>

<p>No lists used...</p>
"
39946554,5033247.0,2016-10-09T17:39:26Z,39946092,0,"<pre><code>def sum_numbers(s):
    total=0
    gt=0 #grand total
    l=len(s)
    for i in range(l):
        if(s[i]!=' '):#find each number
            total = int(s[i])+total*10
        if(s[i]==' ' or i==l-1):#adding to the grand total and also add the last number
            gt+=total
            total=0
    return gt

print(sum_numbers('1 2 3'))
</code></pre>

<p>Here each substring is converted to number and added to grant total</p>
"
39946590,88558.0,2016-10-09T17:43:01Z,39946538,1,"<p>The sample provided does not make much sense because the only change is that the ` character is moved one position to the left.</p>

<p>However, this might do the trick (to keep the dot inside the paranthesis):</p>

<pre><code>text = re.sub(r'\.\s*\)\s*\.', '.)', text)
</code></pre>

<p>Or this to have it outside:</p>

<pre><code>text = re.sub(r'\.\s*\)\s*\.', ').', text)
</code></pre>

<p><strong>Edit:</strong> Or maybe you're looking for this to replace the dot before the opening paranthesis?</p>

<pre><code>text = re.sub(r'\.(?=\s*\(.*?\)\.)', ').', text)
</code></pre>
"
39946657,5459839.0,2016-10-09T17:49:41Z,39946538,1,"<p>I would suggest this to remove a dot before parentheses when there is another one following them:</p>

<pre><code>text = re.sub(r'\.(\s*?\([^)]*\)\s*\.)', r'\1', text)
</code></pre>

<p>See it run on <a href=""https://repl.it/DrvM/3"" rel=""nofollow"">repl.it</a></p>
"
39946925,2141635.0,2016-10-09T18:15:05Z,39946798,0,"<p>Presuming you have comma separated values, you can use a <em>frozenset</em> of the pairings and use a <em>Counter</em> dict to get the counts:</p>

<pre><code>from collections import Counter
import csv

with open(""test.csv"") as f:
    next(f)
    counts = Counter(frozenset(tuple(row[-1].split("","")))
                     for row in csv.reader(f))
    print(counts.most_common())
</code></pre>

<p>If you want all combinations or pairs as per your updated input:</p>

<pre><code>from collections import Counter
from itertools import combinations

def combs(s):
    return  combinations(s.split("",""), 2)

import csv
with open(""test.csv"") as f:
    next(f)
    counts = Counter(frozenset(t)
                     for row in csv.reader(f)
                            for t in combs(row[-1]))
    # counts -&gt; Counter({frozenset(['Cheese', 'Cookie']): 2, frozenset(['Cheese', 'Pie']): 1, frozenset(['Cookie', 'Pie']): 1})
    print(counts.most_common())
</code></pre>

<p>The order of the pairings is irrelevant as <code>frozenset([1,2])</code> and <code>frozenset([2,1])</code> would be considered the same.</p>

<p>If you want to consider all combinations from <code>2-n</code>:</p>

<pre><code>def combs(s):
    indiv_items = s.split("","")
    return chain.from_iterable(combinations(indiv_items, i) for i in range(2, len(indiv_items) + 1))


import csv

with open(""test.csv"") as f:
    next(f)
    counts = Counter(frozenset(t)
                     for row in csv.reader(f)
                         for t in combs(row[-1]))
    print(counts)
    print(counts.most_common())
</code></pre>

<p>Which for:</p>

<pre><code>Receipt,Name,Address,Date,Time,Items
25007,A,ABC,pte,ltd,4/7/2016,10:40,""Cheese,Cookie,Pie""
25008,B,CCC,pte,ltd,4/7/2016,12:40,""Cheese,Cookie""
25009,B,CCC,pte,ltd,4/7/2016,12:40,""Cookie,Cheese,pizza""
25010,B,CCC,pte,ltd,4/7/2016,12:40,""Pie,Cheese,pizza""
</code></pre>

<p>would give you:</p>

<pre><code>Counter({frozenset(['Cheese', 'Cookie']): 3, frozenset(['Cheese', 'pizza']): 2, frozenset(['Cheese', 'Pie']): 2, frozenset(['Cookie', 'Pie']): 1, frozenset(['Cheese', 'Cookie', 'Pie']): 1, frozenset(['Cookie', 'pizza']): 1, frozenset(['Pie', 'pizza']): 1, frozenset(['Cheese', 'Cookie', 'pizza']): 1, frozenset(['Cheese', 'Pie', 'pizza']): 1})
[(frozenset(['Cheese', 'Cookie']), 3), (frozenset(['Cheese', 'pizza']), 2), (frozenset(['Cheese', 'Pie']), 2), (frozenset(['Cookie', 'Pie']), 1), (frozenset(['Cheese', 'Cookie', 'Pie']), 1), (frozenset(['Cookie', 'pizza']), 1), (frozenset(['Pie', 'pizza']), 1), (frozenset(['Cheese', 'Cookie', 'pizza']), 1), (frozenset(['Cheese', 'Pie', 'pizza']), 1)]
</code></pre>
"
39947061,5349916.0,2016-10-09T18:29:09Z,39946666,1,"<p>From the <a href=""https://docs.python.org/3.5/howto/descriptor.html#invoking-descriptors"" rel=""nofollow"">descriptor documentation</a>:</p>

<blockquote>
  <p>The details of invocation depend on whether <code>obj</code> is an object or a class.</p>
</blockquote>

<p>Basically, instances call descriptors as <code>type(b).__dict__['x'].__get__(b, type(b))</code>, while classes call descriptors as <code>B.__dict__['x'].__get__(None, B)</code>. If <code>obj is None</code> means the getter was called from the class, not an instance.</p>

<p>This machinery is used for example to implement <a href=""https://docs.python.org/3.5/howto/descriptor.html#static-methods-and-class-methods"" rel=""nofollow"">classmethods</a>.</p>

<p>The <code>__set__</code> and <code>__delete__</code> do not check for <code>obj is None</code> because they can never be called like this. Only <code>__get__</code> is invoked when called from the class. Doing <code>cls.prop = 2</code> or <code>del cls.prop</code> will directly overwrite or delete the property object, without invoking <code>__set__</code> or <code>__delete__</code>.</p>
"
39947066,1427416.0,2016-10-09T18:29:25Z,39946666,2,"<p>You can see what the effect is by making another version that leaves that test out.  I made a class Property that uses the code you posted, and another BadProperty that leaves out that <code>if</code> block.  Then I made this class:</p>

<pre><code>class Foo(object):
    @Property
    def good(self):
        print(""In good getter"")
        return ""good""

    @good.setter
    def good(self, val):
        print(""In good setter"")

    @BadProperty
    def bad(self):
        print(""In bad getter"")
        return ""bad""

    @bad.setter
    def bad(self, val):
        print(""In bad setter"")
</code></pre>

<p>The similarities and differences can be seen in this example:</p>

<pre><code>&gt;&gt;&gt; x = Foo()

# same
&gt;&gt;&gt; x.good
In good getter
'good'
&gt;&gt;&gt; x.bad
In bad getter
'bad'
&gt;&gt;&gt; x.good = 2
In good setter
&gt;&gt;&gt; x.bad = 2
In bad setter

# different!
&gt;&gt;&gt; Foo.good
&lt;__main__.Property object at 0x0000000002B71470&gt;
&gt;&gt;&gt; Foo.bad
In bad getter
'bad'
</code></pre>

<p>The effect of the <code>if</code> block is to return the raw property object itself if it is accessed via the class.  Without this check, the getter is called even when accessing the descriptor via the class.</p>

<p>The <code>__set__</code> and <code>__del__</code> methods do not need such a check, since the descriptor protocol is not invoke at all when setting/deleting attributes on a class (only on an instance).  This is not totally obvious from the documentation, but can be seen in the difference between the description of <code>__get__</code> vs. those of <code>__set__</code>/<code>__del__</code> in <a href=""https://docs.python.org/3/reference/datamodel.html#object.__get__"" rel=""nofollow"">the docs</a>, where <code>__get__</code> can get the attribute of ""the owner class or an instance"" but <code>__set__</code>/<code>__del__</code> only set/delete the attribute on an instance.</p>
"
39947519,298607.0,2016-10-09T19:15:46Z,39946798,2,"<p>Suppose after processing the CSV file you find the list of items from the CSV file to be:</p>

<pre><code>&gt;&gt;&gt; items=['Cheese,Cookie,Pie', 'Cheese,Cookie,Pie', 'Cake,Cookie,Cheese', 
... 'Cheese,Mousetrap,Pie', 'Cheese,Jam','Cheese','Cookie,Cheese,Mousetrap']
</code></pre>

<p>First determine all possible pairs:</p>

<pre><code>&gt;&gt;&gt; from itertools import combinations
&gt;&gt;&gt; all_pairs={frozenset(t) for e in items for t in combinations(e.split(','),2)}
</code></pre>

<p>Then you can do:</p>

<pre><code>from collections import Counter
pair_counts=Counter()
for s in items:
    for pair in {frozenset(t) for t in combinations(s.split(','), 2)}:
        pair_counts.update({tuple(pair):1})

&gt;&gt;&gt; pair_counts
Counter({('Cheese', 'Cookie'): 4, ('Cheese', 'Pie'): 3, ('Cookie', 'Pie'): 2, ('Cheese', 'Mousetrap'): 2, ('Cookie', 'Mousetrap'): 1, ('Cheese', 'Jam'): 1, ('Mousetrap', 'Pie'): 1, ('Cake', 'Cheese'): 1, ('Cake', 'Cookie'): 1})
</code></pre>

<p>Which can be extended to a more general case:</p>

<pre><code>max_n=max(len(e.split(',')) for e in items)
for n in range(max_n, 1, -1):
    all_groups={frozenset(t) for e in items for t in combinations(e.split(','),n)}
    group_counts=Counter()
    for s in items:
        for group in {frozenset(t) for t in combinations(s.split(','), n)}:
            group_counts.update({tuple(group):1})      
    print 'group length: {}, most_common: {}'.format(n, group_counts.most_common())     
</code></pre>

<p>Prints:</p>

<pre><code>group length: 3, most_common: [(('Cheese', 'Cookie', 'Pie'), 2), (('Cheese', 'Mousetrap', 'Pie'), 1), (('Cheese', 'Cookie', 'Mousetrap'), 1), (('Cake', 'Cheese', 'Cookie'), 1)]
group length: 2, most_common: [(('Cheese', 'Cookie'), 4), (('Cheese', 'Pie'), 3), (('Cookie', 'Pie'), 2), (('Cheese', 'Mousetrap'), 2), (('Cookie', 'Mousetrap'), 1), (('Cheese', 'Jam'), 1), (('Mousetrap', 'Pie'), 1), (('Cake', 'Cheese'), 1), (('Cake', 'Cookie'), 1)]
</code></pre>
"
39947574,6941046.0,2016-10-09T19:20:12Z,39937160,0,"<p>This is the best answer that I was able to come up with so far. It's fast, but not quite fast enough. I'm still posting it because I'm probably going to abandon this question and don't want to leave out any progress I've made.</p>

<pre><code>def answer(l):
    num_dict = {}
    ans_set = set()

    for a2, a in enumerate(l):
        num_dict[(a, a2)] = []

    for x2, x in enumerate(l):
        for y2, y in enumerate(l):
            if (y, y2) != (x, x2) and y % x == 0:
                pair = (y, y2)
                num_dict[(x, x2)].append(pair)

    for x in num_dict:
        for y in num_dict[x]:
            for z in num_dict[y]:
                ans_set.add((x[0], y[0], z[0]))

    return len(ans_set)
</code></pre>
"
39947694,677824.0,2016-10-09T19:32:02Z,39896985,2,"<p>First of all, I want to commend you for usage non linear rectifying. According to what Geoffrey Hinton inventor of Boltzmann machine believe, non linear rectifier is a best feet for activities of human brain. </p>

<p>But for other parts you've chosen I propose you to change NN architecture. For predictions of stock market you should use some recurrent NN: easiest candidates could be Elman or Jordan networks. Or you can try more complicated, like LSTM network. </p>

<p>Another part of advice, I propose to modify what you feed in NN. In general, I recommend you to apply scaling and normalization. For example don't feed in NN raw price. Modify it in one of the following ways ( those proposals are not written in stone ): 
1. feed in NN percentages of changes of price.
2. If you feed in NN 30 values, and want to predict two values, then subtract from 30 + 2 values minimums of all 32 values, and try to predict 2 values, but basing on 30. Then just add to result the minimum of 32 values.</p>

<p>Don't feed just dates in the NN. It says to NN nothing about making prediction. Instead feed in NN date and time as categorical value. Categorical means that you transform datetime in more then one entry. For example instead of giving to NN 2016/09/10  you can consider some of the following.</p>

<ol>
<li>year of trading most probably will not give any useful information. So you can omit year of trading.</li>
<li>09 stands for number of month or about September. You have choice either feed in NN number of month, but I strongly recommend you make 12 inputs in NN, and in case of January give at first NN input 1, and zeros for other eleven. In this way you'll train your network to separate trading period in January from trading period in June or December. Also I propose to do categorical input of day of week in the same way. Because trading in Monday differs from trading on Friday, especially in the day of NFP. </li>
<li>For hours I propose to use encoding by periods of 6 - 8 hours. It will help you to train network to take into account different trading sessions: Asia, Frankfurt, London, New-York.</li>
<li>If you decide to feed in NN some indicators then for some indicators consider thermometer encoding. As usually thermometer encoding is needed for indicators like ADX.</li>
</ol>

<p>According to your question in comments about how to use minimum I'll give you simplified example. 
Let's say you want to use for training NN following close prices for eur/usd:<br/>
1.1122, 1.1132, 1.1152, 1.1156, 1.1166, 1.1173, 1.1153, 1.1150, 1.1152, 1.1159.
Instead of windows size for learning 30 I'll demonstrate learning with window size 3 ( just for simplicity sake ) and prediction window size 2.<br/>
In total data used for prediction equals to 3. Output will be 2. For learning we will use first 5 values, or:<br/>
1.1122, 1.1132, 1.1152, 1.1156, 1.1166<br/>
then another 5 values or:<br/>
1.1132, 1.1152, 1.1156, 1.1166, 1.1173<br/>
In the first window minimal value is: 1.1122.<br/>
Then you subtract 1.1122 from each value:<br/>
0, 0.002, 0.003, 0.0033, 0.0034. As input you feed in NN 0, 0.002, 0.003. As output from NN you expect 0.0033, 0.0034. If you want to make it learn much faster, feed in NN normalized and scaled values. Then each time you'll need to make de-normalization and de-scaling of inputs. <br/><br/></p>

<p>Another way, feed in NN percentage of changes of price. Let me know if you need sample for it.<br/><br/></p>

<p>And one more important piece of advice. Don't use just NN for making trading. Never!!! Better way to do it is invent some system with some percentage of success. For example 30%. Then use NN in order to increase success percentage of success to 60%. <br/><br/></p>

<p>I also want to provide for you also example of thermometer encoding for some indicators. Consider ADX indicator and following examples:<br/><br/></p>

<p>a.>10 >20 >30 >40 <br />
    1    0   0   0<br />
 b. >10 >20 >30 >40<br />
     1    1   0   0<br />
example a provides input in NN with ADX greater then 10.
Example b provides input in NN with ADX greater then 20.
<br/>
You can modify thermometer encoding for providing inputs for stochastic. As usually stochastic has meaning in ranges 0 - 20, and 80 - 100 and in seldom cases in range 20 - 80. But as always you can try and see.</p>
"
39948580,6936386.0,2016-10-09T21:11:24Z,39947975,0,"<p>Try a form of recursion:</p>

<p>(Python 2.7.6)</p>

<pre><code>def back():
    global f,m,i
    if f&lt;m:
        s=m//f
        i+=s
        m-=s*f
    elif m&lt;f:
        s=f//m
        i+=s
        f-=s*m
    else:
        return False
    return True
while True:
    f=int(raw_input('f = '))
    m=int(raw_input('m = '))
    i=0
    while True:
        if f==m==1:
            print 'Output:',str(i)
            break
        else:
            if not back():
                print 'Output: impossible'
                break
    print
</code></pre>

<p>(Python 3.5.2)</p>

<pre><code>def back():
    global f,m,i
    if f&lt;m:
        s=m//f
        i+=s
        m-=s*f
    elif m&lt;f:
        s=f//m
        i+=s
        f-=s*m
    else:
        return False
    return True
while True:
    f=int(input('f = '))
    m=int(input('m = '))
    i=0
    while True:
        if f==m==1:
            print('Output:',str(i))
            break
        else:
            if not back():
                print('Output: impossible')
                break
    print()
</code></pre>

<p>Note: I am a Python 3.5 coder so I have tried to backdate my code, please let me know if there is something wrong with it.</p>

<p>The input format is also different: instead of <code>f = ""some_int""</code> it is now <code>f = some_int</code>, and the output is formatted similarly.</p>
"
39948598,1749513.0,2016-10-09T21:13:37Z,39947975,2,"<p>This is not a Python question, nor is it really a programming question. This is a problem designed to make you <em>think</em>. As such, if you just get the answer from somebody else, you will gain no knowledge or hindsight from the exercise.</p>

<p>Just add a <code>print(m, f)</code> in your <code>while</code> loop and watch how the numbers evolve for small inputs. For example, try with something like <code>(3, 100)</code>: don't you see any way you could speed things up, rather than repeatedly removing 3 from the bigger number?</p>
"
39948612,2296458.0,2016-10-09T21:15:09Z,39948543,2,"<p>All values between <code>100</code> and <code>40000</code> return <code>0</code> for <code>y = round(300/x**2)</code>.</p>

<p>I assume you meant the following</p>

<pre><code>def y(x):
    return round((300/x)**2)
</code></pre>

<p>In which case you can use <a href=""https://docs.python.org/3.6/library/itertools.html#itertools.groupby"" rel=""nofollow""><code>itertools.groupby</code></a></p>

<pre><code>from itertools import groupby
keys, groups = groupby(range(100, 40000), y)
</code></pre>

<p>Now <code>groups</code> will contain grouped integers that result in the same output from the function <code>y</code>.</p>
"
39948688,2997179.0,2016-10-09T21:23:30Z,39947975,1,"<p>You are on the right track with the top-down approach you posted. You can speed it up by a huge factor if you use integer division instead of repeated subtraction.</p>

<pre><code>def answer(m, f):
    m = int(m)
    f = int(f)
    counter = 0
    while m != 0 and f != 0:
        if f &gt; m:
            m, f = f, m
        print(m, f, counter, sep=""\t"")
        if f != 1 and m % f == 0:
            return ""impossible""
        counter += m // f
        m %= f
    return str(counter - 1)
</code></pre>

<p>Using the above, <code>answer(23333, 30000000000)</code> yields</p>

<pre><code>30000000000 23333   0
23333   15244   1285732
15244   8089    1285733
8089    7155    1285734
7155    934 1285735
934 617 1285742
617 317 1285743
317 300 1285744
300 17  1285745
17  11  1285762
11  6   1285763
6   5   1285764
5   1   1285765
1285769
</code></pre>

<p>and <code>answer(4, 7)</code> yields</p>

<pre><code>7   4   0
4   3   1
3   1   2
4
</code></pre>
"
39948965,6833456.0,2016-10-09T21:58:54Z,39805391,0,"<p>One solution would be to select the form by passing list form into br.form without using br.select_form. </p>

<p>Contents of test.html:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;
  &lt;title&gt;Stuff&lt;/title&gt;
&lt;/head&gt;
    &lt;body&gt;

        &lt;form method=""POST"" &gt;
            &lt;input type=""text"" name=""email""&gt;
        &lt;/form&gt;
        &lt;form method=""POST""&gt;
            &lt;input type=""text"" name=""email""&gt;
        &lt;/form&gt;
        &lt;form method=""POST""&gt;
            &lt;input type=""text"" name=""notemail""&gt;
        &lt;/form&gt;

    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>and the modified python script:</p>

<pre><code>import mechanize
import sys

br = mechanize.Browser()
br.open(""http://localhost/test.html"")

email = ""the@email.com""
for form in br.forms():

    br.form = form

    try:
        textctrl = br.form.find_control(name=""email"")
        textctrl.value = email
        response = br.submit()

        print ""Found email input, Submitted"", response

    except mechanize.ControlNotFoundError:
        print ""No Email control""

    except:
        print ""Unexpected error:"", sys.exc_info()[0]
</code></pre>

<p>This submits form 1 and 2 but not 3. Hope I understood the problem correctly.</p>
"
39948977,4118756.0,2016-10-09T22:01:02Z,39948902,4,"<p>A one-liner solution with <code>numpy</code> and <code>itertools</code>:</p>

<pre><code>[np.reshape(np.array(i), (K, N)) for i in itertools.product([0, 1], repeat = K*N)]
</code></pre>

<p><em>Explanation:</em> the <code>product</code> function returns a Cartesian product of its input. For instance, <code>product([0, 1], [0, 1])</code> returns an iterator that comprises all possible permutations of <code>[0, 1]</code> and <code>[0, 1]</code>. In other words, drawing from a product iterator:</p>

<pre><code>for i, j in product([0, 1], [0, 1]):
</code></pre>

<p>is actually equivalent to running two nested for-loops:</p>

<pre><code>for i in [0, 1]:
    for j in [0, 1]:
</code></pre>

<p>The for-loops above already solve the problem at hand for a specific case of <code>K, N = (1, 0)</code>. Continuing the above line of thought, to generate all possible zero/one states of a vector <code>i</code>, we need to draw samples from an iterator that is equivalent to a nested for-loop of depth <code>l</code>, where <code>l = len(i)</code>. Luckily, <code>itertools</code> provides the framework to do just that with its <code>repeat</code> keyword argument. In the case of OP's problem this permutation depth should be <code>K*N</code>, so that it can be reshaped into a numpy array of proper sizes during each step of the list comprehension.</p>
"
39949136,1029516.0,2016-10-09T22:21:38Z,39948543,2,"<p>First of all: the credit of this solution should go to <a href=""http://stackoverflow.com/a/39948612/1029516"">@CoryKramer</a>, even after the fix in the question.</p>

<pre><code>from itertools import groupby
groups = groupby(range(100, 40000), key=lambda x: round(300000/x**2))
</code></pre>

<p>Anyway there is something you should be aware, due to the division operator you are using: currently you used <em>integer division</em>, but using the <em>true division</em> operator can lead to slightly different grouping due to rounding (<code>300000/x**2</code> vs <code>300000.0/x**2</code>).</p>

<pre><code>from itertools import groupby
groups_int = groupby(range(100, 40000), key=lambda x: round(300000/x**2))
groups_true = groupby(range(100, 40000), key=lambda x: round(300000.0/x**2))


res_int  = [(g[0], [n for n in g[1]]) for g in groups_int]
res_true = [(g[0], [n for n in g[1]]) for g in groups_true]

for v_int, v_true in zip(res_int, res_true):
    # show the min and the max for each grouping 
    print v_int[0], min(v_int[1]), 'to', max(v_int[1]), '---', min(v_true[1]), 'to', max(v_true[1])

30.0 100 to 100 --- 100 to 100
29.0 101 to 101 --- 101 to 102
28.0 102 to 103 --- 103 to 104
27.0 104 to 105 --- 105 to 106
26.0 106 to 107 --- 107 to 108
25.0 108 to 109 --- 109 to 110
24.0 110 to 111 --- 111 to 112
23.0 112 to 114 --- 113 to 115
22.0 115 to 116 --- 116 to 118
21.0 117 to 119 --- 119 to 120
20.0 120 to 122 --- 121 to 124
19.0 123 to 125 --- 125 to 127
18.0 126 to 129 --- 128 to 130
17.0 130 to 132 --- 131 to 134
16.0 133 to 136 --- 135 to 139
15.0 137 to 141 --- 140 to 143
14.0 142 to 146 --- 144 to 149
13.0 147 to 151 --- 150 to 154
12.0 152 to 158 --- 155 to 161
11.0 159 to 165 --- 162 to 169
10.0 166 to 173 --- 170 to 177
9.0 174 to 182 --- 178 to 187
8.0 183 to 193 --- 188 to 200
7.0 194 to 207 --- 201 to 214
6.0 208 to 223 --- 215 to 233
5.0 224 to 244 --- 234 to 258
4.0 245 to 273 --- 259 to 292
3.0 274 to 316 --- 293 to 346
2.0 317 to 387 --- 347 to 447
1.0 388 to 547 --- 448 to 774
0.0 548 to 39999 --- 775 to 39999
</code></pre>

<hr>

<h2>Alternative solution</h2>

<p>Here I propose another solution reversing the problem: the turning point of the rounding is when a value reaches <code>xxx.5</code>, so we can try to reverse the equation and solve</p>

<p><img src=""http://latex.codecogs.com/gif.latex?x%20%3D%20%5Csqrt%7B%5Cfrac%7B300000%7D%7By%20&plus;%200.5%7D%7D"" alt=""formula""></p>

<p>computing <code>x</code> for each <code>y</code> integer between 0 and 30 (we know it doing a bit of domain analysis or simply peeking from the previous solution :P).</p>

<pre><code>points = [(y, (300000.0/(y + 0.5))**.5) for y in range(30,0,-1)]
# [(30, 99.17694073609294), (29, 100.84389681792216), (28, 102.59783520851542), (27, 104.44659357341871), (26, 106.3990353197863), (25, 108.46522890932809), (24, 110.65666703449763), (23, 112.98653657320641), (22, 115.47005383792515), (21, 118.12488464372366), (20, 120.97167578182678), (19, 124.03473458920845), (18, 127.34290799340266), (17, 130.93073414159542), (16, 134.8399724926484), (15, 139.12166872805048), (14, 143.83899044561525), (13, 149.07119849998597), (12, 154.91933384829667), (11, 161.51457061744966), (10, 169.03085094570332), (9, 177.7046633277277), (8, 187.86728732554485), (7, 200.0), (6, 214.83446221182987), (5, 233.5496832484569), (4, 258.19888974716116), (3, 292.7700218845599), (2, 346.41016151377545), (1, 447.21359549995793)]
</code></pre>

<p>Then for each point we can compute your result grouping all the integers between the points we just calculated:</p>

<pre><code>from math import ceil, floor
res = [(low[0], range(int(ceil(low[1])), int(floor(up[1]))+1)) for low, up in zip(points[:-1], points[1:])]
</code></pre>
"
39949231,8418.0,2016-10-09T22:36:51Z,39890593,1,"<p>I think mainly because they are slowly moving away from the <code>appcfg.py</code>, to start using the <a href=""https://cloud.google.com/sdk/"" rel=""nofollow"">Cloud SDK</a> instead, where <code>application</code> is not supported. You can set your default application so you won't need to use command line all the time.</p>
"
39949255,1514983.0,2016-10-09T22:40:08Z,39948485,0,"<p>The answer is in Python headers but may not be obvious.</p>

<p>Python headers declare 2 somewhat static objects here, with couple of macros:</p>

<pre><code>/* Don't use these directly */
PyAPI_DATA(struct _longobject) _Py_FalseStruct, _Py_TrueStruct;

/* Use these macros */
#define Py_False ((PyObject *) &amp;_Py_FalseStruct)
#define Py_True ((PyObject *) &amp;_Py_TrueStruct)

/* Macros for returning Py_True or Py_False, respectively */
#define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True
#define Py_RETURN_FALSE return Py_INCREF(Py_False), Py_False
</code></pre>

<p>It seems that both <code>True</code> and <code>False</code> are in fact Python objects and all values in Python that are <code>True</code> or <code>False</code> are in fact references to these two global <code>Py_True</code> and <code>Py_False</code> objects. When such object is returned using <code>Py_RETURN_TRUE</code>, the reference count is incremented.</p>

<p>This means that every C pointer that points to PyObject of value <code>Py_True</code> in fact points to same memory address. Therefore checking if <code>PyObject</code> is true or false is as simple as:</p>

<pre><code>/* here I create Python boolean with value of True */
PyObject *b = Py_RETURN_TRUE;
/* now we compare the address of pointer with address of Py_True */
if (b == Py_True)
{ /* it's true! */ }
else
{ /* it's false */ }
</code></pre>

<p>It's generally good idea to use <code>int PyBool_Check(PyObject*)</code> to verify if object in question is Python boolean.</p>
"
39949523,839689.0,2016-10-09T23:22:02Z,39949497,3,"<p>You can't use multi-character constructs like <code>\d+</code> in a character class.</p>

<p>So you can do it by brute force like this:</p>

<pre><code>re.findall(r""\(|\)|\d+|-|\*|/"", st)
</code></pre>

<p>Or you can use a character class for single-character tokens, alternated with other things:</p>

<pre><code>re.findall(r""[()\-*/]|\d+"", st)
</code></pre>
"
39949551,5249307.0,2016-10-09T23:26:03Z,39948902,2,"<p>An alternative approach to using <code>itertools.product</code> that is faster:</p>

<pre><code>def using_shifts(K, N):
    shifter = numpy.arange(K*N).reshape( (K, N) )
    return [(x &gt;&gt; shifter) % 2 for x in range(2 ** (K*N))]
</code></pre>

<p>How does this work?  We are exploiting that each desired array of <code>0</code> and <code>1</code> will correspond to the binary expansion of an integer.  For an integer <code>x</code> to find bit at index <code>y</code> we need to compute <code>(x &gt;&gt; y) % 2</code>.  </p>

<p>So here we can use <code>numpy</code> operations to find an array of indexes using the corresponding array operations.</p>

<p>Compared to <code>itertools.product</code>, relative timing is:</p>

<ul>
<li><code>using_shifts(2, 5)</code>: 1.9ms</li>
<li><code>using_itertools(2, 5)</code>: 2.8ms</li>
</ul>
"
39949904,267540.0,2016-10-10T00:31:34Z,39949719,0,"<blockquote>
  <p>I am certain that it is actually connected because when I try to
  makemigrations. I checked MySQL workbench and all my models are synced
  into the database.</p>
</blockquote>

<p>It sounds more like you are connecting to a database that was already being used for something, or you have run <code>migrate</code> on this database before. <code>makemigrations</code> does not make any changes to the database. The only thing it does is to create a set of files in the migrations folder of your apps.</p>

<p>There could be several </p>

<p>The content_type_id comes from the <a href=""https://docs.djangoproject.com/en/1.10/ref/contrib/contenttypes/"" rel=""nofollow"">content types frame work</a> which means one of the libraries you are using probably uses a generic foreign key.</p>

<p>There are several things you could try, if this is a new installation without any data, just drop the database and start again.</p>
"
39950290,5349916.0,2016-10-10T01:43:37Z,39950130,0,"<p>You can either use raw files or modules such as <code>pickle</code> to store data easily.</p>

<pre><code>import cPickle as pickle
from quippy import Potential
try:  # try previously calculated value
    with open('/tmp/pot_store.pkl') as store:
        pot = pickle.load(store)
except OSError:  # fall back to calculating it from scratch
    pot = quippy.Potential(""Potential  xml_label=gap_h2o_2b_ccsdt_3b_ccsdt"",param_filename=""gp.xml"")
    with open('/tmp/pot_store.pkl', 'w') as store:
        pot = pickle.dump(pot, store)
</code></pre>

<p>There are various optimizations to this, e.g. checking whether your pickle file is older than the file generating it's value.</p>
"
39950446,4483861.0,2016-10-10T02:11:48Z,39950130,0,"<p>I found one solution, but I'm interested in alternatives. You can divide the script into two parts: </p>

<p>start.py:</p>

<pre><code>from quippy import Potential
from ase import atoms
pot=Potential(...  etc...
</code></pre>

<p>body.py:</p>

<pre><code>for i in range(max_int):
      print ""doing things""
# etc...
</code></pre>

<p>Then enter python interpreter and run the start-script only once but the body as much as needed:</p>

<pre><code>me@laptop:~/dir$ python
&gt;&gt;&gt; execfile('start.py')
&gt;&gt;&gt; execfile('body.py')
&gt;&gt;&gt; #(change code of ""body.py"" in editor)
&gt;&gt;&gt; execfile('body.py') # again without reloading ""start.py""
</code></pre>

<p>So this means a terminal is occupied and the script is affected, but it works. </p>
"
39950583,5161084.0,2016-10-10T02:34:46Z,39950111,0,"<p>I think the <a href=""https://docs.python.org/3/library/itertools.html#itertools.accumulate"" rel=""nofollow""><code>itertools.accumulate</code></a> may meet your needs, but it's return value may be different from what you expect.</p>

<p>For instance:</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from itertools import accumulate

def callback(seq, i):
    """"""
        seq: the sequence you will modified
        i: the number of times this function is called
    """"""
    seq.append(seq[-1] + seq[-2])
    return seq

res = accumulate([[0, 1]] + list(range(1, 8 + 1)), callback)
for item in res:
    print(item)
</code></pre>

<p>The <code>[0, 1]</code> is the init sequence, and the <code>8</code> is the number of times you want to call the <code>callback</code> function. </p>

<p>And the result of above code is this:</p>

<pre><code>In [48]: run test.py
[0, 1]
[0, 1, 1]
[0, 1, 1, 2]
[0, 1, 1, 2, 3]
[0, 1, 1, 2, 3, 5]
[0, 1, 1, 2, 3, 5, 8]
[0, 1, 1, 2, 3, 5, 8, 13]
[0, 1, 1, 2, 3, 5, 8, 13, 21]
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]
</code></pre>

<p>The last one is you wanted.</p>
"
39951125,298607.0,2016-10-10T03:58:45Z,39950951,1,"<p>Suppose we have:</p>

<pre><code>&gt;&gt;&gt; df=pd.DataFrame({'col':['NaN']*10})
</code></pre>

<p>You can use <code>.apply</code> to convert:</p>

<pre><code>&gt;&gt;&gt; new_df=df.apply(float, axis=1)
&gt;&gt;&gt; type(new_df[0])
&lt;type 'numpy.float64'&gt;
</code></pre>
"
39951175,4177078.0,2016-10-10T04:06:54Z,39950951,1,"<p>Yes, you can do this when reading the csv file.</p>

<pre><code>df = pd.read_csv('test.csv', names=['t', 'v'], dtype={'v':np.float64})
</code></pre>

<p>Check the docs of <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow"">pandas.read_csv</a>. There are some parameters is useful for your application:</p>

<ul>
<li>names </li>
<li>dtype</li>
<li>na_values</li>
</ul>

<p>Hope this would be helpful.</p>
"
39951608,2901002.0,2016-10-10T05:09:48Z,39951581,2,"<p><a href=""http://pandas.pydata.org/pandas-docs/stable/advanced.html#creating-a-multiindex-hierarchical-index-object"" rel=""nofollow""><code>MultiIndex</code></a> is called first and second columns and if first level has duplicates by default it 'sparsified' the higher levels of the indexes to make the console output a bit easier on the eyes.</p>

<p>You can show data in first level of <code>MultiIndex</code> by setting <a href=""http://pandas.pydata.org/pandas-docs/stable/options.html#available-options"" rel=""nofollow""><code>display.multi_sparse</code></a> to <code>False</code>.</p>

<p>Sample:</p>

<pre><code>df = pd.DataFrame({'A':[1,1,3],
                   'B':[4,5,6],
                   'C':[7,8,9]})

df.set_index(['A','B'], inplace=True)

print (df)
     C
A B   
1 4  7
  5  8
3 6  9

#temporary set multi_sparse to False
#http://pandas.pydata.org/pandas-docs/stable/options.html#getting-and-setting-options
with pd.option_context('display.multi_sparse', False):
    print (df)
     C
A B   
1 4  7
1 5  8
3 6  9
</code></pre>

<p>EDIT by edit of question:</p>

<p>I think problem is type of value <code>11948</code> is <code>string</code>, so it is omited.</p>

<p>EDIT1 by file:</p>

<p>You can simplify your solution by add parameter <code>usecols</code> in <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow""><code>read_csv</code></a> and then aggregating by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.sum.html"" rel=""nofollow""><code>GroupBy.sum</code></a>:</p>

<pre><code>import pandas as pd
import numpy as np

df2 = pd.read_table(r'tbsm_trdar_selng_utf8.txt' , 
                    sep='|' ,
                    header=None ,
                    usecols=[0 ,1, 2, 3 ,4, 11,12 ,82],
                    names=['STDR_YM_CD', 'TRDAR_CD', 'TRDAR_CD_NM', 'SVC_INDUTY_CD', 'SVC_INDUTY_CD_NM', 'THSMON_SELNG_AMT', 'THSMON_SELNG_CO', 'STOR_CO'],
                    dtype = { '0' : int})


df4_agg = df2.groupby(['STDR_YM_CD', 'TRDAR_CD' ]).sum()
print(df4_agg.head(10))
                     THSMON_SELNG_AMT  THSMON_SELNG_CO  STOR_CO
STDR_YM_CD TRDAR_CD                                            
201301     11947           1966588856            74798       73
           11948           3404215104            89064      116
           11949           1078973946            42005       45
           11950           1759827974            93245       71
           11953            779024380            21042       84
           11954           2367130386            94033      128
           11956            511840921            23340       33
           11957            329738651            15531       50
           11958           1255880439            42774      118
           11962           1837895919            66692       68
</code></pre>
"
39951856,2336654.0,2016-10-10T05:37:55Z,39950951,1,"<p>I'd use the <code>converters</code> option in <code>read_csv</code>.  In this case, we are aiming to convert the column in question to numeric values and treat everything else as <code>numpy.nan</code> which includes string version of <code>'NaN'</code></p>

<pre><code>converter = lambda x: pd.to_numeric(x, 'coerce')
df = pd.read_csv(StringIO(txt), delim_whitespace=True, converters={1: converter}, header=None)
df
</code></pre>

<p><a href=""http://i.stack.imgur.com/mPA8c.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mPA8c.png"" alt=""enter image description here""></a></p>

<pre><code>df.dtypes

0     object
1    float64
dtype: object
</code></pre>
"
39952326,6947311.0,2016-10-10T06:22:19Z,39765738,2,"<p><code>sklearn.cluster.KMeans</code> don't support input data with type <code>blaze.interactive._Data</code> which is the type of data_numeric in your code.</p>

<p>You can use <code>data_cluster.fit(data_numeric.peek())</code> to fit the transferred data_numeric with type <code>DataFrame</code> supported by <code>sklearn.cluster.KMeans</code>.</p>
"
39955283,5741205.0,2016-10-10T09:32:11Z,39954668,2,"<p>you can do it this way:</p>

<pre><code>In [84]: df
Out[84]:
               A               B
0     some value      [[L1, L2]]
1  another value  [[L3, L4, L5]]

In [85]: (df['B'].apply(lambda x: pd.Series(x[0]))
   ....:         .stack()
   ....:         .reset_index(level=1, drop=True)
   ....:         .to_frame('B')
   ....:         .join(df[['A']], how='left')
   ....: )
Out[85]:
    B              A
0  L1     some value
0  L2     some value
1  L3  another value
1  L4  another value
1  L5  another value
</code></pre>
"
39955362,5847976.0,2016-10-10T09:36:56Z,39955222,7,"<p>Your issue is that you are not puting sub-dictionaries inside dataRows. The fix would be this:</p>

<pre><code>for i, row in enumerate(rows):
    dataRows[row] = {}
    for key, value in data.items():
        dataRows[row][key] = value[i]
</code></pre>
"
39955393,5899959.0,2016-10-10T09:38:25Z,39955222,2,"<p>Following code works for me:</p>

<pre><code>rows = [2, 21]

data = {'x': [46, 35], 'y': [20, 30]}

dataRows = {}

for i, row in enumerate(rows):
  dataRows[row] = {}
  dataRows[row]['x'] = data['x'][i]
  dataRows[row]['y'] = data['y'][i]

print dataRows
</code></pre>

<p>UPDATE:</p>

<p>You can also use collections.defaultdict() to avoid assigning dict to dataRows in every iteration.</p>

<pre><code>import collections

rows = [2, 21]

data = {'x': [46, 35], 'y': [20, 30]}

dataRows = collections.defaultdict(dict)

for i, row in enumerate(rows):
  for key, value in data.items():
    dataRows[row][key] = value[i]

print dataRows
</code></pre>
"
39955429,5249307.0,2016-10-10T09:40:26Z,39955222,0,"<p>In a line:</p>

<pre><code>&gt;&gt;&gt; {r: {k: v[i] for k, v in data.items()} for i, r in enumerate(rows)}
{2: {'x': 46, 'y': 20}, 21: {'x': 35, 'y': 30}}
</code></pre>
"
39955506,6096652.0,2016-10-10T09:44:24Z,39954668,0,"<p>I can't find a elegant way to handle this, but the following codes can work...</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame([{""a"":1,""b"":[[1,2]]},{""a"":4, ""b"":[[3,4,5]]}])
z = []
for k,row in df.iterrows():
    for j in list(np.array(row.b).flat):
        z.append({'a':row.a, 'b':j})
result = pd.DataFrame(z)
</code></pre>
"
39955582,279627.0,2016-10-10T09:48:18Z,39955222,0,"<p>Here's a Python 3 solution with only one explicit loop:</p>

<pre><code>{r: dict(zip(data.keys(), d)) for r, *d in zip(rows, *data.values())}
</code></pre>
"
39956010,5741205.0,2016-10-10T10:15:05Z,39955336,2,"<p>you can do it using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reindex.html"" rel=""nofollow"">reindex()</a> method and custom sorting:</p>

<pre><code>In [26]: table
Out[26]:
                     population
windspeed               120km/h 60km/h 90km/h
admin0 admin1 admin2
cntry1 state1 city1           0    700    210
              city2           0    100      0
       state2 city3           0     70      0
              city4           0    180    370
cntry2 state3 city5           0    890      0
              city6         360    120    420
       state4 city7           0    740      0

In [27]: cols = sorted(table.columns.tolist(), key=lambda x: int(x[1].replace('km/h','')))

In [28]: cols
Out[28]: [('population', '60km/h'), ('population', '90km/h'), ('population', '120km/h')]

In [29]: table = table.reindex(columns=cols)

In [30]: table
Out[30]:
                     population
windspeed                60km/h 90km/h 120km/h
admin0 admin1 admin2
cntry1 state1 city1         700    210       0
              city2         100      0       0
       state2 city3          70      0       0
              city4         180    370       0
cntry2 state3 city5         890      0       0
              city6         120    420     360
       state4 city7         740      0       0
</code></pre>
"
39956406,5067311.0,2016-10-10T10:40:50Z,39956313,4,"<p>Your problem seems very simple to vectorize. For each pair of rows of <code>B</code> you want to compute</p>

<pre><code>P[i,j] = np.exp(-np.sum(np.abs(B[i,:] - B[j,:])))
</code></pre>

<p>You can make use of array broadcasting and introduce a third dimension, summing along the last one:</p>

<pre><code>P2 = np.exp(-np.sum(np.abs(B[:,None,:] - B),axis=-1))
</code></pre>

<p>The idea is to reshape the first occurence of <code>B</code> to shape <code>(N,1,M)</code> while the second <code>B</code> is left with shape <code>(N,M)</code>. With array broadcasting, the latter is equivalent to <code>(1,N,M)</code>, so</p>

<pre><code>B[:,None,:] - B
</code></pre>

<p>is of shape <code>(N,N,M)</code>. Summing along the last index will then result in the <code>(N,N)</code>-shape correlation array you're looking for.</p>

<hr>

<p>Note that if you were using <code>scipy</code>, you would be able to do this using <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html"" rel=""nofollow""><code>scipy.spatial.distance.cdist</code></a> (or, equivalently, a combination of <code>scipy.spatial.distance.pdist</code> and <code>scipy.spatial.distance.squareform</code>), without unnecessarily computing the lower triangular half of this symmetrix matrix. Using <a href=""http://stackoverflow.com/users/3293881/divakar"">@Divakar</a>'s suggestion in comments for the simplest solution this way:</p>

<pre><code>from scipy.spatial.distance import cdist
P3 = 1/np.exp(cdist(B, B, 'minkowski',1))
</code></pre>

<p><code>cdist</code> will compute the Minkowski distance in 1-norm, which is exactly the sum of the absolute values of coordinate differences.</p>
"
39956469,2901002.0,2016-10-10T10:44:19Z,39955336,4,"<p>Solution with subtotals and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.MultiIndex.from_arrays.html"" rel=""nofollow""><code>MultiIndex.from_arrays</code></a>. Last <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow""><code>concat</code></a> and  all <code>Dataframes</code>, <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_index.html"" rel=""nofollow""><code>sort_index</code></a> and add all <code>sum</code>:</p>

<pre><code>#replace km/h and convert to int
df.windspeed = df.windspeed.str.replace('km/h','').astype(int)
print (df)
    FID  admin0  admin1 admin2  windspeed  population
0     0  cntry1  state1  city1         60         700
1     1  cntry1  state1  city1         90         210
2     2  cntry1  state1  city2         60         100
3     3  cntry1  state2  city3         60          70
4     4  cntry1  state2  city4         60         180
5     5  cntry1  state2  city4         90         370
6     6  cntry2  state3  city5         60         890
7     7  cntry2  state3  city6         60         120
8     8  cntry2  state3  city6         90         420
9     9  cntry2  state3  city6        120         360
10   10  cntry2  state4  city7         60         740

#pivoting
table = pd.pivot_table(df,
                       index=[""admin0"",""admin1"",""admin2""], 
                       columns=[""windspeed""], 
                       values=[""population""],
                       fill_value=0)
print (table)
                    population          
windspeed                   60   90   120
admin0 admin1 admin2                     
cntry1 state1 city1         700  210    0
              city2         100    0    0
       state2 city3          70    0    0
              city4         180  370    0
cntry2 state3 city5         890    0    0
              city6         120  420  360
       state4 city7         740    0    0
</code></pre>



<pre><code>#groupby and create sum dataframe by levels 0,1
df1 = table.groupby(level=[0,1]).sum()
df1.index = pd.MultiIndex.from_arrays([df1.index.get_level_values(0), 
                                       df1.index.get_level_values(1)+ '_sum', 
                                       len(df1.index) * ['']])
print (df1)
                   population          
windspeed                 60   90   120
admin0                                 
cntry1 state1_sum         800  210    0
       state2_sum         250  370    0
cntry2 state3_sum        1010  420  360
       state4_sum         740    0    0

df2 = table.groupby(level=0).sum()
df2.index = pd.MultiIndex.from_arrays([df2.index.values + '_sum',
                                       len(df2.index) * [''], 
                                       len(df2.index) * ['']])
print (df2)
             population          
windspeed           60   90   120
cntry1_sum         1050  580    0
cntry2_sum         1750  420  360

#concat all dataframes together, sort index
df = pd.concat([table, df1, df2]).sort_index(level=[0])
</code></pre>



<pre><code>#add km/h to second level in columns
df.columns = pd.MultiIndex.from_arrays([df.columns.get_level_values(0),
                                       df.columns.get_level_values(1).astype(str) + 'km/h'])

#add all sum
df.loc[('All_sum','','')] = table.sum().values
print (df)
                             population               
                                 60km/h 90km/h 120km/h
admin0     admin1     admin2                          
cntry1     state1     city1         700    210       0
                      city2         100      0       0
           state1_sum               800    210       0
           state2     city3          70      0       0
                      city4         180    370       0
           state2_sum               250    370       0
cntry1_sum                         1050    580       0
cntry2     state3     city5         890      0       0
                      city6         120    420     360
           state3_sum              1010    420     360
           state4     city7         740      0       0
           state4_sum               740      0       0
cntry2_sum                         1750    420     360
All_sum                            2800   1000     360
</code></pre>

<p>EDIT by comment:</p>

<pre><code>def f(x):
    print (x)
    if (len(x) &gt; 1):
        return x.sum()

df1 = table.groupby(level=[0,1]).apply(f).dropna(how='all')
df1.index = pd.MultiIndex.from_arrays([df1.index.get_level_values(0), 
                                       df1.index.get_level_values(1)+ '_sum', 
                                       len(df1.index) * ['']])
print (df1)
                   population              
windspeed                 60     90     120
admin0                                     
cntry1 state1_sum       800.0  210.0    0.0
       state2_sum       250.0  370.0    0.0
cntry2 state3_sum      1010.0  420.0  360.0
</code></pre>
"
39956909,6748546.0,2016-10-10T11:09:21Z,39956782,4,"<pre><code>input_list = []
input_number = 1
while True:

    input_list.append(raw_input('Enter percentage {} (in decimal form):'.format(input_number))

    if float(input_list[-1]) &gt; 1:     # Last input is larger than one, remove last input and print reason
        input_list.remove(input_list[-1])
        print('The input is larger than one.')
        continue

    total = sum([float(s) for s in input_list])
    if total &gt; 1:    # Total larger than one, remove last input and print reason
        input_list.remove(input_list[-1])
        print('The sum of the percentages is larger than one.')
        continue

    if total == 1:    # if the sum equals one: exit the loop
        break

    input_number += 1
</code></pre>
"
39957721,5741205.0,2016-10-10T11:56:08Z,39957573,-1,"<p>It can be done pretty easily using Pandas module:</p>

<pre><code>import pandas as pd

df = pd.read_csv('/path/to/file.csv')

df.groupby(['Name','Date']).Time.apply(list).reset_index().to_csv('d:/temp/out.csv', index=False)
</code></pre>

<p>D:\temp\out.csv:</p>

<pre><code>Name,Date,Time
A,3/7/2016,""['10:40', '11.30']""
A,4/7/2016,['12:40']
B,4/7/2016,['07.35']
</code></pre>
"
39957830,6869965.0,2016-10-10T12:01:45Z,39764582,0,"<p>I finally ended up with the solution below:<br>
- First of all, based on the <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix"" rel=""nofollow"">scipy documentation</a>, the LiL (linked list) format seems to be ideal for this sort of operations. (However I never did any actual comparison!)<br>
- I've used the functions described <a href=""http://stackoverflow.com/a/15162737/6869965"">here</a> to swap rows and columns.<br>
- Following the <a href=""http://stackoverflow.com/a/39791339/6869965"">suggestion of elyase</a>, I defined a 200*200 'window' in the 'upper left' corner of the matrix and implemented a 'window score' which was simply equal to the number of non-zero elements inside the window.<br>
- To identify the columns to swap, I checked which column contains the least non-zero elements inside the window, and which column contains the most non-zero elements outside the window. In case of a tie, the number of non-zero elements in the whole column was the tie-breaker (if this was tied as well, I've chosen randomly).<br>
- The method for swapping rows was identical.</p>

<pre><code>import numpy as np
import scipy.sparse
import operator

def swap_rows(mat, a, b):
    ''' See link in description'''

def swap_cols(mat, a, b) :
    ''' See link in description'''

def windowScore(lilmatrix,window):
    ''' Return no. of non-zero elements inside window. '''
    a=lilmatrix.nonzero()

    return sum([1 for i,j in list(zip(a[0],a[1])) if i&lt;window and j&lt;window])

def colsToSwap(lilmatrix,window):
    ''' Determine columns to be swapped.
    In: lil_matrix, window (to what col_no is it considered ""left"") 
    Out: (minColumnLeft,maxColumnRight) columns inside/outside of window w/ least/most NZ elements'''

    # Locate non-zero elements
    a=lilmatrix.nonzero()

    totalCols=lilmatrix.get_shape()[1]

    # Store no. of NZ elements for each column {in the window,in the whole table}, initialize with zeros
    colScoreWindow=np.zeros(totalCols)
    colScoreWhole=np.zeros(totalCols)

    ### Set colScoreWindow scores
    # Unique row indices
    rows_uniq={k for k in a[0] if k&lt;window}
    for k in rows_uniq:
        # List of tuples w/ location of each NZ element in current row
        gen=((row,col) for row,col in list(zip(a[0],a[1])) if row==k)
        for row,col in gen:
           # Increment no. of NZ elements in current column in colScoreWindow
            colScoreWindow[col]+=1

    ### Set colScoreWhole scores
    # Unique row indices
    rows_uniq={k for k in a[0]}
    for k in rows_uniq:
        # List of tuples w/ location of each NZ element in current row
        gen=((row,col) for row,col in list(zip(a[0],a[1])) if row==k)
        for row,col in gen:
            # Increment no. of NZ elements in current column in colScoreWhole
            colScoreWhole[col]+=1


    # Column inside of window w/ least NZ elements
    minColumnLeft=sorted(list(zip(np.arange(totalCols),colScoreWindow,colScoreWhole,np.random.rand(totalCols)))[:window], key=operator.itemgetter(1,2,3))[0][0]
    # Column outside of window w/ most NZ elements
    maxColumnRight=sorted(list(zip(np.arange(totalCols),colScoreWindow,colScoreWhole,np.random.rand(totalCols)))[window:], key=operator.itemgetter(1,2,3))[-1][0]

    return (minColumnLeft,maxColumnRight)

def rowsToSwap(lilmatrix,window):
    ''' Same as colsToSwap, adjusted for rows.'''
</code></pre>

<p>After running a suitable number of iterations of <code>colsToSwap</code> and <code>rowsToSwap</code> and the actual swapping functions, the number of non-zero elements inside the window converges to a maximum. Note that the method is not optimized at all, and there is much room for improvement. For example, I suspect that reducing the number of sparse matrix type conversions and/or the <code>a=lilmatrix.nonzero()</code> call would significantly speed it up.</p>
"
39958153,2867928.0,2016-10-10T12:19:21Z,39958095,4,"<p>The inner list should be based on columns:</p>

<pre><code>mat = [[ 0 for x in range(column)] for y in range(row)]
</code></pre>

<p>Here is an example:</p>

<pre><code>In [73]: row = 3
In [74]: column = 4
In [78]: mat = [[ 0 for x in range(column)] for y in range(row)]

In [79]: 

In [79]: for x in range(row): # row is 2 
             for y in range(column): # column is 3
                 mat[x][y] = 5
   ....:         

In [80]: mat
Out[80]: [[5, 5, 5, 5], [5, 5, 5, 5], [5, 5, 5, 5]]
</code></pre>
"
39958283,6671342.0,2016-10-10T12:27:10Z,39958095,1,"<p>I think it should be:</p>

<pre><code>&gt;&gt;&gt; for x in range(column):
...     for y in range(row):
...             mat[x][y] = int(""number: "")
...
1
2
3
4
5
6
&gt;&gt;&gt; mat
[[1, 2], [3, 4], [5, 6]]
</code></pre>
"
39959012,5276520.0,2016-10-10T13:06:13Z,39957573,-1,"<p>If you don't want to use Pandas, this is a possible solution. It's not the most elegant since your csv format is relatively clunky to parse. If you can change the format to use a non-whitespace field separator, using a proper csv parsing library (like <code>pandas</code> or Python's built-in <code>csv</code> module) would be preferable.</p>

<pre><code>import re

datePattern = re.compile(r""(\d+/\d+/\d+)\s+(\d+[:.]\d+)"")
companyPattern = re.compile(r""^\s+\d+\s+(\w+)"")
companyDict = {}

for i, line in enumerate(open('sample_data.csv')):
    # skip csv header
    if i == 0:
        continue

    timestampMatch = datePattern.search(line)
    companyMatch   = companyPattern.search(line)

    # filter out any malformed lines which don't match
    if timestampMatch is None or companyMatch is None:
        continue

    date = timestampMatch.group(1)
    time = timestampMatch.group(2)
    company = companyMatch.group(1)

    companyDict.setdefault(company, []).append(""{} {}"".format(date, time))
</code></pre>

<p>Note that the time field is inconsistent as to whether it uses <code>.</code> or <code>:</code> for the hour/minute delimiter so I've taken this into account.</p>

<p>Running this on your sample data results in the following value for <code>companyDict</code>:</p>

<pre><code>{'A': ['3/7/2016 10:40', '3/7/2016 11.30', '4/7/2016 12:40'], 'B': ['4/7/2016 07.35']} 
</code></pre>
"
39959302,2901002.0,2016-10-10T13:20:42Z,39954668,0,"<p>Faster solution with <code>chain.from_iterable</code> and <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html"" rel=""nofollow""><code>numpy.repeat</code></a>:</p>

<pre><code>df = pd.DataFrame({'A':['a','b'],
                   'B':[[['A1', 'A2']],[['A1', 'A2', 'A3']]]})

print (df)
   A               B
0  a      [[A1, A2]]
1  b  [[A1, A2, A3]]


df1 = pd.DataFrame({ ""A"": np.repeat(df.A.values, 
                                    [len(x) for x in (chain.from_iterable(df.B))]),
                     ""B"": list(chain.from_iterable(chain.from_iterable(df.B)))})

print (df1)
   A   B
0  a  A1
1  a  A2
2  b  A1
3  b  A2
4  b  A3
</code></pre>

<p><strong>Timings</strong>:</p>

<pre><code>A = np.unique(np.random.randint(0, 1000, 1000))
B = [[list(string.ascii_letters[:random.randint(3, 10)])] for _ in range(len(A))]
df = pd.DataFrame({""A"":A, ""B"":B})
print (df)
       A                                 B
0      0        [[a, b, c, d, e, f, g, h]]
1      1                       [[a, b, c]]
2      3     [[a, b, c, d, e, f, g, h, i]]
3      5                 [[a, b, c, d, e]]
4      6     [[a, b, c, d, e, f, g, h, i]]
5      7           [[a, b, c, d, e, f, g]]
6      8              [[a, b, c, d, e, f]]
7     10              [[a, b, c, d, e, f]]
8     11           [[a, b, c, d, e, f, g]]
9     12     [[a, b, c, d, e, f, g, h, i]]
10    13        [[a, b, c, d, e, f, g, h]]
...
...

In [67]: %timeit pd.DataFrame({ ""A"": np.repeat(df.A.values, [len(x) for x in (chain.from_iterable(df.B))]),""B"": list(chain.from_iterable(chain.from_iterable(df.B)))})
1000 loops, best of 3: 818 Âµs per loop

In [68]: %timeit ((df['B'].apply(lambda x: pd.Series(x[0])).stack().reset_index(level=1, drop=True).to_frame('B').join(df[['A']], how='left')))
10 loops, best of 3: 103 ms per loop
</code></pre>
"
39959484,2296458.0,2016-10-10T13:30:00Z,39959409,3,"<p>You can use <a href=""https://docs.python.org/3/library/os.html#os.environ"" rel=""nofollow""><code>os.environ</code></a></p>

<pre><code>import os
win_path = os.environ['WINDIR']
</code></pre>

<p><code>WINDIR</code> is an environment variable set by windows that will point to <code>%SystemRoot%</code></p>
"
39959511,3293881.0,2016-10-10T13:31:49Z,39959435,4,"<p><strong>Approach #1</strong></p>

<p>One approach based on <code>cumsum</code> and <code>argmax</code> -</p>

<pre><code>A[np.arange(A.shape[0]),(A!=0).cumsum(1).argmax(1)] = 0
</code></pre>

<p>Sample run -</p>

<pre><code>In [59]: A
Out[59]: 
array([[2, 0, 3, 4],
       [5, 6, 7, 0],
       [8, 9, 0, 0]])

In [60]: A[np.arange(A.shape[0]),(A!=0).cumsum(1).argmax(1)] = 0

In [61]: A
Out[61]: 
array([[2, 0, 3, 0],
       [5, 6, 0, 0],
       [8, 0, 0, 0]])
</code></pre>

<p><strong>Approach #2</strong></p>

<p>One more based on <code>argmax</code> and hopefully more efficient -</p>

<pre><code>A[np.arange(A.shape[0]),A.shape[1] - 1 - (A[:,::-1]!=0).argmax(1)] = 0
</code></pre>

<hr>

<p><strong>Explanation</strong></p>

<p>One of the uses of <code>argmax</code> is to get ID  of the <strong>first</strong> occurence of the <code>max</code> element along an axis in an array . In the first approach we get the cumsum along the rows and get the first max ID, which represents the last non-zero elem. This is because <code>cumsum</code> on the leftover elements won't increase the sum value after that last non-zero element. </p>

<p>Let's re-run that case in a bit more detailed manner -</p>

<pre><code>In [105]: A
Out[105]: 
array([[2, 0, 3, 4],
       [5, 6, 7, 0],
       [8, 9, 0, 0]])

In [106]: (A!=0)
Out[106]: 
array([[ True, False,  True,  True],
       [ True,  True,  True, False],
       [ True,  True, False, False]], dtype=bool)

In [107]: (A!=0).cumsum(1)
Out[107]: 
array([[1, 1, 2, 3],
       [1, 2, 3, 3],
       [1, 2, 2, 2]])

In [108]: (A!=0).cumsum(1).argmax(1)
Out[108]: array([3, 2, 1])
</code></pre>

<p>Finally, we use <code>fancy-indexing</code> to use those as the column indices to set appropriate elements in <code>A</code>.</p>

<p>In the second approach, when we use <code>argmax</code> on the boolean array, we simply got the first occurence of <code>True</code>, which we used on a row-flipped version of the input array. As such, we would have the last non-zero elem in the original order. Rest of the idea there, is the same.</p>
"
39960675,2141635.0,2016-10-10T14:31:54Z,39957573,0,"<p>Presuming your data actually looks like:</p>

<pre><code>Receipt,Name,Address,Date,Time,Items
25007,A,ABC pte ltd,4/7/2016,10:40,""Cheese, Cookie, Pie""
25008,A,CCC pte ltd,4/7/2016,11:30,""Cheese, Cookie""
25009,B,CCC pte ltd,4/7/2016,07:35,""Chocolate""
25010,A,CCC pte ltd,4/7/2016,12:40,"" Butter, Cookie""
</code></pre>

<p>then it is pretty trivial to group:</p>

<pre><code>from collections import defaultdict
from csv import reader
with open(""test.csv"") as f:
    next(f) # skip header
    group_dict = defaultdict(list)
    for _, name, _, dte, time, _ in reader(f):
        group_dict[name].append((dte, time))

from  pprint import pprint as pp

pp(dict(group_dict))
</code></pre>

<p>which would give you:</p>

<pre><code>'A': [('4/7/2016', '10:40'), ('4/7/2016', '11:30'), ('4/7/2016', '12:40')],
 'B': [('4/7/2016', '07:35')]}
</code></pre>

<p>If you don't want the date repeating, then also group on that:</p>

<pre><code>with open(""test.csv"") as f:
    next(f) # skip header
    group_dict = defaultdict(list)
    for _, name, _, dte, time, _ in reader(f):
        group_dict[name, dte].append(time)

from  pprint import pprint as pp

pp(dict(group_dict))
</code></pre>

<p>Which would give you:</p>

<pre><code>{('A', '4/7/2016'): ['10:40', '11:30', '12:40'], ('B', '4/7/2016'): ['07:35']}
</code></pre>
"
39961127,1079354.0,2016-10-10T14:55:38Z,39960754,2,"<p>Your utilization of <code>round</code> means that your coin flip function will tend towards even numbers if the values you get from your time-based random operation are equidistant from one another (i.e. you ""flip"" your coin more consistently every half second due to your computer internals).</p>

<p><a href=""https://docs.python.org/3/library/functions.html#round"" rel=""nofollow"">From the documentation</a>:</p>

<blockquote>
  <p>For the built-in types supporting <code>round()</code>, values are rounded to the closest multiple of 10 to the power minus <em>ndigits</em>; if two multiples are equally close, rounding is done toward the even choice (so, for example, both <code>round(0.5)</code> and <code>round(-0.5)</code> are 0, and <code>round(1.5)</code> is 2).</p>
</blockquote>

<p>It appears that both of your methods suffer from this sort of bias; if they're executed too quickly after one another, or too close to a single timestamp, then you can tend to get <em>one</em> value out of it:</p>

<pre><code>&gt;&gt;&gt; [dice() for x in range(11)]
[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
&gt;&gt;&gt; [coin() for x in range(11)]
['Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes', 'Dimes']
</code></pre>

<p>The only realistic thing that you could do is to regenerate the time sample if the values are sufficiently close to one another so that you don't run into time-based biases like this, or generate ten time samples and take the average of those instead.  Principally, if your computer moves quickly enough and executes these functions fast enough, it <em>will</em> likely pull the same timestamp, which will lead to a strong time-based bias.</p>
"
39961212,229602.0,2016-10-10T15:00:23Z,39960941,3,"<p>I'd go with the set. It's much more readable. The string of <code>or</code>s can be faster in some circumstances since the operator short circuits and there is no overhead of constructing the list of items each time but I don't think it's worth the readability sacrifice. Here is a quick and dirty benchmark. This is with Python 2.7</p>

<pre><code> def t1(x):
   return (x == ""Foo"" or x == ""Bar"" or x == ""Baz"" or x == ""Quux"")                                                                                    


 def t2(x):
   return x in {""Foo"", ""Bar"", ""Baz"", ""Quux""}

 [2.7.9]&gt;&gt;&gt; import timeit
 [2.7.9]&gt;&gt;&gt; timeit.timeit(lambda : t1(""Quux""))                                                                                                                  
 0.22514700889587402
 [2.7.9]&gt;&gt;&gt; timeit.timeit(lambda : t1(""Foo""))                                                                                                                   
 0.18890380859375
 [2.7.9]&gt;&gt;&gt; timeit.timeit(lambda : t2(""Quux""))                                                                                                                  
 0.27969884872436523
 [2.7.9]&gt;&gt;&gt; timeit.timeit(lambda : t2(""Foo""))                                                                                                                   
 0.25904297828674316
</code></pre>

<p>Python 3 numbers.</p>

<pre><code> [3.4.2]&gt;&gt;&gt; timeit.timeit(lambda : t1(""Quux""))
 0.25126787397312
 [3.4.2]&gt;&gt;&gt; timeit.timeit(lambda : t1(""Foo""))
 0.1722603400121443
 [3.4.2]&gt;&gt;&gt; timeit.timeit(lambda : t2(""Quux""))
 0.18982669000979513
 [3.4.2]&gt;&gt;&gt; timeit.timeit(lambda : t2(""Foo""))
 0.17984321201220155
</code></pre>
"
39961343,847552.0,2016-10-10T15:07:22Z,39960941,1,"<p>Obviously in your case it's better to use <code>in</code> operator. It's just much more readable.</p>

<p>In more complex cases when it's not possible to use <code>in</code> operator, you may use <a href=""https://docs.python.org/3/library/functions.html#all"" rel=""nofollow""><code>all</code></a> and <a href=""https://docs.python.org/3/library/functions.html#any"" rel=""nofollow""><code>any</code></a> functions:</p>

<pre><code>operations = {'EQUAL', 'NOT EQUAL', 'LESS', 'GREATER'}
condition1 = any(curr_op.startswith(op) for op in operations)

condition2 = all([
    self.Operation == ""EQUAL"",
    isinstance(self.LeftHandSide, int),
    isinstance(self.RightHandSide, int),
])
</code></pre>
"
39962293,2846140.0,2016-10-10T16:03:50Z,39958110,1,"<blockquote>
  <p>But how to run track() strictly before reboot to not miss any possible output in log?</p>
</blockquote>

<p>You could <code>await</code> the first subprocess creation before running the second one. </p>

<blockquote>
  <p>And how to retrieve return values of both coroutines?</p>
</blockquote>

<p><a href=""https://docs.python.org/3.5/library/asyncio-task.html#asyncio.gather"" rel=""nofollow""><code>asyncio.gather</code></a> returns the aggregated results.</p>

<p>Example:</p>

<pre><code>async def main():
    process_a = await asyncio.create_subprocess_shell([...])
    process_b = await asyncio.create_subprocess_shell([...])
    return await asyncio.gather(monitor_a(process_a), monitor_b(process_b))

loop = asyncio.get_event_loop()
result_a, result_b = loop.run_until_complete(main())
</code></pre>
"
39962542,4522780.0,2016-10-10T16:17:55Z,39888949,1,"<p>You can replace the <code>for</code> loop with <code>writelines</code> by passing a genexp to it and replace <code>zip</code> with <code>izip</code> from <code>itertools</code> in method 2. This may come close to <code>paste</code> or surpass it.</p>

<pre><code>with open(file1, 'rb') as fin1, open(file2, 'rb') as fin2, open(output, 'wb') as fout:
    fout.writelines(b""{}\t{}"".format(*line) for line in izip(fin1, fin2))
</code></pre>

<p>If you don't want to embed <code>\t</code> in the format string, you can use <code>repeat</code> from <code>itertools</code>;  </p>

<pre><code>    fout.writelines(b""{}{}{}"".format(*line) for line in izip(fin1, repeat(b'\t'), fin2))
</code></pre>

<p>If the files are of same length, you can do away with <code>izip</code>.  </p>

<pre><code>with open(file1, 'rb') as fin1, open(file2, 'rb') as fin2, open(output, 'wb') as fout:
    fout.writelines(b""{}\t{}"".format(line, next(fin2)) for line in fin1)
</code></pre>
"
39962759,161801.0,2016-10-10T16:30:59Z,39939093,0,"<p>It looks like you have an old version of SymPy. Try upgrading to the newest version (1.0 at the time of writing). </p>
"
39963449,984421.0,2016-10-10T17:14:38Z,39962499,2,"<p>The main problem with your example is how you are handling endline characters. If you completely replace them in the input, the output will no longer line up correctly, and so won't make any sense. To fix that, the <code>readable_whitespace</code> function should look something like this:</p>

<pre><code>def readable_whitespace(line):
    end = len(line.rstrip('\r\n'))
    return line[:end] + repr(line[end:])[1:-1] + '\n'
</code></pre>

<p>This will handle all types of endline sequence, and ensures that the lines are displayed correctly when printed.</p>

<p>The other minor problem is due to a typo:</p>

<pre><code>text1 = [readable_whitespace(line) for line in text1]
text1 = [readable_whitespace(line) for line in text2]
# --^ oops!    
</code></pre>

<p>Once these fixes are made, the output will look like this:</p>

<pre><code>- AAABAAA\n
?    ^
+ AAAAAAA\n
?    ^
+ \n
- BBB\n
?    --
+ BBB
</code></pre>

<p>which should hopefully now make sense to you.</p>
"
39963893,494631.0,2016-10-10T17:44:27Z,39963745,0,"<p>I had the similar issue in the past. I have found a key to this bug in <a href=""https://github.com/boto/boto3/issues/134"" rel=""nofollow"">https://github.com/boto/boto3/issues/134</a> .</p>

<p>You can use undocumented trick:</p>

<pre><code>import botocore


def s3_list(bucket, s3path_or_prefix, public=False):
    bsession = boto3.Session(aws_access_key_id=settings.AWS['ACCESS_KEY'],
                             aws_secret_access_key=settings.AWS['SECRET_ACCESS_KEY'],
                             region_name=settings.AWS['REGION_NAME'])
    client = bsession.client('s3')
    if public:
        client.meta.events.register('choose-signer.s3.*', botocore.handlers.disable_signing)
    result = client.list_objects(Bucket=bucket, Delimiter='/', Prefix=s3path_or_prefix)
    return [obj['Prefix'] for obj in result.get('CommonPrefixes')]
</code></pre>
"
39964056,5306028.0,2016-10-10T17:55:39Z,39963364,-1,"<p>Maybe it's because you're typing multiple letters at once. Your program then checks if e. g. ""ABC"" matches ""A"". Try entering single letters.</p>
"
39964818,3293881.0,2016-10-10T18:46:59Z,39964555,2,"<p>Since we are assuming that <code>ixs</code> could be <em>sparsey</em>, we could modify the strategy to get the summations of rows from the <code>zero-th</code> row and rest of the rows separately based on the given row indices. So, we could use the <code>bincount</code> method for the <code>non-zero-th</code> indexed rows summation and add it with the <code>(zero-th row x no. of zeros</code> in <code>ixs</code>).</p>

<p>Thus, the second approach could be modified, like so -</p>

<pre><code>nzmask = ixs!=0
nzsum = np.bincount(ixs[nzmask]-1, minlength=mat.shape[0]-1).dot(mat[1:])
row0_sum = mat[0]*(len(ixs) - np.count_nonzero(nzmask))
out = nzsum + row0_sum
</code></pre>

<p>We could extend this strategy to the first approach as well, like so -</p>

<pre><code>out = mat[0]*(len(ixs) - len(nzidx)) + mat[ixs[nzidx]].sum(axis=0)
</code></pre>

<p>If we are working with lots of non-zero indices that are repeated, we could alternatively make use of <code>np.take</code> with focus on performance. Thus, <code>mat[ixs[nzidx]]</code> could be replaced by <code>np.take(mat,ixs[nzidx],axis=0)</code> and similarly <code>mat[ixs]</code> by <code>np.take(mat,ixs,axis=0)</code>. With such repeated indices based indexing <code>np.take</code> brings out some noticeable speedup as compared to simply indexing.</p>

<p>Finally, we could use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html"" rel=""nofollow""><code>np.einsum</code></a> to perform these row ID based selection and summing, like so -</p>

<pre><code>nzmask = ixs!=0
unq,tags = np.unique(ixs[nzmask],return_inverse=1)
nzsum = np.einsum('ji,jk-&gt;k',np.arange(len(unq))[:,None] == tags,mat[unq])
out = mat[0]*(len(ixs) - np.count_nonzero(nzmask)) + nzsum
</code></pre>

<h2>Benchmarking</h2>

<p>Let's list out all the five approaches posted thus far in this post and also include the two approaches posted in the question for some runtime testing as functions -</p>

<pre><code>def org_indexing_app(mat,ixs):
    return mat[ixs].sum(axis=0)

def org_bincount_app(mat,ixs):
    return np.bincount(ixs, minlength=mat.shape[0]).dot(mat)

def indexing_modified_app(mat,ixs):
    return np.take(mat,ixs,axis=0).sum(axis=0)

def bincount_modified_app(mat,ixs):
    nzmask = ixs!=0
    nzsum = np.bincount(ixs[nzmask]-1, minlength=mat.shape[0]-1).dot(mat[1:])
    row0_sum = mat[0]*(len(ixs) - np.count_nonzero(nzmask))
    return nzsum + row0_sum

def simply_indexing_app(mat,ixs):
    nzmask = ixs!=0
    nzsum = mat[ixs[nzmask]].sum(axis=0)
    return mat[0]*(len(ixs) - np.count_nonzero(nzmask)) + nzsum

def take_app(mat,ixs):
    nzmask = ixs!=0
    nzsum = np.take(mat,ixs[nzmask],axis=0).sum(axis=0)
    return mat[0]*(len(ixs) - np.count_nonzero(nzmask)) + nzsum

def unq_mask_einsum_app(mat,ixs):
    nzmask = ixs!=0
    unq,tags = np.unique(ixs[nzmask],return_inverse=1)
    nzsum = np.einsum('ji,jk-&gt;k',np.arange(len(unq))[:,None] == tags,mat[unq])
    return mat[0]*(len(ixs) - np.count_nonzero(nzmask)) + nzsum
</code></pre>

<p><strong>Timings</strong></p>

<p>Case #1 (<code>ixs</code> is 95% sparsey) :</p>

<pre><code>In [301]: # Setup input
     ...: mat = np.random.rand(20,4)
     ...: ixs = np.random.randint(0,10,(100000))
     ...: ixs[np.random.rand(ixs.size)&lt;0.95] = 0 # Make it approx 95% sparsey
     ...: 

In [302]: # Timings
     ...: %timeit org_indexing_app(mat,ixs)
     ...: %timeit org_bincount_app(mat,ixs)
     ...: %timeit indexing_modified_app(mat,ixs)
     ...: %timeit bincount_modified_app(mat,ixs)
     ...: %timeit simply_indexing_app(mat,ixs)
     ...: %timeit take_app(mat,ixs)
     ...: %timeit unq_mask_einsum_app(mat,ixs)
     ...: 
100 loops, best of 3: 4.89 ms per loop
1000 loops, best of 3: 428 Âµs per loop
100 loops, best of 3: 3.29 ms per loop
1000 loops, best of 3: 329 Âµs per loop
1000 loops, best of 3: 537 Âµs per loop
1000 loops, best of 3: 462 Âµs per loop
1000 loops, best of 3: 1.07 ms per loop
</code></pre>

<p>Case #2 (<code>ixs</code> is 98% sparsey) :</p>

<pre><code>In [303]: # Setup input
     ...: mat = np.random.rand(20,4)
     ...: ixs = np.random.randint(0,10,(100000))
     ...: ixs[np.random.rand(ixs.size)&lt;0.98] = 0 # Make it approx 98% sparsey
     ...: 

In [304]: # Timings
     ...: %timeit org_indexing_app(mat,ixs)
     ...: %timeit org_bincount_app(mat,ixs)
     ...: %timeit indexing_modified_app(mat,ixs)
     ...: %timeit bincount_modified_app(mat,ixs)
     ...: %timeit simply_indexing_app(mat,ixs)
     ...: %timeit take_app(mat,ixs)
     ...: %timeit unq_mask_einsum_app(mat,ixs)
     ...: 
100 loops, best of 3: 4.86 ms per loop
1000 loops, best of 3: 438 Âµs per loop
100 loops, best of 3: 3.5 ms per loop
1000 loops, best of 3: 260 Âµs per loop
1000 loops, best of 3: 318 Âµs per loop
1000 loops, best of 3: 288 Âµs per loop
1000 loops, best of 3: 694 Âµs per loop
</code></pre>
"
39965947,901925.0,2016-10-10T20:05:47Z,39964555,2,"<p>An alternative to <code>bincount</code> is <code>add.at</code>:</p>

<pre><code>In [193]: mat
Out[193]: 
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]])
In [194]: ixs
Out[194]: array([0, 2, 0, 0, 0, 1, 1])

In [195]: J = np.zeros(mat.shape[0],int)
In [196]: np.add.at(J, ixs, 1)
In [197]: J
Out[197]: array([4, 2, 1])

In [198]: np.dot(J, mat)
Out[198]: array([16, 23, 30, 37])
</code></pre>

<p>By the sparsity, you mean, I assume, that <code>ixs</code> might not include all the rows, for example, <code>ixs</code> without the 0s:</p>

<pre><code>In [199]: ixs = np.array([2,1,1])
In [200]: J=np.zeros(mat.shape[0],int)
In [201]: np.add.at(J, ixs, 1)
In [202]: J
Out[202]: array([0, 2, 1])
In [203]: np.dot(J, mat)
Out[203]: array([16, 19, 22, 25])
</code></pre>

<p><code>J</code> still has the <code>mat.shape[0]</code> shape.  But the <code>add.at</code> should scale as the length of <code>ixs</code>.  </p>

<p>A sparse solution would look something like:</p>

<p>Make a sparse matrix from <code>ixs</code> that looks like:</p>

<pre><code>In [204]: I
Out[204]: 
array([[1, 0, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 0, 1, 1],
       [0, 1, 0, 0, 0, 0, 0]])
</code></pre>

<p>sum the rows; sparse does this with matrix multiplication like:</p>

<pre><code>In [205]: np.dot(I, np.ones((7,),int))
Out[205]: array([4, 2, 1])
</code></pre>

<p>then do our dot:</p>

<pre><code>In [206]: np.dot(np.dot(I, np.ones((7,),int)), mat)
Out[206]: array([16, 23, 30, 37])
</code></pre>

<p>Or in sparse code:</p>

<pre><code>In [225]: J = sparse.coo_matrix((np.ones_like(ixs,int),(np.arange(ixs.shape[0]), ixs)))
In [226]: J.A
Out[226]: 
array([[1, 0, 0],
       [0, 0, 1],
       [1, 0, 0],
       [1, 0, 0],
       [1, 0, 0],
       [0, 1, 0],
       [0, 1, 0]])
In [227]: J.sum(axis=0)*mat
Out[227]: matrix([[16, 23, 30, 37]])
</code></pre>

<p><code>sparse</code>, when converting from <code>coo</code> to <code>csr</code> sums duplicates.  I can take advantage that with</p>

<pre><code>In [229]: J = sparse.coo_matrix((np.ones_like(ixs,int), (np.zeros_like(ixs,int), ixs)))
In [230]: J
Out[230]: 
&lt;1x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'
    with 7 stored elements in COOrdinate format&gt;
In [231]: J.A
Out[231]: array([[4, 2, 1]])
In [232]: J*mat
Out[232]: array([[16, 23, 30, 37]], dtype=int32)
</code></pre>
"
39966269,100297.0,2016-10-10T20:30:37Z,39966239,5,"<p>You were almost there, just remove the comma (and pass in a float number, not a string):</p>

<pre><code>""{0:&gt;15.2f}"".format(1464.1000000000001)
</code></pre>

<p>See the <a href=""https://docs.python.org/3/library/string.html#format-specification-mini-language"" rel=""nofollow""><em>Format Specification Mini-Language</em> section</a>:</p>

<blockquote>
<pre><code>format_spec ::=  [[fill]align][sign][#][0][width][,][.precision][type]
fill        ::=  &lt;any character&gt;
align       ::=  ""&lt;"" | ""&gt;"" | ""="" | ""^""
sign        ::=  ""+"" | ""-"" | "" ""
width       ::=  integer
precision   ::=  integer
type        ::=  ""b"" | ""c"" | ""d"" | ""e"" | ""E"" | ""f"" | ""F"" | ""g"" | ""G"" | ""n"" | ""o"" | ""s"" | ""x"" | ""X"" | ""%""
</code></pre>
</blockquote>

<p>Breaking the above format down then:</p>

<pre><code>fill: &lt;empty&gt;
align: &lt;  # left
sign: &lt;not specified&gt;
width: 15
precision: 2
type: `f`
</code></pre>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; ""{0:&gt;15.2f}"".format(1464.1000000000001)
'        1464.10'
</code></pre>

<p>Note that for numbers, the default alignment is to the right, so the <code>&gt;</code> could be omitted.</p>
"
39966275,6888191.0,2016-10-10T20:31:01Z,39966239,3,"<pre><code>""{0:15.2f}"".format(1464.1000000000001)
</code></pre>

<p>I always find this site useful for this stuff:</p>

<p><a href=""https://pyformat.info/"" rel=""nofollow"">https://pyformat.info/</a></p>
"
39967086,4785185.0,2016-10-10T21:33:16Z,39967037,0,"<p>You can index <strong>live_leak</strong>, but <strong>live_leak.links</strong> appears to be some other type of construct that returns the elements of <strong>live_leak</strong>.  Try <strong>live_leak[1]</strong>, perhaps?</p>
"
39967124,4764619.0,2016-10-10T21:36:33Z,39967037,0,"<p>One is looking for links under live_leak the other is just looking at live_leak itself.</p>

<p>for example:
live_leak[1] </p>

<p>should return:
[{'type': 'application/x-shockwave-flash', 'rel': 'enclosure', 'href':
 '<a href=""http://www.liveleak.com/e/abf_1476121939"" rel=""nofollow"">http://www.liveleak.com/e/abf_1476121939</a>'}]</p>
"
39969874,14860.0,2016-10-11T03:01:22Z,39969798,2,"<p>Well, you could <em>start</em> by not unnecessarily duplicating information.</p>

<p>Storing full tuples (number and index) for each multiple is inefficient when you already have that information available.</p>

<p>For example, rather than:</p>

<pre><code>(3, 2): [(16, 4), (6, 7), (6, 8), (9, 10), (24, 11)]
</code></pre>

<p>(the <code>16</code> appears to be wrong there as it's <em>not</em> a multiple of <code>3</code> so I'm guessing you meant <code>15</code>) you could instead opt for:</p>

<pre><code>(3, 2): [15, 6, 9, 24]
(6, 7): ...
</code></pre>

<p>That pretty much halves your storage needs since you can go from the <code>6</code> in the list and find all its indexes by searching the tuples. That will, of course, be extra <em>processing</em> effort to traverse the list but it's probably better to have a slower working solution than a faster non-working one :-)</p>

<hr>

<p>You could reduce the storage even more by not storing the multiples at all, instead running through the tuple list using <code>%</code> to see if you have a multiple.</p>

<hr>

<p>But, of course, this all depends on your <em>actual</em> requirements which would be better off stating the <em>intent</em> of what your trying to achieve rather than pre-supposing a solution.</p>
"
39969921,1048539.0,2016-10-11T03:07:22Z,39969463,1,"<p>Keep a list of all the previous values and check each subsequent iteration.</p>

<pre><code># sets have faster ""val in set?"" checks than lists
# do this once, somewhere in your program
previous_vals = set()

# other stuff here, including main program loop
for x in range(0, 10):
    # find a unique, new random number while 
    # limiting number of tries to prevent infinite looping
    number_of_tries = 0
    MAX_TRIES = 10
    generating_random = true
    while generating_random:    
        minimum = random.randrange(0, max_line,6)
        if minimum not in previous_vals:
            previous_vals.add(minimum)
            generating_random = false
        number_of_tries += 1
        if number_of_tries == MAX_TRIES:
            raise RunTimeError(""Maximum number of random tries reached!"")

    maximum = minimum+6
    listQ = listQ[minimum:maximum]
</code></pre>

<p>Note that <a href=""https://docs.python.org/2/library/stdtypes.html#set.add"" rel=""nofollow"">there are other functions for <code>set</code> than add</a> if you want to modify your example. </p>

<p>I added a maximum number of tries too in order to prevent your code from getting stuck in an infinite loop, since I don't know anything about your input data to know what the likilhood of getting into this situation is.</p>
"
39970466,642070.0,2016-10-11T04:21:57Z,39969798,2,"<p>You rebuild tuples in places like <code>pair = (y, y2 + x2 + 1)</code> and <code>num_dict[(x, x2)].append(pair)</code> when you could build a canonical set of tuples early on and then just put references in the containers. I cobbled up a 2000 item test my machine that works. I have python 3.4 64 bit with a relatively modest 3.5 GIG of RAM...</p>

<pre><code>import random

# a test list that should generate longish lists
l = list(random.randint(0, 2000) for _ in range(2000))

# setup canonical index and sort ascending
sorted_index = sorted((v,i) for i,v in enumerate(l))

num_dict = {}
for idx, vi in enumerate(sorted_index):
    v = vi[0]
    num_dict[vi] = [vi2 for vi2 in sorted_index[idx+1:] if not vi2[0] % v]

for item in num_dict.items():
    print(item)
</code></pre>
"
39970834,2285236.0,2016-10-11T05:08:46Z,39970703,3,"<p>You can create a column of absolute differences:</p>

<pre><code>df['dif'] = (df['values'] - 2).abs()

df
Out: 
  category  values  dif
0        a       1    1
1        b       2    0
2        b       3    1
3        b       4    2
4        c       5    3
5        a       4    2
6        b       3    1
7        c       2    0
8        c       1    1
9        a       0    2
</code></pre>

<p>And then use <code>groupby.transform</code> to check whether the minimum value of each group is equal to the difference you calculated:</p>

<pre><code>df['is_closest'] = df.groupby('category')['dif'].transform('min') == df['dif']

df
Out: 
  category  values  dif is_closest
0        a       1    1       True
1        b       2    0       True
2        b       3    1      False
3        b       4    2      False
4        c       5    3      False
5        a       4    2      False
6        b       3    1      False
7        c       2    0       True
8        c       1    1      False
9        a       0    2      False
</code></pre>

<p><code>df.groupby('category')['dif'].idxmin()</code> would also give you the indices of the closest values for each category. You can use that for mapping too. </p>

<p>For selection:</p>

<pre><code>df.loc[df.groupby('category')['dif'].idxmin()]
Out: 
  category  values  dif
0        a       1    1
1        b       2    0
7        c       2    0
</code></pre>

<p>For assignment:</p>

<pre><code>df['is_closest'] = False
df.loc[df.groupby('category')['dif'].idxmin(), 'is_closest'] = True
df
Out: 
  category  values  dif is_closest
0        a       1    1       True
1        b       2    0       True
2        b       3    1      False
3        b       4    2      False
4        c       5    3      False
5        a       4    2      False
6        b       3    1      False
7        c       2    0       True
8        c       1    1      False
9        a       0    2      False
</code></pre>

<p>The difference between these approaches is that if you check equality against the difference, you would get True for all rows in case of ties. However, with <code>idxmin</code> it will return True for the first occurrence (only one for each group). </p>
"
39970921,2901002.0,2016-10-11T05:19:42Z,39970703,2,"<p>Solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.idxmin.html"" rel=""nofollow""><code>DataFrameGroupBy.idxmin</code></a> - get indexes of minimal values per group and then assign boolean mask by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.isin.html"" rel=""nofollow""><code>Index.isin</code></a> to column <code>isClosest</code>:</p>

<pre><code>idx = (df['values'] - 2).abs().groupby([df['category']]).idxmin()
print (idx)
category
a    0
b    1
c    7
Name: values, dtype: int64

df['isClosest'] = df.index.isin(idx)
print (df)
  category  values isClosest
0        a       1      True
1        b       2      True
2        b       3     False
3        b       4     False
4        c       5     False
5        a       4     False
6        b       3     False
7        c       2      True
8        c       1     False
9        a       0     False
</code></pre>
"
39970930,3065657.0,2016-10-11T05:20:10Z,39970857,6,"<p>Sorting the data and then comparing to the previous would do it:</p>

<pre><code>data = [-2000, 1000, 2000, 3500, 3800, 4500, 4600, 5000, 6000]

lst = [min(data)]          # the min of data goes in solution list
for i in sorted(data[1:]):
  if i-lst[-1] &gt; 999:      # comparing with last element
    lst.append(i)          # of the solution list

print (lst)
</code></pre>

<p>ouput:</p>

<pre><code>[-2000, 1000, 2000, 3500, 4500, 6000]
</code></pre>
"
39971122,6502500.0,2016-10-11T05:45:34Z,39971030,-2,"<p>A slice can't be in your return type as the method just doesn't support this. You can read more about the <code>__index__</code> <a href=""https://www.python.org/dev/peps/pep-0357/"" rel=""nofollow"">special method here</a>.
I could only come up with a workaround that directly calls the function in your class:</p>

<pre><code>class MyThing(object):
        def __init__(self, start, stop, otherstuff):
            self.start = start
            self.stop = stop
            self.otherstuff = otherstuff  

        def __index__(self):
            return slice(self.start, self.stop)

    thing = MyThing(1, 3, 'potato')
    print 'Hello World'[thing.__index__()]
</code></pre>

<p>This will return <code>el</code>.</p>
"
39971377,5349916.0,2016-10-11T06:13:02Z,39971030,5,"<p>TLDR: It's impossible to make custom classes replace <code>slice</code> for builtins types such as <code>list</code> and <code>tuple</code>.</p>

<hr>

<p>The <code>__index__</code> method exists purely to provide an <em>index</em>, which is by definition an integer in python (see <a href=""https://docs.python.org/3.5/reference/datamodel.html#object.__index__"" rel=""nofollow"">the Data Model</a>). You cannot use it for resolving an object to a <code>slice</code>.</p>

<p>I'm afraid that <code>slice</code> seems to be handled specially by python. The interface requires an actual slice; providing its signature (which also includes the <code>indices</code> method) is not sufficient. As you've found out, you cannot inherit from it, so you cannot create new types of <code>slice</code>s. Even Cython will not allow you to inherit from it.</p>

<hr>

<p>So why is <code>slice</code> special? Glad you asked. Welcome to the innards of CPython. Please wash your hands after reading this.</p>

<p>So slice objects are described in <a href=""https://raw.githubusercontent.com/python/cpython/c30098c8c6014f3340a369a31df9c74bdbacc269/Doc/c-api/slice.rst"" rel=""nofollow""><code>slice.rst</code></a>. Note these two guys:</p>

<blockquote>
  <p>.. c:var:: PyTypeObject PySlice_Type</p>
  
  <p>The type object for slice objects.  This is the same as :class:<code>slice</code> in the
     Python layer.</p>
  
  <p>.. c:function:: int PySlice_Check(PyObject *ob)
     Return true if <em>ob</em> is a slice object; <em>ob</em> must not be <em>NULL</em>.</p>
</blockquote>

<p>Now, this is actually implemented in <a href=""https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Include/sliceobject.h"" rel=""nofollow""><code>sliceobject.h</code></a> as :</p>

<pre><code>#define PySlice_Check(op) (Py_TYPE(op) == &amp;PySlice_Type)
</code></pre>

<p>So <em>only</em> the <code>slice</code> type is allowed here. This check is actually used in <a href=""https://github.com/python/cpython/blob/c30098c8c6014f3340a369a31df9c74bdbacc269/Objects/listobject.c#L2406"" rel=""nofollow""><code>list_subscript</code></a> (and <code>tuple subscript</code>, ...) <em>after</em> attempting to use the index protocol (so having <code>__index__</code> on a slice is a bad idea). A custom container class is free to overwrite <code>__getitem__</code> and use its own rules, but that's how <code>list</code> (and <code>tuple</code>, ...) does it.</p>

<p>Now, why is it not possible to subclass <code>slice</code>? Well, <code>type</code> actually has a flag indicating whether something can be subclassed. It is checked <a href=""https://github.com/python/cpython/blob/222b935769b07b8e68ec5b0494c39518d90112d1/Objects/typeobject.c#L1971"" rel=""nofollow"">here</a> and generates the error you have seen:</p>

<pre><code>    if (!PyType_HasFeature(base_i, Py_TPFLAGS_BASETYPE)) {
        PyErr_Format(PyExc_TypeError,
                     ""type '%.100s' is not an acceptable base type"",
                     base_i-&gt;tp_name);
        return NULL;
    }
</code></pre>

<p>I haven't been able to track down how <code>slice</code> (un)sets this value, but the fact that one gets this error means it does. This means you cannot subclass it.</p>

<hr>

<p>Closing remarks: After remembering some long-forgotten C-(non)-skills, I'm fairly sure this is not about optimization in the strict sense. All existing checks and tricks would still work (at least those I've found).</p>

<p>After washing my hands and digging around in the internet, I've found a few references to similar ""issues"". <a href=""http://grokbase.com/t/python/python-list/033r5nks47/type-function-does-not-subtype#20030324rcnwbkfedhzbaf3vmiuer3z4xq"" rel=""nofollow"">Tim Peters has said all there is to say:</a></p>

<blockquote>
  <p>Nothing implemented in C is subclassable unless somebody volunteers the work
  to make it subclassable; nobody volunteered the work to make the <em>[insert name here]</em>
  type subclassable. It sure wasn't at the top of my list <em>wink</em>.</p>
</blockquote>

<p>Also see <a href=""http://stackoverflow.com/a/10114382/5349916"">this thread</a> for a short discussion on non-subclass'able types.</p>

<p>Practically all alternative interpreters replicate the behavior to various degrees: <a href=""https://github.com/nakagami/jython3/blob/master/src/org/python/core/PyType.java#L65"" rel=""nofollow"">Jython</a>, <a href=""https://github.com/dropbox/pyston/issues/596"" rel=""nofollow"">Pyston</a>, <a href=""https://github.com/IronLanguages/main/blob/eedca3ee3c8260ef205a53c5ee11340b092a250a/Languages/IronPython/IronPython/Runtime/Types/NewTypeInfo.cs#L72"" rel=""nofollow"">IronPython</a> and PyPy (didn't find out how they do it, but they do).</p>
"
39972031,100297.0,2016-10-11T07:08:33Z,39971929,15,"<p>Everything between <code>:</code> and the <code>=</code> is a type hint, so <code>primes</code> is indeed defined as <code>List[int]</code>, and initially set to an empty list (and <code>stats</code> is an empty dictionary initially, defined as <code>Dict[str, int]</code>).</p>

<p><code>List[int]</code> and <code>Dict[str, int]</code> are not part of the next syntax however, these were already defined in the Python 3.5 typing hints PEP. The 3.6 <a href=""https://www.python.org/dev/peps/pep-0526/"">PEP 526 â <em>Syntax for Variable Annotations</em></a> proposal <em>only</em> defines the syntax to attach the same hints to variables; before you could only attach type hints to variables with comments (e.g. <code>primes = []  # List[int]</code>).</p>

<p>Both <code>List</code> and <code>Dict</code> are <em>Generic</em> types, indicating that you have a list or dictionary mapping with specific (concrete) contents. </p>

<p>For <code>List</code>, there is only one 'argument' (the elements in the <code>[...]</code> syntax), the type of every element in the list. For <code>Dict</code>, the first argument is the key type, and the second the value type. So <em>all</em> values in the <code>primes</code> list are integers, and <em>all</em> key-value pairs in the <code>stats</code> dictionary are <code>(str, int)</code> pairs, mapping strings to integers.</p>

<p>See the <a href=""https://docs.python.org/3/library/typing.html#typing.List""><code>typing.List</code></a> and <a href=""https://docs.python.org/3/library/typing.html#typing.Dict""><code>typing.Dict</code></a> definitions, the <a href=""https://docs.python.org/3/library/typing.html#generics"">section on <em>Generics</em></a>, as well as <a href=""https://www.python.org/dev/peps/pep-0483"">PEP 483 â <em>The Theory of Type Hints</em></a>.</p>

<p>Like type hints on functions, their use is optional and are also considered <em>annotations</em> (provided there is an object to attach these to, so globals in modules and attributes on classes, but not locals in functions) which you could introspect via the <code>__annotations__</code> attribute. You can attach arbitrary info to these annotations, you are not strictly limited to type hint information.</p>

<p>You may want to read the <a href=""https://www.python.org/dev/peps/pep-0526/"">full proposal</a>; it contains some additional functionality above and beyond the new syntax; it specifies when such annotations are evaluated, how to introspect them and how to declare something as a class attribute vs. instance attribute, for example.</p>
"
39972305,2526441.0,2016-10-11T07:28:53Z,39960941,0,"<p>As suggested by multiple people, go for reability.</p>

<p>performance wise there is a difference, the <code>in</code> operator on sets has an average lookup time of O(1), while for lists it's O(n). You can find this <a href=""https://wiki.python.org/moin/TimeComplexity"" rel=""nofollow"">here</a>.</p>

<p>In your case where the list of possibilities is limited you will hardly notice a difference. However, once this list becomes very large (talking about millions), you can notice a difference.</p>

<p>A simple example can show this: For sets:</p>

<pre><code>operation = 9999999
lookupSet = {i for i in range(0,10000000)}
%timeit operation in lookupSet
&gt;&gt; 10000000 loops, best of 3: 89.4 ns per loop
</code></pre>

<p>where with lists:</p>

<pre><code>operation = 9999999
lookupList =  [i for i in range(0,10000000)]
%timeit operation in lookupList
&gt;&gt; 10 loops, best of 3: 168 ms per loop
</code></pre>
"
39973133,4952130.0,2016-10-11T08:21:24Z,39971929,11,"<p>Indeed, variable annotations are just the next step from <code># type</code> comments as they where defined in <code>PEP 484</code>; the rationale behind this change is highlighted in the <a href=""https://www.python.org/dev/peps/pep-0526/#rationale"">respective PEP section</a>. So, instead of hinting the type with:</p>

<pre><code>primes = []  # type: List[int]
</code></pre>

<p>New syntax was introduced to allow for directly annotating the type with an assignment of the form:</p>

<pre><code>primes: List[int] = []
</code></pre>

<p>which, as @Martijn pointed out, denotes a list of integers by using types available in <a href=""https://docs.python.org/3/library/typing.html""><code>typing</code></a> and initializing it to an empty list.</p>

<hr>

<p>In short, the <a href=""https://docs.python.org/3.6/reference/simple_stmts.html#annotated-assignment-statements"">new syntax introduced</a> simply allows you to annotate with a type after the colon <code>:</code> character and, optionally allows you to assign a value to it:</p>

<pre><code>annotated_assignment_stmt ::=  augtarget "":"" expression [""="" expression]
</code></pre>

<hr>

<p>Additional changes were also introduced along with the new syntax; modules and classes now have an <code>__annotations__</code> attribute, as functions have had for some time, in which the type metadata is attached:</p>

<pre><code>from typing import get_type_hints  # grabs __annotations__
</code></pre>

<p>Now <code>__main__.__annotations__</code> holds the declared types:</p>

<pre><code>&gt;&gt;&gt; from typing import List, get_type_hints
&gt;&gt;&gt; primes: List[int] = []
&gt;&gt;&gt; captain: str
&gt;&gt;&gt; import __main__
&gt;&gt;&gt; get_type_hints(__main__)
{'primes': typing.List&lt;~T&gt;[int]}
</code></pre>

<p><code>captain</code> won't currently show up through <a href=""https://docs.python.org/3.6/library/typing.html#typing.get_type_hints""><code>get_type_hints</code></a> because <code>get_type_hints</code> only returns types that can also be accessed on a module; i.e it needs a value first:</p>

<pre><code>&gt;&gt;&gt; captain = ""Picard""
&gt;&gt;&gt; get_type_hints(__main__)
{'primes': typing.List&lt;~T&gt;[int], 'captain': &lt;class 'str'&gt;}
</code></pre>

<p><sub> Using <code>print(__annotations__)</code> will show <code>'captain': &lt;class 'str'&gt;</code> but you really shouldn't be accessing <code>__annotations__</code> directly.</sub></p>

<p>Similarly, for classes:</p>

<pre><code>&gt;&gt;&gt; get_type_hints(Starship)
ChainMap({'stats': typing.Dict&lt;~KT, ~VT&gt;[str, int]}, {})
</code></pre>

<p>Where a <code>ChainMap</code> is used to grab the annotations for a given class (located in the first mapping) and all annotations defined in the base classes found in its <code>mro</code> (consequent mappings, <code>{}</code> for object).</p>

<p>Along with the new syntax, a new <a href=""https://docs.python.org/3.6/library/typing.html#typing.ClassVar""><code>ClassVar</code></a> type has been added to denote class variables. Yup, <code>stats</code> in your example is actually an <em>instance variable</em>, not a <code>ClassVar</code>.</p>

<hr>

<p>As with type hints from <code>PEP 484</code>, these are completely optional and are of main use for type checking tools (and whatever else you can build based on this information). It is to be provisional when the stable version of Py 3.6 is released so small tweaks might be added in the future.</p>
"
39974148,5547104.0,2016-10-11T09:24:21Z,39966149,1,"<p><code>tf.gradients</code> provides this functionality via its <code>grad_ys</code> argument, see <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#gradients"" rel=""nofollow"">here</a>. In your case, <code>tf.gradients([final_layer], list_of_variables, grad_ys=[_deriv])</code> would compute the gradients you want.</p>

<p>Unfortunately, it looks like the build-in optimizers don't pass a <code>grad_ys</code> argument to <code>tf.gradients</code>. You might have to hack something into the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py#L200"" rel=""nofollow"">compute_gradients</a> method of the optimizer class.</p>
"
39974974,4014959.0,2016-10-11T10:10:30Z,39973360,3,"<p>I'm not completely clear on how you want to handle the 'Z' items in your data, but this code replicates the output for the sample data in <a href=""https://eval.in/658468"" rel=""nofollow"">https://eval.in/658468</a></p>

<pre><code>from __future__ import division

bases = set('ACGT')
#sample = [['TTTT', 'CCCZ'], ['ATTA', 'CZZC']]
sample = [['ATTA', 'TTGA'], ['TTCA', 'TTTA']]

list_of_hets = []
for element in sample:
    hets = []
    for seq in element:
        count_dict = {}
        for base in seq:
            if base in count_dict:
                count_dict[base] += 1
            else:
                count_dict[base] = 1
        print count_dict

        #Calculate frequency of every character
        count = sum(1 for u in seq if u in bases)
        pf = sum((base / count) ** 2 for base in count_dict.values())
        hets.append(1 - pf)
    list_of_hets.append(hets)

print list_of_hets
</code></pre>

<p><strong>output</strong></p>

<pre><code>{'A': 2, 'T': 2}
{'A': 1, 'T': 2, 'G': 1}
{'A': 1, 'C': 1, 'T': 2}
{'A': 1, 'T': 3}
[[0.5, 0.625], [0.625, 0.375]]
</code></pre>

<p>This code could be simplified further by using a collections.Counter instead of the <code>count_dict</code>.</p>

<p>BTW, if the symbol that's not in 'ACGT' is <em>always</em> 'Z' then we can speed up the <code>count</code> calculation. Get rid of <code>bases = set('ACGT')</code> and change</p>

<pre><code>count = sum(1 for u in seq if u in bases)
</code></pre>

<p>to</p>

<pre><code>count = sum(1 for u in seq if u != 'Z')
</code></pre>
"
39975586,851699.0,2016-10-11T10:48:04Z,39964555,0,"<p>After much number crunching (see Conclusions of original Question), the best-performing answer, when the inputs are defined as follows:</p>

<pre><code>rng = np.random.RandomState(1234)
mat = rng.randn(1000, 500)
ixs = rng.choice(rng.randint(mat.shape[0], size=mat.shape[0]/10), size=1000)
</code></pre>

<p>Seems to be:</p>

<pre><code>def sparse_bincount(mat, ixs):
    x = np.bincount(ixs)
    nonzeros, = np.nonzero(x)
    x[nonzeros].dot(mat[nonzeros])
</code></pre>
"
39976539,2901002.0,2016-10-11T11:45:50Z,39976348,2,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow""><code>drop_duplicates</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge.html"" rel=""nofollow""><code>merge</code></a>, if there is only common columns <code>place_id</code> and <code>weather</code> in both <code>DataFrames</code>, you can omit parameter <code>on</code> (it depends of data, maybe <code>on='place_id'</code> is necessary):</p>

<pre><code>df1 = place_df.drop_duplicates(['place_id'])
print (df1)

print (pd.merge(person_df, df1))
</code></pre>

<p>Sample data:</p>

<pre><code>person_df = pd.DataFrame({'place_id':['s','d','f','s','d','f'],
                          'A':[4,5,6,7,8,9]})
print (person_df)
   A place_id
0  4        s
1  5        d
2  6        f
3  7        s
4  8        d
5  9        f

place_df = pd.DataFrame({'place_id':['s','d','f', 's','d','f'],
                         'weather':['y','e','r', 'h','u','i']})
print (place_df)
  place_id weather
0        s       y
1        d       e
2        f       r
3        s       h
4        d       u
5        f       i
</code></pre>



<pre><code>def place_id_to_weather(pid):
    #for first occurence add iloc[0]
    return place_df[place_df['place_id'] == pid]['weather'].iloc[0]

person_df['weather'] = person_df['place_id'].map(place_id_to_weather)
print (person_df)
   A place_id weather
0  4        s       y
1  5        d       e
2  6        f       r
3  7        s       y
4  8        d       e
5  9        f       r
</code></pre>

<hr>

<pre><code>#keep='first' is by default, so can be omit
print (place_df.drop_duplicates(['place_id']))
  place_id weather
0        s       y
1        d       e
2        f       r

print (pd.merge(person_df, place_df.drop_duplicates(['place_id'])))
   A place_id weather
0  4        s       y
1  7        s       y
2  5        d       e
3  8        d       e
4  6        f       r
5  9        f       r
</code></pre>
"
39976650,4642859.0,2016-10-11T11:52:56Z,39976348,0,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow""><code>merge</code></a> to do the operation :</p>

<pre><code>people = pd.DataFrame([['bob', 1], ['alice', 2], ['john', 3], ['paul', 2]], columns=['name', 'place'])

#    name  place
#0    bob      1
#1  alice      2
#2   john      3
#3   paul      2

weather = pd.DataFrame([[1, 'sun'], [2, 'rain'], [3, 'snow'], [1, 'rain']], columns=['place', 'weather'])

#   place weather
#0      1     sun
#1      2    rain
#2      3    snow
#3      1    rain

pd.merge(people, weather, on='place')

#    name  place weather
#0    bob      1     sun
#1    bob      1    rain
#2  alice      2    rain
#3   paul      2    rain
#4   john      3    snow
</code></pre>

<p>In case you have several  weather for the same place, you may want to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow""><code>drop_duplicates</code></a>, then you have the following result :</p>

<pre><code>pd.merge(people, weather, on='place').drop_duplicates(subset=['name', 'place'])

#    name  place weather
#0    bob      1     sun
#2  alice      2    rain
#3   paul      2    rain
#4   john      3    snow
</code></pre>
"
39978523,3538107.0,2016-10-11T13:35:31Z,39978375,1,"<p>You should check out the python documentation.  If you still have questions then come back.</p>

<p><a href=""https://docs.python.org/2/tutorial/modules.html"" rel=""nofollow"">https://docs.python.org/2/tutorial/modules.html</a></p>
"
39978563,6779307.0,2016-10-11T13:37:24Z,39978375,0,"<p>Yes.  Let's say you're writing code in a file called <code>palindrome.py</code>.  Then in another file, or in the python shell you want to use that code.  You would type</p>

<pre><code>import palindrome
</code></pre>

<p>either at the top of the file or just into the shell as a command.  Then you can access functions you've written in <code>palindrome.py</code> with statements like</p>

<pre><code>palindrome.is_palindrome('abba')
</code></pre>

<p>It's important to note that to do this propery, the code in <code>palindrome.py</code> must be in functions.  If you're not sure how to do that, I recommend the official Python tutorial: <a href=""https://docs.python.org/3/tutorial/index.html"" rel=""nofollow"">https://docs.python.org/3/tutorial/index.html</a></p>
"
39978648,5466926.0,2016-10-11T13:41:04Z,39976715,1,"<p>My (possibly wrong :-) guess is that you start the script by something like <code>python checker/main.py</code>, thus the logging configuration in <code>__init__.py</code> is not executed.</p>

<p>Please, take a look at this answer:
<a href=""http://stackoverflow.com/questions/7533480/why-is-init-py-not-being-called"">Why is __init__.py not being called?</a></p>

<p>Moreover, you need to ensure that <code>fileConfig()</code> is called before <code>getLogger()</code> (class body is executed at the time of import). A working setup would be to load the configuration somewhere at the beginning of <code>main.py</code> and instantiate the logger in <code>MyExecutor.__init__()</code>.</p>
"
39978691,5021686.0,2016-10-11T13:42:43Z,39978529,1,"<p>Simple, you initialised b to [], so any attempt to access it by index will be out of range. Lists in Python can increase in size dynamically, but this isn't how you do it. You need .append</p>
"
39978829,6022341.0,2016-10-11T13:49:44Z,39978527,1,"<p>Function can be defined as a combination of <code>pyspark.sql.functions</code>:</p>

<ul>
<li><p>YES - go this way. For example:</p>

<pre><code>def sum_of_squares(col):
    return sum(col * col)

df.select(sum_of_squares(df[""foo""]])

df.groupBy(""foo"").agg(sum_of_squares(df[""bar""]])
</code></pre></li>
<li><p>NO - use RDD.</p></li>
</ul>
"
39978902,5406431.0,2016-10-11T13:53:22Z,39978375,0,"<p>it's easy to write a function in python.</p>

<p>first define a function is_palindromic,</p>

<pre><code>def is_palindromic(num):
    #some code here
    return True
</code></pre>

<p>then define a function is_prime,</p>

<pre><code>def is_prime(num):
    #some code here
    return True
</code></pre>

<p>at last, suppose you have a number 123321, you can call above function,</p>

<pre><code>num = 123321
if is_palindromic(num):
    if is_prime(num):
        print 'this number is a prime!'
</code></pre>

<p>ps, maybe you could try to use some editors like vscode or sublime.</p>
"
39979267,2999853.0,2016-10-11T14:12:28Z,39978529,2,"<p>I don't have any idea about your expected output. I just fixed the error. 
At first <code>append</code> an empty <code>list</code> to <code>b</code> then you can access <code>b[i]</code> and so on. </p>

<p>Code:</p>

<pre><code>a = [
    ['a', 'b'],
    ['c', 'd']
]
print(a)

b = []
for i in range(len(a)):
    b.append([]) #Append an empty list to `b`
    for j in range(len(a[i])):
        if a[0][0] == 'a':
            a[0][0] = 'b'
        else:
            b[i].append(a[i][j]) #No error now

print(b)
</code></pre>
"
39979664,5936628.0,2016-10-11T14:30:25Z,39979293,0,"<p>Your code is giving following error:  </p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""C:\temp\equation1.py"", line 37, in  f3 = x * y - beta * w  NameError: name 'w' is not defined</p>
</blockquote>

<p>Hence we pull symbol 'w' from  sympy symbols as below
<code>x, y, z, w = sp.symbols('x, y, z, w')</code> </p>

<p>You also mentioned you are trying to add  <code>z = w</code> , so once we add that to your code, it works.</p>

<p><strong>Working Code:</strong></p>

<pre><code>import sympy as sp
x, y, z, w = sp.symbols('x, y, z, w')
rho, sigma, beta = sp.symbols('rho, sigma, beta')
z = w
f1 = sigma * (y - x)
f2 = x * (rho - z) - y
f3 = x * y - beta * w
f4 = z - w
print sp.solvers.solve((f1, f2, f3, f4), (x, y, z, w))
</code></pre>

<p><strong>Output:</strong>  </p>

<pre><code>Python 2.7.9 (default, Dec 10 2014, 12:24:55) [MSC v.1500 32 bit (Intel)] on win32
Type ""copyright"", ""credits"" or ""license()"" for more information.
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
[(0, 0, 0), (-sqrt(beta*rho - beta), -sqrt(beta*(rho - 1)), rho - 1), (sqrt(beta*rho - beta), sqrt(beta*(rho - 1)), rho - 1)]
&gt;&gt;&gt; 
</code></pre>
"
39979814,6748546.0,2016-10-11T14:37:06Z,39978529,2,"<p>At <code>b[i][j]</code> you try to access the jth element of the ith element. However, b is just an empty list, so the ith element doesn't exist, let alone the jth element of that ith element. </p>

<p>You can fix this by appending the elements to b. But first, a list (row) should be created (appended to b) for each list (row) in a.</p>

<p>The following example makes use of the <code>enumerate</code> function. It returns the index and the element, so if you want you can still do stuff with i and j, but this will prevent you from gaining an <code>IndexError</code>. You can still safely use <code>a[i][j]</code>, because the indices are derived from <code>a</code>.</p>

<pre><code>a = [
    ['a', 'b'],   
    ['c', 'd']
]
b = []
for i, row in enumerate(a):             # loop over each list (row) in a
    b_row = []                          # list (row) which will be added to b
    for j, element in enumerate(row):   # loop over each element in each list (row) in a
        if a[0][0] == 'a':              # Not sure why each the first element of a has to be checked each loop    # Probably a simplification of the operations.
            a[0][0] = 'b'
        else:
            b_row.append(element)        # append the element to the list(row) that will be added to b
    b.append(b_row)                      # When the row is filled, add it to b

print(b)
</code></pre>

<p>One last comment: it is not recommended to change a list while looping over it in Python (like in the <code>if</code> statement). It is better to apply those changes to your output list, which is b. If it is jsut a simplification of the processessing, just ignore this remark.</p>
"
39979928,2901002.0,2016-10-11T14:42:01Z,39979889,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html"" rel=""nofollow""><code>stack</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a>:</p>

<pre><code>df = df.stack().reset_index()
df.columns = ['a','b','c']

print (df[df.c &gt; 30])
    a  b   c
0   a  a  40
3   a  d  35
6   b  c  35
7   b  d  45
9   c  b  35
10  c  c  45
11  c  d  55
12  d  a  40
13  d  b  45
14  d  c  55
15  d  d  65
</code></pre>

<p>Similar solution:</p>

<pre><code>s = df.stack()
df = s[s &gt; 30].reset_index()
df.columns = ['a','b','c']

print (df)
    a  b   c
0   a  a  40
1   a  d  35
2   b  c  35
3   b  d  45
4   c  b  35
5   c  c  45
6   c  d  55
7   d  a  40
8   d  b  45
9   d  c  55
10  d  d  65
</code></pre>

<p>Another solution:</p>

<pre><code>df1 = df[df &gt; 30].stack().reset_index()
df1.columns = ['a','b','c']
df1.c = df1.c.astype(int)
print (df1)
    a  b   c
0   a  a  40
1   a  d  35
2   b  c  35
3   b  d  45
4   c  b  35
5   c  c  45
6   c  d  55
7   d  a  40
8   d  b  45
9   d  c  55
10  d  d  65
</code></pre>

<hr>

<p>Last you can <code>apply</code> join:</p>

<pre><code>df['d'] = df.astype(str).apply(', '.join, axis=1)
print (df)
    a  b   c         d
0   a  a  40  a, a, 40
1   a  d  35  a, d, 35
2   b  c  35  b, c, 35
3   b  d  45  b, d, 45
4   c  b  35  c, b, 35
5   c  c  45  c, c, 45
6   c  d  55  c, d, 55
7   d  a  40  d, a, 40
8   d  b  45  d, b, 45
9   d  c  55  d, c, 55
10  d  d  65  d, d, 65

print (df.d.tolist())
['a, a, 40', 'a, d, 35', 'b, c, 35', 'b, d, 45', 'c, b, 35', 'c, c, 45', 
'c, d, 55', 'd, a, 40', 'd, b, 45', 'd, c, 55', 'd, d, 65']
</code></pre>
"
39979991,3940864.0,2016-10-11T14:45:07Z,39978375,0,"<p>It is about writing reusable code. I can suggest you to write reusable code in specific function in separate python file and import that file and function too.
For example you need function called sum in other function called ""bodmas"" then write function called sum in one python file say suppose ""allimports.py"":</p>

<pre><code>    def sum(a,b):
      return a+b
</code></pre>

<p>Now suppose your ""bodmas"" named function is some other python file then just import all the required functions and use is normally by calling it.</p>

<pre><code>    from allimports import sum
    def bodmas:
       print(sum(1,1))
</code></pre>

<p>One important thing is be specific while import your module as it will effect performance of your code when length of your code is long.
Suppose you want to use all functions then you can use two option like :</p>

<pre><code>    import allimports
    print(allimports.sum(1,1))
</code></pre>

<p>other option is </p>

<pre><code>    from allimports import *
    print(sum(1,1))
</code></pre>

<p>and for specific imports is as follows:</p>

<pre><code>    from allimports import sum
    print(sum(1,1))
</code></pre>
"
39979998,1832058.0,2016-10-11T14:45:18Z,39977069,1,"<p>The problem is because you use <code>re</code> and you have <code>+</code> in <code>encodec</code>. <code>re</code> treats <code>+</code> in special way so ie. <code>1+2</code> is searching <code>12 or 112 or 1112 etc.</code></p>

<p>Use <code>html_content.find(encoded)</code> which returns position of <code>encodec</code> in <code>html_content</code> or <code>-1</code></p>

<p>Now you will have to use <code>if matched != -1 or counter2 = 2</code> and <code>if matched == -1:</code></p>

<hr>

<p><strong>BTW:</strong> you have mess in code. It could look like this.</p>

<pre><code>from getpass import getpass
from Crypto.Cipher import AES
import base64
import urllib2
import time

# --- constants ---

SECRET_KEY = '1234567890123456'

# --- classes ---

    # empty

# --- functions ---

    # empty

# --- main ---

loggedin = False

# ------ input

print(""\nPlease Authenticate Yourself:"")
#print(""Welcome to Mantis\n"")
user = raw_input(""\nEnter Username:"")
password = getpass(""\nEnter Password:"")

print ""\n....................................................................""

# ------ encrypting

matchstring = ""###{}:::{}"".format(user, password)

cipher = AES.new(SECRET_KEY, AES.MODE_ECB)
encoded = base64.b64encode(cipher.encrypt(matchstring.rjust(32)))

print ""Encrypted Text: \n"", encoded

# ------ checking

# print matchstring #data sent for Authentication
if encoded == ""eiKUr3N8ZT7V7RZlwvnXW2F0F1I4xtktNZZSpNotDh0="":
    print ""\nHello Rishabh !! Is the Login Portal Locked ?""
    print ""\n\nAdministrator Access Granted""
    loggedin = True
else:        
    html = urllib2.urlopen('https://passarchive.blogspot.in').read()
    loggedin = (html.find(encoded) != 1) # True or False

# ------ info

if loggedin:
    user = user.upper()
    print 'Sucessfully Logged in\n'
    print 'Hello', user, ""!\n""

    if user != ""ADMINISTRATOR"":
        print ""Thanks in Advance for using Eagle, the Advanced Data Parsing Algorithm.""
        print ""\nCreator - Rishabh Raghunath, Electrical Engineering Student, MVIT\n""
        time.sleep(1)
        print ""Let's Start !\n""
        print "".....................................................................\n""
else:
   print '\nUserName or Password is Incorrect\n'
   print ""Please Check Your mail in case your Password has been Changed""
   print ""Log in failed.\n""
   time.sleep(5)

# ------ end
</code></pre>
"
39980175,4211135.0,2016-10-11T14:53:00Z,39979358,1,"<p>This is a relatively simple filtering operation. You state that you want to ""take only the columns"" that are the latest date, so I assume that an acceptable result will be a filter <code>DataFrame</code> with just the correct columns. </p>

<p>Here's a simple CSV that is similar to your structure:</p>

<pre><code>DATE,TRADE ID,AVAILABLE STOCK
10/11/2016,123,123
10/11/2016,123,123
10/10/2016,123,123
10/9/2016,123,123
10/11/2016,123,123
</code></pre>

<p>Note that I mixed up the dates a little bit, because it's hacky and error-prone to just assume that the latest dates will be on the top. The following script will filter it appropriately:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.read_csv('data.csv')

# convert the DATE column to datetimes
df['DATE'] = pd.to_datetime(df['DATE'])

# find the latest datetime
latest_date = df['DATE'].max()

# use index filtering to only choose the columns that equal the latest date
latest_rows = df[df['DATE'] == latest_date]
print (latest_rows)

# now you can perform your operations on latest_rows
</code></pre>

<p>In my example, this will print:</p>

<pre><code>        DATE  TRADE ID  AVAILABLE STOCK
0 2016-10-11       123              123
1 2016-10-11       123              123
4 2016-10-11       123              123
</code></pre>
"
39980548,1447525.0,2016-10-11T15:09:00Z,39980323,27,"<p>Below is answering the original first question:</p>

<blockquote>
  <p>Should I use <code>dict</code> or <code>OrderedDict</code> in Python 3.6?</p>
</blockquote>

<p>I think this sentence from the documentation is actually enough to answer your question</p>

<blockquote>
  <p>The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon</p>
</blockquote>

<p><code>dict</code> is not explicitly meant to be an ordered collection, so if you want to stay consistent and not rely on a side effect of the new implementation you should stick with <code>OrderedDict</code>.</p>

<p>Make your code future proof :)</p>

<p>There's a debate about that <a href=""https://news.ycombinator.com/item?id=12460936"">here</a>.</p>
"
39980744,4952130.0,2016-10-11T15:17:53Z,39980323,57,"<blockquote>
  <p>How does the Python 3.6 dictionary implementation perform better than the older one while preserving element order?</p>
</blockquote>

<p>Essentially by keeping two arrays, one holding the entries for the dictionary in the order that they were inserted and the other holding a list of indices.</p>

<p>In the previous implementation a sparse array of type <em>dictionary entries</em> had to be allocated; unfortunately, it also resulted in a lot of empty space since that array was not allowed to be more than <code>2/3</code>s full. This is not the case now since only the <em>required</em> entries are stored and a sparse array of type <em>integer</em> <code>2/3</code>s full is kept. </p>

<p>Obviously creating a sparse array of type ""dictionary entries"" is much more memory demanding than a sparse array for storing ints (<a href=""https://github.com/python/cpython/blob/master/Objects/dict-common.h#L55"">sized <code>8 bytes</code> tops</a> in cases of really large dictionaries) </p>

<hr>

<p><a href=""https://mail.python.org/pipermail/python-dev/2012-December/123028.html"">In the original proposal made by Raymond Hettinger</a>, a visualization of the data structures used can be seen which captures the gist of the idea.</p>

<blockquote>
  <p>For example, the dictionary:</p>

<pre><code>d = {'timmy': 'red', 'barry': 'green', 'guido': 'blue'}
</code></pre>
  
  <p>is currently stored as:</p>

<pre><code>entries = [['--', '--', '--'],
           [-8522787127447073495, 'barry', 'green'],
           ['--', '--', '--'],
           ['--', '--', '--'],
           ['--', '--', '--'],
           [-9092791511155847987, 'timmy', 'red'],
           ['--', '--', '--'],
           [-6480567542315338377, 'guido', 'blue']]
</code></pre>
  
  <p>Instead, the data should be organized as follows:</p>

<pre><code>indices =  [None, 1, None, None, None, 0, None, 2]
entries =  [[-9092791511155847987, 'timmy', 'red'],
            [-8522787127447073495, 'barry', 'green'],
            [-6480567542315338377, 'guido', 'blue']]
</code></pre>
</blockquote>

<p>As you can visually now see, in the original proposal, a lot of space is essentially empty to reduce collisions and make look-ups faster. With the new approach, you reduce the memory required by moving the sparseness where it's really required, in the indices.</p>

<hr>

<blockquote>
  <p>Should you depend on it and/or use it?</p>
</blockquote>

<p>As noted in the documentation, this is considered an implementation detail meaning it is subject to change and you shouldn't depend on it. </p>

<p>Different implementations of Python aren't required to make the dictionary ordered, rather, just support an ordered mapping where that is required (Notable examples are <em><a href=""https://docs.python.org/3.6/whatsnew/3.6.html#pep-520-preserving-class-attribute-definition-order"">PEP 520: Preserving Class Attribute Definition Order</a></em> and <em><a href=""https://docs.python.org/3.6/whatsnew/3.6.html#pep-468-preserving-keyword-argument-order"">PEP 468: Preserving Keyword Argument Order</a></em>)</p>

<p>If you want to write code that preserves the ordering and want it to not break on previous versions/different implementations you should always use <code>OrderedDict</code>. Besides, <code>OrderedDict</code> will most likely eventually become a thin-wrapper around the new <code>dict</code> implementation.</p>
"
39981171,3545273.0,2016-10-11T15:37:19Z,39978405,1,"<p>You have a major problem in your database definition, in the way you store values in it, or in the way you read values from it. I can only explain what you are seeing, but neither why nor how to fix it without:</p>

<ul>
<li>the type of the database</li>
<li>the way you input values in it</li>
<li>the way you extract values to obtain your <em>pseudo unicode</em> string</li>
<li>the actual content if you use direct (<em>native</em>) database access</li>
</ul>

<p>What you get is an ASCII string, where the 8 bits characters are grouped by pair to build 16 bit unicode characters in little endian order. As the expected string has an odd numbers of characters, the last character was (irremediably) lost in translation, because the original string ends with <code>u'\352d'</code> where 0x2d is ASCII code for <code>'-'</code> and 0x35 for <code>'5'</code>. Demo:</p>

<pre><code>def cvt(ustring):
    l = []
    for uc in ustring:
        l.append(chr(ord(uc) &amp; 0xFF)) # low order byte
        l.append(chr((ord(uc) &gt;&gt; 8) &amp; 0xFF)) # high order byte
    return ''.join(l)

cvt(my_string)
'WAGCenter-04190517953516060503-20160605124857-4190-5'
</code></pre>
"
39981337,1126841.0,2016-10-11T15:44:51Z,39981237,1,"<p>The string <code>'100'</code> is indeed less than the string <code>'24'</code>, because <code>'1'</code> is ""alphabetically"" smaller than <code>'2'</code>. You need to compare <em>numbers</em>.</p>

<pre><code>my_age = int(input())
if my_age &gt; 24:
</code></pre>
"
39981347,5747944.0,2016-10-11T15:45:38Z,39981237,5,"<p>You're testing a string value <code>myAge</code> against another string value <code>'24'</code>, as opposed to integer values.</p>

<pre><code>if myAge &gt; ('24'):
     print('You are old, ' + myName)
</code></pre>

<p>Should be</p>

<pre><code>if int(myAge) &gt; 24:
    print('You are old, {}'.format(myName))
</code></pre>

<p>In Python, you can greater-than / less-than against strings, but it doesn't work how you might think. So if you want to test the value of the integer representation of the string, use <code>int(the_string)</code></p>

<pre><code>&gt;&gt;&gt; ""2"" &gt; ""1""
True
&gt;&gt;&gt; ""02"" &gt; ""1""
False
&gt;&gt;&gt; int(""02"") &gt; int(""1"")
True
</code></pre>

<p>You may have also noticed that I changed <code>print('You are old, ' + myName)</code> to <code>print('You are old, {}'.format(myName))</code> -- You should become accustomed to this style of string formatting, as opposed to doing string concatenation with <code>+</code> -- You can read more about it in <a href=""https://docs.python.org/3.5/library/string.html#custom-string-formatting"">the docs.</a> But it really doesn't have anything to do with your core problem.</p>
"
39981354,6494274.0,2016-10-11T15:45:53Z,39981237,1,"<pre><code>print ('What is your name?')
myName = input ()
print ('Hello, ' + myName)
print ('How old are you?, ' + myName)
myAge = input ()
if int(myAge) &gt; 24:
     print('You are old, ' + myName)
else:
     print('You will be old before you know it.')
</code></pre>

<p>Just a small thing about your code. You should convert the input from <code>myAge</code> to an integer (<code>int</code>) <em>(number)</em> and then compare that number to the number 24.;</p>

<p>Also, you should usually not add strings together as it is consider <em>non-pythonic</em> and it slow. Try something like <code>print ('Hello, %s' % myName)</code> instead of <code>print ('Hello, ' + myName)</code>.  </p>

<p><a href=""https://www.tutorialspoint.com/python/python_strings.htm"" rel=""nofollow"">Python Strings Tutorial</a></p>
"
39981383,3652941.0,2016-10-11T15:47:36Z,39981237,0,"<p>Use <code>int(myAge)</code>. I always use <code>raw_input</code> and also, you dont have to print your questions. Instead put the question in with your raw_inputs like so:</p>

<pre><code>myName = raw_input(""Whats your name?"")
print ('Hello, ' + myName)
myAge = raw_input('How old are you?, ' + myName)
if int(myAge) &gt; ('24'):
    print('You are old, ' + myName)
else:
    print('You will be old before you know it.')
</code></pre>
"
39982817,498816.0,2016-10-11T17:07:30Z,39840638,1,"<p>I thin Mayavi uses <code>generators</code> to animate data. This is working for me:</p>

<pre><code>import time
import numpy as np
from mayavi import mlab

f = mlab.figure()
V = np.random.randn(20, 20, 20)
s = mlab.contour3d(V, contours=[0])

@mlab.animate(delay=10)
def anim():
    i = 0
    while i &lt; 5:
        time.sleep(1)
        s.mlab_source.set(scalars=np.random.randn(20, 20, 20))
        i += 1
        yield

anim()
</code></pre>

<p>I used this post as reference ( <a href=""http://stackoverflow.com/questions/14287185/animating-a-mayavi-points3d-plot"">Animating a mayavi points3d plot</a> )</p>
"
39984155,6342575.0,2016-10-11T18:26:06Z,39976348,1,"<p>The map function is your quickest method, the purpose of which is to avoid calling an entire dataframe to run some function repeatedly. This is what you ended up doing in your function i.e. calling an entire dataframe which is fine but not good doing it repeatedly. To tweak your code just a little will significantly speed up your process and only call the place_df dataframe once:</p>

<pre><code>person_df['weather'] = person_df['place_id'].map(dict(zip(place_df.place_id, place_df.weather)))
</code></pre>
"
39986401,2336654.0,2016-10-11T20:43:21Z,39885770,6,"<p><strong><em>setup</em></strong><br>
create 2 time series</p>

<pre><code>from StringIO import StringIO
import pandas as pd


txt1 = """"""2016-10-05 11:50:02.000734    0.50
2016-10-05 11:50:03.000033    0.25
2016-10-05 11:50:10.000479    0.50
2016-10-05 11:50:15.000234    0.25
2016-10-05 11:50:37.000199    0.50
2016-10-05 11:50:49.000401    0.50
2016-10-05 11:50:51.000362    0.25
2016-10-05 11:50:53.000424    0.75
2016-10-05 11:50:53.000982    0.25
2016-10-05 11:50:58.000606    0.75""""""

s1 = pd.read_csv(StringIO(txt1), sep='\s{2,}', engine='python',
                 parse_dates=[0], index_col=0, header=None,
                 squeeze=True).rename('s1').rename_axis(None)

txt2 = """"""2016-10-05 11:50:07.000537    0.50
2016-10-05 11:50:11.000994    0.50
2016-10-05 11:50:19.000181    0.50
2016-10-05 11:50:35.000578    0.50
2016-10-05 11:50:46.000761    0.50
2016-10-05 11:50:49.000295    0.75
2016-10-05 11:50:51.000835    0.75
2016-10-05 11:50:55.000792    0.25
2016-10-05 11:50:55.000904    0.75
2016-10-05 11:50:57.000444    0.75""""""

s2 = pd.read_csv(StringIO(txt2), sep='\s{2,}', engine='python',
                 parse_dates=[0], index_col=0, header=None,
                 squeeze=True).rename('s2').rename_axis(None)
</code></pre>

<hr>

<p><strong><em>TL;DR</em></strong></p>

<pre><code>df = pd.concat([s1, s2], axis=1).ffill().dropna()
overlap = df.index.to_series().diff().shift(-1) \
            .fillna(0).groupby(df.s1.eq(df.s2)).sum()
overlap.div(overlap.sum())

False    0.666657
True     0.333343
Name: duration, dtype: float64
</code></pre>

<hr>

<p><strong><em>explanation</em></strong></p>

<p><strong><em>build base <code>pd.DataFrame</code> <code>df</code></em></strong>  </p>

<ul>
<li>use <code>pd.concat</code> to align indexes</li>
<li>use <code>ffill</code> to let values propagate forward</li>
<li>use <code>dropna</code> to get rid of values of one series prior to the other starting</li>
</ul>

<hr>

<pre><code>df = pd.concat([s1, s2], axis=1).ffill().dropna()
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/Kkebu.png""><img src=""https://i.stack.imgur.com/Kkebu.png"" alt=""enter image description here""></a></p>

<p><strong><em>calculate <code>'duration'</code></em></strong><br>
from current time stamp to next</p>

<pre><code>df['duration'] = df.index.to_series().diff().shift(-1).fillna(0)
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/1ZkA5.png""><img src=""https://i.stack.imgur.com/1ZkA5.png"" alt=""enter image description here""></a></p>

<p><strong><em>calculate overlap</em></strong>  </p>

<ul>
<li><code>df.s1.eq(df.s2)</code> gives boolean series of when <code>s1</code> overlaps with <code>s2</code></li>
<li>use <code>groupby</code> above boolean series to aggregate total duration when <code>True</code> and <code>False</code></li>
</ul>

<hr>

<pre><code>overlap = df.groupby(df.s1.eq(df.s2)).duration.sum()
overlap

False   00:00:33.999548
True    00:00:17.000521
Name: duration, dtype: timedelta64[ns]
</code></pre>

<p><strong><em>percentage of time with same value</em></strong></p>

<pre><code>overlap.div(overlap.sum())

False    0.666657
True     0.333343
Name: duration, dtype: float64
</code></pre>
"
39986894,5741205.0,2016-10-11T21:17:59Z,39986786,3,"<p>Here is a small demonstration of how to use the <code>chunksize</code> parameter when calling <code>HDFStore.select()</code>:</p>

<pre><code>for chunk in store.select('df', columns=['column1', 'column2'],
                          where='column1==5', chunksize=10**6):
    # process `chunk` DF
</code></pre>
"
39986959,2336654.0,2016-10-11T21:22:15Z,39986925,5,"<p>the relevant parameter is <code>mangle_dupe_cols</code></p>

<p>from the <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow"">docs</a></p>

<blockquote>
<pre><code>mangle_dupe_cols : boolean, default True
    Duplicate columns will be specified as 'X.0'...'X.N', rather than 'X'...'X'
</code></pre>
</blockquote>

<p>by default, all of your <code>'a'</code> columns get named <code>'a.0'...'a.N'</code> as specified above.</p>

<p>if you used <code>mangle_dupe_cols=False</code>, importing this <code>csv</code> would produce an error.</p>

<p>you can get all of your columns with </p>

<pre><code>df.filter(like='a')
</code></pre>

<hr>

<p><strong><em>demonstration</em></strong></p>

<pre><code>from StringIO import StringIO
import pandas as pd

txt = """"""a, a, a, b, c, d
1, 2, 3, 4, 5, 6
7, 8, 9, 10, 11, 12""""""

df = pd.read_csv(StringIO(txt), skipinitialspace=True)
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/iQhUw.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/iQhUw.png"" alt=""enter image description here""></a></p>

<pre><code>df.filter(like='a')
</code></pre>

<p><a href=""https://i.stack.imgur.com/1jhmQ.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/1jhmQ.png"" alt=""enter image description here""></a></p>
"
39987094,2550354.0,2016-10-11T21:31:27Z,39971030,3,"<blockquote>
  <blockquote>
    <h1>I'M SORRY FOR THE DARK MAGIC</h1>
  </blockquote>
</blockquote>

<p>Using <a href=""http://clarete.li/forbiddenfruit/"" rel=""nofollow"">Forbiddenfruit</a> and python python's builtin <code>new</code> method I was able to do this:</p>

<pre><code>from forbiddenfruit import curse


class MyThing(int):
    def __new__(cls, *args, **kwargs):
        magic_slice = slice(args[0], args[1])
        curse(slice, 'otherstuff', args[2])  

        return magic_slice

thing = MyThing(1, 3, 'thing')
print 'hello world'[thing]
print thing.otherstuff
</code></pre>

<p>output:</p>

<pre><code>&gt;&gt;&gt; el
&gt;&gt;&gt; thing
</code></pre>

<p><em><strong>I wrote it as a challenge just because everybody said it is impossible, I would never use it on production code IT HAS SO MANY SIDE EFFECTS, you should think again on your structure and needs</strong></em></p>
"
39987117,2336654.0,2016-10-11T21:33:28Z,39987071,6,"<p><code>pd.DataFrame.plot()</code> returns the <code>ax</code> it is plotting to.  You can reuse this for other plots.</p>

<p>Try:</p>

<pre><code>ax = member_df.Age.plot(kind='kde')
member_df.Age.plot(kind='hist', bins=40, ax=ax)
ax.set_xlabel('Age')
</code></pre>

<p><strong><em>example</em></strong><br>
I plot <code>hist</code> first to put in background<br>
Also, I put <code>kde</code> on <code>secondary_y</code> axis  </p>

<pre><code>import pandas as pd
import numpy as np


np.random.seed([3,1415])
df = pd.DataFrame(np.random.randn(100, 2), columns=list('ab'))

ax = df.a.plot(kind='hist')
df.a.plot(kind='kde', ax=ax, secondary_y=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/3k5kQ.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/3k5kQ.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>response to comment</em></strong><br>
using <code>subplot2grid</code>.  just reuse <code>ax1</code></p>

<pre><code>import pandas as pd
import numpy as np

ax1 = plt.subplot2grid((2,3), (0,0))

np.random.seed([3,1415])
df = pd.DataFrame(np.random.randn(100, 2), columns=list('ab'))

df.a.plot(kind='hist', ax=ax1)
df.a.plot(kind='kde', ax=ax1, secondary_y=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/PPiRH.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/PPiRH.png"" alt=""enter image description here""></a></p>
"
39987735,5249307.0,2016-10-11T22:26:13Z,39987708,1,"<p>One approach is to use <a href=""https://docs.python.org/3.6/library/itertools.html#itertools.chain"" rel=""nofollow""><code>itertools.chain</code></a> to glue sublists together</p>

<pre><code>&gt;&gt;&gt; list(itertools.chain(*[[k]*v for k, v in d.items()]))
[1, 1, 1, 10, 10, 5, 5, 5, 5, 5, 5]
</code></pre>

<p>Or if you are dealing with a very large dictionary, then you could avoid constructing the sub lists with <a href=""https://docs.python.org/3.6/library/itertools.html#itertools.chain.from_iterable"" rel=""nofollow""><code>itertools.chain.from_iterable</code></a> and <a href=""https://docs.python.org/3.6/library/itertools.html#itertools.repeat"" rel=""nofollow""><code>itertools.repeat</code></a></p>

<pre><code>&gt;&gt;&gt; list(itertools.chain.from_iterable(itertools.repeat(k, v) for k, v in d.items()))
[1, 1, 1, 10, 10, 5, 5, 5, 5, 5, 5]
</code></pre>

<p>Comparative timings for a very large dictionary with using a list comprehension that uses two loops:</p>

<pre><code>&gt;&gt;&gt; d = {i: i for i in range(100)}
&gt;&gt;&gt; %timeit list(itertools.chain.from_iterable(itertools.repeat(k, v) for k, v in d.items()))
10000 loops, best of 3: 55.6 Âµs per loop
&gt;&gt;&gt; %timeit [k for k, v in d.items() for _ in range(v)]
10000 loops, best of 3: 119 Âµs per loop
</code></pre>

<p>It's not clear whether you want your output sorted (your example code does not sort it), but if so simply presort <code>d.items()</code></p>

<pre><code># same as previous examples, but we sort d.items()
list(itertools.chain(*[[k]*v for k, v in sorted(d.items())]))
</code></pre>
"
39987749,541038.0,2016-10-11T22:27:01Z,39987708,1,"<p><code>[k for k,v in d.items() for _ in range(v)]</code>
... I guess...</p>

<p>if you want it sorted you can do</p>

<p><code>[k for k,v in sorted(d.items()) for _ in range(v)]</code></p>
"
39987754,399680.0,2016-10-11T22:27:29Z,39987708,2,"<p>You can do it using a list comprehension:</p>

<pre><code>[i for i in d for j in range(d[i])]
</code></pre>

<p>yields:</p>

<pre><code>[1, 1, 1, 10, 10, 5, 5, 5, 5, 5, 5]
</code></pre>

<p>You can sort it again to get the list you were looking for.</p>
"
39987767,4785185.0,2016-10-11T22:29:04Z,39987596,1,"<p>Here's a general structure -- assuming that you can generate the dictionaries individually, using each before generating the next.  This sounds like what you might want.  calculate_similarity would be a function containing your ""I have a solution"" code above.</p>

<pre><code>reference = {'1':""U"", '2':""D"", '3':""D"", '4':""U"", '5':""U"",'6':""U""}
while True:
    on_the_spot = generate_dictionary()
    if on_the_spot is None:
        break
    calculate_similarity(reference, on_the_spot)
</code></pre>

<p>If you need to iterate through dictionaries already generated, then you have to have them in an iterable Python structure.  As you generate them, create a list of dictionaries:</p>

<pre><code>victim_list = [
    {'1':""U"", '2':""U"", '3':""D"", '4':""D"", '5':""U"",'6':""D""},
    {'1':""U"", '2':""U"", '3':""U"", '4':""D"", '5':""U"",'6':""D""},
    {'1':""D"", '2':""U"", '3':""U"", '4':""U"", '5':""D"",'6':""D""}
]
for on_the_spot in victim_list:
    # Proceed as above
</code></pre>

<p>Are you familiar with the Python construct <em>generator</em>?  It's like a function that returns its value with a <strong>yield</strong>, not a <strong>return</strong>.  If so, use that instead of the above list.</p>
"
39987842,955926.0,2016-10-11T22:36:26Z,39987596,0,"<p>If you stick your solution in a function, you can call it by name for any two dicts. Also, if you curry the function by breaking up the arguments across nested functions, you can partially apply the first dict to get back a function that just wants the second (or you could use <code>functools.partial</code>), which makes it easy to map:</p>

<pre><code>def similarity (a):
    def _ (b):
        sharedValue = set(a.items()) &amp; set(b.items())
        dictLength = len(a)
        scoreOfSimilarity = len(sharedValue)
        return scoreOfSimilarity/dictLength
    return _
</code></pre>

<p>Aside: the above can also be written as a single expression via nested lambdas:</p>

<pre><code>similarity = lambda a: lambda b: len(set(a.items()) &amp; set(b.items)) / len(a)
</code></pre>

<p>Now you can get the similarity between dictA and the remainder with a map:</p>

<pre><code>otherDicts = [dictB, dictC, dictD]
scores = map(similarity(dictA), otherdicts)
</code></pre>

<p>Now you can use <code>min()</code> (or <code>max()</code>, or whatever) to get the best from the scores list:</p>

<pre><code>winner = min(scores)
</code></pre>

<p>Warning: I have not tested any of the above.</p>
"
39987904,2336654.0,2016-10-11T22:44:04Z,39987860,1,"<p><strong><em>option 1</em></strong><br>
<code>apply</code> + <code>value_counts</code></p>

<pre><code>s = pd.Series([list('ABC'), list('AAA'), list('BA'), list('DD')], name='Things')

pd.concat([s, s.apply(lambda x: pd.Series(x).value_counts()).fillna(0)], axis=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/isZYF.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/isZYF.png"" alt=""enter image description here""></a></p>

<p><strong><em>option 2</em></strong><br>
use <code>pd.DataFrame(s.tolist())</code> + <code>stack</code> / <code>groupby</code> / <code>unstack</code></p>

<pre><code>pd.concat([s,
           pd.DataFrame(s.tolist()).stack() \
             .groupby(level=0).value_counts() \
             .unstack(fill_value=0)],
          axis=1)
</code></pre>
"
39987937,5014455.0,2016-10-11T22:47:09Z,39987877,5,"<p>It seems you are needlessly complicating things. Here is a very Pythonic approach:</p>

<pre><code>&gt;&gt;&gt; import collections
&gt;&gt;&gt; class OrderedCounter(collections.Counter, collections.OrderedDict):
...   pass
... 
&gt;&gt;&gt; lst = [""abc"", ""abc"", ""omg"", ""what"", ""abc"", ""omg""]
&gt;&gt;&gt; counts = OrderedCounter(lst)
&gt;&gt;&gt; counts
OrderedCounter({'abc': 3, 'omg': 2, 'what': 1})
&gt;&gt;&gt; [""{} {}"".format(v,k) if v &gt; 1 else k for k,v in counts.items()]
['3 abc', '2 omg', 'what']
&gt;&gt;&gt; 
</code></pre>
"
39987952,4785185.0,2016-10-11T22:48:10Z,39987877,1,"<p>You've properly used the Counter type to accumulate the needed values.  Now, it's just a matter of a more Pythonic way to generate the results.  Most of all, pull the initialization out of the loop, or you'll lose all but the last entry.</p>

<pre><code>list2 = []
for tpl in have:
    count = """" if tpl[1] == 0 else str(tpl[1])+"" ""
    list2.append(count + tpl[0])
</code></pre>

<p>Now, to throw all of that into a list comprehension:</p>

<pre><code>list2 = [ ("""" if tpl[1] == 0 else str(tpl[1])+"" "") + tpl[0] \
          for tpl in have]
</code></pre>
"
39988073,6095474.0,2016-10-11T23:01:50Z,39987877,1,"<p>Try this:</p>

<pre><code>lst = [""abc"", ""abc"", ""omg"", ""what"", ""abc"", ""omg""]
l = [lst.count(i) for i in lst] # Count number of duplicates
d = dict(zip(lst, l)) # Convert to dictionary
lst = [str(d[i])+' '+i if d[i]&gt;1 else i for i  in d] # Convert to list of strings
</code></pre>
"
39988409,2375593.0,2016-10-11T23:41:32Z,39987860,3,"<p>The most elegant is definitely the CountVectorizer from sklearn. </p>

<p>I'll show you how it works first, then I'll do everything in one line, so you can see how elegant it is. </p>

<h3>First, we'll do it step by step:</h3>

<p>let's create some data</p>

<pre><code>raw = ['ABC', 'AAA', 'BA', 'DD']

things = [list(s) for s in raw]
</code></pre>

<p>Then read in some packages and initialize count vectorizer</p>

<pre><code>from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)
</code></pre>

<p>Next we generate a matrix of counts</p>

<pre><code>matrix = cv.fit_transform(things)

names = [""count_""+n for n in cv.get_feature_names()]
</code></pre>

<p>And save as a data frame</p>

<pre><code>df = pd.DataFrame(data=matrix.toarray(), columns=names, index=raw)
</code></pre>

<p>Generating a data frame like this: </p>

<pre><code>    count_A count_B count_C count_D
ABC 1   1   1   0
AAA 3   0   0   0
BA  1   1   0   0
DD  0   0   0   2
</code></pre>

<h3>Elegant version:</h3>

<p>Everything above in one line</p>

<pre><code>df = pd.DataFrame(data=cv.fit_transform(things).toarray(), columns=[""count_""+n for n in cv.get_feature_names()], index=raw)
</code></pre>

<h3>Timing:</h3>

<p>You mentioned that you're working with a rather large dataset, so I used the %%timeit function to give a time estimate. </p>

<p>Previous response by @piRSquared (which otherwise looks very good!) </p>

<pre><code>pd.concat([s, s.apply(lambda x: pd.Series(x).value_counts()).fillna(0)], axis=1)
</code></pre>

<p><code>100 loops, best of 3: 3.27 ms per loop</code></p>

<p>My answer:</p>

<pre><code>pd.DataFrame(data=cv.fit_transform(things).toarray(), columns=[""count_""+n for n in cv.get_feature_names()], index=raw)
</code></pre>

<p><code>1000 loops, best of 3: 1.08 ms per loop</code></p>

<p>According to my testing, <em>CountVectorizer</em> is about 3x faster. </p>
"
39988445,2644759.0,2016-10-11T23:45:20Z,39988379,4,"<p>Yeah just use lambda:</p>

<pre><code>foo_list = []
foo_list.append(lambda: bar.func1(100))
foo_list.append(lambda: bar.func2([7,7,7,9]))
foo_list.append(lambda: bar.func3(r'C:\Users\user\desktop\output'))

for foo in foo_list:
    print(foo())
</code></pre>
"
39988461,5411494.0,2016-10-11T23:47:27Z,39987877,1,"<p>Another possible solution with comments to help...</p>

<pre><code>import operator

#list
lst = [""abc"", ""abc"", ""omg"", ""what"", ""abc"", ""omg""]

#dictionary
countDic = {}

#iterate lst to populate dictionary: {'what': 1, 'abc': 3, 'omg': 2}
for i in lst:
    if i in countDic:
        countDic[i] += 1
    else:
        countDic[i] = 1

#clean list
lst = []

#convert dictionary to an inverse list sorted by value: [('abc', 3), ('omg', 2), ('what', 1)]
sortedLst = sorted(countDic.items(), key=operator.itemgetter(0))

#iterate sorted list to populate list
for k in sortedLst:
    if k[1] != 1:
        lst.append(str(k[1]) + "" "" + k[0])
    else:
        lst.append(k[0])

#result
print lst
</code></pre>

<p>Output:</p>

<pre><code>['3 abc', '2 omg', 'what']
</code></pre>
"
39988971,2334254.0,2016-10-12T01:03:05Z,39988621,0,"<p>Ok so there is a few important things here, first we need to be able to manage our cwd, for that we will use the os module </p>

<pre><code>import os
</code></pre>

<p>whenever a method operates on a folder it is important to change directories into the folder and back to the parent folder. This can also be achieved with the os module.</p>

<pre><code>def operateOnFolder(folder):
     os.chdir(folder)
     ...
     os.chdir("".."")
</code></pre>

<p>Now we need to do some method for each directory, that comes with this,</p>

<pre><code>for k in os.listdir(""."") if os.path.isdir(k):
    operateOnFolder(k)
</code></pre>

<p>Finally in order to operate on some preexisting FORTRAN file we can use the builtin file operators.</p>

<pre><code>fileSource = open(""someFile.f"",""r"")
fileText = fileSource.read()
fileSource.close()
fileLines = fileText.split(""\n"")
# change a line in the file with -&gt; fileLines[42] = ""the 42nd line""
fileText = ""\n"".join(fileLines)
fileOutput = open(""someFile.f"",""w"")
fileOutput.write(fileText)
</code></pre>

<p>You can create and run your executable <code>output.fx</code> from <code>source.f90</code>::</p>

<pre><code>subprocess.call([""gfortran"",""-o"",""output.fx"",""source.f90""])#create
subprocess.call([""output.fx""])                             #execute
</code></pre>
"
39989023,6190361.0,2016-10-12T01:09:00Z,39988589,1,"<p>The best way to deal with this is by indexing into the rows using a Boolean series as you would in R.</p>

<p>Using your df as an example,</p>

<pre><code>In [5]: df.Col1 == ""what""
Out[5]:
0     True
1    False
2    False
3    False
4    False
5    False
6    False
Name: Col1, dtype: bool

In [6]: df[df.Col1 == ""what""]
Out[6]:
   Col1 Col2  Col3  Col4
0  what  the     0     0
</code></pre>

<p>Now we combine this with the pandas isin function.</p>

<pre><code>In [8]: df[df.Col1.isin([""men"",""rocks"",""mountains""])]
Out[8]:
        Col1      Col2  Col3  Col4
2        men        of     2    16
4      rocks      lips     4    32
6  mountains  history.     6    48
</code></pre>

<p>To filter on multiple columns we can chain them together with &amp; and | operators like so.</p>

<pre><code>In [10]: df[df.Col1.isin([""men"",""rocks"",""mountains""]) | df.Col2.isin([""lips"",""your""])]
Out[10]:
        Col1      Col2  Col3  Col4
2        men        of     2    16
3         to      your     3    24
4      rocks      lips     4    32
6  mountains  history.     6    48

In [11]: df[df.Col1.isin([""men"",""rocks"",""mountains""]) &amp; df.Col2.isin([""lips"",""your""])]
Out[11]:
    Col1  Col2  Col3  Col4
4  rocks  lips     4    32
</code></pre>
"
39990744,2336654.0,2016-10-12T04:51:56Z,39988903,2,"<p>you access it with the <code>name</code> attribute</p>

<pre><code>x.name

0
</code></pre>

<p>take these examples</p>

<pre><code>for i, row in df.iterrows():
    print row.name, i

0 0
1 1
2 2
</code></pre>

<p>Notice that the name attribute is the same as the variable <code>i</code> which is supposed to be the row index.</p>

<p>It's the same for columns</p>

<pre><code>for j, col in df.iteritems():
    print col.name, j

a a
b b
c c
</code></pre>
"
39990804,2901002.0,2016-10-12T04:58:09Z,39990287,2,"<p>You need <a href=""http://pandas.pydata.org/pandas-docs/stable/timeseries.html#custom-business-hour"" rel=""nofollow""><code>Custom Business Hour</code></a> with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.date_range.html"" rel=""nofollow""><code>date_range</code></a>:</p>

<pre><code>cbh = pd.offsets.CustomBusinessHour(start='06:00', 
                                    end='16:00', 
                                    weekmask='Mon Tue Wed Thu Fri Sat')
print (cbh)
&lt;CustomBusinessHour: CBH=06:00-16:00&gt;

day_range = pd.date_range(pd.datetime(2016,9,12,18),pd.datetime.now(),freq=cbh)
print (day_range)
DatetimeIndex(['2016-09-13 06:00:00', '2016-09-13 07:00:00',
               '2016-09-13 08:00:00', '2016-09-13 09:00:00',
               '2016-09-13 10:00:00', '2016-09-13 11:00:00',
               '2016-09-13 12:00:00', '2016-09-13 13:00:00',
               '2016-09-13 14:00:00', '2016-09-13 15:00:00',
               ...
               '2016-10-11 08:00:00', '2016-10-11 09:00:00',
               '2016-10-11 10:00:00', '2016-10-11 11:00:00',
               '2016-10-11 12:00:00', '2016-10-11 13:00:00',
               '2016-10-11 14:00:00', '2016-10-11 15:00:00',
               '2016-10-12 06:00:00', '2016-10-12 07:00:00'],
              dtype='datetime64[ns]', length=252, freq='CBH')
</code></pre>

<p>Test - it omit <code>Sunday</code>:</p>

<pre><code>day_range = pd.date_range(pd.datetime(2016,9,12,18),pd.datetime.now(),freq=cbh)[45:]
print (day_range)
DatetimeIndex(['2016-09-17 11:00:00', '2016-09-17 12:00:00',
               '2016-09-17 13:00:00', '2016-09-17 14:00:00',
               '2016-09-17 15:00:00', '2016-09-19 06:00:00',
               '2016-09-19 07:00:00', '2016-09-19 08:00:00',
               '2016-09-19 09:00:00', '2016-09-19 10:00:00',
               ...
               '2016-10-11 08:00:00', '2016-10-11 09:00:00',
               '2016-10-11 10:00:00', '2016-10-11 11:00:00',
               '2016-10-11 12:00:00', '2016-10-11 13:00:00',
               '2016-10-11 14:00:00', '2016-10-11 15:00:00',
               '2016-10-12 06:00:00', '2016-10-12 07:00:00'],
              dtype='datetime64[ns]', length=207, freq='CBH')
</code></pre>
"
39990811,1332509.0,2016-10-12T04:58:28Z,39989777,2,"<p>TADA!</p>

<pre><code>from itertools import groupby

def group_by_remove(permissions, id_key, groups_key, name_key=None):
    """"""
    @type permissions: C{list} of C{dict} of C{str} to C{object}
    @param id_key: A string that represents the name of the id key, like ""role_id"" or ""module_id""
    @param groups_key: A string that represents the name of the key of the groups like ""modules"" or ""permissions""
    @param name_key: A string that represents the name of the key of names like ""module_name"" (can also be None for no names' key) 
    """"""
    result = []
    permissions_key = lambda permission: permission[id_key]
    # Must sort for groupby to work properly
    sorted_permissions = sorted(permissions, key=permissions_key)
    for key, groups in groupby(sorted_permissions, permissions_key):
        key_result = {}
        groups = list(groups)
        key_result[id_key] = key
        if name_key is not None:
            key_result[name_key] = groups[0][name_key]
        key_result[groups_key] = [{k: v for k, v in group.iteritems() if k != id_key and (name_key is None or k != name_key)} for group in groups]
        result.append(key_result)
    return result

def change_format(initial):
    """"""
    @type initial: C{list}
    @rtype: C{dict} of C{str} to C{list} of C{dict} of C{str} to C{object}
    """"""
    roles_group = group_by_remove(initial, ""role_id"", ""modules"")[0]
    roles_group[""modules""] = group_by_remove(roles_group[""modules""], ""module_id"", ""permissions"", ""module_name"")
    return roles_group

change_format(role_permissions)
</code></pre>

<p>Enjoy :)</p>
"
39991082,6448351.0,2016-10-12T05:27:23Z,39990844,1,"<p>Use this link and Set the DJANGO_CONFIGURATION environment variable to the name of the class you just created, e.g. in bash:</p>

<p>export DJANGO_CONFIGURATION=Dev  </p>

<p><a href=""https://github.com/jazzband/django-configurations"" rel=""nofollow"">Read further here.</a></p>

<p><a href=""https://django-configurations.readthedocs.io/en/stable/"" rel=""nofollow"">And/or here.</a></p>
"
39991875,6842320.0,2016-10-12T06:30:07Z,39765738,0,"<p>Yes,before you fit ,you must need to convert your pandas dataframe into an numpy array,now its works fine...i think @aberger already answered .</p>

<p>thank you!</p>
"
39991951,622121.0,2016-10-12T06:35:32Z,39989777,1,"<p>PyFunctional is pretty good at list manipulation.</p>

<pre><code>from pprint import pprint
from functional import seq
input = [...]  # taken from your example
output =(
    seq(input)  # convert regular python list to Sequence object

    # group by role_id
    .map(lambda e: (e.pop('role_id'), e)).group_by_key()

    # start building role dict
    .map(lambda role_modules: {
        ""role_id"": role_modules[0],
        ""modules"": seq(role_modules[1])
                   # group by (module_id, module_name)
                   .map(lambda e: ( (e.pop('module_id'), e.pop('module_name')), e) ).group_by_key()

                   # start building module/permissions dict
                   .map(lambda module_permissions: {
                       ""module_id"": module_permissions[0][0],
                       ""module_name"": module_permissions[0][1],
                       ""permissions"": module_permissions[1]
                   })
                   # sort by module_id, convert Seq obj to regular list
                   .sorted(key=lambda m:m['module_id']).to_list()
    })
    # sort by role_id, convert Seq obj to regular list
    .sorted(key=lambda r:r['role_id']).to_list()
)

pprint(output)
</code></pre>

<p><strong>RESULT</strong></p>

<pre><code>[{'modules': [{'module_id': 1,
               'module_name': 'ModuleOne',
               'permissions': [{'can_create': True,
                                'can_delete': True,
                                'can_read': True,
                                'can_update': True,
                                'end_point_id': 1,
                                'end_point_name': 'entity'},
                               {'can_create': True,
                                'can_delete': True,
                                'can_read': True,
                                'can_update': True,
                                'end_point_id': 2,
                                'end_point_name': 'management-type'},
                               {'can_create': True,
                                'can_delete': False,
                                'can_read': True,
                                'can_update': True,
                                'end_point_id': 3,
                                'end_point_name': 'ownership-type'}]},
              {'module_id': 2,
               'module_name': 'ModuleTwo',
               'permissions': [{'can_create': True,
                                'can_delete': True,
                                'can_read': True,
                                'can_update': True,
                                'end_point_id': 4,
                                'end_point_name': 'financial-outlay'},
                               {'can_create': True,
                                'can_delete': True,
                                'can_read': True,
                                'can_update': True,
                                'end_point_id': 5,
                                'end_point_name': 'exposure'}]}],
  'role_id': 1}]
</code></pre>
"
39992585,3832970.0,2016-10-12T07:13:03Z,39990657,3,"<p>Note that by itself, a <code>r'(\w+)+'</code> pattern will not cause the <a href=""http://www.regular-expressions.info/catastrophic.html"" rel=""nofollow"">catastrophic backtracking</a>, it will only be ""evil"" inside a longer expression and especially when it is placed next to the start of the pattern since in case subsequent subpatterns fail the engine backtracks to this one, and as the 1+ quantifier inside is again quantified with <code>+</code>, that creates a huge amount of possible variations to try before failing. You may have a look at <a href=""https://regex101.com/r/6ZOhPi/1"" rel=""nofollow"">your regex demo</a> and click the <em>regex debugger</em> on the left to see example regex engine behavior.</p>

<p>The current regex can be written as </p>

<pre><code>r'^(\w+)\s\|{3}\s(\w+)\s\|{3}\s(.*)'
</code></pre>

<p>See the <a href=""https://regex101.com/r/8amfjD/2"" rel=""nofollow"">regex demo</a> where there will be a match if you remove space and <code>.</code> in the second field.</p>

<p><strong>Details</strong>:</p>

<ul>
<li><code>^</code> - start of string (not necessary with <code>re.match</code>)</li>
<li><code>(\w+)</code> - (Group 1) 1+ letters/digits/underscores</li>
<li><code>\s</code> - a whitespace</li>
<li><code>\|{3}</code> - 3 pipe symbols</li>
<li><code>\s(\w+)\s\|{3}\s</code> - see above (<code>(\w+)</code> creates Group 2)</li>
<li><code>(.*)</code> - (Group 3) any 0+ characters other than linebreak characters.</li>
</ul>
"
39992665,2901002.0,2016-10-12T07:18:00Z,39992653,5,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.zfill.html""><code>str.zfill</code></a>:</p>

<pre><code>print (df['month'].astype(str).str.zfill(2))
0    01
1    02
2    03
3    10
4    11
Name: month, dtype: object
</code></pre>
"
39992806,2336654.0,2016-10-12T07:27:10Z,39992653,2,"<p>I'd still pick @jezrael's answer over this, but I like this answer too</p>

<pre><code>df.month.apply('{:02d}'.format)

0    01
1    02
2    03
3    10
4    11
Name: month, dtype: object
</code></pre>
"
39993766,2901002.0,2016-10-12T08:20:09Z,39993744,2,"<p>Use slicing of <code>list</code>:</p>

<pre><code>df[df.columns[2:5]]
</code></pre>

<p>Sample:</p>

<pre><code>df = pd.DataFrame({'A':[1,2,3],
                   'B':[4,5,6],
                   'C':[7,8,9],
                   'D':[1,3,5],
                   'E':[5,3,6],
                   'F':[7,4,3]})

print (df)
   A  B  C  D  E  F
0  1  4  7  1  5  7
1  2  5  8  3  3  4
2  3  6  9  5  6  3

print (df.columns[2:5])
Index(['C', 'D', 'E'], dtype='object')

print (df[df.columns[2:5]])
   C  D  E
0  7  1  5
1  8  3  3
2  9  5  6
</code></pre>

<p>Another solution:</p>

<pre><code>col_list= list(df)
print (col_list[2:5])
['C', 'D', 'E']

print (df[col_list[2:5]])
   C  D  E
0  7  1  5
1  8  3  3
2  9  5  6
</code></pre>
"
39993904,1040597.0,2016-10-12T08:27:32Z,39993507,4,"<p>Your state depends on both <code>lst</code> and previous item you have picked. But you are only considering the <code>lst</code>. That is why you are getting incorrect results. To fix it you just have to add previous item to your dynamic state.</p>

<pre><code>def longest_subsequence(lst, prev=None, mem={}):
  if not lst:
    return []
  if (tuple(lst),prev) not in mem:
    if not prev or lst[0] &gt; prev:
      mem[(tuple(lst),prev)] = max([[lst[0]]+longest_subsequence(lst[1:], lst[0]), longest_subsequence(lst[1:], prev)], key=len)
    else:
     mem[(tuple(lst),prev)] = longest_subsequence(lst[1:], prev)

  return mem[(tuple(lst),prev)]

print longest_subsequence([3,5,6,2,5,4,19,5,6,7,12])
</code></pre>

<p>Note that using the <code>tuple(list)</code> as your dynamic state is not a very good idea. You can simply use the index of the item in the <code>list</code> that you are checking instead of the whole list:</p>

<pre><code>def longest_subsequence(lst, index=0, prev=None, mem={}):
  if index&gt;=len(lst):
    return []
  if (index,prev) not in mem:
    if not prev or lst[index] &gt; prev:
      mem[(index,prev)] = max([[lst[index]]+longest_subsequence(lst, index+1, lst[index]), longest_subsequence(lst, index+1, prev)], key=len)
    else:
      mem[(index,prev)] = longest_subsequence(lst,index+1, prev)

  return mem[(index,prev)]

print longest_subsequence([3,5,6,2,5,4,19,5,6,7,12])
</code></pre>

<p>For more efficient approaches you can check <a href=""http://stackoverflow.com/questions/2631726/how-to-determine-the-longest-increasing-subsequence-using-dynamic-programming"">this</a> question.</p>
"
39994355,5616110.0,2016-10-12T08:51:20Z,39913847,0,"<p>Freeze options:</p>

<ul>
<li><a href=""https://pypi.python.org/pypi/bbfreeze/1.1.3"" rel=""nofollow"">https://pypi.python.org/pypi/bbfreeze/1.1.3</a></li>
<li><a href=""http://cx-freeze.sourceforge.net/"" rel=""nofollow"">http://cx-freeze.sourceforge.net/</a></li>
</ul>

<p>However, your target server should have the environment you want -> you should be able to 'create' it. If it doesn't, you should build your software to match the environment. </p>

<p>I found this handy guide on how to install custom version of python to a virtualenv, assuming you have ssh access: <a href=""http://stackoverflow.com/a/5507373/5616110"">http://stackoverflow.com/a/5507373/5616110</a></p>

<p>In virtualenv, you should be able to pip install anything and you shouldn't need to worry about sudo privileges. Of course, having those and access to package manager like apt makes everything a lot easier.</p>
"
39995065,754456.0,2016-10-12T09:25:24Z,39971625,2,"<p>You can use <code>plt.text()</code> to annotate text onto an axis. By iterating over your x,y points you can place a label at each marker. For example: </p>

<pre><code>import matplotlib.pyplot as plt
x = [1, 2, 3, 4, 5]
y = [1, 1.5, 2, 2.5, 3]

fig, ax = plt.subplots(figsize=(15,10))
plt.plot(x, y, 'ko-')

plt.xlabel('X', fontsize=15)
plt.ylabel('Y', fontsize=15)
plt.xticks(x, fontsize=13)
plt.yticks(y, visible=False)

offset = 0.05
for xp, yp in zip(x,y):
    label = ""%s"" % yp
    plt.text(xp-offset, yp+offset, label, fontsize=12, horizontalalignment='right')

plt.margins(0.1, 0.1)
plt.title('Graph', color='black', fontsize=17) 
ax.axis('scaled') 
ax.grid()
plt.show()
</code></pre>

<p>Gives the following figure:</p>

<p><a href=""https://i.stack.imgur.com/dDP6C.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/dDP6C.png"" alt=""enter image description here""></a></p>
"
39996547,2141635.0,2016-10-12T10:39:59Z,39946092,1,"<p>If you want to be able to handle floats and negative numbers:</p>

<pre><code>def sum_numbers(s):
    sm = i = 0
    while i &lt; len(s):
        t = """"
        while  i &lt; len(s) and not s[i].isspace():
            t += s[i]
            i += 1
        if t:
            sm += float(t)
        else:
            i += 1
    return sm
</code></pre>

<p>Which will work for all cases:</p>

<pre><code>In [9]: sum_numbers('34 3 542 11')
Out[9]: 590.0

In [10]: sum_numbers('1.93 -1 23.12 11')
Out[10]: 35.05

In [11]: sum_numbers('')
Out[11]: 0

In [12]: sum_numbers('123456')
Out[12]: 123456.0
</code></pre>

<p>Or a variation taking slices:</p>

<pre><code>def sum_numbers(s):
    prev = sm = i = 0
    while i &lt; len(s):
        while i &lt; len(s) and not s[i].isspace():
            i += 1
        if i &gt; prev:
            sm += float(s[prev:i])
            prev = i
        i += 1
    return sm
</code></pre>

<p>You could also use <em>itertools.groupby</em> which uses no lists, using a set of allowed chars to group by:</p>

<pre><code>from itertools import groupby


def sum_numbers(s):
    allowed = set(""0123456789-."")
    return sum(float("""".join(v)) for k,v in groupby(s, key=allowed.__contains__) if k)
</code></pre>

<p>which gives you the same output:</p>

<pre><code>In [14]: sum_numbers('34 3 542 11')
Out[14]: 590.0

In [15]: sum_numbers('1.93 -1 23.12 11')
Out[15]: 35.05

In [16]: sum_numbers('')
Out[16]: 0

In [17]: sum_numbers('123456')
Out[17]: 123456.0
</code></pre>

<p>Which if you only have to consider positive ints could just use <em>str.isdigit</em> as the key:</p>

<pre><code>def sum_numbers(s):
    return sum(int("""".join(v)) for k,v in groupby(s, key=str.isdigit) if k)
</code></pre>
"
39996838,3627387.0,2016-10-12T10:55:47Z,39704298,0,"<p>Here <a href=""https://docs.djangoproject.com/en/1.10/_modules/django/#setup"" rel=""nofollow"">https://docs.djangoproject.com/en/1.10/_modules/django/#setup</a> we can see what <code>django.setup</code> actually does.  </p>

<blockquote>
  <p>Configure the settings (this happens as a side effect of accessing the
  first setting), configure logging and populate the app registry.
  Set the thread-local urlresolvers script prefix if <code>set_prefix</code> is True.</p>
</blockquote>

<p>So basically to ensure that setup was already done we can check if apps are ready and settings are configured</p>

<pre><code>from django.apps import apps
from django.conf impor settings

if not apps.ready and not settings.configured:
    django.setup()
</code></pre>
"
39996891,3125566.0,2016-10-12T10:58:40Z,39996814,6,"<p>No. <code>random.randint</code> is not a builtin function. You'll have to <em>import</em> the <code>random</code> module to use the function.</p>

<p>On another note, <code>i</code> was evidently not used in the loop, so you'll conventionally use the underscore <code>_</code> in place of <code>i</code></p>

<pre><code>import random


numbers = []
for _ in range(10):
    numbers.append(random.randint(a, b))
</code></pre>

<p>You'll also notice I have used a list to store all the values from each iteration. In that way, you don't throw away the values from previous iterations.</p>

<p>In case you're not already familiar with lists, you can check out the docs: </p>

<p><a href=""https://docs.python.org/2/tutorial/datastructures.html#data-structures"" rel=""nofollow"">Data structures</a></p>

<p><em>Lists:</em></p>

<blockquote>
  <p>The items of a list are arbitrary Python objects. Lists are formed by
  placing a comma-separated list of expressions in square brackets</p>
</blockquote>

<hr>

<p>On a final note, to print the items from your list, you can use the <a href=""https://docs.python.org/2/library/stdtypes.html#str.join"" rel=""nofollow""><code>str.join</code></a> method, but not after the items in your list have been converted from integers to strings:</p>

<pre><code>output = ', '.join([str(num) for num in numbers])
print(output)
</code></pre>
"
39997027,18601.0,2016-10-12T11:04:56Z,39996814,2,"<p>A couple of points worth mentioning with your original function:</p>

<ul>
<li>the <code>random</code> module is not built-in, you have to explicitly import it.</li>
<li><code>input</code> always returns strings, so you have to convert them to integers, before passing them to <code>random.randint</code></li>
<li><code>i</code> is indeed not use within your for loop. You may as well replace it with <code>_</code> (reinforcing the fact, that you loop because of side-effects, e.g. printing and not the variable itself).</li>
<li>More of a stylistic side-note regarding function names: PEP8 (Python style guide) encourages the use of lowercase in combination with underscore to separate words and not camel case (<code>random_number</code>vs <code>RandomNumber</code>)</li>
</ul>

<p>Here's a working example:</p>

<pre><code>import random 

def random_numbers():
    """"""
    Asking the user for a min and max, then printing ten random numbers between min and max.
    """"""
    print(""Give me two numbers, a min and a max"")
    a = int(input(""Select min. ""))
    b = int(input(""Select max. ""))

    numbers = [random.randint(a, b) for i in range(10)]
    print(','.join(str(n) for n in numbers))

random_numbers()
</code></pre>
"
39997213,2696355.0,2016-10-12T11:15:54Z,39996295,3,"<p>I don't think there is a ""better"" way to achieve what you want.</p>

<hr>

<p>If you really don't want to use the <code>_option_string_actions</code> attribute, you could process the <code>parser.format_usage()</code> to retrieve the options, but doing this, you will get only the short options names.</p>

<p>If you want both short and long options names, you could process the <code>parser.format_help()</code> instead.</p>

<p>This process can be done with a very simple regular expression: <code>-+\w+</code></p>

<pre><code>import re

OPTION_RE = re.compile(r""-+\w+"")
PARSER_HELP = """"""usage: test_args_2.py [-h] [--foo FOO] [--bar BAR]

optional arguments:
  -h, --help         show this help message and exit
  --foo FOO, -f FOO  a random options
  --bar BAR, -b BAR  a more random option
""""""

options = set(OPTION_RE.findall(PARSER_HELP))

print(options)
# set(['-f', '-b', '--bar', '-h', '--help', '--foo'])
</code></pre>

<hr>

<p>Or you could first make a dictionnary which contains the argument parser configuration and then build the argmuent parser from it. Such a dictionnary could have the option names as key and the option configuration as value. Doing this, you can access the options list via the dictionnary keys flattened with <a href=""https://docs.python.org/2/library/itertools.html?highlight=itertools#itertools.chain"" rel=""nofollow"">itertools.chain</a>:</p>

<pre><code>import argparse
import itertools

parser_config = {
    ('--foo', '-f'): {""help"": ""a random options"", ""type"": str},
    ('--bar', '-b'): {""help"": ""a more random option"", ""type"": int, ""default"": 0}
}

parser = argparse.ArgumentParser()
for option, config in parser_config.items():
    parser.add_argument(*option, **config)

print(parser.format_help())
# usage: test_args_2.py [-h] [--foo FOO] [--bar BAR]
# 
# optional arguments:
#   -h, --help         show this help message and exit
#   --foo FOO, -f FOO  a random options
#   --bar BAR, -b BAR  a more random option

print(list(itertools.chain(*parser_config.keys())))
# ['--foo', '-f', '--bar', '-b']
</code></pre>

<p>This last way is what I would do, if I was reluctant to use <code>_option_string_actions</code>.</p>
"
39997271,3026320.0,2016-10-12T11:19:22Z,39996295,0,"<p>I have to agree with Tryph's answer.</p>

<p>Not pretty, but you can retrieve them from <code>parser.format_help()</code>:</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--foo', '-f', type=str)
goal = parser._option_string_actions.keys()

def get_allowed_arguments(parser):
    lines = parser.format_help().split('\n')
    line_index = 0
    number_of_lines = len(lines)
    found_optional_arguments = False
    # skip the first lines until the section 'optional arguments'
    while line_index &lt; number_of_lines:
        if lines[line_index] == 'optional arguments:':
            found_optional_arguments = True
            line_index += 1
            break
        line_index += 1
    result_list = []
    if found_optional_arguments:
        while line_index &lt; number_of_lines:
            arg_list = get_arguments_from_line(lines[line_index])
            if len(arg_list) == 0:
                break
            result_list += arg_list
            line_index += 1
    return result_list

def get_arguments_from_line(line):
    if line[:2] != '  ':
        return []
    arg_list = []
    i = 2
    N = len(line)
    inside_arg = False
    arg_start = 2
    while i &lt; N:
        if line[i] == '-' and not inside_arg:
            arg_start = i
            inside_arg = True
        elif line[i] in [',',' '] and inside_arg:
            arg_list.append(line[arg_start:i+1])
            inside_arg = False
        i += 1
    return arg_list

answer = get_allowed_arguments(parser)
</code></pre>

<p>There's probably a regular expressions alternative to the above mess...</p>
"
39997371,2781698.0,2016-10-12T11:24:36Z,39997334,2,"<p><code>s == s.max()</code> will evaluate to a boolean (due to the <code>==</code> in between the variables). The next step is storing that value in <code>is_max</code>.</p>
"
39997375,1472064.0,2016-10-12T11:24:47Z,39997297,1,"<p>The sudo environment may not contain your <code>PYTHONPATH</code></p>

<p><code>/etc/sudoers</code> contains Defaults <code>env_reset</code>.
Simply add Defaults <code>env_keep += ""PYTHONPATH""</code> to <code>/etc/sudoers</code> and it will work just fine with <code>sudo</code>.</p>
"
39997405,2380830.0,2016-10-12T11:26:21Z,39997334,2,"<p>The code</p>

<p><code>is_max = s == s.max()</code></p>

<p>is evaluated as</p>

<p><code>is_max = (s == s.max())</code></p>

<p>The bit in parentheses is evaluated first, and that is either <code>True</code> or <code>False</code>. The result is assigned to <code>is_max</code>.</p>
"
39997436,2901002.0,2016-10-12T11:28:03Z,39997334,2,"<p>In pandas <code>s</code> is very often <code>Series</code> (column in <code>DataFrame</code>).</p>

<p>So you compare all values in <code>Series</code> with <code>max</code> value of <code>Series</code> and get boolean mask. Output is in <code>is_max</code>. And then set style <code>'background-color: yellow'</code> only to cell of table where is <code>True</code> value - where is max value.</p>

<p>Sample:</p>

<pre><code>s = pd.Series([1,2,3])
print (s)
0    1
1    2
2    3
dtype: int64

is_max = s == s.max()
print (is_max)
0    False
1    False
2     True
dtype: bool
</code></pre>
"
39997846,1076965.0,2016-10-12T11:48:33Z,39997334,0,"<blockquote>
  <p>is_max is EQUAL TO comparison of s and s_max</p>
</blockquote>
"
39998175,5399734.0,2016-10-12T12:04:58Z,39997334,0,"<p>According to the document, <a href=""https://docs.python.org/3/reference/expressions.html#evaluation-order"" rel=""nofollow"">Evaluation order</a>:</p>

<blockquote>
  <p>Notice that while evaluating an assignment, the right-hand side is
  evaluated before the left-hand side.</p>
</blockquote>

<p>This is quite reasonable, for you have to know the value of an expression before assigning it to a variable.</p>

<p>So Python first evaluates <code>s.max()</code>, followed by checking if the calculated value is equal to <code>s</code>, resulting in a boolean result, and then assign this boolean to a variable called <code>is_max</code>.</p>

<p>See also: <a href=""https://docs.python.org/3/reference/simple_stmts.html#assignment-statements"" rel=""nofollow"">Assignment statements</a></p>
"
39998522,6451573.0,2016-10-12T12:22:19Z,39998424,6,"<p><code>isfile</code> checks for <em>regular</em> file.</p>

<p>You could workaround it like this by checking if it exists but not a directory or a symlink:</p>

<pre><code>def deleteFile(filename):
    if os.path.exists(filename) and not os.path.isdir(filename) and not os.path.islink(filename):
        os.remove(filename)
</code></pre>
"
40000523,6736042.0,2016-10-12T13:55:55Z,39987596,0,"<p>Thanks to everyone for participation on the answer. Here is result that does what I need:</p>

<pre><code>def compareTwoDictionaries(self, absolute, reference, listOfDictionaries):
    #look only for absolute fit, yes or no
    if (absolute == True):
        similarity = reference == listOfDictionaries
    else:
        #return items that are the same between two dictionaries
        shared_items = set(reference.items()) &amp; set(listOfDictionaries.items())
        #return the length of the dictionary for further calculation of %
        dictLength = len(reference)
        #return the length of shared_items for further calculation of %
        scoreOfSimilarity = len(shared_items)
        #return final score: similarity
        similarity = scoreOfSimilarity/dictLength
    return similarity
</code></pre>

<p>Here is the call of the function</p>

<pre><code>for dict in victim_list:
                output = oandaConnectorCalls.compareTwoDictionaries(False, reference, dict)
</code></pre>

<p>""Reference"" dict and ""victim_list"" dict are used as described above.</p>
"
40000970,5626104.0,2016-10-12T14:16:08Z,39877725,3,"<p>If <code>track_1</code> and <code>track_2</code> are timestamps of beeps and both catch all of the beeps, then there's no need to build a waveform and do cross-correlation. Just find the average delay between the two arrays of beep timestamps:</p>

<pre><code>import numpy as np

frequency = 44100
num_samples = 10 * frequency
actual_time_delay = 0.020
timestamp_noise = 0.001

# Assumes track_1 and track_2 are timestamps of beeps, with a delay and noise added
track_1 = np.array([1,2,10000])
track_2 = np.array([1,2,10000]) + actual_time_delay*frequency + 
          frequency*np.random.uniform(-timestamp_noise,timestamp_noise,len(track_1))

calculated_time_delay = np.mean(track_2 - track_1) / frequency
print('Calculated time delay (s):',calculated_time_delay)
</code></pre>

<p>This yields approximately <code>0.020</code> s depending on the random errors introduced and is about as fast as it gets.</p>

<p><strong>EDIT:</strong> If extra or missing timestamps need to be handled, then the code could be modified as the following. Essentially, if the error on the timestamp randomness is bounded, then perform statistical ""mode"" function on the delays between all the values. Anything within the timestamp randomness bound is clustered together and identified, then the original identified delays are then averaged. </p>

<pre><code>import numpy as np
from scipy.stats import mode

frequency = 44100
num_samples = 10 * frequency
actual_time_delay = 0.020

# Assumes track_1 and track_2 are timestamps of beeps, with a delay, noise added,
#   and extra/missing beeps
timestamp_noise = 0.001
timestamp_noise_threshold = 0.002
track_1 = np.array([1,2,5,10000,100000,200000])
track_2 = np.array([1,2,44,10000,30000]) + actual_time_delay*frequency 
track_2 = track_2 + frequency*np.random.uniform(-timestamp_noise,timestamp_noise,len(track_2))

deltas = []
for t1 in track_1:
    for t2 in track_2:
        deltas.append( t2 - t1)
sorted_deltas = np.sort(deltas)/frequency
truncated_deltas = np.array(sorted_deltas/(timestamp_noise_threshold)+0.5, 
dtype=int)*timestamp_noise_threshold

truncated_time_delay = min(mode(truncated_deltas).mode,key=abs)
calculated_time_delay = np.mean(sorted_deltas[truncated_deltas == truncated_time_delay])

print('Calculated time delay (s):',calculated_time_delay)
</code></pre>

<p>Obviously, if too many beeps are missing or extra beeps exist then the code begins to fail at some point, but in general it should work well and be way faster than trying to generate a whole waveform and performing correlation. </p>
"
40001247,5315126.0,2016-10-12T14:29:31Z,40000718,0,"<p>The 1st question is easy to answer, you could use the <code>numpy.maximum()</code> function to find the element wise maximum value in each cell, across multiple dataframes</p>

<pre><code>dic ['four'] = pd.DataFrame(np.maximum(dic['one'].values,dic['two'].values,dic['three'].values),columns = list('ABC'))
</code></pre>
"
40001276,2336654.0,2016-10-12T14:30:24Z,40000718,3,"<p>consider the <code>dict</code> <code>dfs</code> which is a dictionary of <code>pd.DataFrame</code>s</p>

<pre><code>import pandas as pd
import numpy as np

np.random.seed([3,1415])
dfs = dict(
    one=pd.DataFrame(np.random.randint(1, 10, (5, 5))),
    two=pd.DataFrame(np.random.randint(1, 10, (5, 5))),
    three=pd.DataFrame(np.random.randint(1, 10, (5, 5))),
)
</code></pre>

<p>the best way to handle this is with a <code>pd.Panel</code> object, which is the higher dimensional object analogous to <code>pd.DataFrame</code>.</p>

<pre><code>p = pd.Panel(dfs)
</code></pre>

<p>then the answers you need are very straighforward</p>

<p><strong><em>max</em></strong><br>
<code>p.max(axis='items')</code> or <code>p.max(0)</code></p>

<p><strong><em>penultimate</em></strong><br>
<code>p.apply(lambda x: np.sort(x)[-2], axis=0)</code></p>
"
40001523,1527176.0,2016-10-12T14:41:21Z,39957761,4,"<p>According to the code (in Python 2.7), the <code>getElementsByName</code> method relays on the <code>_get_elements_by_tagName_helper</code> function, which code is:</p>

<pre><code>def _get_elements_by_tagName_helper(parent, name, rc):
    for node in parent.childNodes:
        if node.nodeType == Node.ELEMENT_NODE and \
            (name == ""*"" or node.tagName == name):
            rc.append(node)
        _get_elements_by_tagName_helper(node, name, rc)
    return rc
</code></pre>

<p>What this means is that the order in the <code>getElementByName</code> is the same that you have in the <code>childNodes</code>.</p>

<p>But this is true only if the <code>tagName</code> appears only in the same level. Notice the recursive call of <code>_get_elements_by_tagName_helper</code> inside the same function, which means that elements with the same <code>tagName</code> that are placed deeper in the tree will be interleaved with the ones you have in a higher level.</p>

<p>If by <em>document</em> you mean an XML text file or a string, the question is then moved to whether or not the parser respects the order when creating the elements in the DOM. 
If you use the <code>parse</code> function from the <code>xml.dom.minidom</code>, it relays on the <code>pyexpat</code> library, that in turns use the <code>expat</code> C library.</p>

<p>So, the short answer would be:</p>

<blockquote>
  <p>If you have the tagName only present in the same level of hierarchy in the XML DOM, then the order is respected. If you have the same tagName in other nodes deeper in the tree, those elements will be interleaved with the ones of higher level. The respected order is the order of the elements in the minidom document object, which order depends on the parser.</p>
</blockquote>

<p>Look this example:</p>

<pre><code>&gt;&gt;&gt; import StringIO
&gt;&gt;&gt; from xml.dom.minidom import parseString
&gt;&gt;&gt; s = '''&lt;head&gt;
...   &lt;tagName myatt=""1""/&gt;
...   &lt;tagName myatt=""2""/&gt;
...   &lt;tagName myatt=""3""/&gt;
...   &lt;otherTag&gt;
...     &lt;otherDeeperTag&gt;
...       &lt;tagName myatt=""3.1""/&gt;
...       &lt;tagName myatt=""3.2""/&gt;
...       &lt;tagName myatt=""3.3""/&gt;
...     &lt;/otherDeeperTag&gt;
...   &lt;/otherTag&gt; 
...   &lt;tagName myatt=""4""/&gt;
...   &lt;tagName myatt=""5""/&gt;
... &lt;/head&gt;'''
&gt;&gt;&gt; doc = parseString(s)
&gt;&gt;&gt; for e in doc.getElementsByTagName('tagName'):
...     print e.getAttribute('myatt')
... 
1
2
3
3.1
3.2
3.3
4
5
</code></pre>

<p>It seems the parser respects the ordering structure of the xml string (most parsers respect that order because it is easier to respect it) but I couldn't find any documentation that confirms it. I mean, it could be the (strange) case that the parser, depending on the size of the document, moves from using a list to a hash table to store the elements, and that could break the order. Take into account that the XML standard does not specify order of the elements, so a parser that does not respect order would be complaint too.</p>
"
40003530,748858.0,2016-10-12T16:18:18Z,40003067,7,"<p>To me, it looks like the memory savings come from the lack of a <code>__weakref__</code> on the instance.</p>

<p>So if we have:</p>

<pre><code>class Spam1(object):
    __slots__ = ('__dict__',)

class Spam2(object):
    __slots__ = ('__dict__', '__weakref__')

class Spam3(object):
    __slots__ = ('foo',)

class Eggs(object):
    pass

objs = Spam1(), Spam2(), Spam3(), Eggs()
for obj in objs:
    obj.foo = 'bar'

import sys
for obj in objs:
    print(type(obj).__name__, sys.getsizeof(obj))
</code></pre>

<p>The results (on python 3.5.2) are:</p>

<pre><code>Spam1 48
Spam2 56
Spam3 48
Eggs 56
</code></pre>

<p>We see that <code>Spam2</code> (which has a <code>__weakref__</code>) is the same size as <code>Eggs</code> (a traditional class).</p>

<p>Note that normally, this savings is going to be completely insignificant (and prevents you from using weak-references in your slots enabled classes).  Generally, savings from <code>__slots__</code> come from the fact that they don't create a <code>__dict__</code> in the first place.  Since <code>__dict__</code> are implemented using a somewhat sparse table (in order to help avoid hash collisions and maintain O(1) lookup/insert/delete), there's a fair amount of space that isn't used for each dictionary that your program creates.  If you add <code>'__dict__'</code> to your <code>__slots__</code> though, you miss out on this optimization (a dict is still created). </p>

<p>To explore this a little more, we can add more slots:</p>

<pre><code>class Spam3(object):
    __slots__ = ('foo', 'bar')
</code></pre>

<p>Now if we re-run, we see that it takes:</p>

<pre><code>Spam1 48
Spam2 56
Spam3 56
Eggs 56
</code></pre>

<p>So each slot takes 8 bytes on the <em>instance</em> (for me -- likely because 8 bytes is <code>sizeof(pointer)</code> on my system).  Also note that <code>__slots__</code> is implemented by making descriptors (which live on the <em>class</em>, not the instance).  So, the instance (even though you might find <code>__slots__</code> listed via <code>dir(instance)</code>) isn't actually carrying around a <code>__slots__</code> value) -- That's being carried around by the <em>class</em>.</p>

<p>This also has the consequence that your slots enabled class can't set ""default"" values... e.g. the following code doesn't work:</p>

<pre><code>class Foo(object):
    __slots__ = ('foo',)
    foo = 'bar'
</code></pre>

<p>So to boil it down:</p>

<ul>
<li>each ""slot"" on an instance takes up the size of a pointer on your system.</li>
<li>without <code>__slots__ = ('__dict__',)</code> a <code>__dict__</code> slot and a <code>__weakref__</code> slot is created on the instance</li>
<li>with <code>__slots__ = ('__dict__',)</code>, a <code>__dict__</code> slot is created but a <code>__weakref__</code> slot is not create on the instance.</li>
<li>In neither case is <code>__slots__</code> actually put on the <em>instance</em>.  It lives on the <em>class</em> (even though you might see it from <code>dir(instance)</code>).</li>
<li>The savings you reap from using <code>__slots__</code> in this way is likely to be insignificant.  Real savings from <code>__slots__</code> happen when you do not create a <code>dict</code> for the instance (since <code>dict</code> take up a more storage than the sum of the storage required for their contents due to somewhat sparse packing data in the data-structure).  On top of that, there are downsides to using slots this way (e.g. no weak-references to your instances).</li>
</ul>
"
40003588,2901002.0,2016-10-12T16:21:14Z,40003559,4,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow""><code>groupby</code></a> with aggregating <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.sum.html"" rel=""nofollow""><code>sum</code></a>:</p>

<pre><code>print (df.groupby(['A','B'], as_index=False)['count'].sum())
   A  B  count
0  a  1     11
1  a  2      1
2  c  3      7

print (df.groupby(['A','B'])['count'].sum().reset_index())
   A  B  count
0  a  1     11
1  a  2      1
2  c  3      7
</code></pre>
"
40003693,4879688.0,2016-10-12T16:26:55Z,39888949,-1,"<p>Method #1 is the fastest because it uses native (instead of Python) code to concatenate the files. However it is definitively cheating.</p>

<p>If you want to cheat, you may also consider also writing your own C extension for Python - it may be even faster, depending on your coding skills.</p>

<p>I am afraid Method #4 would not be working, since you are concatenating lists with string. I would go with <code>writer.writerow(line1 + line2)</code> instead. You may use <code>delimiter</code> parameter of both <code>csv.reader</code> and <code>csv.writer</code> to customise the delimiter (see <a href=""https://docs.python.org/2/library/csv.html"" rel=""nofollow"">https://docs.python.org/2/library/csv.html</a>).</p>
"
40004171,3919475.0,2016-10-12T16:53:15Z,39987596,1,"<p>Based on your problem setup, there appears to be no alternative to looping through the input list of dictionaries. However, there is a multiprocessing trick that can be applied here. </p>

<p>Here is your input:</p>

<pre><code>dict_a = {'1': ""U"", '2': ""D"", '3': ""D"", '4': ""U"", '5': ""U"", '6': ""U""}
dict_b = {'1': ""U"", '2': ""U"", '3': ""D"", '4': ""D"", '5': ""U"", '6': ""D""}
dict_c = {'1': ""U"", '2': ""U"", '3': ""U"", '4': ""D"", '5': ""U"", '6': ""D""}
dict_d = {'1': ""D"", '2': ""U"", '3': ""U"", '4': ""U"", '5': ""D"", '6': ""D""}
other_dicts = [dict_b, dict_c, dict_d]
</code></pre>

<p>I have included @gary_fixler's map technique as <code>similarity1</code>, in addition to the <code>similarity2</code> function that I will use for the loop technique.</p>

<pre><code>def similarity1(a):
    def _(b):
        shared_value = set(a.items()) &amp; set(b.items())
        dict_length = len(a)
        score_of_similarity = len(shared_value)
        return score_of_similarity / dict_length
    return _

def similarity2(c):
    a, b = c
    shared_value = set(a.items()) &amp; set(b.items())
    dict_length = len(a)
    score_of_similarity = len(shared_value)
    return score_of_similarity / dict_length
</code></pre>

<p>We are evaluating 3 techniques here: <br />
(1) @gary_fixler's map <br />
(2) simple loop through the list of dicts<br />
(3) multiprocessing the list of dicts</p>

<p>Here are the execution statements:</p>

<pre><code>print(list(map(similarity1(dict_a), other_dicts)))
print([similarity2((dict_a, dict_v)) for dict_v in other_dicts])

max_processes = int(multiprocessing.cpu_count()/2-1)
pool = multiprocessing.Pool(processes=max_processes)
print([x for x in pool.map(similarity2, zip(itertools.repeat(dict_a), other_dicts))])
</code></pre>

<p>You will find that all 3 techniques produce the same result:</p>

<pre><code>[0.5, 0.3333333333333333, 0.16666666666666666]
[0.5, 0.3333333333333333, 0.16666666666666666]
[0.5, 0.3333333333333333, 0.16666666666666666]
</code></pre>

<p>Note that, for multiprocessing, you have <code>multiprocessing.cpu_count()/2</code> cores (with each core having hyper-threading). Assuming that you have nothing else running on your system, and your program has no I/O or synchronization needs (as is the case for our problem), you will often get optimum performance with <code>multiprocessing.cpu_count()/2-1</code> processes, the <code>-1</code> being for the parent process.</p>

<p>Now, to time the 3 techniques:</p>

<pre><code>print(timeit.timeit(""list(map(similarity1(dict_a), other_dicts))"",
                    setup=""from __main__ import similarity1, dict_a, other_dicts"", 
                    number=10000))

print(timeit.timeit(""[similarity2((dict_a, dict_v)) for dict_v in other_dicts]"",
                    setup=""from __main__ import similarity2, dict_a, other_dicts"", 
                    number=10000))

print(timeit.timeit(""[x for x in pool.map(similarity2, zip(itertools.repeat(dict_a), other_dicts))]"",
                    setup=""from __main__ import similarity2, dict_a, other_dicts, pool"", 
                    number=10000))
</code></pre>

<p>This produces the following results on my laptop:</p>

<pre><code>0.07092539698351175
0.06757041101809591
1.6528456939850003
</code></pre>

<p>You can see that the basic loop technique performs the best. The multiprocessing was significantly worse than the other 2 techniques, because of the overhead of creating processes and passing data back and forth. This does not mean that multiprocessing is not useful here. Quite the contrary. Look at the results for a larger number of input dictionaries:</p>

<pre><code>for _ in range(7):
    other_dicts.extend(other_dicts)
</code></pre>

<p>This extends the dictionary list to 384 items. Here are the timing results for this input:</p>

<pre><code>7.934810006991029
8.184540337068029
7.466550623998046
</code></pre>

<p>For any larger set of input dictionaries, the multiprocessing technique becomes the most optimum. </p>
"
40004375,5741205.0,2016-10-12T17:03:49Z,39988589,1,"<p>If I understood your question correctly you can do it either using boolean indexing as <a href=""http://stackoverflow.com/a/39989023/5741205"">@uhjish has already shown in his answer</a> or using <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental"" rel=""nofollow"">query()</a> method:</p>

<pre><code>In [30]: search_list = ['rocks','mountains']

In [31]: df
Out[31]:
        Col1      Col2  Col3  Col4
0       what       the     0     0
1        are    curves     1     8
2        men        of     2    16
3         to      your     3    24
4      rocks      lips     4    32
5        and   rewrite     5    40
6  mountains  history.     6    48
</code></pre>

<p><code>.query()</code> method:</p>

<pre><code>In [32]: df.query('Col1 in @search_list and Col4 &gt; 40')
Out[32]:
        Col1      Col2  Col3  Col4
6  mountains  history.     6    48

In [33]: df.query('Col1 in @search_list')
Out[33]:
        Col1      Col2  Col3  Col4
4      rocks      lips     4    32
6  mountains  history.     6    48
</code></pre>

<p>using boolean indexing:</p>

<pre><code>In [34]: df.ix[df.Col1.isin(search_list) &amp; (df.Col4 &gt; 40)]
Out[34]:
        Col1      Col2  Col3  Col4
6  mountains  history.     6    48

In [35]: df.ix[df.Col1.isin(search_list)]
Out[35]:
        Col1      Col2  Col3  Col4
4      rocks      lips     4    32
6  mountains  history.     6    48
</code></pre>

<p><strong>UPDATE:</strong> using function:</p>

<pre><code>def find_queries(df, qry, debug=0, **parms):
    if debug:
        print('[DEBUG]: Query:\t' + qry.format(**parms))
    return df.query(qry.format(**parms))

In [31]: find_queries(df, 'Col1 in {Col1} and Col4 &gt; {Col4}', Col1='@search_list', Col4=40)
    ...:
Out[31]:
        Col1      Col2  Col3  Col4
6  mountains  history.     6    48

In [32]: find_queries(df, 'Col1 in {Col1} and Col4 &gt; {Col4}', Col1='@search_list', Col4=10)
Out[32]:
        Col1      Col2  Col3  Col4
4      rocks      lips     4    32
6  mountains  history.     6    48
</code></pre>

<p>including debugging info (print query):</p>

<pre><code>In [40]: find_queries(df, 'Col1 in {Col1} and Col4 &gt; {Col4}', Col1='@search_list', Col4=10, debug=1)
[DEBUG]: Query: Col1 in @search_list and Col4 &gt; 10
Out[40]:
        Col1      Col2  Col3  Col4
4      rocks      lips     4    32
6  mountains  history.     6    48
</code></pre>
"
40004924,6858122.0,2016-10-12T17:35:44Z,40004871,1,"<p>use this to iterate over any value <code>df.ix[row_value,col_value]</code> for finding the column index use this function</p>

<pre><code>def find_column_number(column_name):
    x=list(df1.columns.values)
    print column_name
    col_lenth= len(x)
    counter=0
    count=[]
    while counter&lt;col_lenth:
        if x[counter]==column_name:
            count=counter
        counter+=1
    return count
</code></pre>
"
40004947,5626112.0,2016-10-12T17:36:55Z,40004871,2,"<pre><code>for i, row in df.iterrows():
</code></pre>

<p>returns a <code>Series</code> for each row where the <code>Series</code> name is the <code>index</code> of the row you are iterating through. you could simply do</p>

<pre><code>d = { 'prod_code': ['040201060AAAIAI', '040201060AAAIAJ', '040201060AAAIAI', '040201060AAAIAI', '040201060AAAIAI', '040201060AAAIAI', '040301060AAAKAG', '040301060AAAKAK', '040301060AAAKAK', '040301060AAAKAX', '040301060AAAKAK', '040301060AAAKAK'], 'month': ['2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01', '2016-01-01', '2016-02-01', '2016-03-01'], 'cost': [43, 45, 46, 41, 48, 59, 8, 9, 10, 12, 15, 13] }
df = pd.DataFrame.from_dict(d)
df.set_index('prod_code', inplace=True)
for i, row in df.iterrows():
    print row.name, row['cost']

040201060AAAIAI 43
040201060AAAIAJ 45
040201060AAAIAI 46
040201060AAAIAI 41
040201060AAAIAI 48
040201060AAAIAI 59
040301060AAAKAG 8
040301060AAAKAK 9
040301060AAAKAK 10
040301060AAAKAX 12
040301060AAAKAK 15
040301060AAAKAK 13
</code></pre>

<p>you can learn more about it <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html#pandas-dataframe-iterrows"" rel=""nofollow"">here</a></p>
"
40005377,1425689.0,2016-10-12T18:01:14Z,40004517,0,"<blockquote>
  <p>What is the reason of having two ways of extending a bytearray?</p>
</blockquote>

<ul>
<li>An operator is not chainable like function calls, whereas a method is.</li>
<li>The <code>+=</code> operator cannot be used with nonlocal variables.</li>
<li>The <code>+=</code> is slightly faster</li>
<li><code>.extend()</code> may/might/could sometimes possibly be more readable</li>
</ul>

<blockquote>
  <p>Are they performing exactly the same task?</p>
</blockquote>

<p>Depends on how you look at it. The implementation is not the same, but the result usually is. For a bunch of examples and explanations, maybe try the SO search, and for example this question: <a href=""http://stackoverflow.com/questions/3653298/concatenating-two-lists-difference-between-and-extend"">Concatenating two lists - difference between &#39;+=&#39; and extend()</a></p>

<blockquote>
  <p>Which one should be used when?</p>
</blockquote>

<p>If you care about the small performance difference, use the operator when you can. Other than that, just use whichever you like to look at, of course given the restrictions I mentioned above.</p>

<blockquote>
  <p>But the main question is why += and .extend do not share the same internal function to do the actual work of extending a bytearray.</p>
</blockquote>

<p>Because one is faster but has limitations, so we need the other for the cases where we do have the limitations.</p>

<p><br>
<strong>Bonus</strong>:</p>

<p>The increment operator might cause some funny business with tuples:</p>

<blockquote>
  <p>if you put a list in a tuple and use the += operator to extend the list, the increment succeeds and you get a TypeError</p>
</blockquote>

<p>Source: <a href=""https://emptysqua.re/blog/python-increment-is-weird-part-ii/"" rel=""nofollow"">https://emptysqua.re/blog/python-increment-is-weird-part-ii/</a></p>
"
40005797,5066140.0,2016-10-12T18:25:12Z,39674713,1,"<p>Below is an example that sets up time series data to train an LSTM.  The model output is nonsense as I only set it up to demonstrate how to build the model.</p>

<pre><code>import pandas as pd
import numpy as np
# Get some time series data
df = pd.read_csv(""https://raw.githubusercontent.com/plotly/datasets/master/timeseries.csv"")
df.head()
</code></pre>

<p>Time series dataframe:</p>

<pre><code>Date      A       B       C      D      E      F      G
0   2008-03-18  24.68  164.93  114.73  26.27  19.21  28.87  63.44
1   2008-03-19  24.18  164.89  114.75  26.22  19.07  27.76  59.98
2   2008-03-20  23.99  164.63  115.04  25.78  19.01  27.04  59.61
3   2008-03-25  24.14  163.92  114.85  27.41  19.61  27.84  59.41
4   2008-03-26  24.44  163.45  114.84  26.86  19.53  28.02  60.09
</code></pre>

<p>You can build put inputs into a vector and then use pandas <code>.cumsum()</code> function to build the sequence for the time series:</p>

<pre><code># Put your inputs into a single list
df['single_input_vector'] = df[input_cols].apply(tuple, axis=1).apply(list)
# Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements
df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])
# Use .cumsum() to include previous row vectors in the current row list of vectors
df['cumulative_input_vectors'] = df.single_input_vector.cumsum()
</code></pre>

<p>The output can be set up in a similar way, but it will be a single vector instead of a sequence:</p>

<pre><code># If your output is multi-dimensional, you need to capture those dimensions in one object
# If your output is a single dimension, this step may be unnecessary
df['output_vector'] = df[output_cols].apply(tuple, axis=1).apply(list)
</code></pre>

<p>The input sequences have to be the same length to run them through the model, so you need to pad them to be the max length of your cumulative vectors:</p>

<pre><code># Pad your sequences so they are the same length
from keras.preprocessing.sequence import pad_sequences

max_sequence_length = df.cumulative_input_vectors.apply(len).max()
# Save it as a list   
padded_sequences = pad_sequences(df.cumulative_input_vectors.tolist(), max_sequence_length).tolist()
df['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)
</code></pre>

<p>Training data can be pulled from the dataframe and put into numpy arrays.  <strong>Note that the input data that comes out of the dataframe will not make a 3D array.  It makes an array of arrays, which is not the same thing.</strong></p>

<p>You can use hstack and reshape to build a 3D input array.</p>

<pre><code># Extract your training data
X_train_init = np.asarray(df.padded_input_vectors)
# Use hstack to and reshape to make the inputs a 3d vector
X_train = np.hstack(X_train_init).reshape(len(df),max_sequence_length,len(input_cols))
y_train = np.hstack(np.asarray(df.output_vector)).reshape(len(df),len(output_cols))
</code></pre>

<p>To prove it:</p>

<pre><code>&gt;&gt;&gt; print(X_train_init.shape)
(11,)
&gt;&gt;&gt; print(X_train.shape)
(11, 11, 6)
&gt;&gt;&gt; print(X_train == X_train_init)
False
</code></pre>

<p>Once you have training data you can define the dimensions of your input layer and output layers.</p>

<pre><code># Get your input dimensions
# Input length is the length for one input sequence (i.e. the number of rows for your sample)
# Input dim is the number of dimensions in one input vector (i.e. number of input columns)
input_length = X_train.shape[1]
input_dim = X_train.shape[2]
# Output dimensions is the shape of a single output vector
# In this case it's just 1, but it could be more
output_dim = len(y_train[0])
</code></pre>

<p>Build the model:</p>

<pre><code>from keras.models import Model, Sequential
from keras.layers import LSTM, Dense

# Build the model
model = Sequential()

# I arbitrarily picked the output dimensions as 4
model.add(LSTM(4, input_dim = input_dim, input_length = input_length))
# The max output value is &gt; 1 so relu is used as final activation.
model.add(Dense(output_dim, activation='relu'))

model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=['accuracy'])
</code></pre>

<p>Finally you can train the model and save the training log as history:</p>

<pre><code># Set batch_size to 7 to show that it doesn't have to be a factor or multiple of your sample size
history = model.fit(X_train, y_train,
              batch_size=7, nb_epoch=3,
              verbose = 1)
</code></pre>

<p>Output:</p>

<pre><code>Epoch 1/3
11/11 [==============================] - 0s - loss: 3498.5756 - acc: 0.0000e+00     
Epoch 2/3
11/11 [==============================] - 0s - loss: 3498.5755 - acc: 0.0000e+00     
Epoch 3/3
11/11 [==============================] - 0s - loss: 3498.5757 - acc: 0.0000e+00 
</code></pre>

<p>That's it.  Use <code>model.predict(X)</code> where <code>X</code> is the same format (other than the number of samples) as <code>X_train</code> in order to make predictions from the model.</p>
"
40007005,4724196.0,2016-10-12T19:40:41Z,40005701,2,"<p>You can get access to a <code>user</code> and <code>http_session</code> attributes of your <code>message</code> by changing the decorators in <code>consumers.py</code> to match the docs:</p>

<blockquote>
  <p>You get access to a userâs normal Django session using the <code>http_session decorator</code> - that gives you a <code>message.http_session</code> attribute that behaves just like <code>request.session</code>. You can go one further and use <code>http_session_user</code> which will provide a <code>message.user</code> attribute as well as the session attribute.  </p>
</blockquote>

<p>so the <a href=""https://github.com/jacobian/channels-example/blob/master/chat/consumers.py"" rel=""nofollow""><code>consumers.py</code></a> file in the example should become the following:  </p>

<pre><code>from channels.auth import http_session_user, channel_session_user, channel_session_user_from_http  

@channel_session_user_from_http  
def ws_connect(message):  
    ...  
    ...


@channel_session_user  
def ws_receive(message):  
    # You can check for the user attr like this  
    log.debug('%s', message.user)  
    ...  
    ...  


@channel_session_user  
def ws_disconnect(message):  
    ...  
    ...  
</code></pre>

<p><strong>Notice the decorators change</strong>.<br>
Also, i suggest you don't build anything upon that example</p>

<p>for more details see: <a href=""https://channels.readthedocs.io/en/stable/getting-started.html#authentication"" rel=""nofollow"">https://channels.readthedocs.io/en/stable/getting-started.html#authentication</a>  </p>
"
40007285,901925.0,2016-10-12T19:58:25Z,39996295,0,"<p>First a note on the <code>argparse</code> docs - it's basically a how-to-use document, not a formal API.  The standard for what <code>argparse</code> does is the code itself, the unit tests (<code>test/test_argparse.py</code>), and a paralyzing concern for backward compatibility.</p>

<p>There's no 'official' way of accessing <code>allowed arguments</code>, because users usually don't need to know that (other than reading the <code>help/usage</code>).</p>

<p>But let me illustrate with a simple parser in an iteractive session:</p>

<pre><code>In [247]: parser=argparse.ArgumentParser()
In [248]: a = parser.add_argument('pos')
In [249]: b = parser.add_argument('-f','--foo')
</code></pre>

<p><code>add_argument</code> returns the Action object that it created.  This isn't documented, but obvious to any one who has created a parser interactively.</p>

<p>The <code>parser</code> object has a <code>repr</code> method, that displays major parameters.  But it has many more attributes, which you can see with <code>vars(parser)</code>, or <code>parser.&lt;tab&gt;</code> in Ipython.</p>

<pre><code>In [250]: parser
Out[250]: ArgumentParser(prog='ipython3', usage=None, description=None, formatter_class=&lt;class 'argparse.HelpFormatter'&gt;, conflict_handler='error', add_help=True)
</code></pre>

<p>The Actions too have <code>repr</code>; the Action subclass is determined by the <code>action</code> parameter.</p>

<pre><code>In [251]: a
Out[251]: _StoreAction(option_strings=[], dest='pos', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)
In [252]: b
Out[252]: _StoreAction(option_strings=['-f', '--foo'], dest='foo', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)
</code></pre>

<p><code>vars(a)</code> etc can be used to see all attributes.</p>

<p>A key <code>parser</code> attribute is <code>_actions</code>, a list of all defined Actions.  This is the basis for all parsing.  Note it includes the <code>help</code> action that was created automatically.  Look at <code>option_strings</code>; that determines whether the Action is positional or optional.</p>

<pre><code>In [253]: parser._actions
Out[253]: 
[_HelpAction(option_strings=['-h', '--help'], dest='help', nargs=0, const=None, default='==SUPPRESS==', type=None, choices=None, help='show this help message and exit', metavar=None),
 _StoreAction(option_strings=[], dest='pos',....),
 _StoreAction(option_strings=['-f', '--foo'], dest='foo', ...)]
</code></pre>

<p><code>_option_string_actions</code> is a dictionary, mapping from <code>option_strings</code> to Actions (the same objects that appear in <code>_actions</code>).  References to those Action objects appear all over the place in <code>argparse</code> code.</p>

<pre><code>In [255]: parser._option_string_actions
Out[255]: 
{'--foo': _StoreAction(option_strings=['-f', '--foo'],....),
 '--help': _HelpAction(option_strings=['-h', '--help'],...),
 '-f': _StoreAction(option_strings=['-f', '--foo'], dest='foo',...),
 '-h': _HelpAction(option_strings=['-h', '--help'], ....)}

In [256]: list(parser._option_string_actions.keys())
Out[256]: ['-f', '--help', '-h', '--foo']
</code></pre>

<p>Note that there is a key for each <code>-</code> string, long or short; but there's nothing for <code>pos</code>, the positional has an empty <code>option_strings</code> parameter.</p>

<p>If that list of keys is what you want, use it, and don't worry about the <code>_</code>.  It does not have a 'public' alias.</p>

<p>I can understand parsing the <code>help</code> to discover the same; but that's a lot of work to just avoid using a 'private' attribute.  If you worry about the undocumented attribute being changed, you should also worry about the help format being changed.  That isn't part of the docs either.</p>

<p><code>help</code> layout is controlled by <code>parser.format_help</code>.  The <code>usage</code> is created from information in <code>self._actions</code>.  Help lines from information in</p>

<pre><code>    for action_group in self._action_groups:
        formatter.add_arguments(action_group._group_actions)
</code></pre>

<p>(you don't want to get into <code>action groups</code> do you?).</p>

<p>There is another way of getting the <code>option_strings</code> - collect them from the <code>_actions</code>:</p>

<pre><code>In [258]: [a.option_strings for a in parser._actions]
Out[258]: [['-h', '--help'], [], ['-f', '--foo']]
</code></pre>

<p>===================</p>

<p>Delving in to code details a bit:</p>

<p><code>parser.add_argument</code> creates an Action, and then passes it to <code>parser._add_action</code>.  This is the method the populates both <code>.actions</code> and  <code>action.option_strings</code>.</p>

<pre><code>    self._actions.append(action)
    for option_string in action.option_strings:
        self._option_string_actions[option_string] = action
</code></pre>
"
40008192,3063935.0,2016-10-12T20:54:53Z,39993507,0,"<p>So I had just discovered that Tempux's answer did not work for all cases.</p>

<p>I went back and through about encapsulating the entire state into the memoization dictionary  and thus added <code>tuple(lis)</code> as part of the key. Also, the <code>lst</code> index trick may not be as easy to implement since I am mutating <code>lst</code> through the recursion, hence why I am using <code>tuple()</code> as my keys.</p>

<p>The reasoning behind what I did is that multiple <code>lis</code> may have the same <code>[-1]</code> value. So, with this new state, the code is:</p>

<pre><code>def longest_subsequence(lst, lis=[],mem={}):
  if not lst:
    return lis
  if (tuple(lst),tuple(lis)) not in mem:
    if not lis or lst[0] &gt; lis[-1]:
      mem[(tuple(lst),tuple(lis))] = max([longest_subsequence(lst[1:], lis+[lst[0]]), longest_subsequence(lst[1:], lis)], key=len)
    else:
     mem[(tuple(lst),tuple(lis))] = longest_subsequence(lst[1:], lis)
  return mem[(tuple(lst),tuple(lis))]
</code></pre>

<p>This works for all cases I have tested so far.</p>
"
40008332,622121.0,2016-10-12T21:05:22Z,40008155,3,"<p>I think there are many corner cases but here is one of solutions.</p>

<pre><code>import inspect


class Parent(object):
     def method_of_parent(self):
         pass
class SubClass(Parent):
     def method_of_subclass(self):
         pass

def get_subclass_methods(child_cls):
    parents = inspect.getmro(child_cls)[1:]
    parents_methods = set()
    for parent in parents:
        members = inspect.getmembers(parent, predicate=inspect.ismethod)
        parents_methods.update(members)

    child_methods = set(inspect.getmembers(child_cls, predicate=inspect.ismethod))

    child_only_methods = child_methods - parents_methods
    return [m[0] for m in child_only_methods]

print(get_subclass_methods(SubClass))
</code></pre>

<p>Result</p>

<pre><code>['method_of_subclass']
</code></pre>
"
40008834,3293881.0,2016-10-12T21:40:55Z,40006169,3,"<p>Here's a vectorized approach using <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasted</code></a> summations -</p>

<pre><code># Gather the elements sorted by the keys in (row,col) order of a dense 
# 2D array for both nn and nnn
sidx0 = np.ravel_multi_index(np.array(nn.keys()).T,(4,4)).argsort()
a0 = np.array(nn.values())[sidx0].reshape(4,4)

sidx1 = np.ravel_multi_index(np.array(nnn.keys()).T,(4,4)).argsort()
a1 = np.array(nnn.values())[sidx1].reshape(4,4)

# Perform the summations keep the first axis aligned for nn and nnn parts
parte0 = a0[:,:,None,None,None] + a0[:,None,:,None,None] + \
     a0[:,None,None,:,None] + a0[:,None,None,None,:]

parte1 = a1[:,:,None,None,None] + a1[:,None,:,None,None] + \
     a1[:,None,None,:,None] + a1[:,None,None,None,:]    

# Finally add up sums from nn and nnn for final output    
out = parte0[...,None,None,None,None] + parte1[:,None,None,None,None]
</code></pre>

<p><strong>Runtime test</strong></p>

<p>Function defintions -</p>

<pre><code>def vectorized_approach(nn,nnn):
    sidx0 = np.ravel_multi_index(np.array(nn.keys()).T,(4,4)).argsort()
    a0 = np.array(nn.values())[sidx0].reshape(4,4)    
    sidx1 = np.ravel_multi_index(np.array(nnn.keys()).T,(4,4)).argsort()
    a1 = np.array(nnn.values())[sidx1].reshape(4,4)
    parte0 = a0[:,:,None,None,None] + a0[:,None,:,None,None] + \
         a0[:,None,None,:,None] + a0[:,None,None,None,:]    
    parte1 = a1[:,:,None,None,None] + a1[:,None,:,None,None] + \
         a1[:,None,None,:,None] + a1[:,None,None,None,:]
    return parte0[...,None,None,None,None] + parte1[:,None,None,None,None]

def original_approach(nn,nnn):
    params = np.zeros([4, 4, 4, 4, 4, 4, 4, 4, 4])
    for (i,j,k,l,m,jj,kk,ll,mm), val in np.ndenumerate(params):    
        params[i,j,k,l,m,jj,kk,ll,mm] = nn[(i,j)] + nn[(i,k)] + nn[(i,l)] + \
                                        nn[(i,m)] + nnn[(i,jj)] + \
                                        nnn[(i,kk)] + nnn[(i,ll)] + nnn[(i,mm)]
    return params
</code></pre>

<p>Setup inputs -</p>

<pre><code># Setup inputs
x = np.random.rand(30)
nn = {(0,0): x[0], (1,1): x[1], (2,2): x[2], (3,3): x[3], (0,1): x[4],
      (1,0): x[4], (0,2): x[5], (2,0): x[5], (0,3): x[6], (3,0): x[6],
      (1,2): x[7], (2,1): x[7], (1,3): x[8], (3,1): x[8], (2,3): x[9],
      (3,2): x[9]}

nnn = {(0,0): x[10], (1,1): x[11], (2,2): x[12], (3,3): x[13], (0,1): x[14],
       (1,0): x[14], (0,2): x[15], (2,0): x[15], (0,3): x[16], (3,0): x[16],
       (1,2): x[17], (2,1): x[17], (1,3): x[18], (3,1): x[18], (2,3): x[19],
       (3,2): x[19]}
</code></pre>

<p>Timings -</p>

<pre><code>In [98]: np.allclose(original_approach(nn,nnn),vectorized_approach(nn,nnn))
Out[98]: True

In [99]: %timeit original_approach(nn,nnn)
1 loops, best of 3: 884 ms per loop

In [100]: %timeit vectorized_approach(nn,nnn)
1000 loops, best of 3: 708 Âµs per loop
</code></pre>

<p>Welcome to <strong><code>1000x+</code></strong> speedup!</p>
"
40009138,636626.0,2016-10-12T22:04:21Z,40009015,3,"<p>It's an optimisation technique called interning. CPython recognises the <a href=""https://github.com/python/cpython/blob/c21f17c6e9d3218812b593f6ec5647e91c16896b/Objects/codeobject.c#L59"" rel=""nofollow"">equal values of <strong>string constants</strong></a> and doesn't allocate extra memory for new instances but simply points to the same one (interns it), giving both the same <code>id()</code>. </p>

<p>One can play around to confirm that only constants are treated this way (simple operations like <code>b</code> are recognised):</p>

<pre><code># Two string constants
a = ""aaaa""
b = ""aa"" + ""aa""

# Prevent interpreter from figuring out string constant
c = ""aaa""
c += ""a""

print id(a)         # 4509752320
print id(b)         # 4509752320
print id(c)         # 4509752176 !!
</code></pre>

<p>However you can manually force a string to be mapped to an already existing one using <a href=""https://docs.python.org/2/library/functions.html#intern"" rel=""nofollow""><code>intern()</code></a>:</p>

<pre><code>c = intern(c)

print id(a)         # 4509752320
print id(b)         # 4509752320
print id(c)         # 4509752320 !!
</code></pre>

<p>Other interpreters may do it differently. Since strings are immutable, changing one of the two will not change the other.</p>
"
40009292,2666182.0,2016-10-12T22:18:14Z,40009015,0,"<p>It doesn't store an array of all possible strings, instead it has a hash table which point to memory addresses of all currently declared strings, indexed by the hash of the string.</p>

<p>For example</p>

<p>when you say <code>a = 'foo'</code>, it first hashes the string <code>foo</code> and checks if an entry already exists in the hash table. If yes, then variable <code>a</code> now references that address. </p>

<p>If no entry is found in the table, python allocates memory to store the string, hashes <code>foo</code> and adds an entry in the table with the address of the allocated memory.</p>

<p>See:</p>

<ol>
<li><a href=""http://stackoverflow.com/questions/2987958/how-is-the-is-keyword-implemented-in-python"">How is the &#39;is&#39; keyword implemented in Python?</a></li>
<li><a href=""https://en.wikipedia.org/wiki/String_interning"" rel=""nofollow"">https://en.wikipedia.org/wiki/String_interning</a></li>
</ol>
"
40010101,5505161.0,2016-10-12T23:46:15Z,40010066,2,"<p>The reason you're not calculating the vowels is because the score variable is not getting incremented. To increment it, you have to set the variable score to previous score + 1.</p>

<p>This should work:</p>

<pre><code>for letter in name:
    if letter in vowel:
        score+=1
</code></pre>

<p>Edit: It's worth writing that score+=1 is the same as score=score+1</p>

<p>I worked out the error - instead of creating name = first, second, initialize name to first+second. You will get the results you want. The reason it was failing is because name=first, second creates a tuple, and iterating through the tuple makes letter = ""Kate"", ""John"" etc, and not the actual individual characters.</p>
"
40010243,6950186.0,2016-10-13T00:04:23Z,40010175,6,"<pre><code>&gt;&gt;&gt; d = {'a': [[1, 2, 3], [1, 2, 3]], 'b': [[2, 4, 1], [1, 6, 1]]}
&gt;&gt;&gt; {k: map(sum, zip(*v)) for k, v in d.items()}
{'a': [2, 4, 6], 'b': [3, 10, 2]}
</code></pre>
"
40010268,5014455.0,2016-10-13T00:06:46Z,40010175,4,"<p>The following will work on Python 2 or 3:</p>

<pre><code>&gt;&gt;&gt; {k: [a + b for a, b in zip(*v)] for k, v in d.items()}
{'a': [2, 4, 6], 'b': [3, 10, 2]}
</code></pre>

<p>The issue with your code is you are mapping <code>add_element</code> to every individual element in <code>v</code> inside your dictionary comprehension. This passes a one-dimensional list to <code>zip</code> in <code>add_element</code>, resulting in your error (since individual integers don't support iteration.</p>
"
40010367,1219006.0,2016-10-13T00:18:42Z,40010175,4,"<p>To fix your original code, the only change you need to make is:</p>

<pre><code>return {k: list(map(add_element, v)) for k, v in dicty.items()}
</code></pre>

<p>-></p>

<pre><code>return {k: add_element(v) for k, v in dicty.items()}
</code></pre>

<p>Because <code>zip(*lst)</code> is trying to transpose multiple rows into columns, but you are only passing it single rows through your original <code>map</code></p>
"
40010535,1219006.0,2016-10-13T00:40:29Z,40010066,1,"<pre><code>a = [""John"", ""Kate"", ""Oli""]
b = [""Green"", ""Fletcher"", ""Nelson""]
vowel = {""a"", ""e"", ""i"", ""o"", ""u""}
names = (first + ' ' + last for first in a for last in b)

for name in names:
    score = len(name) + sum(c in vowel for c in name.lower())
    print ""Full Name: {name} Score: {score}"".format(name=name, score=score)
</code></pre>

<hr>

<pre><code>Full Name: John Green Score: 13
Full Name: John Fletcher Score: 16
Full Name: John Nelson Score: 14
Full Name: Kate Green Score: 14
Full Name: Kate Fletcher Score: 17
Full Name: Kate Nelson Score: 15
Full Name: Oli Green Score: 13
Full Name: Oli Fletcher Score: 16
Full Name: Oli Nelson Score: 14
</code></pre>
"
40012160,5349916.0,2016-10-13T04:11:16Z,40012062,5,"<p>The <code>\a</code> does not capitalize a letter - it is the <a href=""https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals"" rel=""nofollow"">bell escape sequence</a>.</p>

<p>The <a href=""https://docs.python.org/3/library/stdtypes.html#textseq"" rel=""nofollow""><code>str.title</code></a> simply capitalizes the first letter of any group of letters. Since the bell is not a letter, it has the same meaning as a space. The following produces equivalent capitalization:</p>

<pre><code>name = ""de anna""
print(name.title())
</code></pre>

<p>Anyways, there are no capitalize/uncapitalize magic characters in python. Simply write the name properly. If you want both a proper and a lower-case version, create the <em>later</em> via <code>str.lower</code>:</p>

<pre><code>name = ""George von Trapp""
print(name, ':', name.lower())
</code></pre>

<hr>

<p>If you <strong>really</strong> want to go from <code>""georg van trapp""</code> (I'm just pretending the discussion about <code>\a</code> is over) to <code>""Georg van Trapp""</code> - welcome to having-to-decide-about-the-semantics-of-the-language-you-are-emulating.</p>

<ul>
<li><p>A simple approach is to upper-case every word, but fix some known ones.</p>

<pre><code>name = ""georg van trapp""
proper_name = name.title()
proper_name.replace(' Von ', ' von ').replace(' Zu ', ' zu ').replace(' De ', ' de ')
print(name, ':', proper_name)
</code></pre></li>
<li><p>You can do that with a <code>list</code>-and-loop approach for less headache as well:</p>

<pre><code>lc_words = ['von', 'van', 'zu', 'von und zu', 'de', ""d'"", 'av', 'af', 'der', 'Teer', ""'t"", ""'n"", ""'s""]
name = ""georg van trapp""
proper_name = name.title()
for word in lc_words:
    proper_name = proper_name.replace(' %s ' % word.title(), ' %s ' % word)
print(name, ':', proper_name)
</code></pre></li>
<li><p>If names are of the form <code>First Second byword Last</code>, you can capitalize everything but the second-to-last word:</p>

<pre><code>name = ""georg fritz ferdinand hannibal van trapp""
proper_name = name.title().split()  # gets you the *individual* words, capitalized
proper_name = ' '.join(proper_name[:-2] + [proper_name[-2].lower(), proper_name[-1]])
print(name, ':', proper_name)
</code></pre></li>
<li><p>Any words that are shorter than four letters (warning, not feasible for some names!!!)</p>

<pre><code>name = ""georg fritz theodores ferdinand markus hannibal von und zu trapp""
proper_name = ' '.join(word.title() if len(word) &gt; 3 else word.lower() for word in name.split())
print(name, ':', proper_name)
</code></pre></li>
</ul>
"
40012240,5399734.0,2016-10-13T04:21:55Z,40012062,0,"<blockquote>
  <p>What do you code to produce a name like <code>""George von Trapp""</code> where I
  want to uncapitalize a normally capitalized letter?</p>
</blockquote>

<p>Letters are not auto-capitalized in Python. In your case, <code>""de\aanna""</code>(I think you should use <code>""de anna""</code> instead) is capitalised because you called <code>title()</code> on it. If I didn't misunderstand your question, what you want is simply to disable such ""auto-capitalizing"".</p>

<p>Just don't call <code>title()</code>:</p>

<pre><code>name = ""George von Trapp""
print(name.lower())
</code></pre>
"
40012496,6288435.0,2016-10-13T04:49:09Z,40012062,1,"<p>Why not just roll your own function for it?</p>

<pre><code> def capitalizeName(name):
     #split the name on spaces
     parts = name.split("" "")

     # define a list of words to not capitalize
     do_not_cap = ['von']

     # for each part of the name,
     # force the word to lowercase
     # then check if it is a word in our forbidden list
     # if it is not, then go ahead and capitalize it
     # this will leave words in that list in their uncapitalized state
     for i,p in enumerate(parts):
          parts[i] = p.lower()
          if p.lower() not in do_not_cap:
              parts[i] = p.title()

      # rejoin the parts of the word
      return "" "".join(parts)
</code></pre>

<p>The point to the <code>do_not_cap</code> list is that allows you to further define parts you may not want to capitalize very easily.  For example, some names may have a ""de"" in it you may not want capitalized.</p>

<p>This is what it looks like with an example:</p>

<pre><code>name = ""geOrge Von Trapp""
capitalizeName(name)
# ""George von Trapp""
</code></pre>
"
40013319,6052183.0,2016-10-13T05:57:22Z,40012745,1,"<p>Since you run this on 2 different instances it might be best to store the values in some sort of config/json file that will be cleared each time you run ""configure"".</p>

<pre><code>import json

def configure(config_file):
   print config_file[opt_name] # do something with options in file

   # clear config file
   with open(""CONFIG_FILE.JSON"", ""wb"") as f: config = json.dump([], f)

def main():
    # load config file
    with open(""CONFIG_FILE.JSON"", ""rb"") as f: config = json.load(f)

    # use the configure opt only when called and supply the config json to it
    if sys.argv[0] == ['configure']:
       configure(config)
       return

    # parse options example (a bit raw, and should be done in different method anyway)
    parser = OptionParser()
    parser.add_option(""-q"", action=""store_false"", dest=""verbose"")
    config_file[""q""] = OPTION_VALUE
</code></pre>
"
40014466,6451573.0,2016-10-13T07:09:07Z,40014390,2,"<p>Just create another instance</p>

<pre><code>var_1 = type(var_2)()
</code></pre>

<p>Note that if you're not sure whether the object has a non-default constructor, you cannot rely on the above, but you can use <code>copy</code> or <code>deepcopy</code> (you get a ""non-empty"" object.</p>

<pre><code>import copy
var_1 = copy.copy(var_2)  # or copy.deepcopy
</code></pre>

<p>You could use both combined with the latter as a fallback mechanism</p>

<p>Note: <code>deepcopy</code> will ensure that your second object is completely independent from the first (If there are lists of lists, for instance)</p>
"
40014469,6748546.0,2016-10-13T07:09:24Z,40014390,2,"<pre><code>a = 1              # a is an int
a_type = type(a)   # a_type now contains the int-type
b = '1'            # '1' is a string
c = a_type(b)      # c is now an int with the value 1
</code></pre>

<p>So you can get the type of a variable using <code>type()</code>. You can then store this type in a variable and you can then use that variable just like you would use <code>int(b)</code>, <code>str(b)</code>, <code>float(b)</code> etc. </p>
"
40014532,5159284.0,2016-10-13T07:12:48Z,39763091,0,"<p>I was going through spacy library more, and I finally figured out the solution through dependency management. Thanks to <a href=""https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py"" rel=""nofollow"">this</a> repo, I figured out how to include adjectives as well in my subjective verb object (making it SVAO's), as well as taking out compound subjects in the query. Here goes my solution:</p>

<pre><code>from nltk.stem.wordnet import WordNetLemmatizer
from spacy.en import English

SUBJECTS = [""nsubj"", ""nsubjpass"", ""csubj"", ""csubjpass"", ""agent"", ""expl""]
OBJECTS = [""dobj"", ""dative"", ""attr"", ""oprd""]
ADJECTIVES = [""acomp"", ""advcl"", ""advmod"", ""amod"", ""appos"", ""nn"", ""nmod"", ""ccomp"", ""complm"",
              ""hmod"", ""infmod"", ""xcomp"", ""rcmod"", ""poss"","" possessive""]
COMPOUNDS = [""compound""]
PREPOSITIONS = [""prep""]

def getSubsFromConjunctions(subs):
    moreSubs = []
    for sub in subs:
        # rights is a generator
        rights = list(sub.rights)
        rightDeps = {tok.lower_ for tok in rights}
        if ""and"" in rightDeps:
            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == ""NOUN""])
            if len(moreSubs) &gt; 0:
                moreSubs.extend(getSubsFromConjunctions(moreSubs))
    return moreSubs

def getObjsFromConjunctions(objs):
    moreObjs = []
    for obj in objs:
        # rights is a generator
        rights = list(obj.rights)
        rightDeps = {tok.lower_ for tok in rights}
        if ""and"" in rightDeps:
            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == ""NOUN""])
            if len(moreObjs) &gt; 0:
                moreObjs.extend(getObjsFromConjunctions(moreObjs))
    return moreObjs

def getVerbsFromConjunctions(verbs):
    moreVerbs = []
    for verb in verbs:
        rightDeps = {tok.lower_ for tok in verb.rights}
        if ""and"" in rightDeps:
            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == ""VERB""])
            if len(moreVerbs) &gt; 0:
                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))
    return moreVerbs

def findSubs(tok):
    head = tok.head
    while head.pos_ != ""VERB"" and head.pos_ != ""NOUN"" and head.head != head:
        head = head.head
    if head.pos_ == ""VERB"":
        subs = [tok for tok in head.lefts if tok.dep_ == ""SUB""]
        if len(subs) &gt; 0:
            verbNegated = isNegated(head)
            subs.extend(getSubsFromConjunctions(subs))
            return subs, verbNegated
        elif head.head != head:
            return findSubs(head)
    elif head.pos_ == ""NOUN"":
        return [head], isNegated(tok)
    return [], False

def isNegated(tok):
    negations = {""no"", ""not"", ""n't"", ""never"", ""none""}
    for dep in list(tok.lefts) + list(tok.rights):
        if dep.lower_ in negations:
            return True
    return False

def findSVs(tokens):
    svs = []
    verbs = [tok for tok in tokens if tok.pos_ == ""VERB""]
    for v in verbs:
        subs, verbNegated = getAllSubs(v)
        if len(subs) &gt; 0:
            for sub in subs:
                svs.append((sub.orth_, ""!"" + v.orth_ if verbNegated else v.orth_))
    return svs

def getObjsFromPrepositions(deps):
    objs = []
    for dep in deps:
        if dep.pos_ == ""ADP"" and dep.dep_ == ""prep"":
            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == ""PRON"" and tok.lower_ == ""me"")])
    return objs

def getAdjectives(toks):
    toks_with_adjectives = []
    for tok in toks:
        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]
        adjs.append(tok)
        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])
        tok_with_adj = "" "".join([adj.lower_ for adj in adjs])
        toks_with_adjectives.extend(adjs)

    return toks_with_adjectives

def getObjsFromAttrs(deps):
    for dep in deps:
        if dep.pos_ == ""NOUN"" and dep.dep_ == ""attr"":
            verbs = [tok for tok in dep.rights if tok.pos_ == ""VERB""]
            if len(verbs) &gt; 0:
                for v in verbs:
                    rights = list(v.rights)
                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]
                    objs.extend(getObjsFromPrepositions(rights))
                    if len(objs) &gt; 0:
                        return v, objs
    return None, None

def getObjFromXComp(deps):
    for dep in deps:
        if dep.pos_ == ""VERB"" and dep.dep_ == ""xcomp"":
            v = dep
            rights = list(v.rights)
            objs = [tok for tok in rights if tok.dep_ in OBJECTS]
            objs.extend(getObjsFromPrepositions(rights))
            if len(objs) &gt; 0:
                return v, objs
    return None, None

def getAllSubs(v):
    verbNegated = isNegated(v)
    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != ""DET""]
    if len(subs) &gt; 0:
        subs.extend(getSubsFromConjunctions(subs))
    else:
        foundSubs, verbNegated = findSubs(v)
        subs.extend(foundSubs)
    return subs, verbNegated

def getAllObjs(v):
    # rights is a generator
    rights = list(v.rights)
    objs = [tok for tok in rights if tok.dep_ in OBJECTS]
    objs.extend(getObjsFromPrepositions(rights))

    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)
    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) &gt; 0:
        objs.extend(potentialNewObjs)
        v = potentialNewVerb
    if len(objs) &gt; 0:
        objs.extend(getObjsFromConjunctions(objs))
    return v, objs

def getAllObjsWithAdjectives(v):
    # rights is a generator
    rights = list(v.rights)
    objs = [tok for tok in rights if tok.dep_ in OBJECTS]

    if len(objs)== 0:
        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]

    objs.extend(getObjsFromPrepositions(rights))

    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)
    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) &gt; 0:
        objs.extend(potentialNewObjs)
        v = potentialNewVerb
    if len(objs) &gt; 0:
        objs.extend(getObjsFromConjunctions(objs))
    return v, objs

def findSVOs(tokens):
    svos = []
    verbs = [tok for tok in tokens if tok.pos_ == ""VERB"" and tok.dep_ != ""aux""]
    for v in verbs:
        subs, verbNegated = getAllSubs(v)
        # hopefully there are subs, if not, don't examine this verb any longer
        if len(subs) &gt; 0:
            v, objs = getAllObjs(v)
            for sub in subs:
                for obj in objs:
                    objNegated = isNegated(obj)
                    svos.append((sub.lower_, ""!"" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))
    return svos

def findSVAOs(tokens):
    svos = []
    verbs = [tok for tok in tokens if tok.pos_ == ""VERB"" and tok.dep_ != ""aux""]
    for v in verbs:
        subs, verbNegated = getAllSubs(v)
        # hopefully there are subs, if not, don't examine this verb any longer
        if len(subs) &gt; 0:
            v, objs = getAllObjsWithAdjectives(v)
            for sub in subs:
                for obj in objs:
                    objNegated = isNegated(obj)
                    obj_desc_tokens = generate_left_right_adjectives(obj)
                    sub_compound = generate_sub_compound(sub)
                    svos.append(("" "".join(tok.lower_ for tok in sub_compound), ""!"" + v.lower_ if verbNegated or objNegated else v.lower_, "" "".join(tok.lower_ for tok in obj_desc_tokens)))
    return svos

def generate_sub_compound(sub):
    sub_compunds = []
    for tok in sub.lefts:
        if tok.dep_ in COMPOUNDS:
            sub_compunds.extend(generate_sub_compound(tok))
    sub_compunds.append(sub)
    for tok in sub.rights:
        if tok.dep_ in COMPOUNDS:
            sub_compunds.extend(generate_sub_compound(tok))
    return sub_compunds

def generate_left_right_adjectives(obj):
    obj_desc_tokens = []
    for tok in obj.lefts:
        if tok.dep_ in ADJECTIVES:
            obj_desc_tokens.extend(generate_left_right_adjectives(tok))
    obj_desc_tokens.append(obj)

    for tok in obj.rights:
        if tok.dep_ in ADJECTIVES:
            obj_desc_tokens.extend(generate_left_right_adjectives(tok))

    return obj_desc_tokens
</code></pre>

<p>Now when you pass query such as:</p>

<pre><code>from spacy.en import English
parser = English()

sentence = u""""""
Donald Trump is the worst president of USA, but Hillary is better than him
""""""

parse = parser(sentence)
print(findSVAOs(parse))
</code></pre>

<p>You will get the following:</p>

<pre><code>[(u'donald trump', u'is', u'worst president'), (u'hillary', u'is', u'better')]
</code></pre>

<p>Thank you @Krzysiek for your solution too, I actually was unable to go deep into your library to modify it. I rather tried modifying the above mentioned link to solve my problem.</p>
"
40015480,3124746.0,2016-10-13T08:04:08Z,40015439,7,"<p>Because it returns an iterator, it omit storing the full list in the memory. So that you can easily iterate over it in the future not making pain to memory. Possibly you even don't need a full list, but the part of it, until your condition is matched.</p>

<p>You can find this <a href=""https://docs.python.org/3/glossary.html#term-iterator"">docs</a> useful, because iterators are awesome.</p>

<blockquote>
  <p>An object representing a stream of data. Repeated calls to the iteratorâs <code>__next__()</code> method (or passing it to the built-in function <code>next()</code>) return successive items in the stream. When no more data are available a <code>StopIteration</code> exception is raised instead. At this point, the iterator object is exhausted and any further calls to its <code>__next__()</code> method just raise <code>StopIteration</code> again. Iterators are required to have an <code>__iter__()</code> method that returns the iterator object itself so every iterator is also iterable and may be used in most places where other iterables are accepted. One notable exception is code which attempts multiple iteration passes. A container object (such as a <code>list</code>) produces a fresh new iterator each time you pass it to the <code>iter()</code> function or use it in a for loop. Attempting this with an iterator will just return the same exhausted iterator object used in the previous iteration pass, making it appear like an empty container. </p>
</blockquote>
"
40015658,6260170.0,2016-10-13T08:14:25Z,40015439,4,"<p>In Python 3, many functions (not just <code>map</code> but <code>zip</code>, <code>range</code> and others) return an iterator rather than the full list. This puts an emphasis on a <em>lazy</em> approach, where each item is generated sequentially on the fly until the iterator is exhausted, so that an entire list does not need to be held in memory unnecessarily. Iterators are not always suitable, for example if you need to access the entire list at once or particular list indexes.</p>

<p>In your example, if you had a much more intensive calculation like <code>map(lambda x: x+1, range(large_number))</code> (the equivalent would be <code>xrange</code> in Python 2) then it would make little sense to generate a list immediately rather than process each item of the iterator one by one. However, in this case, and many others, a generator expression would suffice and be arguable more readable, like <code>(x+1 for x in range(large_number))</code>.</p>

<p>Note that for <code>map()</code>/<code>list(map)</code> in Python 3 is not quite equivalent to <code>iter(map())</code>/<code>map()</code> in Python 2 as explained in answers to this <a href=""http://stackoverflow.com/questions/12015521/python-3-vs-python-2-map-behavior"">question</a>.</p>
"
40015733,799163.0,2016-10-13T08:17:55Z,40015439,5,"<p>I think the reason why map still exists <em>at all</em> when generator expressions also exist, is that it can take multiple iterator arguments that are all looped over and passed into the function:</p>

<pre><code>&gt;&gt;&gt; list(map(min, [1,2,3,4], [0,10,0,10]))
[0,2,0,4]
</code></pre>

<p>That's slightly easier than using zip:</p>

<pre><code>&gt;&gt;&gt; list(min(x, y) for x, y in zip([1,2,3,4], [0,10,0,10]))
</code></pre>

<p>Otherwise, it simply doesn't add anything over generator expressions.</p>
"
40015905,736308.0,2016-10-13T08:27:25Z,40015439,6,"<p>Guido answers this question <a href=""https://docs.python.org/3/whatsnew/3.0.html"">here</a>: ""<em>since creating a list would just be wasteful</em>"".  </p>

<p>He also says that the correct transformation is to use a regular <code>for</code> loop.</p>

<p>Converting <code>map()</code> from 2 to 3 might not just be a simple case of sticking a <code>list( )</code> around it.  Guido also says:</p>

<p>""If the input sequences are not of equal length, <code>map()</code> will stop at the termination of the shortest of the sequences. For full compatibility with <code>map()</code> from Python 2.x, also wrap the sequences in <code>itertools.zip_longest()</code>, e.g.</p>

<pre><code>map(func, *sequences)
</code></pre>

<p>becomes  </p>

<pre><code>list(map(func, itertools.zip_longest(*sequences)))
</code></pre>

<p>""</p>
"
40016402,100297.0,2016-10-13T08:52:13Z,40016359,5,"<p>Just concatenate the results:</p>

<pre><code>l[:3] + l[-3:]
</code></pre>

<p>There is no dedicated syntax to combine disjoint slices.</p>
"
40016409,6919239.0,2016-10-13T08:52:29Z,40016359,2,"<p>No, but you can use:</p>

<pre><code>l[:3] + l [-3:]
</code></pre>
"
40016450,5318186.0,2016-10-13T08:54:06Z,40016359,3,"<p>Do you mean like:</p>

<pre><code>l[:3]+l[-3:]
</code></pre>

<p>then using variable:</p>

<pre><code>l[:x]+l[-y:]
</code></pre>
"
40016555,3782161.0,2016-10-13T08:59:38Z,40006763,1,"<p>When a episode ends you are breaking the while loop before updating the Q function. Therefore, when the reward received by the agent is different from zero (the agent has reached the goal state), the Q function is never updated in that reward.</p>

<p>You should check for the end of the episode in the last part of the while loop.</p>
"
40016560,1874054.0,2016-10-13T08:59:54Z,40016431,5,"<p>He is one solution that may work (using <code>search</code> and <code>group</code> suggested by Wiktor):</p>

<pre><code>&gt;&gt;&gt; for t in items :
...   re.search(r'([0-9]+(g|ml))', t).group(1)
... 
'320g'
'320g'
'250g'
'330ml'
'225ml'
</code></pre>

<p>Indeed a better solution (thanks Wiktor) would be to test if there is a match :</p>

<pre><code>&gt;&gt;&gt; res = []
&gt;&gt;&gt; for t in items :
...   m = re.search(r'(\d+(g|ml))', t)
...   if m:
...     res.append(m.group(1))

print res 
</code></pre>
"
40016591,645551.0,2016-10-13T09:01:29Z,40016373,5,"<p>1) <code>result.get();</code> does not start the thread. It only <strong>waits</strong> for the result. The parallel thread is launched with <code>std::async(called_from_async)</code> call (or whenever the compiler decides).</p>

<p>However <code>std::cout</code> is guaranteed to be internally thread safe. So the result you are showing us <strong>should not</strong> ever happen. There's a race condition, but you can't mix both outputs like that. If it really happens (which I doubt) then you might be dealing with a compiler bug.</p>

<p>2) Your calls will run parallely. On how many cores it depends on OS and other processes running on your machine. But there's a good chance that all will be used (assuming that you have control over whole ecosystem and no other cpu-intensive processes are running in the background).</p>

<p>There is no multiprocessing-like lib for C++ (at least not in the std). If you wish to run subprocesses then there are several options, e.g. forking or <code>popen</code> syscalls.</p>
"
40016602,6411000.0,2016-10-13T09:01:55Z,40016359,2,"<p>If it is allowed to change list, You can use this:</p>

<pre><code>del(a[n:-n])
</code></pre>

<p>If not, create new list and then do this.</p>

<pre><code>b = [x for x in a]
del(b[n:-n])
</code></pre>
"
40016604,799163.0,2016-10-13T09:01:59Z,40016192,0,"<p>Something like</p>

<pre><code>from collections import defaultdict
result = defaultdict(dict)

for somestring, namesdict in initialdata.items():
    for name, amount in namesdict.items():
        result[name][something] = amount
</code></pre>

<p>Would do it, but with 8 million items it may become time to look at databases.</p>
"
40016633,2562137.0,2016-10-13T09:03:24Z,40016431,-1,"<p><a href=""https://regex101.com/r/gy5YTp/4"" rel=""nofollow"">https://regex101.com/r/gy5YTp/4</a></p>

<p>Match any digit with <code>\d+</code> then create a matching but non selecting group with <code>(?:ml|g)</code> this will match ml or g. </p>

<pre><code>import re

items = ['avuhovi Grillikaapeli 320g', 'Savuhovi 333ml Kisamakkara 320g', 'Savuhovi Raivo 250g', 'AitoMaku str.garl.sal.dres.330ml', 'Rydbergs 225ml Hollandaise sauce']

groupedWeights = [re.findall('(\d+(?:ml|g))', i) for i in items]
flattenedWeights = [y for x in groupedWeights for y in x]

print(flattenedWeights)
</code></pre>

<p>The match that we make returns a list of lists of weights found so we need to flatten that with <code>[y for x in groupedWeights for y in x]</code></p>

<p>That is if you ever have more than one weight in an element. Otherwise we can take the first element of each list like this.</p>

<pre><code>weights = [re.findall('(\d+(?:ml|g))', i)[0] for i in items]
</code></pre>
"
40017060,3832970.0,2016-10-13T09:22:54Z,40016950,4,"<p>The <code>\s</code> matches both <em>horizontal</em> and <em>veritcal</em> whitespace symbols. If you have a <code>re.VERBOSE</code>, you can match a normal space with an escaped space <code>\ </code>. Or, you may exclude <code>\r</code> and <code>\n</code> from <code>\s</code> with <code>[^\S\r\n]</code> to match horizontal whitespace.</p>

<p>Use</p>

<pre><code>ex = re.compile(r""""""(
    (\(?0\d{4}\)?)?       # Area code
    ([^\S\r\n]*-*\.*)?   # seperator   ((HERE))
    (\(?\d{6}\)?)        # Local number
     )"""""", re.VERBOSE)
</code></pre>

<p>See the <a href=""https://regex101.com/r/TefKkm/3"" rel=""nofollow"">regex demo</a></p>

<p>Also, the <code>-</code> outside a character class does not require escaping.</p>
"
40018475,6898066.0,2016-10-13T10:28:41Z,40012745,0,"<p>I tried writing some script to help you but it was a bit beyond my (current) Newbie skill level.
However, the tools/approach I started taking may be able to help. Try using <code>sys.argv</code> (which generates a list of all arguments from when the script is run), and then using some regular expressions (<code>import re</code>...).</p>

<p>I hope this helps someone else help you. (:</p>
"
40018719,948550.0,2016-10-13T10:40:13Z,40018398,41,"<p>I think you're seeing over-allocation patterns this is a <a href=""https://github.com/python/cpython/blob/3.5/Objects/listobject.c#L42"" rel=""nofollow"">sample from the source</a>:</p>

<pre><code>/* This over-allocates proportional to the list size, making room
 * for additional growth.  The over-allocation is mild, but is
 * enough to give linear-time amortized behavior over a long
 * sequence of appends() in the presence of a poorly-performing
 * system realloc().
 * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...
 */

new_allocated = (newsize &gt;&gt; 3) + (newsize &lt; 9 ? 3 : 6);
</code></pre>

<hr>

<p>Printing the sizes of list comprehensions of lengths 0-88 you can see the pattern matches:</p>

<pre><code># create comprehensions for sizes 0-88
comprehensions = [sys.getsizeof([1 for _ in range(l)]) for l in range(90)]

# only take those that resulted in growth compared to previous length
steps = zip(comprehensions, comprehensions[1:])
growths = [x for x in list(enumerate(steps)) if x[1][0] != x[1][1]]

# print the results:
for growth in growths:
    print(growth)
</code></pre>

<p>Results (format is <code>(list length, (old total size, new total size))</code>):</p>

<pre><code>(0, (64, 96)) 
(4, (96, 128))
(8, (128, 192))
(16, (192, 264))
(25, (264, 344))
(35, (344, 432))
(46, (432, 528))
(58, (528, 640))
(72, (640, 768))
(88, (768, 912))
</code></pre>

<hr>

<p>The over-allocation is done for performance reasons allowing lists to grow without allocating more memory with every growth (better <a href=""https://en.wikipedia.org/wiki/Amortized_analysis"" rel=""nofollow"">amortized</a> performance).</p>

<p>A probable reason for the difference with using list comprehension, is that list comprehension can not deterministically calculate the size of the generated list, but <code>list()</code> can. This means comprehensions will continuously grow the list as it fills it using over-allocation until finally filling it.</p>

<p>It is possible that is will not grow the over-allocation buffer with unused allocated nodes once its done (in fact, in most cases it wont, that would defeat the over-allocation purpose).</p>

<p><code>list()</code>, however, can add some buffer no matter the list size since it knows the final list size in advance.</p>

<hr>

<p>Another backing evidence, also from the source, is that we see <a href=""https://github.com/python/cpython/blob/3.5/Python/compile.c#L3374"" rel=""nofollow"">list comprehensions invoking <code>LIST_APPEND</code></a>, which indicates usage of <code>list.resize</code>, which in turn indicates consuming the pre-allocation buffer without knowing how much of it will be filled. This is consistent with the behavior you're seeing.</p>

<hr>

<p>To conclude, <code>list()</code> will pre-allocate more nodes as a function of the list size</p>

<pre><code>&gt;&gt;&gt; sys.getsizeof(list([1,2,3]))
60
&gt;&gt;&gt; sys.getsizeof(list([1,2,3,4]))
64
</code></pre>

<p>List comprehension does not know the list size so it uses append operations as it grows, depleting the pre-allocation buffer:</p>

<pre><code># one item before filling pre-allocation buffer completely
&gt;&gt;&gt; sys.getsizeof([i for i in [1,2,3]]) 
52
# fills pre-allocation buffer completely
# note that size did not change, we still have buffered unused nodes
&gt;&gt;&gt; sys.getsizeof([i for i in [1,2,3,4]]) 
52
# grows pre-allocation buffer
&gt;&gt;&gt; sys.getsizeof([i for i in [1,2,3,4,5]])
68
</code></pre>
"
40019900,3124746.0,2016-10-13T11:37:10Z,40018398,23,"<p>Thanks everyone for helping me to understand that awesome Python.</p>

<p>I don't want to make question that massive(that why i'm posting answer), just want to show and share my thoughts.</p>

<p>As @ReutSharabani noted correctly: ""list() deterministically determines list size"". You can see it from that graph.</p>

<p><a href=""https://i.stack.imgur.com/JrqC9.png""><img src=""https://i.stack.imgur.com/JrqC9.png"" alt=""graph of sizes""></a></p>

<p>When you <code>append</code> or using list comprehension you always have some sort of boundaries that extends when you reach some point. And with <code>list()</code> you have almost the same boundaries, but they are floating.</p>

<p><strong>UPDATE</strong></p>

<p>So thanks to @ReutSharabani, @tavo, @SvenFestersen</p>

<p>To sum up: <code>list()</code> preallocates memory depend on list size, list comprehension cannot do that(it requests more memory when it needed, like <code>.append()</code>). That's why <code>list()</code> store more memory.</p>

<p>One more graph, that show <code>list()</code> preallocate memory. So green line shows <code>list(range(830))</code> appending element by element and for a while memory not changing.</p>

<p><a href=""https://i.stack.imgur.com/yoV85.png""><img src=""https://i.stack.imgur.com/yoV85.png"" alt=""list() preallocates memory""></a></p>

<p><strong>UPDATE 2</strong></p>

<p>As @Barmar noted in comments below, <code>list()</code> must me faster than list comprehension, so i ran <code>timeit()</code> with <code>number=1000</code> for length of <code>list</code> from <code>4**0</code> to <code>4**10</code> and the results are</p>

<p><a href=""https://i.stack.imgur.com/WNSnO.png""><img src=""https://i.stack.imgur.com/WNSnO.png"" alt=""time measurements""></a></p>
"
40020376,548225.0,2016-10-13T11:58:06Z,40020326,3,"<p>You can use this regex:</p>

<pre><code>&gt;&gt;&gt; s = ""12 word word2""
&gt;&gt;&gt; print re.sub(r'\b[0-9]+\b\s*', '', s)
word word2
</code></pre>

<p><code>\b</code> is used for word boundary and <code>\s*</code> will remove 0 or more spaces after your number word.</p>
"
40020421,1252759.0,2016-10-13T12:00:33Z,40020326,6,"<p>Using a regex is probably a bit overkill here depending whether you need to preserve whitespace:</p>

<pre><code>s = ""12 word word2""
s2 = ' '.join(word for word in s.split() if not word.isdigit())
# 'word word2'
</code></pre>
"
40020470,6209196.0,2016-10-13T12:02:48Z,40020326,1,"<p>Without using any external library you could do:</p>

<pre><code>stringToFormat = ""12 word word2""
words = """"
for word in stringToFormat.split("" ""):
    try:
        int(word)
    except ValueError:
        words += ""{} "".format(word)
print(words)
</code></pre>
"
40022269,1754752.0,2016-10-13T13:22:56Z,40022102,2,"<pre><code>import re
re.sub('\s+\n','\n',string)
</code></pre>

<p>Edit: better version from comments:</p>

<pre><code>re.sub(r'\s+$', '', string, flags=re.M)
</code></pre>
"
40022271,3125566.0,2016-10-13T13:23:00Z,40022102,6,"<p>You can <em>split</em> the string into lines, <em>strip</em> off all whitespaces on the right using <code>rstrip</code>, then add a new line at the end of each line:</p>

<pre><code>''.join([line.rstrip()+'\n' for line in string.splitlines()])
</code></pre>
"
40022448,6225617.0,2016-10-13T13:31:27Z,40022102,-1,"<p>As you can find <a href=""http://stackoverflow.com/questions/8270092/python-remove-all-whitespace-in-a-string"">here</a>, </p>

<p>To remove all whitespace characters (space, tab, newline, and so on) you can use split then join:</p>

<pre><code>sentence = ''.join(sentence.split())
</code></pre>

<p>or a regular expression:</p>

<pre><code>import re
pattern = re.compile(r'\s+')
sentence = re.sub(pattern, '', sentence)
</code></pre>
"
40023032,3142347.0,2016-10-13T13:55:18Z,40022757,0,"<p>The error is saying what is happening: you did not define the name <code>analytics</code> used in line 9 previously</p>

<pre><code>daily_upload = analytics.management().uploads().uploadData(
</code></pre>

<p>You need to go over the first steps in order to use that and import it. You will also need an account since it requires auth.</p>

<p><a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/quickstart/service-py"" rel=""nofollow"">https://developers.google.com/analytics/devguides/reporting/core/v3/quickstart/service-py</a></p>

<p>After you setup, you should read</p>

<p><a href=""https://developers.google.com/analytics/devguides/reporting/core/v3/coreDevguide"" rel=""nofollow"">https://developers.google.com/analytics/devguides/reporting/core/v3/coreDevguide</a></p>

<p>With that you should be ok!</p>
"
40023593,771848.0,2016-10-13T14:18:29Z,39935335,1,"<p>From what I understand, the <em><code>pyopenssl</code> package version installed system-wide is not up-to-date</em>. Upgrade it:</p>

<pre><code>sudo pip install --upgrade pyopenssl
</code></pre>

<p>Or, remove it and install the latest in your virtual environment:</p>

<pre><code>$ sudo pip uninstall pyopenssl
$ # activate virtual environment
(myvirtualenv) $ pip install --upgrade pyopenssl
</code></pre>
"
40023778,1126841.0,2016-10-13T14:26:08Z,40023663,2,"<p><code>isDebug</code> is state that applies to the application of a function <code>cmd_exec</code>. Sounds like a use-case for a class to me.</p>

<pre><code>class CommandExecutor(object):

    def __init__(self, debug):
        self.debug = debug

    def execute(self, cmd):
        if self.debug:
            commandStart = datetime.datetime.now()
            print commandStart
            print cmd
        ...
        ... executing commands
        ...
        if self.debug:
            print datetime.datetime.now() - command_start

def main(args):
    ce = CommandExecutor(args.debug)
    ce.execute(cmd1)
    ce.execute(cmd2)
</code></pre>
"
40023796,6926025.0,2016-10-13T14:26:58Z,39876729,0,"<p>So the problem here is in the win32security.SetFileSecurity() function. As per MSDN, this function is obsolete (see: <a href=""https://msdn.microsoft.com/en-us/library/windows/desktop/aa379577(v=vs.85).aspx"" rel=""nofollow"">https://msdn.microsoft.com/en-us/library/windows/desktop/aa379577(v=vs.85).aspx</a>) and has been replaced by SetNamedSecurityInfo. I switched, and everything appears to work well. Thanks anyway! </p>
"
40023830,4211135.0,2016-10-13T14:28:17Z,40023663,2,"<p>Python has a built-in <code>__debug__</code> variable that could be useful.</p>

<pre><code>if __debug__:
    print 'information...'
</code></pre>

<p>When you run your program as <code>python test.py</code>, <code>__debug__</code> is <code>True</code>. If you run it as <code>python -O test.py</code>, it will be <code>False</code>.</p>

<p>Another option which I do in my projects is set a global <code>DEBUG</code> var at the beginning of the file, after importing:</p>

<pre><code>DEBUG = True
</code></pre>

<p>You can then reference this <code>DEBUG</code> var in the scope of the function.</p>
"
40024462,5741205.0,2016-10-13T14:55:27Z,40024406,3,"<p>you can reuse the same <code>cols_to_use</code> list for selecting columns in desired order:</p>

<pre><code>df_ret = pd.read_csv(filepath, index_col=False, usecols=cols_to_use)[cols_to_use]
</code></pre>
"
40024499,2615075.0,2016-10-13T14:57:22Z,40024017,2,"<p>The key to reversing the regex is the <a href=""https://github.com/django/django/blob/master/django/utils/regex_helper.py#L50"" rel=""nofollow""><code>normalize()</code></a> function, which takes a regex and returns a list of possibilities. Each possibility is a tuple of a placeholder string and a list of parameters. </p>

<p>The <code>reverse()</code> function first checks if the parameters match the list of parameters for the placeholder. If it matches, the placeholder string is formatted with the arguments passed to <code>reverse()</code> (with a simple <code>'string' % parameters</code>). This string is then matched against the original regex. That's necessary because the parameters for the placeholder string don't have any restrictions, so checking against the original regex ensures that e.g. a group with <code>\d+</code> only contains numbers. </p>

<p>If a string formatted with the passed parameters matches the original regex, it's a valid url, and returned. </p>
"
40024533,5987.0,2016-10-13T14:59:36Z,40023663,1,"<p>You can use a module to create variables that are shared. This is better than a global because it only affects code that is specifically looking for the variable, it doesn't pollute the global namespace. It also lets you define something without your main module needing to know about it.</p>

<p>This works because modules are shared objects in Python. Every <code>import</code> gets back a reference to the same object, and modifications to the contents of that module get shared immediately, just like a global would.</p>

<p>my_debug.py:</p>

<pre><code>isDebug = false
</code></pre>

<p>main.py:</p>

<pre><code>import my_debug

def cmd_exec(cmd):
    if my_debug.isDebug:
        # ...

def main():
    # ...
    if args.debug:
        my_debug.isDebug = True
</code></pre>
"
40024699,2901002.0,2016-10-13T15:07:10Z,40024666,4,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""nofollow""><code>reset_index</code></a>:</p>

<pre><code>distcounts = df.groupby('a')['b'].nunique().reset_index()
</code></pre>

<p>Sample:</p>

<pre><code>df = pd.DataFrame({'a':[7,8,8],
                   'b':[4,5,6]})

print (df)
   a  b
0  7  4
1  8  5
2  8  6

distcounts = df.groupby('a')['b'].nunique().reset_index()
print (distcounts)
   a  b
0  7  1
1  8  2
</code></pre>
"
40024797,6515149.0,2016-10-13T15:12:14Z,40016025,0,"<p>Why not make Address a many to many relationship on Person? That seems to be a more natural expression of your data. </p>

<p>But regardless, you can't really enforce the many to many realation on the db. You could perhaps override the save of Person to check for an address. But I would prefer to handle it in the form logic. </p>
"
40024825,2901002.0,2016-10-13T15:13:46Z,40024721,4,"<p>You can use:</p>

<pre><code>idx = pd.IndexSlice
df = testdf2.loc[:, idx[['sum', 'count'], 'A']]
print (df)
                                                    sum count
Provider                                              A     A
Employer Year Division Name EmployeeId PersonId              
Z        2012 C        H    14         14        311.17   2.0
                                       15           NaN   NaN
         2013 C        H    14         14        386.29   2.0
</code></pre>

<p>Another solution:</p>

<pre><code>df = testdf2.loc[:, (slice('sum','count'), ['A'])]
print (df)
                                                    sum count
Provider                                              A     A
Employer Year Division Name EmployeeId PersonId              
Z        2012 C        H    14         14        311.17   2.0
                                       15           NaN   NaN
         2013 C        H    14         14        386.29   2.0
</code></pre>
"
40024829,6207849.0,2016-10-13T15:13:57Z,40024666,3,"<p>Another alternative using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html"" rel=""nofollow""><code>Groupby.agg</code></a> instead:</p>

<pre><code>df.groupby('a', as_index=False).agg({'b': 'nunique'})
</code></pre>
"
40024904,2336654.0,2016-10-13T15:17:13Z,40024721,5,"<p>Use <code>xs</code> for cross section</p>

<pre><code>testdf2.xs('A', axis=1, level=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/nx1np.png""><img src=""https://i.stack.imgur.com/nx1np.png"" alt=""enter image description here""></a></p>

<p>Or keep the column level with <code>drop_level=False</code></p>

<pre><code>testdf2.xs('A', axis=1, level=1, drop_level=False)
</code></pre>

<p><a href=""https://i.stack.imgur.com/tNenr.png""><img src=""https://i.stack.imgur.com/tNenr.png"" alt=""enter image description here""></a></p>
"
40025403,5910563.0,2016-10-13T15:40:03Z,40025238,1,"<p>Not a complete answer, but here are some bits you should be aware of:</p>

<ul>
<li>Imports might happen in conditional or try-catch blocks. So depending on a setting of an environment variable, module A might or might not import module B.</li>
<li>There's a wide variety of import syntax: <code>import A</code>, <code>from A import B</code>, <code>from A import *</code>, <code>from . import A</code>, <code>from .. import A</code>, <code>from ..A import B</code> as well as their versions with <code>A</code> replaced with sub-modules.</li>
<li>Imports can happen in any executable context - the top-level of the file, in a function, in a class definition etc.</li>
<li><code>eval</code> can evaluate code with imports. Up to you if you consider such code to be a dependency.</li>
</ul>

<p>The standard library <a href=""https://docs.python.org/3.5/library/modulefinder.html"" rel=""nofollow"">modulefinder</a> module might help.</p>
"
40025789,1113623.0,2016-10-13T15:59:41Z,40025616,2,"<p>I'm not too familiar with tkinter so i can't show you ze code, but you should look at tkinter's event system, particularly firing custom events.</p>

<p><a href=""http://stackoverflow.com/questions/270648/tkinter-invoke-event-in-main-loop"">This question</a> may help you get started</p>
"
40025839,209246.0,2016-10-13T16:02:07Z,39840638,1,"<p>If you use the <code>wx</code> backend, you can call <code>wx.Yield()</code> periodically if you want to interact with your data during some long-running function. In the following example, <code>wx.Yield()</code> is called for every iteration of some ""long running"" function, <code>animate_sleep</code>. In this case, you could start the program with <code>$ ipython --gui=wx &lt;program_name.py&gt;</code></p>

<pre><code>import time
import numpy as np
from mayavi import mlab
import wx

V = np.random.randn(20, 20, 20)
f = mlab.figure()
s = mlab.contour3d(V, contours=[0])

def animate_sleep(x):
    n_steps = int(x / 0.01)
    for i in range(n_steps):
        time.sleep(0.01)
        wx.Yield()

for i in range(5):

    animate_sleep(1)

    V = np.random.randn(20, 20, 20)

    # Update the plot with the new information
    s.mlab_source.set(scalars=V)
</code></pre>
"
40025910,3125566.0,2016-10-13T16:04:53Z,40025837,2,"<p><code>sympy_parser</code> is a <em>module</em>, and modules are not a <em>callable</em>. What you want is <a href=""http://docs.sympy.org/dev/modules/parsing.html"" rel=""nofollow""><code>sympy_parser.parse_expr</code></a>:</p>

<pre><code>&gt;&gt;&gt; from sympy.parsing import sympy_parser
&gt;&gt;&gt; r ='3/2'
&gt;&gt;&gt; r = sympy_parser.parse_expr(r)
&gt;&gt;&gt; r
3/2
</code></pre>
"
40025975,6411000.0,2016-10-13T16:08:36Z,40025616,1,"<p>You can use <code>Queue</code>, from <code>Queue</code> module.</p>

<pre><code>def get_queue(self, queue):
    status = queue.get()
    if status:
        self.do_your_action()
    self.master.after(1000, self.get_queue)
</code></pre>

<p>And in the progress bar window, You pass queue and put something in it when progress bar finishes.</p>
"
40026172,6620609.0,2016-10-13T16:18:17Z,40016025,0,"<p>As previously said, there's no way to enforce a relationship directly on the database. </p>

<p>However, you can take care of it by validating the model before saving using the <code>clean()</code> method. It will be automatically triggered on save for Django models.</p>

<pre><code>class Person(models.Model):
    .
    .
    .
    def clean(self):
        if len(self.addresses) == 0:
            raise ValidationError('At least one address is required.')
</code></pre>
"
40026199,5276797.0,2016-10-13T16:19:57Z,40024294,0,"<p>This may not be the most general solution, but it solves your problem:</p>

<p><strong>First</strong>, isolate the right half:</p>

<pre><code>r = df[['x1', 'x2', 'y1', 'y2']].dropna(how='all')
</code></pre>

<p><strong>Second</strong>, use <code>dropna</code> applied column by column to compress the data:</p>

<pre><code>r_compressed = r.apply(
    lambda g: g.dropna().reset_index(drop=True),
    axis=0
).set_index(r.index[::2])
</code></pre>

<p>You need to drop the index otherwise pandas will attempt to realign the data. The original index is reapplied at the end (but only with every second index label) to facilitate reinsertion of the left half and the <code>t</code> column.</p>

<p>Output (note the index values):</p>

<pre><code>    x1   x2   y1   y2
0  1.0  2.0  2.0  4.0
3  1.0  2.0  2.0  3.0
5  1.0  2.0  2.0  3.0
</code></pre>

<p><strong>Third</strong>, isolate left half:</p>

<pre><code>l = df[['a', 'b', 'c', 'd']].dropna(how='all')
</code></pre>

<p><strong>Fourth</strong>, incorporate the left half and <code>t</code> column to compressed right half:</p>

<pre><code>out = r_compressed.combine_first(l)
out['t'] = df['t']
</code></pre>

<p>Output:</p>

<pre><code>     a    b    c    d   x1   x2   y1   y2    t
0  NaN  NaN  NaN  NaN  1.0  2.0  2.0  4.0  0.2
1  1.0  2.0  NaN  NaN  NaN  NaN  NaN  NaN  0.4
3  NaN  NaN  NaN  NaN  1.0  2.0  2.0  3.0  1.8
5  NaN  NaN  NaN  NaN  1.0  2.0  2.0  3.0  3.8
7  NaN  NaN  2.0  5.0  NaN  NaN  NaN  NaN  4.2
</code></pre>
"
40027326,5747944.0,2016-10-13T17:24:46Z,40027108,2,"<p>The cause of this problem is, as we previously discussed, insufficient permissions to perform the network operation. The remedy to the problem was to run the process as an administrator and/or to modify the system policy to allow connections on the restricted port.</p>
"
40027843,3142347.0,2016-10-13T17:56:30Z,40027392,2,"<blockquote>
  <p>The current version of Python uses reference counting to keep track of allocated memory. Each object in Python has a reference count which indicates how many objects are pointing to it. When this reference count reaches zero the object is freed. This works well for most programs. However, there is one fundamental flaw with reference counting and it is due to something called reference cycles. The simplest example of a reference cycle is one object that refers to itself. For example:</p>
</blockquote>

<pre><code>&gt;&gt;&gt; l = []
&gt;&gt;&gt; l.append(l)
&gt;&gt;&gt; del l
</code></pre>

<blockquote>
  <p>The reference count for the list created is now one. However, since it cannot not be reached from inside Python and cannot possibly be used again, it should be considered garbage. In the current version of Python, this list will never be freed.</p>
  
  <p>Creating reference cycles is usually not good programming practice and can almost always be avoided. However, sometimes it is difficult to avoid creating reference cycles and other times the programmer does not even realize it is happening. For long running programs such as servers this is especially troublesome. People do not want their servers to run out of memory because reference counting failed to free unreachable objects. For large programs it is difficult to find how reference cycles are being created.</p>
</blockquote>

<p>Source: <a href=""http://arctrix.com/nas/python/gc/"" rel=""nofollow"">http://arctrix.com/nas/python/gc/</a></p>

<p>The link below has the sample example you are using and it also explains:</p>

<p><a href=""http://www.digi.com/wiki/developer/index.php/Python_Garbage_Collection"" rel=""nofollow"">http://www.digi.com/wiki/developer/index.php/Python_Garbage_Collection</a></p>
"
40027920,5747944.0,2016-10-13T18:00:34Z,40027675,1,"<p>If you call a selenium driver without any arguments, the path to the webdriver executable must be in the system PATH environment variables.</p>

<p>Alternatively, you can specify the path explicitly as such:</p>

<pre><code>driver = webdriver.Firefox(""path/to/the/FireFoxExecutable"")
</code></pre>
"
40029176,2901002.0,2016-10-13T19:16:36Z,40029071,2,"<p>I think there is problem with not align index of column <code>data['numerical_column']</code>.</p>

<p>So need convert it to <code>numpy array</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.values.html"" rel=""nofollow""><code>values</code></a>:</p>

<pre><code>new_series = pd.Series(data['numerical_column'].values , index=data['dates'])
</code></pre>

<p>Sample:</p>

<pre><code>import pandas as pd
import datetime

data = pd.DataFrame({
'dates': {0: datetime.date(1980, 1, 31), 1: datetime.date(1980, 2, 29), 
          2: datetime.date(1980, 3, 31), 3: datetime.date(1980, 4, 30), 
          4: datetime.date(1980, 5, 31), 5: datetime.date(1980, 6, 30)}, 
'numerical_column': {0: 1, 1: 4, 2: 5, 3: 3, 4: 1, 5: 0}})
print (data)
        dates  numerical_column
0  1980-01-31                 1
1  1980-02-29                 4
2  1980-03-31                 5
3  1980-04-30                 3
4  1980-05-31                 1
5  1980-06-30                 0

new_series = pd.Series(data['numerical_column'].values , index=data['dates'])
print (new_series)
dates
1980-01-31    1
1980-02-29    4
1980-03-31    5
1980-04-30    3
1980-05-31    1
1980-06-30    0
dtype: int64
</code></pre>

<p>But method with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow""><code>set_index</code></a> is nicer, but slowier:</p>

<pre><code>#[60000 rows x 2 columns]
data = pd.concat([data]*10000).reset_index(drop=True)

In [65]: %timeit pd.Series(data['numerical_column'].values , index=data['dates'])
1000 loops, best of 3: 308 Âµs per loop

In [66]: %timeit data.set_index('dates')['numerical_column']
1000 loops, best of 3: 1.28 ms per loop
</code></pre>

<p><strong>Verification</strong>:</p>

<p>If index of column has same index, it works nice:</p>

<pre><code>s = data.set_index('dates')['numerical_column']
df = s.to_frame()
print (df)
            numerical_column
dates                       
1980-01-31                 1
1980-02-29                 4
1980-03-31                 5
1980-04-30                 3
1980-05-31                 1
1980-06-30                 0

new_series = pd.Series(df['numerical_column'] , index=data['dates'])
print (new_series)
dates
1980-01-31    1
1980-02-29    4
1980-03-31    5
1980-04-30    3
1980-05-31    1
1980-06-30    0
Name: numerical_column, dtype: int64
</code></pre>
"
40029336,208880.0,2016-10-13T19:25:52Z,40028498,1,"<p>The <em>One Obvious Way</em> is function annotations.</p>

<pre><code>class CheckBox(enum.Enum):
    Off = 0
    On = 1

def setCheckState(self, value: CheckBox):
    ...
</code></pre>

<p>This says quite clearly that <code>value</code> should be an instance of <code>CheckBox</code>.  Having <code>Enum</code> just makes that a bit easier.</p>

<p>Annotations themselves aren't directly supported in 2.7, though.  Common workarounds include putting that information in the function doc string (where various tools can find it) or in comments (as we already knew).</p>

<p>If looking for a method for your own code: use an annotating decorator.  This has the advantage of continuing to work in 3+:</p>

<pre><code>class annotate(object):
    def __init__(self, **kwds):
        self.kwds = kwds
    def __call__(self, func):
        func.__annotations__ = self.kwds

@annotate(value=CheckBox)
def setCheckState(self, value):
    ...
</code></pre>

<p>To be a robust decorator it should check that the contents of <code>kwds</code> matches the function parameter names.</p>
"
40029368,651744.0,2016-10-13T19:28:13Z,40028380,0,"<p>This looks suspiciously like a bug related to size of some buffer, since 8192 is a power of two.</p>

<p>The main thing here is to isolate exactly where the failure is occurring. If I were debugging this, I would </p>

<ol>
<li><p>Take a closer look at the output from <code>json.dumps</code>, by printing several characters on either side of position 8191, ideally the integer character code (unicode, ASCII, or whatever). </p></li>
<li><p>If that looks OK, I would try capturing the output from the python script as a file and read that directly in the node server (i.e. don't run a python script). </p></li>
<li><p>If that works, then create a python script that takes that file and outputs it without manipulation and have your node server execute that python script instead of the one it is using now.</p></li>
</ol>

<p>That should help you figure out where the problem is occurring. From comments, I suspect that this is essentially a bug that you cannot control, unless you can increase the python buffer size enough to guarantee your data will never blow the buffer. 8K is pretty small, so that might be a realistic solution.</p>

<p>If that is inadequate, then you might consider processing the data on the the node server, to remove every character at <code>n * 8192</code>, if you can consistently rely on that.  Good luck. </p>
"
40029515,6207849.0,2016-10-13T19:37:24Z,40027233,2,"<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.axes.html"" rel=""nofollow""><code>.axes</code></a> to return a list of the index axis labels as well as the column axis labels and then you could access it via slice-notation.</p>

<pre><code>pn.axes
#[Index(['X', 'Y', 'Z'], dtype='object'),
# Index(['a', 'b', 'c'], dtype='object'),
# Index(['A', 'B', 'C'], dtype='object')]
</code></pre>

<p>Then, you could provide the slices to retrieve the objects:</p>

<pre><code>pn.axes[0]
#Index(['X', 'Y', 'Z'], dtype='object')

df.axes[0]
#Index(['a', 'b', 'c'], dtype='object')
</code></pre>
"
40029630,1547004.0,2016-10-13T19:44:05Z,40028755,0,"<p>You can do it, but you'd basically have to copy/modify a lot of the code out of <code>json.encoder</code> because the encoding functions aren't really designed to be partially overridden.</p>

<p>Basically, copy the entirety of <code>_make_iterencode</code> from <code>json.encoder</code> and make the changes so that your special dictionary gets printed without newline indents.  Then monkeypatch the json package to use your modified version, run the json dump, then undo the monkeypatch (if you want).</p>

<p>The <code>_make_iterencode</code> function is pretty long, so I've only posted the portions that need to be changed.</p>

<pre><code>import json
import json.encoder

def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,
    ...
    def _iterencode_dict(dct, _current_indent_level):
        ...
        if _indent is not None:
            _current_indent_level += 1
            if '$special' in dct:
                newline_indent = ''
                item_separator = _item_separator
            else:
                newline_indent = '\n' + (' ' * (_indent * _current_indent_level))
                item_separator = _item_separator + newline_indent
            yield newline_indent
        ...
        if newline_indent is not None:
            _current_indent_level -= 1
            if '$special' not in dct:
                yield '\n' + (' ' * (_indent * _current_indent_level))

def main():
    data = {
        'x': [1, {'$special': 'a'}, 2],
        'y': {'$special': 'b'},
        'z': {'p': True, 'q': False},
    }

    orig_make_iterencoder = json.encoder._make_iterencode
    json.encoder._make_iterencode = _make_iterencode
    print(json.dumps(data, indent=2))
    json.encoder._make_iterencode = orig_make_iterencoder
</code></pre>
"
40029760,3124746.0,2016-10-13T19:51:49Z,39946092,0,"<p>If we omit the fact <a href=""https://docs.python.org/3/library/functions.html#eval"" rel=""nofollow""><code>eval</code></a> is <a href=""http://stackoverflow.com/questions/1832940/is-using-eval-in-python-a-bad-practice"">evil</a>, we can solve that problem with it.</p>

<pre><code>def sum_numbers(s):
    s = s.replace(' ', '+')
    return eval(s)
</code></pre>

<p>Yes, that simple. But i won't put that thing in production.</p>

<p>And sure we need to test that:</p>

<pre><code>from hypothesis import given
import hypothesis.strategies as st


@given(list_num=st.lists(st.integers(), min_size=1))
def test_that_thing(list_num):
    assert sum_numbers(' '.join(str(i) for i in list_num)) == sum(list_num)

test_that_thing()
</code></pre>

<p>And it would raise nothing.</p>
"
40030066,4952130.0,2016-10-13T20:10:23Z,40029807,2,"<p>Well, the <code>type</code> is of course <code>MyMetaClass</code>. <code>metaclass_callable</code> is initially 'selected' as the metaclass since <a href=""https://github.com/python/cpython/blob/master/Python/bltinmodule.c#L100"" rel=""nofollow"">it's been specified in the <code>metaclass</code> kwarg</a> and as such, it's <code>__call__</code> (a simple function call) is going to be performed. </p>

<p>It just so happens that calling it will <code>print</code> and then invoke <code>MyMetaClass.__call__</code> (which calls <code>type.__call__</code> since <code>__call__</code> hasn't been overridden for <code>MyMetaClass</code>). <a href=""https://github.com/python/cpython/blob/master/Objects/typeobject.c#L2693"" rel=""nofollow""><em>There</em> the assignment of <code>cls.__class__</code> is made</a> to <code>MyMetaClass</code>.  </p>

<blockquote>
  <p><code>metaclass_callable</code> is called once and then appears to be unrecoverable</p>
</blockquote>

<p>Yes, it is only initially invoked and then hands control over to <code>MyMetaClass</code>. I'm not aware of any class attribute that keeps that information around. </p>

<blockquote>
  <p>derived classes do not use (as far as I can tell) <code>metaclass_callable</code> in any way.</p>
</blockquote>

<p>Nope, if no <code>metaclass</code> is explicitly defined, <a href=""https://github.com/python/cpython/blob/master/Python/bltinmodule.c#L130"" rel=""nofollow"">the best match for the metaclasses of <code>bases</code></a> (here <code>MyClass</code>) will be used (resulting in <code>MyMetaClass</code>).</p>

<hr>

<p>As for question <code>2</code>, pretty sure everything you can do with a callable is also possible by using an instance of type with <code>__call__</code> overridden accordingly. As to <em>why</em>, you might not want to go full blown class-creation if you simply want to make minor changes when actually creating a class.</p>
"
40030142,2867928.0,2016-10-13T20:14:46Z,40029807,7,"<p>Regarding your first question the metaclass should be <code>MyMetaclass</code> (which it's so):</p>

<pre><code>In [7]: print(type(MyClass), type(MyDerived))
&lt;class '__main__.MyMetaclass'&gt; &lt;class '__main__.MyMetaclass'&gt;
</code></pre>

<p>The reason is that if the metaclass is not an instance of type python calls the methaclass by passing these arguments to it <code>name, bases, ns, **kwds</code> (see <code>new_class</code>) and since you are returning your real metaclass in that function it gets the correct type for metaclass.</p>

<p>And about the second question:</p>

<blockquote>
  <p>What is the purpose of accepting an arbitrary callable?</p>
</blockquote>

<p>There is no special purpose, <strong>it's actually the nature of metaclasses</strong> which is because that making an instance from a class always calls the metaclass by calling it's <code>__call__</code> method:</p>

<pre><code>Metaclass.__call__()
</code></pre>

<p>Which means that you can pass any callable as your metaclass. So for example if you test it with a nested function the result will still be the same:</p>

<pre><code>In [21]: def metaclass_callable(name, bases, namespace):
             def inner():
                 return MyMetaclass(name, bases, namespace)
             return inner()
   ....: 

In [22]: class MyClass(metaclass=metaclass_callable):
             pass
   ....: 

In [23]: print(type(MyClass), type(MyDerived))
&lt;class '__main__.MyMetaclass'&gt; &lt;class '__main__.MyMetaclass'&gt;
</code></pre>

<hr>

<p>For more info here is how Python crates a class:</p>

<p>It calls the <code>new_class</code> function which it calls <code>prepare_class</code> inside itself, then as you can see inside the <code>prepare_class</code> python calls the <code>__prepare__</code> method of the appropriate metaclass, beside of finding the proper meta (using <code>_calculate_meta</code> function ) and creating the appropriate namespace for the class.</p>

<p>So all in one here is the hierarchy of executing a metacalss's methods:</p>

<ol>
<li><code>__prepare__</code> <sup>1</sup></li>
<li><code>__call__</code></li>
<li><code>__new__</code></li>
<li><code>__init__</code></li>
</ol>

<p>And here is the source code:</p>

<pre><code># Provide a PEP 3115 compliant mechanism for class creation
def new_class(name, bases=(), kwds=None, exec_body=None):
    """"""Create a class object dynamically using the appropriate metaclass.""""""
    meta, ns, kwds = prepare_class(name, bases, kwds)
    if exec_body is not None:
        exec_body(ns)
    return meta(name, bases, ns, **kwds)

def prepare_class(name, bases=(), kwds=None):
    """"""Call the __prepare__ method of the appropriate metaclass.

    Returns (metaclass, namespace, kwds) as a 3-tuple

    *metaclass* is the appropriate metaclass
    *namespace* is the prepared class namespace
    *kwds* is an updated copy of the passed in kwds argument with any
    'metaclass' entry removed. If no kwds argument is passed in, this will
    be an empty dict.
    """"""
    if kwds is None:
        kwds = {}
    else:
        kwds = dict(kwds) # Don't alter the provided mapping
    if 'metaclass' in kwds:
        meta = kwds.pop('metaclass')
    else:
        if bases:
            meta = type(bases[0])
        else:
            meta = type
    if isinstance(meta, type):
        # when meta is a type, we first determine the most-derived metaclass
        # instead of invoking the initial candidate directly
        meta = _calculate_meta(meta, bases)
    if hasattr(meta, '__prepare__'):
        ns = meta.__prepare__(name, bases, **kwds)
    else:
        ns = {}
    return meta, ns, kwds


def _calculate_meta(meta, bases):
    """"""Calculate the most derived metaclass.""""""
    winner = meta
    for base in bases:
        base_meta = type(base)
        if issubclass(winner, base_meta):
            continue
        if issubclass(base_meta, winner):
            winner = base_meta
            continue
        # else:
        raise TypeError(""metaclass conflict: ""
                        ""the metaclass of a derived class ""
                        ""must be a (non-strict) subclass ""
                        ""of the metaclasses of all its bases"")
    return winner
</code></pre>

<hr>

<p><sub>
1. Note that it get called implicitly inside the <em>new_class</em> function and before the return.
</sub></p>
"
40030475,5741205.0,2016-10-13T20:34:10Z,40030362,1,"<p>Is that what you want?</p>

<pre><code>In [52]: df = pd.DataFrame(np.arange(30).reshape(5,6), columns=list('abcdef'))

In [53]: df
Out[53]:
    a   b   c   d   e   f
0   0   1   2   3   4   5
1   6   7   8   9  10  11
2  12  13  14  15  16  17
3  18  19  20  21  22  23
4  24  25  26  27  28  29

In [54]: df[[0,2,4]]
Out[54]:
    a   c   e
0   0   2   4
1   6   8  10
2  12  14  16
3  18  20  22
4  24  26  28
</code></pre>

<p>concatenating (reshaping) columns <code>0</code>,<code>2</code>,<code>4</code> into single series:</p>

<pre><code>In [68]: df[[0,2,4]].values.T.reshape(-1,)
Out[68]: array([ 0,  6, 12, 18, 24,  2,  8, 14, 20, 26,  4, 10, 16, 22, 28])

In [69]: pd.Series(df[[0,2,4]].values.T.reshape(-1,))
Out[69]:
0      0
1      6
2     12
3     18
4     24
5      2
6      8
7     14
8     20
9     26
10     4
11    10
12    16
13    22
14    28
dtype: int32
</code></pre>
"
40030589,2901002.0,2016-10-13T20:41:37Z,40030362,0,"<p>I think you can convert all values to <code>lists</code> and then create <code>Series</code>, but then lost indices:</p>

<pre><code>df = pd.DataFrame({'A':[1,2,3],
                   'B':[4,5,6],
                   'C':[7,8,9],
                   'D':[1,3,5],
                   'E':[5,3,6],
                   'F':[7,4,3]})

print (df)
   A  B  C  D  E  F
0  1  4  7  1  5  7
1  2  5  8  3  3  4
2  3  6  9  5  6  3

row_count = 1

print (df.iloc[row_count, range (2, 4)])
C    8
D    3
Name: 1, dtype: int64

dfnow = pd.Series([df.iloc[row_count,0]]  + df.iloc[row_count, range (2, 4)].tolist())
print (dfnow)
0    2
1    8
2    3
dtype: int64
</code></pre>

<hr>

<p>Or you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow""><code>concat</code></a>, then indices are column names:</p>

<pre><code>row_count = 1

a = df.iloc[row_count, range (2, 4)]
b = df.iloc[row_count, range (4, 6)]

print (a)
C    8
D    3
Name: 1, dtype: int64

print (b)
E    3
F    4
Name: 1, dtype: int64

print (pd.concat([a,b]))
C    8
D    3
E    3
F    4
Name: 1, dtype: int64
</code></pre>

<p>But if need add scalar (<code>a</code>), it is a bit complicated - need <code>Series</code>:</p>

<pre><code>row_count = 1

a = pd.Series(df.iloc[row_count, 0], index=[df.columns[0]])
b = df.iloc[row_count, range (2, 4)]
c = df.iloc[row_count, range (4, 6)]

print (a)
A    2
dtype: int64

print (b)
C    8
D    3
Name: 1, dtype: int64

print (c)
E    3
F    4
Name: 1, dtype: int64

print (pd.concat([a,b,c]))
A    2
C    8
D    3
E    3
F    4
dtype: int64
</code></pre>
"
40030695,100297.0,2016-10-13T20:48:30Z,40028755,2,"<p>The <code>json</code> module is not really designed to give you that much control over the output; indentation is mostly meant to aid readability while debugging.</p>

<p>Instead of making <code>json</code> produce the output, you could <em>transform</em> the output using the standard library <a href=""https://docs.python.org/3/library/tokenize.html"" rel=""nofollow""><code>tokenize</code> module</a>:</p>

<pre><code>import tokenize
from io import BytesIO


def inline_special(json_data):
    def adjust(t, ld,):
        """"""Adjust token line number by offset""""""
        (sl, sc), (el, ec) = t.start, t.end
        return t._replace(start=(sl + ld, sc), end=(el + ld, ec))

    def transform():
        with BytesIO(json_data.encode('utf8')) as b:
            held = []  # to defer newline tokens
            lastend = None  # to track the end pos of the prev token
            loffset = 0     # line offset to adjust tokens by
            tokens = tokenize.tokenize(b.readline)
            for tok in tokens:
                if tok.type == tokenize.NL:
                    # hold newlines until we know there's no special key coming
                    held.append(adjust(tok, loffset))
                elif (tok.type == tokenize.STRING and
                        tok.string == '""$special""'):
                    # special string, collate tokens until the next rbrace
                    # held newlines are discarded, adjust the line offset
                    loffset -= len(held)
                    held = []
                    text = [tok.string]
                    while tok.exact_type != tokenize.RBRACE:
                        tok = next(tokens)
                        if tok.type != tokenize.NL:
                            text.append(tok.string)
                            if tok.string in ':,':
                                text.append(' ')
                        else:
                            loffset -= 1  # following lines all shift
                    line, col = lastend
                    text = ''.join(text)
                    endcol = col + len(text)
                    yield tokenize.TokenInfo(
                        tokenize.STRING, text, (line, col), (line, endcol),
                        '')
                    # adjust any remaining tokens on this line
                    while tok.type != tokenize.NL:
                        tok = next(tokens)
                        yield tok._replace(
                            start=(line, endcol),
                            end=(line, endcol + len(tok.string)))
                        endcol += len(tok.string)
                else:
                    # uninteresting token, yield any held newlines
                    if held:
                        yield from held
                        held = []
                    # adjust and remember last position
                    tok = adjust(tok, loffset)
                    lastend = tok.end
                    yield tok

    return tokenize.untokenize(transform()).decode('utf8')
</code></pre>

<p>This reformats your sample successfully:</p>

<pre><code>import json

data = {
    'x': [1, {'$special': 'a'}, 2],
    'y': {'$special': 'b'},
    'z': {'p': True, 'q': False}
}

&gt;&gt;&gt; print(inline_special(json.dumps(data, indent=2)))
{
  ""x"": [
    1,
    {""$special"": ""a""},
    2
  ],
  ""y"": {""$special"": ""b""},
  ""z"": {
    ""p"": true,
    ""q"": false
  }
}
</code></pre>
"
40030740,1461187.0,2016-10-13T20:50:36Z,40006395,0,"<p>What you are trying to is write a UDAF (User Defined Aggregate Function) as opposed to a UDF (User Defined Function).  UDAFs are functions that work on data grouped by a key.  Specifically they need to define how to merge multiple values in the group in a single partition, and then how to merge the results across partitions for key.  There is currently no way in python to implement a UDAF, they can only be implemented in scala.  </p>

<p>But, you can work around it in Python.  You can use collect set to gather your grouped values and then use a regular UDF to do what you want with them.  The only caveat is collect_set only works on primative values, so you will need to encode them down to a string.</p>

<pre><code>from pyspark.sql.types import StringType
from pyspark.sql.functions import col, collect_set, concat_ws, udf

def myFunc(data_list):
    for val in data_list:
        b, c = data.split(',')
        # do something

    return &lt;whatever&gt;

myUdf = udf(myFunc, StringType())

df.withColumn('data', concat_ws(',', col('B'), col('C'))) \
  .groupBy('A').agg(collect_list('data').alias('data'))
  .withColumn('data', myUdf('data'))
</code></pre>

<p>Use collect_set if you want deduping.  Also, if you have lots of values for some of your keys, this will be slow because all values for a key will need to be collected in a single partition somewhere on your cluster.  If your end result is a value you build by combining the values per key in some way (for example summing them) it might be faster to implement it using the <a href=""http://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.aggregateByKey"" rel=""nofollow"">RDD aggregateByKey</a> method which lets you build an intermediate value for each key in a partition before shuffling data around.</p>
"
40030794,1381627.0,2016-10-13T20:53:56Z,40028498,0,"<p>That will do the trick</p>

<pre><code>import collections

def create_enum(container, start_num, *enum_words):
    return collections.namedtuple(container, enum_words)(*range(start_num, start_num + len(enum_words)))

Switch = create_enum('enums', 1, 'On', 'Off')
</code></pre>

<p><em>Switch</em> is your enum:</p>

<pre><code>In [20]: Switch.On
Out[20]: 1

In [21]: Switch.Off
Out[21]: 2
</code></pre>

<p>OK, I got the error of my ways - I mixed up representation with value.</p>

<p>Nevertheless, if you want to enumerate a larger range - in my <strong>fake</strong> approach you don't have to add values manually. Of course, if you have sequential numbers.</p>

<p>And I hate extra typing :-)</p>
"
40031043,2336654.0,2016-10-13T21:09:08Z,40030362,1,"<p>consider the <code>df</code></p>

<pre><code>from string import ascii_uppercase
import pandas as pd
import numpy as np

df = pd.DataFrame(np.arange(150).reshape(-1, 15),
                  columns=list(ascii_uppercase[:15]))
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/84S4N.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/84S4N.png"" alt=""enter image description here""></a></p>

<p>use <code>np.r_</code> to construct the array neccesary for the slice you want</p>

<pre><code>np.r_[0, 8:14]

array([ 0,  8,  9, 10, 11, 12, 13])
</code></pre>

<p>then slice</p>

<pre><code>df.iloc[:, np.r_[0, 8:14]]
</code></pre>

<p><a href=""https://i.stack.imgur.com/RBwdB.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/RBwdB.png"" alt=""enter image description here""></a></p>
"
40031127,7014698.0,2016-10-13T21:15:49Z,40029807,2,"<p>Concerning question 1, I think the ""metaclass"" of a class <code>cls</code> should be understood as <code>type(cls)</code>. That way of understanding is compatible with Python's error message in the following example:</p>

<pre><code>&gt;&gt;&gt; class Meta1(type): pass
... 
&gt;&gt;&gt; class Meta2(type): pass
... 
&gt;&gt;&gt; def metafunc(name, bases, methods):
...     if methods.get('version') == 1:
...         return Meta1(name, bases, methods)
...     return Meta2(name, bases, methods)
... 
&gt;&gt;&gt; class C1:
...     __metaclass__ = metafunc
...     version = 1
... 
&gt;&gt;&gt; class C2:
...     __metaclass__ = metafunc
...     version = 2
... 
&gt;&gt;&gt; type(C1)
&lt;class '__main__.Meta1'&gt;
&gt;&gt;&gt; type(C2)
&lt;class '__main__.Meta2'&gt;
&gt;&gt;&gt; class C3(C1,C2): pass
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: Error when calling the metaclass bases
    metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases
</code></pre>

<p>I.e., according to the error message, the metaclass of a class is a class, even though the callable used to construct the class can be just anything.</p>

<p>Concerning the second question, indeed with a subclass of type used as a metaclass, you can do the same as with any other callable. In particular, it is possible that it yields something that is not its instance:</p>

<pre><code>&gt;&gt;&gt; class Mockup(type):
...     def __new__(cls, name, bases, methods):
...         return Meta1(name, bases, methods)
... 
&gt;&gt;&gt; class Foo:
...     __metaclass__ = Mockup
... 
&gt;&gt;&gt; type(Foo)
&lt;class '__main__.Meta1'&gt;
&gt;&gt;&gt; isinstance(Foo, Mockup)
False
&gt;&gt;&gt; Foo.__metaclass__
&lt;class '__main__.Mockup'&gt;
</code></pre>

<p>As to why Python gives the freedom of using any callable: The previous example shows that it is actually irrelevant whether the callable is a type or not.</p>

<p>BTW, here is a fun example: It is possible to code metaclasses that, themselves, have a metaclass different from <code>type</code>---let's call it a metametaclass. The metametaclass implements what happens when a metaclass is called. In that way, it is possible to create a class with two bases whose metaclasses are <em>not</em> subclass of each other (compare with Python's error message in the example above!). Indeed, only the metaclass of the resulting class is subclass of the metaclass of the bases, and this metaclass is created on the fly:</p>

<pre><code>&gt;&gt;&gt; class MetaMeta(type):
...     def __call__(mcls, name, bases, methods):
...         metabases = set(type(X) for X in bases)
...         metabases.add(mcls)
...         if len(metabases) &gt; 1:
...             mcls = type(''.join([X.__name__ for X in metabases]), tuple(metabases), {})
...         return mcls.__new__(mcls, name, bases, methods)
... 
&gt;&gt;&gt; class Meta1(type):
...     __metaclass__ = MetaMeta
... 
&gt;&gt;&gt; class Meta2(type):
...     __metaclass__ = MetaMeta
... 
&gt;&gt;&gt; class C1:
...     __metaclass__ = Meta1
... 
&gt;&gt;&gt; class C2:
...     __metaclass__ = Meta2
... 
&gt;&gt;&gt; type(C1)
&lt;class '__main__.Meta1'&gt;
&gt;&gt;&gt; type(C2)
&lt;class '__main__.Meta2'&gt;
&gt;&gt;&gt; class C3(C1,C2): pass
... 
&gt;&gt;&gt; type(C3)
&lt;class '__main__.Meta1Meta2'&gt;
</code></pre>

<p>What is less fun: The preceding example won't work in Python 3. If I understand correctly, Python 2 creates the class and checks whether its metaclass is a subclass of all its bases, whereas Python 3 <em>first</em> checks whether there is one base whose metaclass is superclass of the metaclasses of all other bases, and only <em>then</em> creates the new class. That's a regression, from my point of view. But that shall be the topic of a new question that I am about to post...</p>

<p><strong>Edit</strong>: The new question is <a href=""http://stackoverflow.com/questions/40031906/is-it-possible-to-dynamically-create-a-metaclass-for-a-class-with-several-bases"">here</a></p>
"
40031225,3293881.0,2016-10-13T21:22:32Z,40030448,2,"<p><strong>Approach #1 :</strong> Memory permitting, here's a vectorized approach using <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasting</code></a> -</p>

<pre><code># Craate +,-2 limits usind ind
r = x[ind[:,None]] + [-2,2]

# Use limits to get inside matches and sum over the iterative and last dim
mask = (refVals &gt;= r[:,None,None,None,0]) &amp; (refVals &lt;= r[:,None,None,None,1])
out = mask.sum(axis=(0,3))
</code></pre>

<hr>

<p><strong>Approach #2 :</strong> If running out of memory with the previous one, we could use a loop and use NumPy boolean arrays and that could be more efficient than masked arrays. Also, we would perform one more level of <code>sum-reduction</code>, so that we would be dragging less data with us when moving across iterations. Thus, the alternative implementation would look something like this -</p>

<pre><code>out = np.zeros(refVals.shape[:2]).astype(np.int16)
x_ind = x[ind]
for i in x_ind:
    out += ((refVals &gt;= i-2) &amp; (refVals &lt;= i+2)).sum(-1)
</code></pre>

<p><strong>Approach #3 :</strong> Alternatively, we could replace that limit based comparison with <a href=""http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.isclose.html"" rel=""nofollow""><code>np.isclose</code></a> in approach #2. Thus, the only step inside the loop would become -</p>

<pre><code>out += np.isclose(refVals,i,atol=2).sum(-1)
</code></pre>
"
40031676,1634191.0,2016-10-13T21:57:00Z,40031569,2,"<p>First, convert the dates to datetimes, then use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html"" rel=""nofollow""><code>DataFrame.diff</code></a>:</p>

<pre><code>df.date = pd.to_datetime(df.date)
df.date.diff()
</code></pre>

<p>yields:</p>

<pre><code>0                 NaT
1     0 days 00:00:00
2     0 days 01:15:38
3     1 days 00:00:00
4     1 days 00:00:00
5     3 days 00:00:00
6   -1 days +00:00:00
7   -4 days +00:00:00
Name: date, dtype: timedelta64[ns]
</code></pre>

<p>If you want the average, you can do something like</p>

<pre><code>df.date.diff().mean() # or possibly df.date.diff().abs().mean()
# Timedelta('0 days 00:10:48.285714')
</code></pre>
"
40032153,748858.0,2016-10-13T22:36:39Z,40031906,0,"<p>Here's an example that shows some options that you have in python3.x.  Specifically, <code>C3</code> has a metaclass that is created dynamically, but in a lot of ways, explicitly.  <code>C4</code> has a metaclass that is created dynamically within it's metaclass <em>function</em>.  <code>C5</code> is just to demonstrate that it too has the same metaclass properies that <code>C4</code> has.  (We didn't lose anything through inheritance which <em>can</em> happen if you use a function as a metaclass instead of a <code>type</code> ...)</p>

<pre><code>class Meta1(type):
    def foo(cls):
        print(cls)


class Meta2(type):
    def bar(cls):
        print(cls)


class C1(object, metaclass=Meta1):
    """"""C1""""""


class C2(object, metaclass=Meta2):
    """"""C2""""""


class C3(C1, C2, metaclass=type('Meta3', (Meta1, Meta2), {})):
    """"""C3""""""

def meta_mixer(name, bases, dct):
    cls = type('MixedMeta', tuple(type(b) for b in bases), dct)
    return cls(name, bases, dct)


class C4(C1, C2, metaclass=meta_mixer):
    """"""C4""""""


C1.foo()
C2.bar()

C3.foo()
C3.bar()

C4.foo()
C4.bar()

class C5(C4):
    """"""C5""""""

C5.foo()
C5.bar()
</code></pre>

<p>It should be noted that we're playing with fire here (the same way that you're playing with fire in your original example).  There is no guarantee that the metaclasses will play nicely in cooperative multiple inheritance.  If they weren't designed for it, chances are that you'll run into bugs using this at <em>some</em> point.  If they <em>were</em> designed for it, there'd be no reason to be doing this hacky runtime mixing :-).</p>
"
40032158,6352803.0,2016-10-13T22:37:13Z,40031406,0,"<p>This Python code gives me this output: <code>['organic cane sugar', 'whole-wheat flour', 'mono &amp; diglycerides']</code>
It requires that the ingredients come after ""Ingredients: "" and that all ingredients are listed before a ""."", as in your case.</p>

<pre><code>import re
text = """"""Ingredients: organic cane sugar, whole-wheat flour,
   mono &amp; diglycerides. Manufactured in a facility that uses nuts.""""""

# Search everything that comes after 'Ingredients: ' and before '.'
m = re.search('(?&lt;=Ingredients: ).+?(?=\.)', text, re.DOTALL) # DOTALL: make . match newlines too
items = m.group(0).replace('\n', ' ').split(',') # Turn newlines into   spaces, make a list of items separated by ','
items = [ i.strip() for i in items ] # Remove leading whitespace in each item
print items
</code></pre>
"
40032300,108205.0,2016-10-13T22:53:02Z,40031906,1,"<p>In Python3  at the time the metaclass is used it have to be ready, and it can't  know about the bases of the final (non-meta) class in order to dynamically create a metaclass at that point. </p>

<p>But ynstead of complicating things (I confess I could not wrap my head around your need for a meta-meta-class) - you can simply use normal class hierarchy with collaborative use of <code>super</code> for your metaclasses.
You can even build the final metaclass dynamically with a simple
call to <code>type</code>: </p>

<pre><code>class A(type):
    def __new__(metacls, name, bases,attrs):
        attrs['A'] = ""Metaclass A processed""
        return super().__new__(metacls, name, bases,attrs)


class B(type):
    def __new__(metacls, name, bases,attrs):
        attrs['B'] = ""Metaclass A processed""
        return super().__new__(metacls, name, bases,attrs)


C = type(""C"", (A, B), {})

class Example(metaclass=C): pass
</code></pre>

<p>And:</p>

<pre><code>In[47] :Example.A
Out[47]: 'Metaclass A processed'

In[48]: Example.B
Out[48]: 'Metaclass A processed'
</code></pre>

<p>If your metaclasses are not designed to be collaborative in the first place, it will be very tricky to create any automatic method to combine them - and it would possibly involve monkey-patching the call to <code>type.__new__</code> in some of the metaclasses constructors. </p>

<p>As for not needing to explictly build <code>C</code>, you can use a normal function as the metaclass parameter, that will inspect the bases and build a dynamic derived metaclass. :</p>

<pre><code>def Auto(name, bases, attrs):
    basemetaclasses = []
    for base in bases:
        metacls = type(base)
        if isinstance(metacls, type) and metacls is not type and not metacls in basemetaclasses:
            basemetaclasses.append(metacls)
    dynamic = type(''.join(b.__name__ for b in basemetaclasses), tuple(basemetaclasses), {})
    return dynamic(name, bases, attrs)
</code></pre>

<p>(This code is very similar to yours - but I used a three-line explicit <code>for</code> instead of a <code>set</code> in order to preserve the metaclass order - which might matter)</p>

<p>You have them to pass Auto as an metaclass for derived classes, but otherwise it works as in your example:</p>

<pre><code>In [61]: class AA(metaclass=A):pass

In [62]: class BB(metaclass=B):pass

In [63]: class CC(AA,BB): pass
---------------------------------------------------------------------------
...
TypeError:   metaclass conflict
...

In [66]: class CC(AA,BB, metaclass=Auto): pass

In [67]: type(CC)
Out[67]: __main__.AB

In [68]: CC.A
Out[68]: 'Metaclass A processed'
</code></pre>
"
40032361,2747160.0,2016-10-13T22:58:45Z,39996295,1,"<p>This started as a joke answer, but I've learned something since - so I'll post it.</p>

<p>Assume, we know the maximum length of an option allowed. Here is a nice answer to the question in this situation:</p>

<pre><code>from itertools import combinations

def parsable(option):
    try:
        return len(parser.parse_known_args(option.split())[1]) != 2
    except:
        return False

def test(tester, option):
    return any([tester(str(option) + ' ' + str(v)) for v in ['0', '0.0']])

def allowed_options(parser, max_len=3, min_len=1):
    acceptable = []
    for l in range(min_len, max_len + 1):
        for option in combinations([c for c in [chr(i) for i in range(33, 127)] if c != '-'], l):
            option = ''.join(option)
            acceptable += [p + option for p in ['-', '--'] if test(parsable, p + option)]
    return acceptable
</code></pre>

<p>Of course this is very pedantic as the question doesn't require any specific runtime. So I'll ignore that here. I'll also disregard, that the above version produces a mess of output because <a href=""http://stackoverflow.com/a/2829036/2747160"">one can get rid of it easily</a>.</p>

<p>But more importantly, this method detected the following interesting <code>argparse</code> ""features"":</p>

<ul>
<li>In in the OP example, <code>argparse</code> would also allow <code>--fo</code>. This has to be a bug.</li>
<li>But further, in the OP example again, <code>argparse</code> would also allow <code>-fo</code> (ie. setting <code>foo</code> to <code>o</code> without space or anything). This is documented and intended, but I didn't know it.</li>
</ul>

<p>Because of this, a correct solution is a bit longer and would look something like this (only <code>parsable</code> changes, I'll omit the other methods):</p>

<pre><code>def parsable(option):
    try:
        default = vars(parser.parse_known_args(['--' + '0' * 200])[0])
        parsed, remaining = parser.parse_known_args(option.split())
        if len(remaining)  == 2:
            return False
        parsed = vars(parsed)
        for k in parsed.keys():
            try:
                if k in default and default[k] != parsed[k] and float(parsed[k]) != 0.0:
                    return False  # Filter '-fx' cases where '-f' is the argument and 'x' the value.
            except:
                return False
        return True
    except:
        return False
</code></pre>

<p><strong>Summary</strong>: Besides all the restrictions (runtime and fixed maximum option length), this is the only answer that correctly respects the real <code>parser</code> behavior - however buggy it may even be. So here you are, a perfect answer that is absolutely useless.</p>
"
40032433,2422776.0,2016-10-13T23:05:38Z,40032408,2,"<p><code>=</code> is the assignment operator. For equality checks, you should use the <code>==</code> operator:</p>

<pre><code>if is_prime == True:
</code></pre>

<p>Or better yet, since <code>is_prime</code> is a boolean expression in its own right, just evaluate it:</p>

<pre><code>if is_prime:
</code></pre>
"
40032646,4974980.0,2016-10-13T23:28:00Z,40031724,0,"<p>For <code>render()</code>, you can pass context as a plain dictionary. If you're going to use a keyword argument, use <code>context</code> not <code>context_instance</code> <a href=""https://docs.djangoproject.com/en/1.10/topics/http/shortcuts/#render"" rel=""nofollow"">(docs)</a>. That's why you're getting <code>render() got an unexpected keyword argument 'context_instance'</code>:</p>

<pre><code>def home(request):
    """"""Renders the home page.""""""

    assert isinstance(request, HttpRequest)
    return render(request, 'app/index.html', context={'title':'Home Page', 'year':datetime.now().year})
</code></pre>
"
40032656,953863.0,2016-10-13T23:29:15Z,40032219,2,"<p>This an issue of scope. By default Python variables are local in scope. In servy, where you set inez to a new value, Python assumes this is a new local variable because you have not specifically declared it to be global. Therefore when you call serx the second time, the global variable inez is unchanged. Here is a simpler example to illustrate the issue.</p>

<pre><code>a = ""hello""

def b():
    print(a)

def c():
    a = ""world""
    print(a)
    b()

b()
c()
</code></pre>

<p>That is a nasty bug that has tripped me up many times. One of the great reasons to avoid global variables if at all possible.</p>

<p>There are other issues with the above code, such as using recursion where a loop should probably be used. I would suggest reading about python's scoping rules (<a href=""http://stackoverflow.com/questions/291978/short-description-of-scoping-rules"">Short Description of Scoping Rules</a>), try restructuring to avoid recursion and then posting your code on <a href=""http://codereview.stackexchange.com"">http://codereview.stackexchange.com</a>.</p>
"
40032792,3832970.0,2016-10-13T23:46:57Z,40032632,2,"<p>Use a raw string literal:</p>

<pre><code>re.search(r'''
            (?&lt;=Changed\spaths:\n)  
            (?:\s{3}[AMD]\s.*\n)*
            (?=\n)    
            ''', revision, re.VERBOSE)
</code></pre>

<p>See this fixed <a href=""http://ideone.com/xlYIMF"" rel=""nofollow"">Python demo</a>. </p>

<p>The main issue is that you have to pass it as a raw string literal, or use <code>\\n</code> instead of <code>\n</code>. Otherwise, <code>\n</code> (being a literal newline) is ignored inside the regex pattern, is treated as formatting whitespace (read more about that in the <a href=""https://docs.python.org/2/library/re.html#re.VERBOSE"" rel=""nofollow"">Python <code>re</code> docs</a>).</p>

<p>Also, note you corrupted the lookahead by enclosing it with <code>[...]</code> (it became a character class part) and the <code>|</code> inside character classes are treated as literal pipes (thus, here, they should be removed).</p>
"
40032877,99989.0,2016-10-13T23:57:28Z,40031906,0,"<p>In 95% of cases, it should be possible to use the machinery introduced in Python 3.6 due to <a href=""https://www.python.org/dev/peps/pep-0447"" rel=""nofollow"">PEP 447</a> to do <em>most</em> of what metaclasses can do using special new hooks.  In that case, you will not need to combine metaclasses since your hooks can call super and their behavior is combined due to inheritance.</p>

<p>As for your general case, I believe that mgilson is right and that you are probably making things too complicated.  I have yet to see a case for combining metaclasses that is not covered by PEP 447.</p>
"
40032915,1394353.0,2016-10-14T00:02:17Z,40023663,0,"<p>Specifically for this, I would use partials/currying, basically pre-filling a variable.</p>

<pre><code>import sys
from functools import partial
import datetime

def _cmd_exec(cmd, isDebug=False):
    if isDebug:
        command_start = datetime.datetime.now()
        print command_start
        print cmd

    else:
        print 'isDebug is false' + cmd

    if isDebug:
        print datetime.datetime.now() - command_start
    return

#default, keeping it as is...
cmd_exec = _cmd_exec

#switch to debug
def debug_on():
    global cmd_exec

    #pre-apply the isDebug optional param
    cmd_exec = partial(_cmd_exec, isDebug=True)


def main():
    if ""-d"" in sys.argv:
        debug_on()

    cmd_exec(""cmd1"")
    cmd_exec(""cmd2"")

main()
</code></pre>

<p>In this case, I check for <code>-d</code> on the command line to turn on debug mode and I do pre-populate isDebug on the function call by creating a new function with <code>isDebug = True</code>.</p>

<p>I think even other modules will see this modified cmd_exec, because I replaced the function at the module level.</p>

<p>output:</p>

<p><strong>jluc@explore$ py test_so64.py</strong><br>
<code>isDebug is falsecmd1
isDebug is falsecmd2</code></p>

<p><strong>jluc@explore$ py test_so64.py -d</strong><br>
<code>2016-10-13 17:00:33.523016
cmd1
0:00:00.000682
2016-10-13 17:00:33.523715
cmd2
0:00:00.000009</code></p>
"
40032985,5847976.0,2016-10-14T00:11:08Z,40032909,0,"<p>I've just added prints between triangles to reproduce the expected output. Is that what you want ?</p>

<pre><code>base=int(input(""Enter the triangle size: ""))
for i in range(1, base + 1):
    print (('*' * i) + (' ' * (base - i)))
print()
for i in range(1, base + 1)[::-1]:
    print (('*' * i) + (' ' * (base - i)))
for i in range(1, base + 1):
    print (' ' * (base - i) + ('*' * i))
print()
for i in range(1, base + 1)[::-1]:
    print (' ' * (base - i) + ('*' * i))
</code></pre>

<p>Output:</p>

<pre><code>Enter the triangle size: 4
*   
**  
*** 
****

****
*** 
**  
*   
   *
  **
 ***
****

****
 ***
  **
   *
</code></pre>
"
40033097,1397919.0,2016-10-14T00:25:57Z,40033066,2,"<p>You can set up the script to run via <code>cron</code>, configuring time as <code>@reboot</code></p>

<p>With python scripts, you will not need to compile it. You might need to install it, depending on what assumptions your script makes about its environment. </p>
"
40033424,795325.0,2016-10-14T01:12:14Z,40022982,0,"<p>To rephrase your question, are you asking how to pre-populate the form fields? If so consider using <a href=""https://docs.djangoproject.com/en/dev/topics/forms/formsets/#using-initial-data-with-a-formset"" rel=""nofollow"">initial values in your formsets</a></p>
"
40033661,190597.0,2016-10-14T01:44:17Z,40033471,1,"<p>You could get this UserWarning if <code>df_masked</code> is a sub-DataFrame of some other DataFrame.
In particular, if data had been <em>copied</em> from the original DataFrame to <code>df_masked</code> then, Pandas emits the UserWarning to alert you that modifying <code>df_masked</code> will not affect the original DataFrame. </p>

<p>If you do not intend to modify the original DataFrame, then you are free to ignore the UserWarning.</p>

<p><a href=""http://stackoverflow.com/a/38810015/190597"">There are ways</a> to shut off the UserWarning on a per-statement basis. In particular, you could use <code>df_masked.is_copy = False</code>. </p>

<p>If you run into this UserWarning a lot, then instead of silencing the UserWarnings one-by-one, I think it is better to leave them be as you are developing your code. Be aware of what the UserWarning means, and if the modifying-the-child-does-not-affect-the-parent issue does not affect you, then ignore it. When your code is ready for production, or if you are experienced enough to not need the warnings, shut them off entirely with</p>

<pre><code>pd.options.mode.chained_assignment = None
</code></pre>

<p>near the top of your code.</p>

<hr>

<p>Here is a simple example which demonstrate the problem and (a) solution:</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'swallow':['African','European'], 'cheese':['gouda', 'cheddar']})
df_masked = df.iloc[1:]
df_masked.is_copy = False   # comment-out this line to see the UserWarning
df_masked.loc[:, 'swallow'] = 'forest'
</code></pre>

<hr>

<p>The reason why the UserWarning exists is to help alert new users to the fact that
<a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#why-does-assignment-fail-when-using-chained-indexing"" rel=""nofollow"">chained-indexing</a> such as</p>

<pre><code>df.iloc[1:].loc[:, 'swallow'] = 'forest'
</code></pre>

<p>will not affect <code>df</code> when the result of the first indexer (e.g. <code>df.iloc[1:]</code>)
returns a copy. </p>
"
40034280,190597.0,2016-10-14T03:06:01Z,40033948,1,"<p>I found this by mimicking the code in <a href=""https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/patches.py#L1125"" rel=""nofollow""><code>patches.Arrow.__init__</code></a>:</p>

<pre><code>import numpy as np
import matplotlib.patches as patches
from matplotlib import pyplot as plt
from matplotlib import animation
import matplotlib.transforms as mtransforms

# Create figure
fig, ax = plt.subplots()

# Axes labels and title are established
ax.set_xlabel('x')
ax.set_ylabel('y')

ax.set_ylim(-2,2)
ax.set_xlim(-2,2)
ax.set_aspect('equal', adjustable='box')

N = 20
x = np.linspace(-1,1,N) 
y  = np.linspace(-1,1,N) 
dx = np.sin(x)
dy = np.cos(y)

patch = patches.Arrow(x[0], y[0], dx[0], dy[0])

def init():
    ax.add_patch(patch)
    return patch,

def animate(t):
    L = np.hypot(dx[t], dy[t])

    if L != 0:
        cx = float(dx[t]) / L
        sx = float(dy[t]) / L
    else:
        # Account for division by zero
        cx, sx = 0, 1

    trans1 = mtransforms.Affine2D().scale(L, 1)
    trans2 = mtransforms.Affine2D.from_values(cx, sx, -sx, cx, 0.0, 0.0)
    trans3 = mtransforms.Affine2D().translate(x[t], y[t])
    trans = trans1 + trans2 + trans3
    patch._patch_transform = trans.frozen()
    return patch,

anim = animation.FuncAnimation(fig, animate, 
                               init_func=init, 
                               interval=20,
                               frames=N,
                               blit=False)

plt.show()
</code></pre>
"
40034308,1832058.0,2016-10-14T03:08:27Z,40033948,2,"<p>Add <code>ax.clear()</code> before <code>ax.add_patch(patch)</code> but will remove all elements from plot.</p>

<pre><code>def animate(t):

    ax.clear() 

    patch = plt.Arrow(x[t], y[t], dx[t], dy[t] )
    ax.add_patch(patch)

    return patch,
</code></pre>

<hr>

<p><strong>EDIT:</strong> removing one patch</p>

<ul>
<li><p>using <code>ax.patches.pop(index)</code>. </p>

<p>In your example is only one patch so you can use <code>index=0</code></p>

<pre><code>def animate(t):

    ax.patches.pop(0) 

    patch = plt.Arrow(x[t], y[t], dx[t], dy[t] )
    ax.add_patch(patch)

    return patch,
</code></pre></li>
<li><p>using <code>ax.patches.remove(object)</code></p>

<p>It needs <code>global</code> to get/set external <code>patch</code> with <code>Arrow</code></p>

<pre><code>def animate(t):

    global patch

    ax.patches.remove(patch) 

    patch = plt.Arrow(x[t], y[t], dx[t], dy[t] )
    ax.add_patch(patch)

    return patch,
</code></pre></li>
</ul>

<hr>

<p><strong>BTW:</strong> to get list of properties which you can use with <code>update()</code></p>

<pre><code>print( patch.properties().keys() )

dict_keys(['aa', 'clip_path', 'patch_transform', 'edgecolor', 'path', 'verts', 'rasterized', 'linestyle', 'transform', 'picker', 'capstyle', 'children', 'antialiased', 'sketch_params', 'contains', 'snap', 'extents', 'figure', 'gid', 'zorder', 'transformed_clip_path_and_affine', 'clip_on', 'data_transform', 'alpha', 'hatch', 'axes', 'lw', 'path_effects', 'visible', 'label', 'ls', 'linewidth', 'agg_filter', 'ec', 'facecolor', 'fc', 'window_extent', 'animated', 'url', 'clip_box', 'joinstyle', 'fill'])
</code></pre>

<p>so you can use <code>update</code> to change color - `facecolor</p>

<pre><code>def animate(t):
    global patch

    t %= 20 # get only 0-19 to loop animation and get color t/20 as 0.0-1.0

    ax.patches.remove(patch)

    patch = patches.Arrow(x[t], y[t], dx[t], dy[t])

    patch.update({'facecolor': (t/20,t/20,t/20,1.0)})

    ax.add_patch(patch)

    return patch,
</code></pre>
"
40035425,2901002.0,2016-10-14T05:19:33Z,40035276,1,"<p>I think the easiest is to assign <code>list</code> of <code>lists</code>, only you need same length of <code>lists</code> as <code>length</code> of <code>DataFrame</code>:</p>

<pre><code>arr = [[0.2, 0.34, 0.45, 0.28], [0.12, 0.3, 0.41, 0.39]]
print (arr)
[[0.2, 0.34, 0.45, 0.28], [0.12, 0.3, 0.41, 0.39]]

print (len(arr))
2
print (len(df))
2

df[""'Rec'""] = arr
print (df)
   'Location'     'Rec  ID'  'Duration'                    'Rec'
0           0  Houston  126        17.0  [0.2, 0.34, 0.45, 0.28]
1           1  Chicago  338        19.3  [0.12, 0.3, 0.41, 0.39]
</code></pre>

<hr>

<p>If use <code>numpy array</code>, first convert <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tolist.html"" rel=""nofollow""><code>tolist</code></a>:</p>

<pre><code>arr = np.array([[0.2, 0.34, 0.45, 0.28], [0.12, 0.3, 0.41, 0.39]])
print (arr)
[[ 0.2   0.34  0.45  0.28]
 [ 0.12  0.3   0.41  0.39]]

df[""'Rec'""] = arr.tolist()

print (df)
   'Location'     'Rec  ID'  'Duration'                    'Rec'
0           0  Houston  126        17.0  [0.2, 0.34, 0.45, 0.28]
1           1  Chicago  338        19.3  [0.12, 0.3, 0.41, 0.39]
</code></pre>
"
40035491,2598279.0,2016-10-14T05:24:50Z,40031287,0,"<p>If you mean matrix multiplication by rotation.</p>

<p>You can convert both to numpy arrays and perform it as</p>

<pre><code>lh = largest_haloes.values
rotated_array = lh.dot(rot)
</code></pre>

<p>You can also do</p>

<pre><code>x = pd.DataFrame(data=rot,index=['X','Y','Z'])
rotated_df = largest_haloes.dot(x)
</code></pre>
"
40035729,596041.0,2016-10-14T05:44:19Z,40034950,0,"<p>Your problem is caused by the fact that python dictionaries are unordered. Try using a <a href=""https://docs.python.org/2/library/collections.html#collections.OrderedDict"" rel=""nofollow"">OrderedDict</a> instead of <code>dict</code> and you should be fine. The OrderedDict works just like a normal <code>dict</code> but with ordering retained, at a small performance cost.</p>

<p>Note that while you could create an OrderedDict from a dict literal (like I did here at first), that dict would be unordered, so the ordering might not be guaranteed. Using a list of <code>(key, value)</code> pairs preserves the ordering in all cases.</p>

<pre><code>from collections import OrderedDict

test1=OrderedDict([('integer_set', '{#integer_list#?}'), ('integer_list', '#integer_range#(?,#integer_range#)*'), ('integer_range', '#integer#(..#integer#)?'), ('integer', '[+-]?\\d+')])
test2=OrderedDict([('b', '#a#'), ('f', '#e#'), ('c', '#b#'), ('e', '#d#'), ('d', '#c#'), ('g', '#f#'), ('a', 'correct')])
def change(pat_dict:{str:str}):
    print('Expanding: ',pat_dict)
    num=0
    while num&lt;len(pat_dict):
        inv_pat_dict = {v: k for k, v in pat_dict.items()}
        for value in pat_dict.values():
            for key in pat_dict.keys():
                if key in value:
                    repl='#'+key+'#'
                    repl2='('+pat_dict[key]+')'
                    value0=value.replace(repl,repl2)
                    pat_dict[inv_pat_dict[value]]=value0
        num+=1
    print('Result: ',pat_dict)

change(test1)
change(test2)
</code></pre>
"
40035870,3627387.0,2016-10-14T05:55:36Z,40034950,0,"<p>Try this one. Your problem is due to mutating starting dict. You need to change its copy.</p>

<pre><code>test1={'integer_set': '{#integer_list#?}', 'integer_list': '#integer_range#(?,#integer_range#)*', 'integer_range': '#integer#(..#integer#)?', 'integer': '[+-]?\\d+'}
test2={'b': '#a#', 'f': '#e#', 'c': '#b#', 'e': '#d#', 'd': '#c#', 'g': '#f#', 'a': 'correct'}
def change(d):
    new_d = d.copy()
    for k in d.keys():
        for nk, v in new_d.items():
            if k in v:
                new_d[nk] = v.replace('#{}#'.format(k), '({})'.format(new_d[k]))
    return new_d

test1 = change(test1)
test2 = change(test2)
</code></pre>
"
40039137,7018208.0,2016-10-14T09:02:27Z,40034570,3,"<p>I got the same question.
Try to follow the [official instruction] to install tensorflow: <a href=""https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation</a></p>

<pre><code># Mac OS X
$ sudo easy_install pip
$ sudo easy_install --upgrade six
</code></pre>

<p>Then, select the correct binary to install:</p>

<pre><code># Ubuntu/Linux 64-bit, CPU only, Python 2.7
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl

# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7
# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see ""Install from sources"" below.
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl

# Mac OS X, CPU only, Python 2.7:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl

# Mac OS X, GPU enabled, Python 2.7:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0rc0-py2-none-any.whl

# Ubuntu/Linux 64-bit, CPU only, Python 3.4
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl

# Ubuntu/Linux 64-bit, GPU enabled, Python 3.4
# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see ""Install from sources"" below.
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp34-cp34m-linux_x86_64.whl

# Ubuntu/Linux 64-bit, CPU only, Python 3.5
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl

# Ubuntu/Linux 64-bit, GPU enabled, Python 3.5
# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see ""Install from sources"" below.
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl

# Mac OS X, CPU only, Python 3.4 or 3.5:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py3-none-any.whl

# Mac OS X, GPU enabled, Python 3.4 or 3.5:
$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0rc0-py3-none-any.whl
</code></pre>

<p>Install TensorFlow:</p>

<pre><code># Python 2
$ sudo pip install --upgrade $TF_BINARY_URL

# Python 3
$ sudo pip3 install --upgrade $TF_BINARY_URL
</code></pre>
"
40039924,1145666.0,2016-10-14T09:39:53Z,40039516,1,"<p>Apparently, a <code>datetime</code> object does not abide by the format specifier rules. When cast to a <code>str</code>, it works:</p>

<pre><code>print ""{d: &lt;12}"".format(str(datetime.datetime.now()))
</code></pre>
"
40040701,3728414.0,2016-10-14T10:16:40Z,40040453,2,"<p>You just have to split your lines into words, store them somewhere, and then, after having read all of your files and stored their words, pick 100 with <code>random.sample</code>. It it what I did in the code below. However, I am not quite sure if it is able to deal with 170 novels, since it will likely result in a lot of memory usage.</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)
words = []

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             for line in f:
                 for word in line.split():
                     words.append(word)

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 

random_sample_input = random.sample(words, 100)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>In the above code, the more words a novel has, the more likely is to be represented in the output sample. That may or may not be the desired behaviour. If you want each novel to have the same ponderation, you can select, let's say, 100 words from it to add in the <code>words</code> variable, and then select 100 hundred words from there at the end. It will also have the side effect of using a lot less memory, since only one novel will be stored at a time.</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)
words = []

for text in files:
    try:
        novel = []
        with open(text, 'rt', encoding='utf-8') as f:
             # number of lines from txt file
             for line in f:
                 for word in line.split():
                     novel.append(word)
             words.append(random.sample(novel, 100))


    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 


random_sample_input = random.sample(words, 100)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8') 
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>

<p>Third version, this one will deal with sentences instead of words, and keep the punctuation. Also, each book has the same ""weight"" on the final sentences kept, regardless of its size. Keep in mind that the sentence detection is done by an algorithm that is quite clever, but not infallible.</p>

<pre><code>import random
import os
import glob
import sys
import errno
import nltk.data

path = '/home/clement/Documents/randomPythonScripts/data/*.txt'
files = glob.glob(path)

sentence_detector = nltk.data.load('tokenizers/punkt/dutch.pickle')
listOfSentences = []

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
            fullText = f.read()
        listOfSentences += [x.replace(""\n"", "" "").replace(""  "","" "").strip() for x in random.sample(sentence_detector.tokenize(fullText), 30)]

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR:
            raise

random_sample_input = random.sample(listOfSentences, 15)
print(random_sample_input)

# This block of code writes the result of the previous to a new file
random_sample_output = open(""randomsample"", ""w"", encoding='utf-8')
random_sample_input = map(lambda x: x+""\n"", random_sample_input)
random_sample_output.writelines(random_sample_input)
random_sample_output.close()
</code></pre>
"
40040710,5199146.0,2016-10-14T10:17:07Z,40025238,0,"<p>As suggested in a comment: the other answers are valid, but one of the fundamental problems is that your examples only work for 'simple' scripts or files: A lot of more complex code will use things like dynamic imports: consider the following:</p>

<pre><code>path, task_name = ""module.function"".rsplit(""."", 1);
module = importlib.import_module(path);
real_func = getattr(module, task_name); 
real_func();
</code></pre>

<p>The actual original string could be obfuscated, or pulled from a DB, or a file or...</p>

<p>There are alternatives to importlib, but this is on top of the exec type stuff you might see in @horia's good answer.</p>
"
40040763,1859772.0,2016-10-14T10:19:26Z,40040453,3,"<p>A few suggestions:</p>

<p>Take random sentences, not words or lines. NE taggers will work much better if input is grammatical sentences. So you need to use a sentence splitter.</p>

<p>When you iterate over the files, <code>random_sample_input</code> contains lines from only the last file. You should move the block of code that writes the selected content to a file inside the for-loop. You can then write the selected sentences to either one file or into separate files. E.g.:</p>

<pre><code>out = open(""selected-sentences.txt"", ""w"")

for text in files:
    try:
        with open(text, 'rt', encoding='utf-8') as f:
             sentences = sentence_splitter.split(f.read())
             for sentence in random.sample(sentences, 100):
                 print &gt;&gt; out, sentence

    except IOError as exc:
    # Do not fail if a directory is found, just ignore it.
        if exc.errno != errno.EISDIR: 
            raise 

out.close()
</code></pre>

<p>[edit] Here is how you should be able to use an NLTK sentence splitter:</p>

<pre><code>import nltk.data
sentence_splitter = nltk.data.load(""tokenizers/punkt/dutch.pickle"")
text = ""Dit is de eerste zin. Dit is de tweede zin.""
print sentence_splitter.tokenize(text)
</code></pre>

<p>Prints:</p>

<pre><code>[""Dit is de eerste zin."", ""Dit is de tweede zin.""]
</code></pre>

<p>Note you'd need to download the Dutch tokenizer first, using <code>nltk.download()</code> from the interactive console.</p>
"
40040953,1643661.0,2016-10-14T10:28:43Z,40040453,1,"<p>This solves both problems:</p>

<pre><code>import random
import os
import glob
import sys
import errno

path = '/Users/roelsmeets/Desktop/libris_corpus_clean/*.txt'
files = glob.glob(path)

with open(""randomsample"", ""w"", encoding='utf-8') as random_sample_output:
    for text in files:
        try:
            with open(text, 'rt', encoding='utf-8') as f:
                # number of lines from txt file
                random_sample_input = random.sample(f.read().split(), 10)

        except IOError as exc:
            # Do not fail if a directory is found, just ignore it.
            if exc.errno != errno.EISDIR:
            raise

        # This block of code writes the result of the previous to a new file
        random_sample_input = map(lambda x: x + ""\n"", random_sample_input)
        random_sample_output.writelines(random_sample_input)
</code></pre>
"
40041063,4549554.0,2016-10-14T10:34:10Z,40040564,2,"<p>Try following code and let me know in case of any issues:</p>

<pre><code>WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//input[@type=""file""][@name=""qqfile""]'))).send_keys(""/path/to/Gandalf.jpg"")
</code></pre>

<p>P.S. You should replace string <code>""/path/to/Gandalf.jpg""</code> with actual path to file</p>
"
40042236,5741205.0,2016-10-14T11:37:11Z,40041845,2,"<p>I would choose Pandas, because it's much faster, has an excellent and extremely rich API, a source code looks much cleaner and better, etc.</p>

<p>BTW the following line can be easily rewritten:</p>

<pre><code>managers[managers['ShopId'].isin(sales[sales['Year2010'] &gt; 1500]['ShopId'])]['ManagerName'].values
</code></pre>

<p>as:</p>

<pre><code>ShopIds = sales.ix[sales['Year2010'] &gt; 1500, 'ShopId']
managers.query('ShopId in @ShopIds')['ManagerName'].values
</code></pre>

<p>IMO it's pretty easy to read and understand</p>

<p>PS you may also want to store your data in a <code>SQL</code>-able database and use SQL or to store it in HDF Store and use <code>where</code> parameter - in both cases you can benefit from indexing ""search"" columns</p>
"
40042283,4474436.0,2016-10-14T11:39:46Z,40041845,2,"<p>Creating classes that operate on dataframes is not a good idea, because it'll hide away the fact that you're using a data frame, and open the way to  very bad decisions (like iterating over a dataframe with a <code>for</code> loop).</p>

<p>Solution 1: Denormalize the data. 
You don't have to keep your data in a normal form. Normal form is preferrable when you have to keep your entries consistent throughout the database. This is not a database, you don't do constant inserts, updates and deletes. So just denormalize it, and work with one large dataframe, as it's clearly more convenient, and better suits your needs. </p>

<p>Solution 2: Use a database. 
You can dump your data into a SQLite database (pandas has a built-in function for that), and execute all kinds of crazy queries on it. In my personal opition, SQL queries are much more readable than the stuff you posted. 
If you do this kind of analysis regularly, and the data structure remains the same, this may be a preferrable solution. You can dump data ino a db, and then use SQLAlchemy to work with it.</p>

<p>Solution 3. Create your own datarame.
You can inherit from <code>pandas.DataFrame</code> and add custom methods to it. You need to dig into the guts of <code>pandas</code> for that, though, to see how to implement those methods. This way you can create, for example, custom methods of accessing certain parts of a dataframe.</p>

<p>Unless you know pandas really well, I'd go for solutions 1 or 2. If you need more flexibility, and the data manipulation is different every time, use 1. If you need to execute roughly the same analysis every time, use 2 (especially if your data analysis code is a part of a bigger application).</p>

<p>Also, I don't understand why ""adding more lines of code"" is bad. By breaking up a huge one-liner into many expressions, you don't increase the <em>actual</em> complexity, and you decrease the <em>perceived</em> complexity. Maybe all you need to do is just refactor your code, and pack some operations into reusable functions?</p>
"
40042425,266852.0,2016-10-14T11:47:07Z,39935335,0,"<p>Alternatively, you can install Anaconda Python from: <a href=""https://www.continuum.io/downloads"" rel=""nofollow"">https://www.continuum.io/downloads</a></p>

<p>This installation includes BS out of the box as most of the common libraries you will use. Plus it makes library installation quite easy.</p>
"
40042659,3293881.0,2016-10-14T11:59:12Z,40042573,4,"<p>You could reshape to split the first axis into two axes, such that latter of those axes is of length <code>2</code> and then flip the array along that axis with <code>[::-1]</code> and finally reshape back to original shape.</p>

<p>Thus, we would have an implementation like so -</p>

<pre><code>a.reshape(-1,2,*a.shape[1:])[:,::-1].reshape(a.shape)
</code></pre>

<p>Sample run -</p>

<pre><code>In [170]: a = np.random.randint(0,9,(6,3))

In [171]: order = [1,0,3,2,5,4]

In [172]: a[order]
Out[172]: 
array([[0, 8, 5],
       [4, 5, 6],
       [0, 0, 2],
       [7, 3, 8],
       [1, 6, 3],
       [2, 4, 4]])

In [173]: a.reshape(-1,2,*a.shape[1:])[:,::-1].reshape(a.shape)
Out[173]: 
array([[0, 8, 5],
       [4, 5, 6],
       [0, 0, 2],
       [7, 3, 8],
       [1, 6, 3],
       [2, 4, 4]])
</code></pre>

<p>Alternatively, if you are looking to efficiently create those constantly flipping indices <code>order</code>, we could do something like this -</p>

<pre><code>order = np.arange(data.shape[0]).reshape(-1,2)[:,::-1].ravel()
</code></pre>
"
40043941,771848.0,2016-10-14T13:05:50Z,40043715,1,"<p>You can actually let <code>BeautifulSoup</code> <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#parsing-only-part-of-a-document"" rel=""nofollow"">parse only the tags you are interested in via <code>SoupStrainer</code></a>:</p>

<pre><code>from bs4 import BeautifulSoup, SoupStrainer

only_addresses = SoupStrainer(""span"", itemprop=""address"")
soup = BeautifulSoup(html_doc, ""html.parser"", parse_only=only_addresses)
</code></pre>

<p>If you though have some ""addresses"" before the ""Top-10 today"" and some after but you are interested in those coming before it, you can make a custom <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#a-function"" rel=""nofollow"">searching function</a>:</p>

<pre><code>def search_addresses(tag):
    return tag.name == ""span"" and tag.get(""itemprop"") == ""address"" and \
           tag.find_next(""h2"", text=lambda text: text and ""Top-10 today"" in text)

addresses = soup.find_all(search_addresses)
</code></pre>

<p>It does not look trivial, but the idea is simple - we are using <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all-next-and-find-next"" rel=""nofollow""><code>find_next()</code></a> for every ""address"" to check if ""Top-10 today"" heading exists after it.</p>
"
40044975,4211135.0,2016-10-14T13:55:37Z,40044814,3,"<p>As noted in the comments, you are using an immutable data structure for data items that you are attempting to change. Without further context, it looks like you want a dictionary, not a list of tuples, and it also looks like you want the second item in the tuple (the letter) to be the key, since you are planning on modifying the number. </p>

<p>Using these assumptions, I recommend converting the list of tuples to a dictionary and then using normal dictionary assignment. This also assumes that <em>order</em> is not important (if it is, you can use an <code>OrderedDict</code>) and that the same letter does not appear twice (if it does, only the last number will be in the dict).</p>

<pre><code>&gt;&gt;&gt; lst =  [(1, 'q'), (2, 'w'), (3, 'e'), (4, 'r')]
&gt;&gt;&gt; item_dict = dict(i[::-1] for i in lst)
&gt;&gt;&gt; item_dict
{'q': 1, 'r': 4, 'e': 3, 'w': 2}
&gt;&gt;&gt; item_dict['w'] = 6
&gt;&gt;&gt; item_dict
{'q': 1, 'r': 4, 'e': 3, 'w': 6}
</code></pre>
"
40045156,4014959.0,2016-10-14T14:04:45Z,40044814,5,"<p>You can simply scan through the list looking for a tuple with the desired letter and replace the whole tuple (you can't modify tuples), breaking out of the loop when you've found the required item. Eg,</p>

<pre><code>lst = [(1, 'q'), (2, 'w'), (3, 'e'), (4, 'r')]

def update(item, num):
    for i, t in enumerate(lst):
        if t[1] == item:
            lst[i] = num, item
            break

update('w', 6)
print(lst)
</code></pre>

<p><strong>output</strong></p>

<pre><code>[(1, 'q'), (6, 'w'), (3, 'e'), (4, 'r')]
</code></pre>

<p>However, you should seriously consider using a dictionary instead of a list of tuples. Searching a dictionary is much more efficient than doing a linear scan over a list.</p>
"
40045176,6640099.0,2016-10-14T14:05:28Z,40044814,1,"<p>Tuples are an immutable object. Which means once they're created, you can't go changing there contents.</p>

<p>You can, work around this however, by replaceing the tuple you want to change. Possibly something such as this:</p>

<pre><code>def change_item_in_list(lst, item, num):
    for pos, tup in enumerate(lst):
        if tup[1] == item:
            lst[pos] = (num, item)
            return

l = [(1, 'q'), (2, 'w'), (3, 'e'), (4, 'r')]
print(l)
change_item_in_list(l, 'w', 6)
print(l)
</code></pre>

<p>But as @brianpck has already said, you probably want a (ordered)-dictionary instead of a list of tuples.</p>
"
40045478,3293881.0,2016-10-14T14:19:34Z,40044714,2,"<p>You could do that with a series of reductions, like so -</p>

<pre><code>p1 = np.tensordot(re,ewp,axes=(1,0))
p2 = np.tensordot(p1,ewp,axes=(1,0))
out = p2**2
</code></pre>

<p><strong>Explanation</strong></p>

<p>First off, we could separate it out into two groups of operations :</p>

<pre><code>Group 1: R(i,j,k) , &lt; wj &gt; , &lt; wk &gt; 
Group 2: R(i,l,m) , &lt; wl &gt; , &lt; wl &gt; 
</code></pre>

<p>The operations performed within these two groups are identical. So, one could compute for one group and derive the final output based off it.</p>

<p>Now, to compute <code>R(i,j,k)</code> , &lt; <code>wj</code> >, &lt; <code>wk</code> and end up with <code>(i)</code> , we need to perform element-wise multiplication along the second and third axes of <code>R</code> with <code>w</code> and then perform <code>sum-reduction</code> along those axes. Here, we are doing it in two steps with two <code>tensordots</code> -</p>

<pre><code>[1] R(i,j,k) , &lt; wj &gt; to get p1(i,k)
[2] p1(i,k) , &lt; wk &gt; to get p2(i)
</code></pre>

<p>Thus, we end up with a vector <code>p2</code>. Similarly with the second group, the result would be an identical vector. So, to get to the final output, we just need to square that vector, i.e. <code>p**2</code>.</p>
"
40045991,5276797.0,2016-10-14T14:45:10Z,40039531,1,"<p><strong>First</strong>, keep all strings starting with <code>1</code> or <code>nan</code>:</p>

<pre><code>keep = frame['RR'].str.startswith(""1"", na=True)
keep1 = keep[keep]  # will be used at the end
</code></pre>

<p><strong>Second</strong>, keep strings starting with <code>2</code> or <code>3</code> that are not in the first dataframe <code>rr1</code>:</p>

<pre><code>rr1 = frame.loc[frame['RR'].str.startswith(""1"", na=False), 'RR']
keep2 = ~frame.loc[
            (frame['RR'].str.startswith(""2"")) | (frame['RR'].str.startswith(""3"")), 'RR'
        ].str.slice(1).isin(rr1.str.slice(1))
</code></pre>

<p><strong>Third</strong>, keep other strings that are not in <code>rr1</code> after adding a leading <code>1</code>:</p>

<pre><code>import numpy as np
keep3 = ~(""1"" + frame.loc[
            ~frame['RR'].str.slice(0,1).isin([np.nan, ""1"", ""2"", ""3""]), 'RR'
        ]).isin(rr1)
</code></pre>

<p><strong>Finally</strong>, put everything together:</p>

<pre><code>frame[pd.concat([keep1, keep2, keep3]).sort_index()]
</code></pre>
"
40046073,208880.0,2016-10-14T14:49:37Z,40042345,1,"<p>It looks like you are trying to use the backported enum from the 3.4 stdlib, but you installed <code>enum</code> -- you need to install <code>enum34</code>.</p>
"
40046832,6381964.0,2016-10-14T15:27:37Z,39891202,2,"<p>Ultimately, you will need and want to write your own wrapper/script to do this. And since you are using a distribution of Linux, this is relatively easy.</p>

<p>On a Linux OS, the simplest way to issue a print job is to open a <a href=""http://docs.python.org/3/library/subprocess.html"" rel=""nofollow"" title=""subprocess"">subprocess</a> to the <a href=""http://www.computerhope.com/unix/ulpr.htm"" rel=""nofollow"">lpr</a>. Generally, using <code>lpr</code> lets you access the printer without the need to be logged in as root (being a superuser), which is desirable considering the amount of damage that can be done while logged in as a ""superuser"".</p>

<p>Code like the following:</p>

<pre><code>import subprocess
lpr = subprocess.Popen(""/usr/bin/lpr"", stdin=subprocess.PIPE)
lpr.stdin.write(data_to_send_to_printer)
</code></pre>

<p>Should be a good jumping off point for you. Essentially, this code should allow you to accomplish what you need.</p>

<p><strong>Be careful though; depending on your privilege levels, a call to open a subprocess might need root level/Superuser permissions.</strong></p>

<p>Subprocesses generally <a href=""http://stackoverflow.com/questions/22233454/will-a-python-subprocess-popen-call-inherit-root-privs-if-the-calling-script-i"" title=""inherit"">inherit</a> the User IDs and access rights by the user that is running the command. For example, if the subprocess is created by a root user, then you will need root user/Superuser rights to access that subprocess.</p>

<p>For more information, check out the hyperlinks I've included in the post.</p>

<p>Good luck!</p>
"
40047237,6748105.0,2016-10-14T15:47:44Z,39891202,3,"<h2>The Problem</h2>

<p>To send data down this route:</p>

<p>Client computer ---> Server (Windows machine) ---> printer (dot-matrix)</p>

<p>...and to <em>not</em> let Windows mess with the data; instead to send the raw data, including printer control codes, straight from the client computer.</p>

<h2>My Solution</h2>

<p>Here's how I solved a near-identical problem for a small in-house database application:</p>

<p>Step 1) Make the printer network-accessible without Windows getting its fingers in the data routed to it. I accomplished this by installing the printer using the ""Generic/Text Only"" driver, then installing 
<a href=""https://sourceforge.net/projects/rawprintserver/"" rel=""nofollow"">RawPrintServer</a> on the Windows machine connected to the printer.</p>

<p>Step 2) Send raw data over the network to the TCP/IP port specified when you set up RawPrintServer (default is 9100). There are various ways to do that, here's what I did:</p>

<pre><code>data = b""\x1B@A String To Print\x1B@"" # be sure to use the right codes for your printer
ip_addr = 123.123.123.123 # address of the machine with the printer
port = 9100 # or whatever you set it to
s = socket.socket()
try:
    s.connect((ip_addr, port))
    s.send(data)
except:
    # deal with the error
finally:
    s.close()
</code></pre>

<h2>Background</h2>

<p>I thought about the problem in two parts:</p>

<ol>
<li>Client machine: spitting out the data I need from Python with the correct formatting/control codes for my printer, and sending it across the network</li>
<li>Print server machine: transmitting the data to the locally connected printer</li>
</ol>

<p>Number 1 is the easy part. There are actually <a href=""https://pypi.python.org/pypi?%3Aaction=search&amp;term=escpos&amp;submit=search"" rel=""nofollow"">some libraries in PyPI</a> that may help with all the printer codes, but I found most of them are aimed at the little point-of-sale label printers, and were of limited use to me. So I just hard-coded what I needed into my Python program.</p>

<p>Of course, the way you choose to solve number 2 will effect how you send the data from Python. I chose the TCP/IP route to avoid dealing with Samba and Windows print issues.</p>

<p>As you probably discovered, Windows normally tries very hard to convert whatever you want to print to a bitmap and run the printer in graphics mode. We can use the generic driver and dump the data straight into the (local) printer port in order to prevent this.</p>

<p>The missing link, then, is getting from the network to the local printer port on the machine connected to the printer. Again, there are various ways to solve this. You could attempt to access the Windows printer share in some way. If you go the TCP/IP route like I did, you could write your own print server in Python. In my case, the RawPrintServer program ""just worked"" so I didn't investigate any further. Apparently all it does is grab incoming data from TCP port 9100 and shove it into the local printer port. Obviously you'll have to be sure the firewall isn't blocking the incoming connections on the print server machine. This method does not require the printer to be ""shared"" as far as Windows is concerned.</p>

<p>Depending on your situation (if you use DHCP), you might need to do some extra work to get the server's IP address in Python. In my case, I got the IP for free because of the peculiarity of my application.</p>

<p>This solution seems to be working out very well for me. I've got an old Panasonic printer running in Epson ESC/P compatibility mode connected to a Windows 7 machine, which I can print to from any other computer on the local network. Incidentally, this general idea should work regardless of what OS the client computer is running.</p>
"
40047273,1433712.0,2016-10-14T15:49:23Z,40047029,1,"<p>Python is showing you the binary data represented in <a href=""https://en.wikipedia.org/wiki/Hexadecimal"" rel=""nofollow"">hexadecimal</a> when the characters do not correspond with a regular <a href=""https://en.wikipedia.org/wiki/ASCII"" rel=""nofollow"">ascii</a> character. For example <code>\xa3</code> is a byte of hexidecimal value <code>A3</code> which is <code>10100011</code> in binary. <code>T</code> on the other hand could be printed as <code>\x54</code> which is a byte of binary value <code>01010100</code>. Since you used the <code>print</code> function, python assumes you are trying to convert the binary data to a human readable string, so instead of <code>\x54</code> it showed the corresponding character <code>T</code>.</p>

<p>You can use the following code to get an array of binary strings that represent your data:</p>

<pre><code>data = '\xa3\x95\x80\x80YFMT\x00BBnNZ\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Type,Length,Name,Format,Columns'

decimalArray = map(ord,data)
byteArray = map(lambda x: ""{0:b}"".format(x), decimalArray)
print byteArray
</code></pre>

<p>Here is the output:</p>

<pre><code>['10100011', '10010101', '10000000', '10000000', '1011001', '1000110', '1001101', '1010100', '0', '1000010', '1000010', '1101110', '1001110', '1011010', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1010100', '1111001', '1110000', '1100101', '101100', '1001100', '1100101', '1101110', '1100111', '1110100', '1101000', '101100', '1001110', '1100001', '1101101', '1100101', '101100', '1000110', '1101111', '1110010', '1101101', '1100001', '1110100', '101100', '1000011', '1101111', '1101100', '1110101', '1101101', '1101110', '1110011']
</code></pre>
"
40047450,2034487.0,2016-10-14T15:58:58Z,40047291,3,"<p>You can iterate through the results of <code>df.to_dict(orient=""records"")</code>:</p>

<pre><code>data = pd.read_csv(""some-url"")
for row in data.to_dict(orient=""records""):
    # For each loop, `row` will be filled with a key:value dict where each
    # key takes the value of the column name.
    # Use this dict to create a record for your db insert, eg as raw SQL or
    # to create an instance for an ORM like SQLAlchemy.
</code></pre>

<p>I do a similar thing to pre-format data for SQLAlchemy inserts, although I'm using Pandas to merge data from multiple sources rather than just reading the file.</p>

<p><em>Side note: There will be plenty of other ways to do this without Pandas and just iterate through the lines of the file. However Pandas's intuituve handling of CSVs makes it an attractive shortcut to do what you need.</em></p>
"
40047765,5574466.0,2016-10-14T16:18:01Z,40047742,1,"<p>Always remember to commit the changes that you make. In this case: <code>connection.commit()</code>. Except it looks like you overrode your <code>connection</code> variable.</p>
"
40048058,224671.0,2016-10-14T16:37:27Z,40048006,3,"<p>We could use a <a href=""http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays"" rel=""nofollow"">boolean array</a> as index for filtering.</p>

<pre><code>&gt;&gt;&gt; x[:, x[0] &gt; 5]
array([[ 5.5,  6. ,  7. ],
       [ 2. ,  3. ,  4. ]])
</code></pre>

<ul>
<li><code>x[0]</code> selects the first row</li>
<li><code>x[0] &gt; 5</code> creates an array of boolean, checking whether an element is > 5 or not. (This is <code>[False, False, False, False, True, True, True]</code>.)</li>
<li><p>When we write <code>some_array[boolean_array]</code>, we  only keep the elements in <code>some_array</code> which the corresponding value in <code>boolean_array</code> is True. For instance, </p>

<pre><code>&gt;&gt;&gt; numpy.array([2, 4, 6, 8])[numpy.array([True, False, False, True])]
array([2, 8])
</code></pre></li>
<li><p>Since we are going to select columns, the boolean array <code>x[0] &gt; 5</code> should be placed in the second axis. We select the whole first axis with <code>:</code>. Thus the final expression is <code>x[:, x[0] &gt; 5]</code>.</p></li>
</ul>
"
40048128,3123089.0,2016-10-14T16:41:48Z,40048006,0,"<p>Or the enumerate function:</p>

<pre><code>    res = []
    for i, _ in enumerate(x):
    res.append([])
    for j, val in enumerate(x[i]):
        if j &gt; 5:
            res[i].append(val)
</code></pre>
"
40048515,1422451.0,2016-10-14T17:07:33Z,40041757,1,"<p>Consider outputting data frame initially as is to a temp file. Then, during creation of the <em>MainCSV</em>, read in temp file, iteratively writing lines, then destroy temp file. Also, prior to writing dataframe to csv, create the three blank columns. </p>

<p>Below assumes you want two tasks: 1) three blank columns and 2) writing dataframe values below Amazon/Ebay/Kindle row headers. Example data uses random normal values and scalar values for <em>wdvad</em>, <em>wevad</em>, <em>wdmpv</em>, <em>wempv</em> are the string literals of their names:</p>

<pre><code>import csv, os
import pandas as pd

# TEMP DF CSV
dirname = os.path.dirname(os.path.abspath(__file__))    
df = pd.DataFrame([np.random.normal(loc=3.0, scale=1.0, size=24)*1000 for i in range(7)],
                  index=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 
                         'Friday', 'Saturday', 'Sunday'])
df['Blank1'], df['Blank2'], df['Blank3'] = None, None, None   

df.to_csv(os.path.join(dirname, 'temp.csv'))       # OUTPUT TEMP DF CSV

# MAIN CSV
csvfilename = os.path.join(dirname, 'MainFile.csv')
tempfile = os.path.join(dirname, 'temp.csv')

wdvad = 'wdvad'; wevad = 'wevad'; wdmpv = 'wdmpv'; wempv = 'wempv'

with open(csvfilename, 'w', newline='') as output_file:
    writer = csv.writer(output_file)
    writer.writerow([""""])
    writer.writerow(["""",""Amazon"",""Weekday"",""Weekend""])
    writer.writerow(["""",""Ebay"",wdvad,wevad])
    writer.writerow(["""",""Kindle"",wdmpv,wempv])
    writer.writerow([""""])

    with open(tempfile, 'r') as data_file:
        for line in data_file:
            line = line.replace('\n', '')
            row = line.split("","")            
            writer.writerow(row)

os.remove(tempfile)                               # DESTROY TEMP DF CSV
</code></pre>

<p><strong>Output</strong></p>

<p><a href=""https://i.stack.imgur.com/qVOCL.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/qVOCL.png"" alt=""CSV File Output""></a></p>
"
40048637,131187.0,2016-10-14T17:15:15Z,40048309,4,"<p><strong>i_out</strong> has not been declared as a symbol.</p>

<pre><code>&gt;&gt;&gt; from sympy import *
&gt;&gt;&gt; var('D_PWM, Rsense, A i_out')
(D_PWM, Rsense, A, i_out)
&gt;&gt;&gt; eqn=Eq(i_out,D_PWM * (A/Rsense))
&gt;&gt;&gt; solve(eqn,Rsense)
[A*D_PWM/i_out]
</code></pre>
"
40048869,3657742.0,2016-10-14T17:29:06Z,40048323,3,"<p><code>pandas</code> doesn't have a great way to handle general recursive calculations.  There may be some trick to vectorize it, but if you can take the dependency, this is relatively painless and very fast with <code>numba</code>.</p>

<pre><code>@numba.njit
def make_b(a):
    b = np.zeros_like(a)
    b[0] = 1
    for i in range(1, len(a)):
        b[i] = min(b[i-1], a[i-1]) + 2

    return b

df['b'] = make_b(df['a'].values)

df
Out[73]: 
    a   b
0   1   1
1   1   3
2   5   3
3   2   5
4   7   4
5   8   6
6  16   8
7  16  10
8  16  12
</code></pre>
"
40049860,3019689.0,2016-10-14T18:29:40Z,39730467,1,"<p>Not to detract from the accepted answer, which does solve the problem of getting updated axis limits, but is this perhaps an example of the XY problem?   If what you want to do is draw a box around the axes, then you don't actually <em>need</em> the <code>xlim</code> and <code>ylim</code> in data coordinates.  Instead, you just need to use the <code>ax.transAxes</code> transform which causes both <code>x</code> and <code>y</code> data to be interpreted in normalized coordinates instead of data-centered coordinates:</p>

<pre><code>ax.plot([0,0,1,1,0],[0,1,1,0,0], lw=3, transform=ax.transAxes)
</code></pre>

<p>The great thing about this is that your line will stay around the edges of the axes <strong><em>even if the <code>xlim</code> and <code>ylim</code> subsequently change</em></strong>.</p>

<p>You can also use <code>transform=ax.xaxis.get_transform()</code> or <code>transform=ax.yaxis.get_transform()</code> if you want only <code>x</code> or only <code>y</code> to be defined in normalized coordinates, with the other one in data coordinates.</p>
"
40050122,5771269.0,2016-10-14T18:47:31Z,39963364,0,"<p>Since your letters print relative to the origin (0, 0), the trick is to divide the screen into portions (say thirds) and for each letter, move to the center of the appropriate portion before drawing the letter.</p>

<p>I've reworked your code below to do the above (for up to three letters) and simplify some of your logic.  The letter functions themselves are unchanged so I left out the bulk of them below, just splice in your original code there:</p>

<pre><code>from turtle import Turtle, Screen

SCREEN_WIDTH, SCREEN_HEIGHT = 1200, 600
SCREEN_MARGIN = 0.1 * SCREEN_WIDTH
MAXIMUM_CHARACTERS = 3
CHARACTER_WIDTH = (SCREEN_WIDTH - (2 * SCREEN_MARGIN)) / MAXIMUM_CHARACTERS
COLORS = ['purple', 'blue', 'black']

velcro = Turtle()
screen = Screen()

def letter_A():
    velcro.left(90)
    velcro.forward(150)
    velcro.right(90)
    velcro.forward(150)
    velcro.right(90)
    velcro.forward(150)
    velcro.right(180)
    velcro.forward(75)
    velcro.left(90)
    velcro.forward(150)

# ...

def letter_Z():
    velcro.forward(225)
    velcro.left(140)
    velcro.forward(300)
    velcro.right(140)
    velcro.forward(225)

letter_lookup = {
    'A': letter_A,
    'B': letter_B,
    'C': letter_C,
    'D': letter_D,
    'E': letter_E,
    'F': letter_F,
    'G': letter_G,
    'H': letter_H,
    'I': letter_I,
    'J': letter_J,
    'K': letter_K,
    'L': letter_L,
    'M': letter_M,
    'N': letter_N,
    'O': letter_O,
    'P': letter_P,
    'Q': letter_Q,
    'R': letter_R,
    'S': letter_S,
    'T': letter_T,
    'U': letter_U,
    'V': letter_V,
    'W': letter_W,
    'X': letter_X,
    'Y': letter_Y,
    'Z': letter_Z,
}

screen.setup(width=SCREEN_WIDTH, height=SCREEN_HEIGHT)

screen.bgcolor('pink')
velcro.color(""purple"", ""blue"")
velcro.pensize(""12"")

color_string = "", "".join(COLORS[:-1]) + "" or "" + COLORS[-1]

velcroColor = screen.textinput(""Pick a Color"", ""Please chose from the colors "" + color_string + "", to draw with"")

if (velcroColor in COLORS):
    velcro.color(velcroColor)

velcroLetters = screen.textinput(""Turtle alphabet"", ""Please enter up to 3 letters from A-Z to draw:"")[:MAXIMUM_CHARACTERS]

center_x = CHARACTER_WIDTH * (len(velcroLetters) - 1) / -2

for letter in velcroLetters:
    velcro.penup()
    velcro.home()
    velcro.goto(center_x, 0)
    velcro.pendown()
    letter_lookup[letter.upper()]()

    center_x += CHARACTER_WIDTH

screen.mainloop()
</code></pre>

<p><a href=""https://i.stack.imgur.com/HEnvs.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/HEnvs.png"" alt=""enter image description here""></a></p>
"
40050134,2141635.0,2016-10-14T18:48:36Z,40035361,1,"<p>You could write your own, if you only care about three decimal places then set  n to 3:</p>

<pre><code>def frac_to_oct(f, n=4):
    # store the number before the decimal point
    whole = int(f)
    rem = (f - whole) * 8
    int_ = int(rem)
    rem = (rem - int_) * 8
    octals = [str(int_)]
    count = 1
    # loop until 8 * rem gives you a whole num or n times
    while rem and count &lt; n:
        count += 1
        int_ = int(rem)
        rem = (rem - int_) * 8
        octals.append(str(int_))
    return float(""{:o}.{}"".format(whole, """".join(octals)))
</code></pre>

<p>Using your input <em>12.325</em>:</p>

<pre><code>In [9]: frac_to_oct(12.325)
Out[9]: 14.2463
In [10]: frac_to_oct(121212.325, 4)
Out[10]: 354574.2463

In [11]: frac_to_oct(0.325, 4)
Out[11]: 0.2463
In [12]: frac_to_oct(2.1, 4)
Out[12]: 2.0631
In [13]:  frac_to_oct(0)
Out[13]: 0.0
In [14]:  frac_to_oct(33)
Out[14]: 41.0
</code></pre>
"
40050158,4962788.0,2016-10-14T18:49:51Z,40035361,0,"<p>Here's the solution, explanation below:</p>

<pre><code>def ToLessThanOne(num): # Change ""num"" into a decimal &lt;1
    while num &gt; 1:
        num /= 10
    return num

def FloatToOctal(flo, places=8): # Flo is float, places is octal places
    main, dec = str(flo).split(""."") # Split into Main (integer component)
                                    # and Dec (decimal component)
    main, dec = int(main), int(dec) # Turn into integers

    res = oct(main)[2::]+""."" # Turn the integer component into an octal value
                             # while removing the ""ox"" that would normally appear ([2::])
                             # Also, res means result

    # NOTE: main and dec are being recycled here

    for x in range(places): 
        main, dec = str((ToLessThanOne(dec))*8).split(""."") # main is integer octal
                                                           # component
                                                           # dec is octal point
                                                           # component
        dec = int(dec) # make dec an integer

        res += main # Add the octal num to the end of the result

    return res # finally return the result
</code></pre>

<p>So you can do <code>print(FloatToOctal(12.325))</code> and it shall print out <code>14.246314631</code></p>

<p>Finally, if you want less octal places (decimal places but in octal) simply add the <code>places</code> argument: <code>print(FloatToOctal(12.325, 3))</code> which returns <code>14.246</code> as is correct according to this website: <a href=""http://coderstoolbox.net/number/"" rel=""nofollow"">http://coderstoolbox.net/number/</a></p>
"
40050292,547156.0,2016-10-14T19:00:36Z,40050238,0,"<p>If you don't mind the performance hit, you could look at the <a href=""https://docs.python.org/2/library/decimal.html"" rel=""nofollow""><code>decimal</code></a> module in the standard library. It's got arbitrary-precision numbers.</p>
"
40050430,6753042.0,2016-10-14T19:09:24Z,39885723,1,"<p>You've just described a Kalman Filtering / data fusion problem. You have an initial state <strong>A</strong> that has some errors and you have some observations <strong>B</strong> that also have some noise. You want to improve your estimate of state <strong>A</strong> by injecting some information from <strong>B</strong>, all while accounting for spatially correlated errors in both datasets. We don't have any prior information about the errors in <strong>A</strong> and <strong>B</strong>, so we can just make it up. Here's an implementation:</p>

<pre><code>import numpy as np

# Make a matrix of the distances between points in an array
def dist(M):
    nx = M.shape[0]
    ny = M.shape[1]
    x = np.ravel(np.tile(np.arange(nx),(ny,1))).reshape((nx*ny,1))
    y = np.ravel(np.tile(np.arange(ny),(nx,1))).reshape((nx*ny,1))
    n,m = np.meshgrid(x,y)
    d = np.sqrt((n-n.T)**2+(m-m.T)**2)
    return d

# Turn a distance matrix into a covariance matrix. Here is a linear covariance matrix.
def covariance(d,scaling_factor):
    c = (-d/np.amax(d) + 1)*scaling_factor
    return c

A = np.array([[1,1,1],[1,1,1],[1,1,1]]) # background state
B = np.array([[34,100,15],[62,17,87],[17,34,60]]) # observations

x = np.ravel(A).reshape((9,1)) # vector representation
y = np.ravel(B).reshape((9,1)) # vector representation

P_a = np.eye(9)*50 # background error covariance matrix (set to diagonal here)
P_b = covariance(dist(B),2) # observation error covariance matrix (set to a function of distance here)

# Compute the Kalman gain matrix
K = P_a.dot(np.linalg.inv(P_a+P_b))

x_new = x + K.dot(y-x)
A_new = x_new.reshape(A.shape)

print(A)
print(B)
print(A_new)
</code></pre>

<p>Now, this method only works if your data are unbiased. So mean(A) must equal mean(B). But you'll still get okay results regardless. Also, you can play with the covariance matrices however you like. I'd recommend reading the Kalman filter wikipedia page for more details. </p>

<p>By the way, the example above yields:</p>

<pre><code>[[ 27.92920141  90.65490699   7.17920141]
 [ 55.92920141   7.65490699  79.17920141]
 [ 10.92920141  24.65490699  52.17920141]]
</code></pre>
"
40050627,5858851.0,2016-10-14T19:24:47Z,40050238,1,"<p>You're relying on a floating point number being exactly equal to 0 (False) for your function to work. Generally speaking, you should avoid testing for equality when dealing with floating point numbers. Instead, it's better to set an acceptable tolerance level for the difference.</p>

<p>Try this instead:</p>

<pre><code>def isRoot(n, root, epsilon=1e-10):
    test = math.log(n, root)%1
    return abs(test - int(round(test))) &lt; epsilon
</code></pre>
"
40050745,2482744.0,2016-10-14T19:32:48Z,40050238,1,"<p>How about this:</p>

<pre><code>def isRoot(n, root):
    power = 1
    while root ** power &lt; n:
        power += 1
    return root ** power == n
</code></pre>

<p>If this is too slow you can also do a sort of binary search to reduce the number of checks.</p>
"
40050791,5987.0,2016-10-14T19:35:41Z,40050238,2,"<p>The best way to avoid the problem is with a different approach that avoids floating point math entirely.</p>

<pre><code>def isRoot(n, root):
    return n &lt;= 1 or (False if n % root != 0 else isRoot(n // root, root))
</code></pre>
"
40050811,4930423.0,2016-10-14T19:37:05Z,40046167,2,"<p>Docker-for-mac only supports connections over the /var/run/docker.sock socket that is listening on your OSX host.</p>

<p>If you try to add this to pycharm, you'll get the following message:</p>

<p><a href=""https://i.stack.imgur.com/4x5i6.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/4x5i6.png"" alt=""Only supported on Linux""></a></p>

<p>""Cannot connect: java.lang.ExceptionInInitializerError, caused by: java.lang.IllegalStateException: Only supported on Linux""</p>

<p>So PyCharm <em>really</em> only wants to connect to a docker daemon over a TCP socket, and has support for the recommended TLS protection of that socket. The Certificates folder defaults to the certificate folder for the default docker-machine machine, ""default"".</p>

<p>It is possible to implement a workaround to expose Docker for Mac via a TCP server if you have socat installed on your OSX machine.</p>

<p>On my system, I have it installed via homebrew:</p>

<pre><code>brew install socat
</code></pre>

<p>Now that's installed, I can run socat with the following parameters:</p>

<pre><code>socat TCP-LISTEN:2376,reuseaddr,fork,bind=127.0.0.1 UNIX-CLIENT:/var/run/docker.sock
</code></pre>

<p>WARNING: this will make it possible for any process running as any user on your whole mac to access your docker-for-mac. The unix socket is protected by user permissions, while 127.0.0.1 is not.</p>

<p>This socat command tells it to listen on 127.0.0.1:2376 and pass connections on to /var/run/docker.sock. The reuseaddr and fork options allow this one command to service multiple connections instead of just the very first one.</p>

<p>I can test that socat is working by running the following command:</p>

<pre><code>docker -H tcp://127.0.0.1:2376 ps
</code></pre>

<p>If you get a successful <code>docker ps</code> response back, then you know that the socat process is doing its job.</p>

<p>Now, in the PyCharm window, I can put the same <code>tcp://127.0.0.1:2376</code> in place. I should get a ""Connection successful"" message back:</p>

<p><a href=""https://i.stack.imgur.com/TqH0E.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/TqH0E.png"" alt=""connection successful""></a></p>

<p>This workaround will require that socat command to be running any time you want to use docker from PyCharm.</p>

<p>If you wanted to do the same thing, but with TLS, you could set up certificates and make them available for both pycharm and socat, and use socat's <code>OPENSSL-LISTEN</code> instead of the <code>TCP-LISTEN</code> feature. I won't go into the details on that for this answer though.</p>
"
40050833,2482744.0,2016-10-14T19:39:19Z,40050238,0,"<p>Just round to an integer and verify:</p>

<pre><code>def isRoot(n, root):
    power = int(round(math.log(n, root)))
    return root ** power == n
</code></pre>
"
40050941,624829.0,2016-10-14T19:47:40Z,40049802,2,"<p>For type consistency with subsequent operations your can do with the results of a <code>transform</code> call, that function tries to cast the resulting Series into the dtype of the selected data it works against. The function source code has this dtype cast explicitly done.</p>

<p>Your boolean data can be turned into dates, thus you obtain a datetime series. Explicitly cast to <code>int</code> to get the expected type:</p>

<pre><code>df.groupby(['B'])['C'].transform(date_test).astype('int64')
</code></pre>
"
40051111,6779307.0,2016-10-14T20:00:01Z,40051073,1,"<pre><code>inputs = []
for i in range(expected_number_of_inputs):
    inputs.append(input('Product Code: '))
for i in inputs:
    print(i)
</code></pre>
"
40051138,7015867.0,2016-10-14T20:01:55Z,40050780,1,"<p>To keep the CPU usual steady I put <code>display.Display().screen()</code> before the loop so that it didn't have to do so much work all the time. The screen shouldn't change so nor should that value so it made sense to set it up before.</p>

<pre><code>import time
from Xlib import display
disp = display.Display().screen()
while True:
    d = disp.root.query_pointer()._data
    print(d[""root_x""], d[""root_y""])
    time.sleep(0.1)
</code></pre>

<p>I've tested it and it stays at about 0.3% for me.</p>

<p>Hope it this helps :)</p>
"
40051147,4577647.0,2016-10-14T20:02:34Z,40051073,0,"<p>You can do this in two separate ways but the way you are trying to do it will not work. You cannot have a variable that is not a list hold two values.</p>

<p>Solution 1: use two variables</p>

<pre><code>w1 = input(""value1"")
w2 = input(""value2"")
print(w1)
print(w2)
</code></pre>

<p>Solution 2: use a list</p>

<pre><code>w = []
w.append(input(""value1""))
w.append(input(""value2""))
print(w[0])
print(w[1])
</code></pre>
"
40051232,6640099.0,2016-10-14T20:08:27Z,40051073,0,"<p>Use a container type instead of a single variable. In this case, <code>list()</code> would seem appropriate:</p>

<pre><code>inputs = [] # use a list to add each input value to
while n == 1:
    inputs.append(input(""Input the product code: "")) # each time the user inputs a string, added it to the inputs list
for i in inputs: # for each item in the inputs list
    print(i) # print the item
</code></pre>

<p><strong>Note:</strong> The above code will not compile. You need to fill in the value for the variable <code>n</code>.</p>
"
40051396,1874016.0,2016-10-14T20:20:25Z,40051205,4,"<p>Pip is a python script. Open it and see : </p>

<p>it begins with <code>#!/usr/bin/python</code></p>

<p>You can either create a symbolic link in the old path to point to the new one, or replace the shebang with the new path. You can also keep your distrib interpreter safe by leaving it be and set the compiled one into a new <strong>virtualenv</strong>.</p>
"
40051595,2901002.0,2016-10-14T20:36:06Z,40051567,3,"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html"" rel=""nofollow""><code>combine_first</code></a>:</p>

<pre><code>print (df_b.combine_first(df_a))
  name  value
0    a    1.0
1    b    2.0
2    c    3.0
3    d    4.0
</code></pre>

<p>Or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html"" rel=""nofollow""><code>fillna</code></a>:</p>

<pre><code>print (df_b.fillna(df_a))
  name  value
0    a    1.0
1    b    2.0
2    c    3.0
3    d    4.0
</code></pre>

<p>Solution with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.update.html"" rel=""nofollow""><code>update</code></a> is not so common as <code>combine_first</code>:</p>

<pre><code>df_b.update(df_a)
print (df_b)
  name  value
0    a    1.0
1    b    2.0
2    c    3.0
3    d    4.0
</code></pre>
"
40052060,389289.0,2016-10-14T21:11:21Z,40051914,4,"<p>You write your code and formulate your question as if it was possible to modify strings in Python. <strong>It is not possible.</strong></p>

<p>Strings are immutable. All functions which operate on strings return new strings. They do not modify existing strings.</p>

<p>This returns a list of strings, but you are not using the result:</p>

<pre><code>string.split(' ')
</code></pre>

<p>This also:</p>

<pre><code>i.split()
</code></pre>

<p>This deletes the variable named <code>char</code>. It does not affect the char itself:</p>

<pre><code>        del char
</code></pre>

<p>This creates a new string which you do not use:</p>

<pre><code>        ''.join(i)
</code></pre>

<p>This also:</p>

<pre><code>        ' '.join(string)
</code></pre>

<p>All in all, almost every line of the code is wrong.</p>

<p>You probably wanted to do this:</p>

<pre><code>def remove_exclamation(string):
    words = string.split(' ')
    rtn_words = []
    for word in words:
        word_without_exclamation = ''.join(ch for ch in word if ch != '!')
        rtn_words.append(word_without_exclamation)
    return ' '.join(rtn_words)
</code></pre>

<p>But in the end, this does the same thing:</p>

<pre><code>def remove_exclamation(string):
    return string.replace('!', '')
</code></pre>
"
40052155,337315.0,2016-10-14T21:18:54Z,40051914,1,"<p>Without clearly knowing the intentions of your function and what you are attempting to do. I have an alternative to the answer that zvone gave.</p>

<p>This option is to remove any characters that you have not defined in an allowed characters list:</p>

<pre><code>characters = ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ""

test_string = ""This is an example!""

test_string = ''.join(list(filter(lambda x: x in characters, test_string)))

print(test_string)
</code></pre>

<p>This outputs:</p>

<blockquote>
  <p>This is an example</p>
</blockquote>

<p>Note, this is the Python 3 version.</p>

<p>Python 2, you do not need the <code>''.join(list())</code></p>

<p>Doing it this way would allow you to define any character that you do not want present in your string, and it will remove them. </p>

<p>You can even do the reverse:</p>

<pre><code>ignore_characters= ""!""

test_string = ""This is an example!""

test_string = ''.join(list(filter(lambda x: x not in ignore_characters, test_string)))

print(test_string)
</code></pre>
"
40052175,6640099.0,2016-10-14T21:20:35Z,40051914,0,"<p>Strings are immutable in Python. And you cannot change them. You can however, re-assign there values.</p>

<p>That is where your problem lies. You never reassign the value of your strings, when you call <code>.split()</code> on them. </p>

<p>But there are also others errors in your program such as:</p>

<ul>
<li>Your indention</li>
<li>The fact that your just returning the string thats passed into the function</li>
<li>Your use of the <code>del</code> statement</li>
<li>etc.</li>
</ul>

<p>Instead, create a new string by iterating through the old one and filtering out the character(s) you do not want, via list comprehension and <code>''.join()</code>.</p>

<pre><code>def remove_exclamation(string):
    return ''.join([char for char in string if char != '!'])
</code></pre>

<p>But as @Moses has already said in the comments, why not just use <code>str.replace()</code>?:</p>

<p><code>string = string.replace('!', '')</code></p>
"
40052375,1394353.0,2016-10-14T21:37:22Z,40051914,0,"<pre><code>def remove_exclamation(string):

    #you think you are splitting string into tokens
    #but you are not assigning the split anywhere...
    string.split(' ')

    #and here you are cycling through individual _chars_ in string which was not affected by the split above ;-)
    for i in string:
        #and now you are splitting a 1-char string and again not assigning it.
        i.split()
</code></pre>

<p>And <code>string</code> is still your input param, which I assume is of <code>type str</code>.  And immutable.</p>

<p>On top of which, if you were import/using the <code>string</code> <strong>module</strong>, you would be shadowing it</p>

<p>A big part of your confusion is knowing when the methods mutate the objects and when they return a new object.  In the case of strings, they never mutate, you need to assign the results to a new variable.</p>

<p>On a list however, and the <code>join()</code> somewhere makes me think you want to use a list, then methods generally change the object in place.</p>

<p>Anyway, on to your question:</p>

<pre><code>def remove_exclamation(inputstring, to_remove=""!""):
    return """".join([c for c in inputstring if c != to_remove])

print (remove_exclamation('This is an example!'))
</code></pre>

<p>output:<br>
<code>This is an example</code></p>
"
40052448,2482744.0,2016-10-14T21:44:01Z,40052328,4,"<p>From <a href=""https://en.wikipedia.org/wiki/Knuth%27s_up-arrow_notation"" rel=""nofollow"">Wikipedia</a>:<a href=""https://i.stack.imgur.com/U0xV7.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/U0xV7.png"" alt=""enter image description here""></a></p>

<p>Suppose you want to store this number (call it <code>N</code>) on a computer, i.e. in binary. This will require <code>k</code> bits, where <code>2^k ~ N</code>. That means <code>k</code> is itself extremely large (just compare <code>2^k</code> with the tower at the end), much too large to be stored on all the hard drives in the world.</p>
"
40053439,5544381.0,2016-10-14T23:36:50Z,39888949,1,"<p>From all four methods I'd take the second. But you have to take care of 
small details in the implementation. 
(with a few improvements it takes <em>0.002 seconds</em> meanwhile the original implementation takes about <em>6 seconds</em>; the file I was working was 1M rows; but there should not be too much difference if the file is 1K times bigger as we are not using almost memory).</p>

<p>Changes from the original implementation:</p>

<ul>
<li>Use iterators if possible, otherwise memory consumption will be penalized and you have to handle the whole file at once.
(mainly if you are using python 2, instead of using zip use itertools.izip)</li>
<li>When you are concatenating strings, use ""%s%s"".format() or similar; otherwise you generate one new string instance each time.</li>
<li>There's no need of writing line by line inside the for. You can use an iterator inside the write.</li>
<li>Small buffers are very interesting but if we are using iterators the difference is very small, but if we try to fetch all data at once (so, for example, we put f1.readlines(1024*1000), it's much slower).</li>
</ul>

<p>Example:</p>

<pre><code>def concat_iter(file1, file2, output):
    with open(output, 'w', 1024) as fo, \
        open(file1, 'r') as f1, \
        open(file2, 'r') as f2:
        fo.write("""".join(""{}\t{}"".format(l1, l2) 
           for l1, l2 in izip(f1.readlines(1024), 
                              f2.readlines(1024))))
</code></pre>

<p>Profiler original solution.</p>

<p>We see that the biggest problem is in write and zip (mainly for not using iterators and having to handle/ process all file in memory).</p>

<pre><code>~/personal/python-algorithms/files$ python -m cProfile sol_original.py 
10000006 function calls in 5.208 seconds

Ordered by: standard name

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000    5.208    5.208 sol_original.py:1(&lt;module&gt;)
    1    2.422    2.422    5.208    5.208 sol_original.py:1(concat_files_zip)
    1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    **9999999    1.713    0.000    1.713    0.000 {method 'write' of 'file' objects}**
    3    0.000    0.000    0.000    0.000 {open}
    1    1.072    1.072    1.072    1.072 {zip}
</code></pre>

<p>Profiler:</p>

<pre><code>~/personal/python-algorithms/files$ python -m cProfile sol1.py 
     3731 function calls in 0.002 seconds

Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
    1    0.000    0.000    0.002    0.002 sol1.py:1(&lt;module&gt;)
    1    0.000    0.000    0.002    0.002 sol1.py:3(concat_iter6)
 1861    0.001    0.000    0.001    0.000 sol1.py:5(&lt;genexpr&gt;)
    1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
 1860    0.001    0.000    0.001    0.000 {method 'format' of 'str' objects}
    1    0.000    0.000    0.002    0.002 {method 'join' of 'str' objects}
    2    0.000    0.000    0.000    0.000 {method 'readlines' of 'file' objects}
    **1    0.000    0.000    0.000    0.000 {method 'write' of 'file' objects}**
    3    0.000    0.000    0.000    0.000 {open}
</code></pre>

<p>And in python 3 is even faster, because iterators are built-in and we dont need to import any library.</p>

<pre><code>~/personal/python-algorithms/files$ python3.5 -m cProfile sol2.py 
843 function calls (842 primitive calls) in 0.001 seconds
[...]
</code></pre>

<p>And also it's very nice to see memory consumption and File System accesses that confirms what we have said before:</p>

<pre><code>$ /usr/bin/time -v python sol1.py
Command being timed: ""python sol1.py""
User time (seconds): 0.01
[...]
Maximum resident set size (kbytes): 7120
Average resident set size (kbytes): 0
Major (requiring I/O) page faults: 0
Minor (reclaiming a frame) page faults: 914
[...]
File system outputs: 40
Socket messages sent: 0
Socket messages received: 0


$ /usr/bin/time -v python sol_original.py 
Command being timed: ""python sol_original.py""
User time (seconds): 5.64
[...]
Maximum resident set size (kbytes): 1752852
Average resident set size (kbytes): 0
Major (requiring I/O) page faults: 0
Minor (reclaiming a frame) page faults: 427697
[...]
File system inputs: 0
File system outputs: 327696
</code></pre>
"
40053690,5411494.0,2016-10-15T00:14:07Z,40053653,3,"<p><code>[\d+]</code> = one digit (<code>0-9</code>) or <code>+</code> character.</p>

<p><code>\d+</code> = one or more digits.</p>
"
40054094,616616.0,2016-10-15T01:35:29Z,40053875,2,"<h3><code>map_blocks</code></h3>

<p>The <a href=""http://dask.pydata.org/en/latest/array-api.html#dask.array.core.map_blocks"" rel=""nofollow"">map_blocks</a> method may be helpful:</p>

<pre><code>dcoor.map_blocks(pdist)
</code></pre>

<h3>Uneven arrays</h3>

<p>It looks like you're doing a bit of fancy slicing to insert particular values into particular locations of an output array.  This will probably be awkward to do with dask.arrays.  Instead, I recommend making a function that produces a numpy array</p>

<pre><code>def myfunc(chunk):
    values = pdist(chunk[0, :, :])
    output = np.zeroes((2048, 2048))
    r = np.arange(2048)
    output[r[:, None] &lt; r] = values
    return output

dcoor.map_blocks(myfunc)
</code></pre>

<h3><code>delayed</code></h3>

<p>Worst case scenario you can always use <a href=""http://dask.pydata.org/en/latest/delayed.html"" rel=""nofollow"">dask.delayed</a></p>

<pre><code>from dask import delayed, compute
coor2 = delayed(coor)
slices = [coor2[i] for i in range(coor.shape[0])]
slices2 = [delayed(pdist)(slice) for slice in slices]
results = compute(*slices2)
</code></pre>
"
40054183,616616.0,2016-10-15T01:55:35Z,40052981,2,"<h3>Single machine scheduling</h3>

<p>Dask.dataframe will use the threaded scheduler by default with as many threads as you have logical cores in your machine.  </p>

<p>As pointed out in the comments, you can control the number of threads or the Pool implementation with keyword arguments to the <code>.compute()</code> method.</p>

<h3>Distributed machine scheduling</h3>

<p>You can use <a href=""http://distributed.readthedocs.io/en/latest/"" rel=""nofollow"">dask.distributed</a> to <a href=""http://distributed.readthedocs.io/en/latest/setup.html"" rel=""nofollow"">deploy dask workers across many nodes in a cluster</a>.  One way to do this with <code>qsub</code> is to start a <code>dask-scheduler</code> locally:</p>

<pre><code>$ dask-scheduler
Scheduler started at 192.168.1.100:8786
</code></pre>

<p>And then use <code>qsub</code> to launch many <code>dask-worker</code> processes, pointed at the reported address:</p>

<pre><code>$ qsub dask-worker 192.168.1.100:8786 ... &lt;various options&gt;
</code></pre>

<p>As of yesterday there is an experimental package that can do this on any DRMAA-enabled system (which includes SGE/qsub-like systems): <a href=""https://github.com/dask/dask-drmaa"" rel=""nofollow"">https://github.com/dask/dask-drmaa</a></p>

<p>After you have done this you can create a <code>dask.distributed.Client</code> object, which will take over as default scheduler</p>

<pre><code>from dask.distributed import Client
c = Client('192.168.1.100:8786')  # now computations run by default on the cluster
</code></pre>

<h3>Multi-threaded performance</h3>

<p>Note that as of Pandas version 0.19 the GIL still isn't released for <code>pd.merge</code>, so I wouldn't expect a huge speed boost from using multiple threads.  If this is important to you then I recommend putting in a comment here: <a href=""https://github.com/pandas-dev/pandas/issues/13745"" rel=""nofollow"">https://github.com/pandas-dev/pandas/issues/13745</a></p>
"
40054236,2225682.0,2016-10-15T02:06:36Z,40054079,0,"<p><code>os.system</code> is not patched, but <a href=""https://docs.python.org/3/library/subprocess.html#subprocess.call"" rel=""nofollow""><code>subprocess.call</code></a> is patched; Replace <code>os.system</code> with <code>subprocess.call</code> (You can also use <a href=""https://docs.python.org/3/library/subprocess.html#subprocess.call"" rel=""nofollow""><code>subprocess.run</code></a> if you are using Python 3.5+)</p>

<pre><code>import subprocess

...

def print_head(i):
    switch = '192.168.182.170'
    response = subprocess.call(""ping "" + switch)
</code></pre>
"
40054250,6553617.0,2016-10-15T02:10:11Z,40054079,0,"<p>The problem is that <code>os.system(""ping -c 5 "" + switch)</code> is running synchronously, because the function is blocking. You should try to do it in different processes.</p>

<p>Here is a concurrent code that does the same.</p>

<pre><code>from multiprocessing import Process
import os

def print_head(i):
    switch='192.168.182.170'
    response = os.system(""ping -c 5 "" + switch)

processes = [Process(target=print_head, args=(i,)) for i in range(1,10)]
for process in processes:
    process.start()
</code></pre>
"
40054364,6043170.0,2016-10-15T02:33:56Z,39824381,4,"<p>You probably havenât compiled the caffe python wrappers in your AWS environment.  For reasons that completely escape me (and several others, <a href=""https://github.com/BVLC/caffe/issues/2440"" rel=""nofollow"">https://github.com/BVLC/caffe/issues/2440</a>) pycaffe is not available as a pypi package, and you have to compile it yourself.  You should follow the compilation/make instructions here or automate it using ebextensions if you are in an AWS EB environment: <a href=""http://caffe.berkeleyvision.org/installation.html#python"" rel=""nofollow"">http://caffe.berkeleyvision.org/installation.html#python</a></p>
"
40055000,6947337.0,2016-10-15T04:38:46Z,40054935,2,"<p>The syntax of the command is <code>conda create -n &lt;name_of_new_env&gt; &lt;packages&gt;</code>. As a result, you created a clean environment named <code>tensorflow</code> with only Python 3.5 installed. Since <code>conda search tensorflow</code> returned nothing, you will have to use <code>pip</code> or some other method of installing the package. Since there is spotty official support for Windows, the conda-forge package (CPU only) at <a href=""https://github.com/conda-forge/tensorflow-feedstock"" rel=""nofollow"">https://github.com/conda-forge/tensorflow-feedstock</a> is probably the best way.</p>

<p>People have also reported success installing Tensorflow with docker, if you have docker set up already.</p>
"
40055245,582782.0,2016-10-15T05:17:48Z,40055177,3,"<p>specify an <strong>object</strong> type for your array, such as:</p>

<pre><code>a = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53']],dtype=object)
</code></pre>

<p>Then, 
<code>a[0][0]+='_1'</code> will do the trick, you will get:</p>

<pre><code>array([['Height_1', 'Weight'],
       ['165', '48'],
       ['168', '50'],
       ['173', '53']], dtype=object)
</code></pre>
"
40055250,5014455.0,2016-10-15T05:18:16Z,40055177,4,"<p>The problem is your array is of <code>dtype('&lt;U6')</code></p>

<pre><code>&gt;&gt;&gt; data = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53']])
&gt;&gt;&gt; data.dtype
dtype('&lt;U6')
&gt;&gt;&gt; 
</code></pre>

<p>It will automatically truncate:</p>

<pre><code>&gt;&gt;&gt; data[0,0] = ""123456789""
&gt;&gt;&gt; data
array([['123456', 'Weight'],
       ['165', '48'],
       ['168', '50'],
       ['173', '53']], 
      dtype='&lt;U6')
&gt;&gt;&gt; 
</code></pre>

<p>You can always specify your dtype as 'object' when you create your array, but this removes a lot of the speed benefits of <code>numpy</code> to begin with.</p>

<p>Alternatively, you can specify a longer string type:</p>

<pre><code>&gt;&gt;&gt; data
array([['Height', 'Weight'],
       ['165', '48'],
       ['168', '50'],
       ['173', '53']], 
      dtype='&lt;U20')
&gt;&gt;&gt; data[0,0]='Height_1'
&gt;&gt;&gt; data
array([['Height_1', 'Weight'],
       ['165', '48'],
       ['168', '50'],
       ['173', '53']], 
      dtype='&lt;U20')
&gt;&gt;&gt; 
</code></pre>

<p>But be careful, as if you set the limit too-long you will be wasting memory:</p>

<pre><code>&gt;&gt;&gt; data = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53'], ['42','88']], dtype='U20')
&gt;&gt;&gt; data.nbytes
800
&gt;&gt;&gt; data = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53'], ['42','88']], dtype='U6')
&gt;&gt;&gt; data.nbytes
240
</code></pre>

<p>If you only need a limited amount of characters, consider using byte-strings (1/4th the memory requirement):</p>

<pre><code>&gt;&gt;&gt; data = np.array([['Height', 'Weight'],['165', '48'],['168', '50'],['173', '53'], ['42','88']], dtype='S20')
&gt;&gt;&gt; data.nbytes
200
&gt;&gt;&gt; 
</code></pre>
"
40055254,5771269.0,2016-10-15T05:18:43Z,40053653,1,"<p>You could also do:</p>

<pre><code>if re.search(r'^[789]\d{9}$', x):
</code></pre>

<p>letting the regex handle the <code>len(x)==10</code> part by using explicit lengths instead of unbounded repetitions.</p>
"
40055675,4954434.0,2016-10-15T06:16:46Z,40053653,0,"<p>I think a general explanation about <code>[]</code> and <code>+</code> is what you need.  </p>

<p><code>[]</code> will match with a single character specified inside.<br>
Eg: <code>[qwe]</code> will match with <code>q</code>, <code>w</code> or <code>e</code>.  </p>

<p><strong>If you want to enter an expression inside <code>[]</code>, you need to use it as <code>[^ expression]</code>.</strong></p>

<p><code>+</code> will match the preceding element one or more times.
Eg: <code>qw+e</code> matches <code>qwe</code>, <code>qwwe</code>, <code>qwwwwe</code>, etc...<br>
Note: this is different from <code>*</code> as <code>*</code> matches preceding element zero or more times. i.e. <code>qw*e</code> matches <code>qe</code> as well.</p>

<p><code>\d</code> matches with numerals. (not just <code>0-9</code>, but numerals from other language scripts as well.)</p>
"
40055892,5714445.0,2016-10-15T06:43:10Z,40055835,3,"<p>If you want to do it the numpy way,</p>

<pre><code>import numpy as np

A = np.array([[1, 1, 1,], [1, 1, 2], [1, 1, 3], [1, 1, 4]])
B = np.array([[0, 0, 0], [1, 0, 2], [1, 0, 3], [1, 0, 4], [1, 1, 0], [1, 1, 1], [1, 1, 4]])
A_rows = A.view([('', A.dtype)] * A.shape[1])
B_rows = B.view([('', B.dtype)] * B.shape[1])

diff_array = np.setdiff1d(A_rows, B_rows).view(A.dtype).reshape(-1, A.shape[1])
</code></pre>

<p>As @Rahul suggested, for a non numpy easy solution,</p>

<pre><code>diff_array = [i for i in A if i not in B]
</code></pre>
"
40055928,4407666.0,2016-10-15T06:47:34Z,40055835,4,"<p>there is a easy solution with <a href=""http://www.secnetix.de/olli/Python/list_comprehensions.hawk"" rel=""nofollow"">list comprehension</a>,</p>

<pre><code>A = [i for i in A if i not in B]
</code></pre>

<p>Result</p>

<pre><code>[[1, 1, 2], [1, 1, 3]]
</code></pre>

<p>List comprehension it's not removing the elements from the array, It's just reassigning, </p>

<p>if you want to remove the elements use this method</p>

<pre><code>for i in B:
     if i in A:
     A.remove(i)
</code></pre>
"
40055932,3873366.0,2016-10-15T06:48:09Z,40055835,3,"<p>Another non-numpy solution:</p>

<pre><code>[i for i in A if i not in B]
</code></pre>
"
40056135,2867928.0,2016-10-15T07:07:33Z,40055835,9,"<p>Here is a Numpythonic approach with <em>broadcasting</em>:</p>

<pre><code>In [83]: A[np.all(np.any((A-B[:, None]), axis=2), axis=0)]
Out[83]: 
array([[1, 1, 2],
       [1, 1, 3]])
</code></pre>

<p>Here is a timeit with other answer:</p>

<pre><code>In [90]: def cal_diff(A, B):
   ....:     A_rows = A.view([('', A.dtype)] * A.shape[1])
   ....:     B_rows = B.view([('', B.dtype)] * B.shape[1])
   ....:     return np.setdiff1d(A_rows, B_rows).view(A.dtype).reshape(-1, A.shape[1])
   ....: 

In [93]: %timeit cal_diff(A, B)
10000 loops, best of 3: 54.1 Âµs per loop

In [94]: %timeit A[np.all(np.any((A-B[:, None]), axis=2), axis=0)]
100000 loops, best of 3: 9.41 Âµs per loop

# Even better with Divakar's suggestion
In [97]: %timeit A[~((A[:,None,:] == B).all(-1)).any(1)]
100000 loops, best of 3: 7.41 Âµs per loop
</code></pre>

<p>Well, if you are looking for a faster way you should looking for ways that reduce the number of comparisons. In this case (without considering the order) you can generate a unique number from your rows and compare the numbers which can be done with summing the items power of two.</p>

<p>Here is the benchmark with Divakar's in1d approach:</p>

<pre><code>In [144]: def in1d_approach(A,B):
   .....:         dims = np.maximum(B.max(0),A.max(0))+1
   .....:         return A[~np.in1d(np.ravel_multi_index(A.T,dims),\
   .....:                          np.ravel_multi_index(B.T,dims))]
   .....: 

In [146]: %timeit in1d_approach(A, B)
10000 loops, best of 3: 23.8 Âµs per loop

In [145]: %timeit A[~np.in1d(np.power(A, 2).sum(1), np.power(B, 2).sum(1))]
10000 loops, best of 3: 20.2 Âµs per loop
</code></pre>

<p>You can use <code>np.diff</code> to get the an order independent result:</p>

<pre><code>In [194]: B=np.array([[0, 0, 0,], [1, 0, 2,], [1, 0, 3,], [1, 0, 4,], [1, 1, 0,], [1, 1, 1,], [1, 1, 4,], [4, 1, 1]])

In [195]: A[~np.in1d(np.diff(np.diff(np.power(A, 2))), np.diff(np.diff(np.power(B, 2))))]
Out[195]: 
array([[1, 1, 2],
       [1, 1, 3]])

In [196]: %timeit A[~np.in1d(np.diff(np.diff(np.power(A, 2))), np.diff(np.diff(np.power(B, 2))))]
10000 loops, best of 3: 30.7 Âµs per loop
</code></pre>

<p>Benchmark with Divakar's setup:</p>

<pre><code>In [198]: B = np.random.randint(0,9,(1000,3))

In [199]: A = np.random.randint(0,9,(100,3))

In [200]: A_idx = np.random.choice(np.arange(A.shape[0]),size=10,replace=0)

In [201]: B_idx = np.random.choice(np.arange(B.shape[0]),size=10,replace=0)

In [202]: A[A_idx] = B[B_idx]

In [203]: %timeit A[~np.in1d(np.diff(np.diff(np.power(A, 2))), np.diff(np.diff(np.power(B, 2))))]
10000 loops, best of 3: 137 Âµs per loop

In [204]: %timeit A[~np.in1d(np.power(A, 2).sum(1), np.power(B, 2).sum(1))]
10000 loops, best of 3: 112 Âµs per loop

In [205]: %timeit in1d_approach(A, B)
10000 loops, best of 3: 115 Âµs per loop
</code></pre>

<p>Timing with larger arrays (Divakar's solution is slightly faster):</p>

<pre><code>In [231]: %timeit A[~np.in1d(np.diff(np.diff(np.power(A, 2))), np.diff(np.diff(np.power(B, 2))))]
1000 loops, best of 3: 1.01 ms per loop

In [232]: %timeit A[~np.in1d(np.power(A, 2).sum(1), np.power(B, 2).sum(1))]
1000 loops, best of 3: 880 Âµs per loop

In [233]:  %timeit in1d_approach(A, B)
1000 loops, best of 3: 807 Âµs per loop
</code></pre>
"
40056251,3293881.0,2016-10-15T07:19:03Z,40055835,9,"<p>Based on <a href=""http://stackoverflow.com/a/38674038/3293881""><code>this solution</code></a> to <a href=""http://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array""><code>Find the row indexes of several values in a numpy array</code></a>, here's a NumPy based solution with less memory footprint and could be beneficial when working with large arrays -</p>

<pre><code>dims = np.maximum(B.max(0),A.max(0))+1
out = A[~np.in1d(np.ravel_multi_index(A.T,dims),np.ravel_multi_index(B.T,dims))]
</code></pre>

<p>Sample run -</p>

<pre><code>In [38]: A
Out[38]: 
array([[1, 1, 1],
       [1, 1, 2],
       [1, 1, 3],
       [1, 1, 4]])

In [39]: B
Out[39]: 
array([[0, 0, 0],
       [1, 0, 2],
       [1, 0, 3],
       [1, 0, 4],
       [1, 1, 0],
       [1, 1, 1],
       [1, 1, 4]])

In [40]: out
Out[40]: 
array([[1, 1, 2],
       [1, 1, 3]])
</code></pre>

<p>Runtime test on large arrays -</p>

<pre><code>In [107]: def in1d_approach(A,B):
     ...:     dims = np.maximum(B.max(0),A.max(0))+1
     ...:     return A[~np.in1d(np.ravel_multi_index(A.T,dims),\
     ...:                     np.ravel_multi_index(B.T,dims))]
     ...: 

In [108]: # Setup arrays with B as large array and A contains some of B's rows
     ...: B = np.random.randint(0,9,(1000,3))
     ...: A = np.random.randint(0,9,(100,3))
     ...: A_idx = np.random.choice(np.arange(A.shape[0]),size=10,replace=0)
     ...: B_idx = np.random.choice(np.arange(B.shape[0]),size=10,replace=0)
     ...: A[A_idx] = B[B_idx]
     ...: 
</code></pre>

<p>Timings with <code>broadcasting</code> based solutions -</p>

<pre><code>In [109]: %timeit A[np.all(np.any((A-B[:, None]), axis=2), axis=0)]
100 loops, best of 3: 4.64 ms per loop # @Kasramvd's soln

In [110]: %timeit A[~((A[:,None,:] == B).all(-1)).any(1)]
100 loops, best of 3: 3.66 ms per loop
</code></pre>

<p>Timing with less memory footprint based solution -</p>

<pre><code>In [111]: %timeit in1d_approach(A,B)
1000 loops, best of 3: 231 Âµs per loop
</code></pre>

<p><strong>Further performance boost</strong></p>

<p><code>in1d_approach</code> reduces each row by considering each row as an indexing tuple. We can do the same a bit more efficiently by introducing matrix-multiplication with <code>np.dot</code>, like so -</p>

<pre><code>def in1d_dot_approach(A,B):
    cumdims = (np.maximum(A.max(),B.max())+1)**np.arange(B.shape[1])
    return A[~np.in1d(A.dot(cumdims),B.dot(cumdims))]
</code></pre>

<p>Let's test it against the previous on much larger arrays -</p>

<pre><code>In [251]: # Setup arrays with B as large array and A contains some of B's rows
     ...: B = np.random.randint(0,9,(10000,3))
     ...: A = np.random.randint(0,9,(1000,3))
     ...: A_idx = np.random.choice(np.arange(A.shape[0]),size=10,replace=0)
     ...: B_idx = np.random.choice(np.arange(B.shape[0]),size=10,replace=0)
     ...: A[A_idx] = B[B_idx]
     ...: 

In [252]: %timeit in1d_approach(A,B)
1000 loops, best of 3: 1.28 ms per loop

In [253]: %timeit in1d_dot_approach(A, B)
1000 loops, best of 3: 1.2 ms per loop
</code></pre>
"
40057021,6599590.0,2016-10-15T08:53:09Z,39692999,0,"<p>Try this solution:</p>

<pre><code>t = nuke.Int_Knob( 'tempMb', 'tempMb' )

for i in nuke.allNodes():
    if not i.knob(""tempMb""):
        if nuke.exists(""sum0""):
            nuke.message(""First tmp knob created"")
            i.addKnob(t)
        else:
            nuke.message(""Second tmp knob created"")
    else: 
        nuke.message(""No knob created"")
</code></pre>

<p><a href=""https://i.stack.imgur.com/fqgbx.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/fqgbx.png"" alt=""enter image description here""></a></p>
"
40057142,1381627.0,2016-10-15T09:05:36Z,40056998,0,"<p>The problem with your code - if  line does not contain <em>full</em> JSON object - it seldom does - it will nearly always fail.</p>

<p>Unlike in Java, in Python JSON is naturally represented by hierarchical mixture of list and dictionary elements. So, if you are looking for list elements in your JSON, you may use recursive search.</p>

<p>If you want to check if your file is valid JSON - the code below is simpler and shorter test</p>

<pre><code>try:
    with open(file_name) as f:
        json_obj = json.load(f)
    except:
        print ""Not valid JSON""
</code></pre>

<p><strong>EDIT</strong>
Is it JSON file or JSON schema?</p>

<p>In the latter, you may just check if your object is list</p>

<pre><code>obj = json.loads(line)
isintance(obj, list)
</code></pre>

<p>As I have already stated, there's no such thing as ""JSON object"" in Python</p>

<p>PS If you read file line by line, and each line is JSON object - it's not a JSON file, it's a file where each string contains JSON. Otherwise, your test will fail on the first line already, which will be just </p>

<pre><code>{
</code></pre>
"
40057403,3677097.0,2016-10-15T09:35:10Z,40057218,1,"<p>This should work for you:</p>

<pre><code>'\d+\s*([a-zA-Z].*\w)\s+\d.*'
</code></pre>

<p>The <code>'\d+</code> will only match the positive quantities, and with less steps.</p>

<p>Now, just extract the info from the capture group.</p>

<p><kbd> <a href=""https://regex101.com/r/UylZqb/1"" rel=""nofollow"">Demo</a> </kbd></p>
"
40057410,2026325.0,2016-10-15T09:35:39Z,40056998,2,"<p>In python, JSON object is converted into <code>dict</code> and JSON list is converted into <code>list</code> datatypes.</p>

<p>So, if you want to check the line content which should be valid <strong>JSON</strong>, is <code>JSON Object</code> or <code>JSON Array</code>, then this code will helps you:-</p>

<pre><code>import json

# assume that, each line is valid json data
obj = json.loads(line)

# if returns true, then JSON Array
isintance(obj, list)

# if returns true, then JSON Object.
isintance(obj, dict)
</code></pre>
"
40057486,5306028.0,2016-10-15T09:45:07Z,39913847,0,"<p>You're probably looking for something like Freeze, which is able to compile your Python application with all its libraries into a static binary:</p>

<p><a href=""https://pypi.python.org/pypi/cx_Freeze"" rel=""nofollow"">PyPi page of Freeze</a></p>

<p><a href=""https://wiki.python.org/moin/Freeze"" rel=""nofollow"">Python Wiki page of Freeze</a></p>

<p><a href=""http://cx-freeze.sourceforge.net/"" rel=""nofollow"">Sourceforge page of Freeze</a></p>
"
40057533,2040251.0,2016-10-15T09:50:36Z,40056061,1,"<p>Once you have a convex hull, you can find two furthest points in linear time. </p>

<p>The idea is to keep two pointers: one of them points to the current edge (and is always incremented by one) and the other one points to a vertex.</p>

<p>The answer is the maximum distance between end points of an edge and the vertex for all edges.</p>

<p>It is possible to show (the proof is neither short nor trivial, so I will not post it here) that if we keep incrementing the second pointer every time after moving the first one as long as it increases the distance between the line that goes through the edge and a vertex, we will find the optimal answer.  </p>
"
40057634,5714445.0,2016-10-15T10:00:42Z,39913847,11,"<p>There are two ways you could go about to solve your problem</p>

<ol>
<li>Use a static builder, like freeze, or pyinstaller, or py2exe</li>
<li>Compile using cython</li>
</ol>

<p>I will explain how you can go about doing it using the second, since the first method is not cross platform and version, and has been explained in other answers. Also, using programs like pyinstaller typically results in huge file sizes, where as using cython will result in a file that's KBs in size</p>

<p>First, install cython. Then, rename your python file (say test.py) into a pyx file </p>

<pre><code>$ sudo pip install cython
$ mv test.py test.pyx
</code></pre>

<p>Then, you can use cython along with GCC to compile it (Cython generates a C file out of a Python .pyx file, and then GCC compiles the C file)
(in reference to <a href=""http://stackoverflow.com/a/22040484/5714445"">http://stackoverflow.com/a/22040484/5714445</a>)</p>

<pre><code>$ cython test.pyx --embed
$ gcc -Os -I /usr/include/python3.5m -o test test.c -lpython3.5m -lpthread -lm -lutil -ldl
</code></pre>

<p>NOTE: Depending on your version of python, you might have to change the last command. To know which version of python you are using, simply use</p>

<pre><code>$ python -V
</code></pre>

<p>You will now have a binary file 'test', which is what you are looking for</p>

<p>NOTE: Cython is used to use C-Type Variable definitions for static memory allocation to speed up Python programs. In your case however, you will still be using traditional Python definitions.</p>

<p>NOTE2: If you are using additional libraries (like opencv, for example), you might have to provide the directory to them using -L and then specify the name of the library using -l in the GCC Flags. For more information on this, please refer to GCC flags</p>
"
40057999,3293881.0,2016-10-15T10:39:59Z,40056275,5,"<p>I would try to answer the second part of the question.</p>

<p>So, with it we are comparing :</p>

<pre><code>A[np.all(np.any((A-B[:, None]), axis=2), axis=0)]  (I)
</code></pre>

<p>and </p>

<pre><code>A[~((A[:,None,:] == B).all(-1)).any(1)]
</code></pre>

<p>To compare with a matching perspective against the first one, we could write down the second approach like this -</p>

<pre><code>A[(((~(A[:,None,:] == B)).any(2))).all(1)]         (II)
</code></pre>

<p>The major difference when considering performance, would be the fact that with the first one, we are getting non-matches with subtraction and then checking for non-zeros with <code>.any()</code>. Thus, <code>any()</code> is made to operate on an array of non-boolean dtype array. In the second approach, instead we are feeding it a boolean array obtained with <code>A[:,None,:] == B</code>.</p>

<p>Let's do a small runtime test to see how <code>.any()</code> performs on <code>int</code> dtype vs <code>boolean array</code> -</p>

<pre><code>In [141]: A = np.random.randint(0,9,(1000,1000)) # An int array

In [142]: %timeit A.any(0)
1000 loops, best of 3: 1.43 ms per loop

In [143]: A = np.random.randint(0,9,(1000,1000))&gt;5 # A boolean array

In [144]: %timeit A.any(0)
10000 loops, best of 3: 164 Âµs per loop
</code></pre>

<p>So, with close to <strong><code>9x</code></strong> speedup on this part, we see a huge advantage to use <code>any()</code> with boolean arrays. This I think was the biggest reason to make the second approach faster.</p>
"
40059388,4996248.0,2016-10-15T13:08:29Z,40059330,2,"<p>Maybe like this:</p>

<pre><code>top = list(x+y for x,y in zip(a,b))
bottom = list(x+y for x,y in zip(c,d))
total = top + bottom

for r in total: print(r)
</code></pre>

<p>Output:</p>

<pre><code>['a', 'a', 'a', 'b', 'b', 'b']
['a', 'a', 'a', 'b', 'b', 'b']
['a', 'a', 'a', 'b', 'b', 'b']
['c', 'c', 'c', 'd', 'd', 'd']
['c', 'c', 'c', 'd', 'd', 'd']
['c', 'c', 'c', 'd', 'd', 'd']
</code></pre>
"
40059403,1478537.0,2016-10-15T13:09:45Z,40059330,2,"<p>You can do it with a one line instruction, that mix list comprehension, zip instructions, and list concatenation with <code>+</code></p>

<pre><code>[aa+bb for aa,bb in zip(a,b)] + [cc+dd for cc,dd in zip(c,d)]
</code></pre>

<p>The whole code </p>

<pre><code>a = [[""a"",""a"",""a""],
    [""a"",""a"",""a""],
    [""a"",""a"",""a""]]
b = [[""b"",""b"",""b""],
    [""b"",""b"",""b""],
    [""b"",""b"",""b""]]
c = [[""c"",""c"",""c""],
    [""c"",""c"",""c""],
    [""c"",""c"",""c""]]
d = [[""d"",""d"",""d""],
    [""d"",""d"",""d""],
    [""d"",""d"",""d""]]
result = [aa+bb for aa,bb in zip(a,b)] + [cc+dd for cc,dd in zip(c,d)]
</code></pre>
"
40059476,4495081.0,2016-10-15T13:16:35Z,39919928,0,"<p>I found a workaround: specifying no retries to the <code>jobs_completed()</code> transaction:</p>

<pre><code>@ndb.transactional(xg=True, retries=0)
def jobs_completed(self, job):
</code></pre>

<p>This prevents the automatic repeated execution, instead causing an exception:</p>

<blockquote>
  <p>TransactionFailedError(The transaction could not be committed. Please
  try again.)</p>
</blockquote>

<p>Which is acceptable as I already have in place a back-off/retry safety net for the entire <code>job_completed_task()</code>. Things are OK now.</p>

<p>As for why the rollback didn't happen, the only thing that crosses my mind is that somehow the entity was read (and cached in my object attribute) prior to entering the transaction, thus not being considered part of the (same) transaction. But I couldn't find a code path that would do that, so it's just speculation.</p>
"
40059522,7014980.0,2016-10-15T13:19:35Z,40059330,0,"<p>Since you are talking about arrays, using numpy arrays and <code>hstack()</code> and <code>vstack()</code>:</p>

<pre><code>import numpy as np

ab = np.hstack((np.array(a),np.array(b)))
cd = np.hstack((np.array(c),np.array(d)))

print np.vstack((ab,cd))
</code></pre>

<p>Which leads you:</p>

<pre><code>[['a' 'a' 'a' 'b' 'b' 'b']
 ['a' 'a' 'a' 'b' 'b' 'b']
 ['a' 'a' 'a' 'b' 'b' 'b']
 ['c' 'c' 'c' 'd' 'd' 'd']
 ['c' 'c' 'c' 'd' 'd' 'd']
 ['c' 'c' 'c' 'd' 'd' 'd']]
</code></pre>

<p>Alternatively, using <code>concatenate()</code>:</p>

<pre><code>ab = np.concatenate((np.array(a),np.array(b)),axis=1)
cd = np.concatenate((np.array(c),np.array(d)),axis=1)

print np.concatenate((ab,cd))
</code></pre>
"
40059601,6642340.0,2016-10-15T13:27:52Z,40059330,2,"<p>Assuming, that layout of your map is more complicated than 2x2 blocks:</p>

<pre><code>from itertools import chain
from pprint import pprint


a = [[""a"",""a"",""a""],
     [""a"",""a"",""a""],
     [""a"",""a"",""a""]]
b = [[""b"",""b"",""b""],
     [""b"",""b"",""b""],
     [""b"",""b"",""b""]]
c = [[""c"",""c"",""c""],
     [""c"",""c"",""c""],
     [""c"",""c"",""c""]]
d = [[""d"",""d"",""d""],
     [""d"",""d"",""d""],
     [""d"",""d"",""d""]]
e = [[""e"",""e"",""e""],
     [""e"",""e"",""e""],
     [""e"",""e"",""e""]]
f = [[""f"",""f"",""f""],
     [""f"",""f"",""f""],
     [""f"",""f"",""f""]]

layouts = [
    ((a, b), (c, d)),
    ((a, b, c), (d, e, f)),
    ((a, b), (c, d), (e, f)),
]

for layout in layouts:
    total = [list(chain(*row)) for lrow in layout for row in zip(*lrow)]
    pprint(total)
</code></pre>

<p><strong>output:</strong></p>

<pre><code>[['a', 'a', 'a', 'b', 'b', 'b'],
 ['a', 'a', 'a', 'b', 'b', 'b'],
 ['a', 'a', 'a', 'b', 'b', 'b'],
 ['c', 'c', 'c', 'd', 'd', 'd'],
 ['c', 'c', 'c', 'd', 'd', 'd'],
 ['c', 'c', 'c', 'd', 'd', 'd']]

[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],
 ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],
 ['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'],
 ['d', 'd', 'd', 'e', 'e', 'e', 'f', 'f', 'f'],
 ['d', 'd', 'd', 'e', 'e', 'e', 'f', 'f', 'f'],
 ['d', 'd', 'd', 'e', 'e', 'e', 'f', 'f', 'f']]

[['a', 'a', 'a', 'b', 'b', 'b'],
 ['a', 'a', 'a', 'b', 'b', 'b'],
 ['a', 'a', 'a', 'b', 'b', 'b'],
 ['c', 'c', 'c', 'd', 'd', 'd'],
 ['c', 'c', 'c', 'd', 'd', 'd'],
 ['c', 'c', 'c', 'd', 'd', 'd'],
 ['e', 'e', 'e', 'f', 'f', 'f'],
 ['e', 'e', 'e', 'f', 'f', 'f'],
 ['e', 'e', 'e', 'f', 'f', 'f']]
</code></pre>
"
40059917,494631.0,2016-10-15T13:57:40Z,40059890,0,"<p>You can use <code>locals()['newvarname'] = 12345</code> to create a new variable.
And you can just read your file and fill in this structure as you'd like.</p>

<p>You may write a function to call it and import settings:</p>

<pre><code>import sys
import yaml


def get_settings(filename):
    flocals = sys._getframe().f_back.f_locals  # get locals() from caller
    with open(filename, 'r') as ysfile:
        ydata = yaml.load(ysfile.read())
    for pname in ydata:
        # assume the yaml file contains dict
        flocals[pname] = ydata[pname]  # merge caller's locals with new vars
</code></pre>
"
40060005,3212865.0,2016-10-15T14:07:13Z,40059890,1,"<p>Even though the trick is great in @baldr's answer, I suggest moving the variable assignments out of the function for better readability. Having a function change its caller's context seems very hard to remember and maintain. So, </p>

<pre><code>import yaml

def get_settings(filename):
    with open(filename, 'r') as yf:
        return yaml.load(yf.read())

def somewhere():
    locals().update(get_settings('foo.yaml'))
    print(param1)
</code></pre>

<p>Even then I would strongly suggest simply using the dictionary. Otherwise, you risk very strange bugs. For instance, what if your yaml file contains a <code>open</code> or a <code>print</code> key? They will override python's functions, so the <code>print(param1)</code> in the example will raise a <code>TypeError</code> exception.</p>
"
40060066,1196549.0,2016-10-15T14:12:45Z,40058686,0,"<p>You obviously face a saturation problem, due to some computed component exceeding the range [0,255]. Clamp the values or adjust the gain.</p>

<p>Then there seems to be a swapping of the components somewhere.</p>
"
40060154,1631135.0,2016-10-15T14:20:23Z,39613476,3,"<p>Your requirement of <strong>one database connection per process-pool process</strong> can be easily satisfied if some care is taken on how you instantiate the <code>session</code>, assuming you are working with the orm, in the worker processes.</p>

<p>A simple solution would be to have a global <a href=""http://docs.sqlalchemy.org/en/latest/orm/contextual.html"" rel=""nofollow"">session</a> which you reuse across requests:</p>

<pre><code># db.py
engine = create_engine(""connection_uri"", pool_size=1, max_overflow=0)
DBSession = scoped_session(sessionmaker(bind=engine)) 
</code></pre>

<p>And on the worker task:</p>

<pre><code># task.py
from db import engine, DBSession
def task():
    DBSession.begin() # each task will get its own transaction over the global connection
    ...
    DBSession.query(...)
    ...
    DBSession.close() # cleanup on task end
</code></pre>

<p>Arguments <code>pool_size</code> and <code>max_overflow</code> <a href=""http://docs.sqlalchemy.org/en/latest/core/pooling.html#connection-pool-configuration"" rel=""nofollow"">customize</a> the default <a href=""http://docs.sqlalchemy.org/en/latest/core/pooling.html#sqlalchemy.pool.QueuePool"" rel=""nofollow"">QueuePool</a> used by create_engine.<code>pool_size</code> will make sure your process only keeps 1 connection alive per process in the process pool.</p>

<p>If you want it to reconnect you can use <code>DBSession.remove()</code> which will remove the session from the registry and will make it reconnect at the next DBSession usage. You can also use the <code>recycle</code> argument of <a href=""http://docs.sqlalchemy.org/en/latest/core/pooling.html#sqlalchemy.pool.Pool"" rel=""nofollow"">Pool</a> to make the connection reconnect after the specified amount of time.</p>

<p>During development/debbuging you can use <a href=""http://docs.sqlalchemy.org/en/latest/core/pooling.html#sqlalchemy.pool.AssertionPool"" rel=""nofollow"">AssertionPool</a> which will raise an exception if more than one connection is checked-out from the pool, see <a href=""http://docs.sqlalchemy.org/en/latest/core/pooling.html#switching-pool-implementations"" rel=""nofollow"">switching pool implementations</a> on how to do that.</p>
"
40060245,2285236.0,2016-10-15T14:29:14Z,40059994,1,"<p>You are testing two conditions on the same column so these can be combined (and negated):</p>

<pre><code>frame[~((frame['RR'].str.contains(""^[^123]"", na=False)) &amp; (frame['RR'].isin(series1.str.slice(1))))]
</code></pre>

<p>Here, after <code>~</code> operator, it checks whether a particular row satisfies both conditions - same as the boolean array you get in the end. With <code>~</code>, you turn True's to False's and False's to True's. Finally, <code>frame[condition]</code> returns the rows that satisfy the final condition with boolean indexing.</p>

<p>In a more readable format:</p>

<pre><code>condition1 = frame['RR'].str.contains(""^[^123]"", na=False)
condition2 = frame['RR'].isin(series1.str.slice(1))
frame[~(condition1 &amp; condition2)]
</code></pre>

<p>As an alternative (requires 0.18.0), you can get the indices of the True elements with:</p>

<pre><code>frame.loc[(frame['RR'].str.contains(""^[^123]"", na=False)), 'RR'].isin(series1.str.slice(1))[lambda df: df].index
</code></pre>
"
40061337,100297.0,2016-10-15T16:11:49Z,40061280,12,"<p>You are not indexing. You are yielding a list; the expression <code>yield[0]</code> is really just the same as the following (but without a variable):</p>

<pre><code>lst = [0]
yield lst
</code></pre>

<p>If you look at what <code>next()</code> returned you'd have gotten that list:</p>

<pre><code>&gt;&gt;&gt; def gen1():
...   t = yield[0]
...   assert t
...   yield False
...
&gt;&gt;&gt; g = gen1()
&gt;&gt;&gt; next(g)
[0]
</code></pre>

<p>You don't <em>have</em> to have a space between <code>yield</code> and the <code>[0]</code>, that's all.</p>

<p>The exception is caused by you trying to apply the subscription to the contained <code>0</code> integer:</p>

<pre><code>&gt;&gt;&gt; [0]        # list with one element, the int value 0
[0]
&gt;&gt;&gt; [0][0]     # indexing the first element, so 0
0
&gt;&gt;&gt; [0][0][0]  # trying to index the 0
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: 'int' object is not subscriptable
</code></pre>

<p>If you want to index a value sent to the generator, put parentheses around the <code>yield</code> expression:</p>

<pre><code>t = (yield)[0]
</code></pre>

<p>Demo:</p>

<pre><code>&gt;&gt;&gt; def gen1():
...     t = (yield)[0]
...     print 'Received: {!r}'.format(t)
...     yield False
...
&gt;&gt;&gt; g = gen1()
&gt;&gt;&gt; next(g)
&gt;&gt;&gt; g.send('foo')
Received: 'f'
False
</code></pre>
"
40061352,494631.0,2016-10-15T16:13:21Z,40061297,3,"<p>You can use <code>importlib</code> module: <a href=""https://docs.python.org/3/library/importlib.html"" rel=""nofollow"">https://docs.python.org/3/library/importlib.html</a></p>

<pre><code>import os
import importlib

for student_dir in os.listdir():
    if os.path.isdir(student_dir):
        # here you may add an additional check for not desired folders like 'lib', 'settings', etc
        if not os.path.exists(os.path.join(student_dir, 'answers.py')):
            print(""Error: file answers.py is not found in %s"" % (student_dir))
            continue
        student_module = importlib.import_module(""%s.answers"" % student_dir)
        try:
            student_module.run_test()
            print(""%s: OK"" % student_dir)
        except Exception as ee:
            print(""Error for %s"" % student_dir)
</code></pre>
"
40061601,3293881.0,2016-10-15T16:38:20Z,40061185,3,"<p>You are summing all values in that <code>3x3</code> neighbourhood, but excluding the element itself. So, we can use <a href=""http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"" rel=""nofollow""><code>Scipy's 2D convolution</code></a> and subtract that input array/matrix from it for the desired output, like so -</p>

<pre><code>from scipy.signal import convolve2d

convolve2d(a,np.ones((3,3),dtype=int),'same') - a
</code></pre>

<p>Sample run -</p>

<pre><code>In [11]: a
Out[11]: 
matrix([[0, 0, 0],
        [1, 0, 1],
        [0, 1, 0]])

In [12]: convolve2d(a,np.ones((3,3),dtype=int),'same') - a
Out[12]: 
matrix([[1, 2, 1],
        [1, 3, 1],
        [2, 2, 2]])
</code></pre>

<p>Or simply form a kernel with all ones but zero at the center and use the same 2D convolution -</p>

<pre><code>In [31]: kernel = np.array([[1,1,1],[1,0,1],[1,1,1]])

In [32]: np.asmatrix(convolve2d(a,kernel,'same'))
Out[32]: 
matrix([[1, 2, 1],
        [1, 3, 1],
        [2, 2, 2]])
</code></pre>
"
40062334,7014980.0,2016-10-15T17:50:25Z,40061185,1,"<p>Define a function which computes the sum of all neighbors for a matrix entry, if they exist:</p>

<pre><code>def sumNeighbors(M,x,y):
    l = []
    for i in range(max(0,x-1),x+2): # max(0,x-1), such that no negative values in range() 
        for j in range(max(0,y-1),y+2):
            try:
                t = M[i][j]
                l.append(t)
            except IndexError: # if entry doesn't exist
                pass
    return sum(l)-M[x][y] # exclude the entry itself
</code></pre>

<p>Then you can iterate for every entry in your matrix and pass its result into a new matrix N:</p>

<pre><code>import numpy as np

M = [[1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]] 

M = np.asarray(M)
N = np.zeros(M.shape)

for i in range(M.shape[0]):
    for j in range(M.shape[1]):
        N[i][j] = sumNeighbors(M, i, j)

print ""Original matrix:\n"", M
print ""Summed neighbors matrix:\n"", N
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Original matrix:
[[1 2 3]
 [4 5 6]
 [7 8 9]]
Summed neighbors matrix:
[[ 11.  19.  13.]
 [ 23.  40.  27.]
 [ 17.  31.  19.]]
</code></pre>
"
40062418,6881240.0,2016-10-15T17:59:31Z,40060094,1,"<p>Found the answer here, and there should be a way to make this easier to find:
<a href=""http://stackoverflow.com/questions/19526300/does-argument-unpacking-use-iteration-or-item-getting#19526400"">Does argument unpacking use iteration or item-getting?</a></p>

<p>Summary:
Inheriting from builtins (dict,list...) is problematic since python may ignore your python API entirely (as it did for me) and use the underlying C. This happens in argument unpacking.</p>

<p>Solution: use the available abstractions. In my example add:</p>

<pre><code>from UserDict import UserDict
</code></pre>

<p>and replace all ""dict"" occurrences in the code with UserDict. This solution should be true for lists and tuples. </p>
"
40062555,2141635.0,2016-10-15T18:13:13Z,40057058,0,"<p>You can just keep looping until there is no next page:</p>

<pre><code>import  requests
from bs4 import BeautifulSoup
from urllib.parse import  urljoin

base = ""http://www.reed.co.uk""
url = ""http://www.reed.co.uk/jobs?datecreatedoffset=Today&amp;pagesize=100""

def all_urls():
    r = requests.get(url).content
    soup = BeautifulSoup(r, ""html.parser"")
    # get the urls from the first page
    yield  [urljoin(base, a[""href""]) for a in soup.select(""div.details h3.title a[href^=/jobs]"")]
    nxt = soup.find(""a"", title=""Go to next page"")
    # title=""Go to next page"" is missing when there are no more pages
    while nxt:
        # wash/repeat until no more pages
        r = requests.get(urljoin(base, nxt[""href""])).content
        soup = BeautifulSoup(r, ""html.parser"")
        yield  [urljoin(base, a[""href""]) for a in soup.select(""div.details h3.title a[href^=/jobs]"")]
        nxt = soup.find(""a"", title=""Go to next page"")
</code></pre>

<p>Just loop over the generator function to get the urls from each page:</p>

<pre><code>for u in (all_urls()):
    print(u)
</code></pre>

<p>I also use  <code>a[href^=/jobs]</code> in the selector as there are other tags that match so we make sure we just pull the job paths.</p>

<p>In your own code, the correct way to use a selector would be:</p>

<pre><code>soup.select(""div.results.col-xs-12.col-md-10"")
</code></pre>

<p>You syntax is for <em>find</em> or <em>find_all</em> where you use <code>class_=...</code> for css classes:</p>

<pre><code>soup.find_all(""div"", class_=""results col-xs-12 col-md-10"")
</code></pre>

<p>But that is not the correct selector regardless.</p>

<p>Not sure why you are creating multiple dfs but if that is what you want:</p>

<pre><code>def all_urls():
    r = requests.get(url).content
    soup = BeautifulSoup(r, ""html.parser"")
    yield pd.DataFrame([urljoin(base, a[""href""]) for a in soup.select(""div.details h3.title a[href^=/jobs]"")],
                       columns=[""Links""])
    nxt = soup.find(""a"", title=""Go to next page"")
    while nxt:
        r = requests.get(urljoin(base, nxt[""href""])).content
        soup = BeautifulSoup(r, ""html.parser"")
        yield pd.DataFrame([urljoin(base, a[""href""]) for a in soup.select(""div.details h3.title a[href^=/jobs]"")],
                           columns=[""Links""])
        nxt = soup.find(""a"", title=""Go to next page"")


dfs = list(all_urls())
</code></pre>

<p>That would give you a list of dfs:</p>

<pre><code>In [4]: dfs = list(all_urls())
dfs[0].head()
In [5]: dfs[0].head(10)
Out[5]: 
                                               Links
0  http://www.reed.co.uk/jobs/tufting-manager/308...
1  http://www.reed.co.uk/jobs/financial-services-...
2  http://www.reed.co.uk/jobs/head-of-finance-mul...
3  http://www.reed.co.uk/jobs/class-1-drivers-req...
4  http://www.reed.co.uk/jobs/freelance-middlewei...
5  http://www.reed.co.uk/jobs/sage-200-consultant...
6  http://www.reed.co.uk/jobs/bereavement-support...
7  http://www.reed.co.uk/jobs/property-letting-ma...
8  http://www.reed.co.uk/jobs/graduate-recruitmen...
9  http://www.reed.co.uk/jobs/solutions-delivery-...
</code></pre>

<p>But if you want just one then use the original code with <a href=""https://docs.python.org/3/library/itertools.html#itertools.chain.from_iterable"" rel=""nofollow"">itertools.chain</a>:</p>

<pre><code> from itertools import chain
 df = pd.DataFrame(columns=[""links""], data=list(chain.from_iterable(all_urls())))
</code></pre>

<p>Which will give you all the links in one df:</p>

<pre><code>In [7]:  from itertools import chain
   ...:  df = pd.DataFrame(columns=[""links""], data=list(chain.from_iterable(all_
   ...: urls())))
   ...: 

In [8]: df.size
Out[8]: 675
</code></pre>
"
40062776,3293881.0,2016-10-15T18:35:18Z,40062621,2,"<p>Here's a NumPy based solution  -</p>

<pre><code>df[~(np.triu(df.person1.values[:,None] == df.person2.values)).any(0)]
</code></pre>

<p>Sample run -</p>

<pre><code>In [123]: df
Out[123]: 
  person1 person2 someMetric
0    John   Steve         20
1   Peter   Larry         13
2   Steve    John         19
3   Peter  Parker          5
4   Larry   Peter          7

In [124]: df[~(np.triu(df.person1.values[:,None] == df.person2.values)).any(0)]
Out[124]: 
  person1 person2 someMetric
0    John   Steve         20
1   Peter   Larry         13
3   Peter  Parker          5
</code></pre>
"
40063118,6335503.0,2016-10-15T19:06:56Z,40063034,3,"<p>I don't really know what you mean by saying that those variables are the same, but if indeed they are, than all you have to do is just use <code>set()</code>.</p>

<pre><code>from functools import reduce
from operator import mul
sercnew2 = numpy.zeros((gn, gn, gn, gn))
for x1 in range(gn):
    for x2 in range(x1, gn):
        for x3 in range(x2, gn):
            for x4 in range(x3, gn):
                set_ = [ewp[n] for n in set([x1, x2, x3, x4])]
                sercnew2[x1, x2, x3, x4] = reduce(mul, set_, 1)
</code></pre>

<p>The way it works is that it creates a <code>set()</code> which delete duplicates, and later with <code>reduce</code> function I pick first number from the <code>set_</code>, multiplies it with <code>1</code> (the initializer value), and the result of this is going to be passed to <code>reduce</code> as first argument, and second will be the second item from <code>set_</code>. Sorry for my bad explanation.</p>
"
40063184,5714445.0,2016-10-15T19:14:25Z,40063034,2,"<p>You can do it in a single for loop too. Building on Divakar's Trick for a list of indexes, the first thing we need to do is figure out how to extract only the unique indices of a given element in the 4d array <code>sercnew2</code>.</p>

<p>One of the fastest ways to do this (Refer: <a href=""https://www.peterbe.com/plog/uniqifiers-benchmark"" rel=""nofollow"">https://www.peterbe.com/plog/uniqifiers-benchmark</a>) is using sets. Then, we simply have to initialize <code>sercnew2</code> as an array of ones, rather than zeros.</p>

<pre><code>from itertools import product
import numpy as np

sercnew2 = np.ones((gn, gn, gn, gn))
n_dims=4
idx = list(product(np.arange(gn), repeat=n_dims))

for i,j,k,l in idx:
    unique_items = set((i,j,k,l))
    for ele in unique_items:
        sercnew2[i,j,k,l] *= ewp[ele]
</code></pre>

<p>EDIT: As @unutbu suggested, we could also use the cartesian_product function from <a href=""http://stackoverflow.com/a/11146645/5714445"">http://stackoverflow.com/a/11146645/5714445</a> to speed up the initialization of <code>idx</code></p>

<p>Edit2: In case you are having difficulty understanding what <code>product</code> from <code>itertools</code> does, it provides all permutations. For instance, suppose <code>gn=2</code>, with repeat dimension set to 4, you get</p>

<pre><code>[0, 0, 0, 0]
[0, 0, 0, 1]
[0, 0, 1, 0]
[0, 0, 1, 1]
[0, 1, 0, 0]
[0, 1, 0, 1]
[0, 1, 1, 0]
[0, 1, 1, 1]
[1, 0, 0, 0]
[1, 0, 0, 1]
[1, 0, 1, 0]
[1, 0, 1, 1]
[1, 1, 0, 0]
[1, 1, 0, 1]
[1, 1, 1, 0]
[1, 1, 1, 1]
</code></pre>
"
40063294,5626112.0,2016-10-15T19:23:47Z,40063242,2,"<p>based on your question, you are looking for a cumulative sum of each columns. you could use the <code>cumsum()</code> method</p>

<pre><code>DF.cumsum()

               SPY    AAPL   GOOG
2011-01-10  0.4400 -0.8100 1.8000
2011-01-11  0.4400 -0.8100 1.8000
2011-01-12 -0.6700 -3.5800 0.9400
2011-01-13 -1.5800 -7.6000 0.2600
</code></pre>
"
40063308,2336654.0,2016-10-15T19:25:09Z,40063242,2,"<p>Use <code>DF.shift()</code></p>

<pre><code>DF + DF.shift()
</code></pre>
"
40063414,3293881.0,2016-10-15T19:37:55Z,40063034,3,"<p>Really hoping that I have got it! Here's a vectorized approach -</p>

<pre><code>from itertools import product

n_dims = 4 # Number of dims

# Create 2D array of all possible combinations of X's as rows
idx = np.sort(np.array(list(product(np.arange(gn), repeat=n_dims))),axis=1)

# Get all X's indexed values from ewp array
vals = ewp[idx]

# Set the duplicates along each row as 1s. With the np.prod coming up next,
#these 1s would not affect the result, which is the expected pattern here.
vals[:,1:][idx[:,1:] == idx[:,:-1]] = 1

# Perform product along each row and reshape into multi-dim array
out = vals.prod(1).reshape([gn]*n_dims)
</code></pre>
"
40063593,2028710.0,2016-10-15T19:56:10Z,40062621,0,"<p>an approach in pandas</p>

<pre><code>df = pd.DataFrame(
{'person2':  {0: 'Steve', 1: 'Larry', 2: 'John', 3: 'Parker', 4: 'Peter'}, 
'person1': {0: 'John', 1: 'Peter', 2: 'Steve', 3: 'Peter', 4: 'Larry'}, 
'someMetric': {0: 20, 1: 13, 2: 19, 3: 5, 4: 7}})


print(df)
  person1 person2 someMetric
0    John   Steve         20
1   Peter   Larry         13
2   Steve    John         19
3   Peter  Parker          5
4   Larry   Peter          7


df['ordered-name'] = df.apply(lambda x: '-'.join(sorted([x['person1'],x['person2']])),axis=1)
df = df.drop_duplicates(['ordered-name'])
df.drop(['ordered-name'], axis=1, inplace=True)
print df
</code></pre>

<p>which gives:</p>

<pre><code>  person1 person2  someMetric
0    John   Steve          20
1   Peter   Larry          13
3   Peter  Parker           5
</code></pre>
"
40063647,5626112.0,2016-10-15T20:01:27Z,40063580,1,"<p>there will always be an index in your dataframes. if you don't set 'id' as index, it will be at the same level as other columns and pandas will populate an increasing integer for your index starting from 0.</p>

<pre><code>df = pd.DataFrame([(101,3,'x'), (102,5,'y')], columns=['id', 'A', 'B'])

In[52]: df
Out[52]: 
    id  A  B
0  101  3  x
1  102  5  y
</code></pre>

<p>the index is there so you can slice the original dataframe. such has</p>

<pre><code>df.iloc[0]
Out[53]: 
id    101
A       3
B       x
Name: 0, dtype: object
</code></pre>

<p>so let says you want ID as index and ID as a column, which is very redundant, you could do:</p>

<pre><code>df = pd.DataFrame([(101,3,'x'), (102,5,'y')], columns=['id', 'A', 'B'])
df.set_index('id', inplace=True)
df['id'] = df.index
df
Out[55]: 
     A  B   id
id            
101  3  x  101
102  5  y  102
</code></pre>

<p>with this you can slice by 'id' such has:</p>

<pre><code>df.loc[101]
Out[57]: 
A       3
B       x
id    101
Name: 101, dtype: object
</code></pre>

<p>but it would the same info has :</p>

<pre><code>df = pd.DataFrame([(101,3,'x'), (102,5,'y')], columns=['id', 'A', 'B'])
df.set_index('id', inplace=True)
df.loc[101]

Out[58]: 
A    3
B    x
Name: 101, dtype: object
</code></pre>
"
40063868,298607.0,2016-10-15T20:27:43Z,40063580,1,"<p>Given:</p>

<pre><code>&gt;&gt;&gt; df2=pd.DataFrame([(101,3,'x'), (102,5,'y')], columns=['id', 'A', 'B'])
&gt;&gt;&gt; df2.set_index('id', inplace=True)
&gt;&gt;&gt; df2
     A  B
id       
101  3  x
102  5  y
</code></pre>

<p>For printing purdy, you can produce a copy of the DataFrame with a reset the index and use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_string.html"" rel=""nofollow"">.to_string</a>:</p>

<pre><code>&gt;&gt;&gt; print df2.reset_index().to_string(index=False)
id  A  B
101  3  x
102  5  y
</code></pre>

<p>Then play around with the formatting options so that the output suites your needs:</p>

<pre><code>&gt;&gt;&gt; fmts=[lambda s: u""{:^5}"".format(str(s).strip())]*3
&gt;&gt;&gt; print df2.reset_index().to_string(index=False, formatters=fmts)
id     A      B
101    3      x  
102    5      y
</code></pre>
"
40064285,590335.0,2016-10-15T21:14:06Z,40054935,0,"<p>I was able to run it under the Windows 10 linux subsystem (<a href=""https://msdn.microsoft.com/en-us/commandline/wsl/install_guide"" rel=""nofollow"">https://msdn.microsoft.com/en-us/commandline/wsl/install_guide</a>)</p>

<p>Which is basically a linux environment within windows.</p>
"
40064319,2999346.0,2016-10-15T21:17:53Z,40061307,2,"<p>Yes, there is a chance that using FFTW through the interface <code>pyfftw</code> will reduce your computation time compared to <code>numpy.fft</code> or <code>scipy.fftpack</code>. The performances of these implementations of DFT algorithms can be compared in benchmarks such as <a href=""https://gist.github.com/fnielsen/99b981b9da34ae3d5035"" rel=""nofollow"">this one</a> : some interesting results are reported in <a href=""http://stackoverflow.com/questions/6365623/improving-fft-performance-in-python"">Improving FFT performance in Python</a></p>

<p>I suggest the following code for a test:</p>

<pre><code>import pyfftw
import numpy
import time
import scipy

f = pyfftw.n_byte_align_empty((127,512,512),16, dtype='complex128')
#f = pyfftw.empty_aligned((33,128,128), dtype='complex128', n=16)
f[:] = numpy.random.randn(*f.shape)

# first call requires more time for plan creation
# by default, pyfftw use FFTW_MEASURE for the plan creation, which means that many 3D dft are computed so as to choose the fastest algorithm.
fftf=pyfftw.interfaces.numpy_fft.fftn(f)

#help(pyfftw.interfaces)
tas = time.time()
fftf=pyfftw.interfaces.numpy_fft.fftn(f) # here the plan is applied, nothing else.
tas = time.time()-tas
print ""3D FFT, pyfftw:"", tas

f = pyfftw.n_byte_align_empty((127,512,512),16, dtype='complex128')
#f = pyfftw.empty_aligned((33,128,128), dtype='complex128', n=16)
f[:] = numpy.random.randn(*f.shape)


tas = time.time()
fftf=numpy.fft.fftn(f)
tas = time.time()-tas
print ""3D FFT, numpy:"", tas

tas = time.time()
fftf=scipy.fftpack.fftn(f)
tas = time.time()-tas
print ""3D FFT, scipy/fftpack:"", tas

# first call requires more time for plan creation
# by default, pyfftw use FFTW_MEASURE for the plan creation, which means that many 3D dft are computed so as to choose the fastest algorithm.
f = pyfftw.n_byte_align_empty((128,512,512),16, dtype='complex128')
fftf=pyfftw.interfaces.numpy_fft.fftn(f)

tas = time.time()
fftf=pyfftw.interfaces.numpy_fft.fftn(f) # here the plan is applied, nothing else.
tas = time.time()-tas
print ""3D padded FFT, pyfftw:"", tas
</code></pre>

<p>For a size of 127*512*512, on my modest computer, I got:</p>

<pre><code>3D FFT, pyfftw: 3.94130897522
3D FFT, numpy: 16.0487070084
3D FFT, scipy/fftpack: 19.001199007
3D padded FFT, pyfftw: 2.55221295357
</code></pre>

<p>So <code>pyfftw</code> is significantly faster than <code>numpy.fft</code> and <code>scipy.fftpack</code>. Using padding is even faster, but the thing that is computed is different.</p>

<p>Lastly, <code>pyfftw</code> may seem slower at the first run due to the fact that it uses the flag <code>FFTW_MEASURE</code> according to the <a href=""https://hgomersall.github.io/pyFFTW/pyfftw/interfaces/interfaces.html"" rel=""nofollow"">documentation</a>. It's a good thing if and only if many DFTs of the same size are successively computed.</p>
"
40064454,3510736.0,2016-10-15T21:33:53Z,40064377,7,"<p>In general, it's impossible to find elements in a <code>list</code> in constant time. You could hypothetically maintain both a <code>list</code> and a <code>set</code>, but updating operations will take linear time.</p>

<p>You mention that your motivation is</p>

<blockquote>
  <p>a list, and not a set, is largely due to the need to account for duplicate items within the list. Sets do not allow for duplicates.</p>
</blockquote>

<p>and ask not to focus on the example. If this is your motivation, you might want to use instead of a <code>set</code>, a <code>dict</code> mapping each element to the number of its occurrences. </p>

<p>You might find <a href=""https://docs.python.org/2/library/collections.html#collections.Counter""><code>collections.Counter</code></a> useful in particular:</p>

<pre><code>In [1]: from collections import Counter

In [2]: Counter(['h', 'e', 'l', 'o', 'o', 'z'])
Out[2]: Counter({'e': 1, 'h': 1, 'l': 1, 'o': 2, 'z': 1})
</code></pre>
"
40064571,1547004.0,2016-10-15T21:51:27Z,40064202,1,"<p>The reason this is happening is because how python looks for modules and packages when you run a python script as the <code>__main__</code> script.</p>

<p>When you run <code>python main.py</code>, python will add the parent directory of <code>main.py</code> to the pythonpath, meaning packages and modules within the directory will be importable.  When you moved main.py, you changed the directory that was added to the pythonpath.</p>

<p>Generally, you don't want to rely on this mechanism for importing your modules, because it doesn't allow you to move your script and your package and modules are <em>only</em> importable when running that main script.  What you should do is make sure your <code>package</code> is installed into a directory that is already in the pythonpath.  There are several ways of doing this, but the most common is to create a <code>setup.py</code> script and actually <a href=""https://packaging.python.org/distributing/"" rel=""nofollow"">install your python package</a> for the python installation on your computer.</p>
"
40064653,5714445.0,2016-10-15T21:59:45Z,40064422,1,"<p>Have a look at the imageflow wrapper for tensorflow, which converts a numpy array containing multiple images into a .tfrecords file, which is the suggested format for using tensorflow <a href=""https://github.com/HamedMP/ImageFlow"" rel=""nofollow"">https://github.com/HamedMP/ImageFlow</a>.</p>

<p>You have to install it using </p>

<pre><code>$ pip install imageflow
</code></pre>

<p>Suppose your numpy array containing some 'k' images is <code>k_images</code> and the corresponding k labels (one-hot-encoded) are stored in <code>k_labels</code>, then creating a .tfrecords file with the name 'tfr_file.tfrecords' gets as simple as writing the line</p>

<pre><code>imageflow.convert_images(k_images, k_labels, 'tfr_file')
</code></pre>

<p>Alternatively, Google's Inception model contains a code to read images in a folder assuming each folder represents one label <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/data/build_image_data.py"" rel=""nofollow"">https://github.com/tensorflow/models/blob/master/inception/inception/data/build_image_data.py</a></p>
"
40064812,6335503.0,2016-10-15T22:17:30Z,40064750,2,"<p>I would highly recommend u to create a one function which creates the proper divisor of given N, and after that, the job would be easy.</p>

<pre><code>def get_divs(n):
    return [i for i in range(1, n) if n % i == 0]


def classify(num):
    divs_sum = sum(get_divs(num))
    if divs_sum &gt; num:
        print('{} is abundant number'.format(num))
    elif divs_sum &lt; num:
        print('{} is deficient number'.format(num))
    elif divs_sum == num:
        print('{} is perfect number'.format(num))
</code></pre>
"
40065111,6942925.0,2016-10-15T23:01:29Z,40064750,0,"<p>Somewhere you are misinterpreting something. As is you are printing what kind the number is, as many times as the value of the number. I might be missing something but anyway. </p>

<p>The sum of proper divisors can be found naively through using modulo </p>

<pre><code>def classify1(num):
    div_sum = sum(x for x in range(1, num) if num % x == 0)
    kind = """"
    if div_sum &lt; num:
        kind = ""deficient""
    elif div_sum &gt; num:
        kind = ""abundant""
    else:
        kind = ""perfect""
    print(""{} is a {} number"".format(num, kind))
</code></pre>

<hr>

<p>but for big numbers or maybe numbers this will take a long time. So I welcome you to the divisor function. I just dump it and explain it. </p>

<pre><code>def mark(li: list, x: int):

    for i in range(2*x, len(li), x):
        li[i] = False
    return li


def sieve(lim: int):
    li = [True] * lim
    li[0] = li[1] = 0
    for x in range(2, int(lim ** 0.5) + 1):
        if x:
            li = mark(li, x)
    return [2]+[x for x in range(3, lim, 2) if li[x]]


def factor(num):

    divs = list()
    for prime in primes:
        if prime * prime &gt; num:
            if num &gt; 1:
                divs += [num]
            return divs
        while num % prime == 0:
            num //= prime
            divs += [prime]
    else:
        return divs


def divisors_sum(num):
    """"""
    Function that implements a geometric series to generate the sum of the
     divisors, but it is not the divisor function since removing original
     number.
    """"""
    divs = factor(num)
    div_sum, s = 1, 0
    for div in set(divs):
        s = 0
        for exponent in range(0, divs.count(div) + 1):
            s += div ** exponent
        div_sum *= s
    else:
        return div_sum - num


primes = sieve(limit)
</code></pre>

<p>but.. there is not much explaining due, first prime factor the numbers, and use the divisor function to get the proper divisors sum. That is it. However the speed up is ridiculously much fast. This might seem to over kill the problem but it is just that much more cooler and faster. </p>
"
40065276,5754797.0,2016-10-15T23:29:08Z,40065108,1,"<p>After some simplification, this regular expression meets the requirements stated above and reproduced in the test cases below.</p>

<pre><code>import re

regex = r'(?:section)*\s*(?:[0-9.])*\s*random\s+(?!random)(?:[0-9.])*'

strings = [
   ""random random random random random"",
   ""section 1.2 random 2"",
   ""1.2 random 2"",
   ""1.2. random 2"",
   ""random 2"",
   ""random 2."",
   ""random"",
]

for string in strings:
    m = re.match(regex, string, flags = re.I)
    if m:
        print ""match on"", string
    else:
        print ""non match on"", string
</code></pre>

<p>which gives an output of:</p>

<pre><code>non match on random random random random random
match on section 1.2 random 2
match on 1.2 random 2
match on 1.2. random 2
match on random 2
match on random 2.
non match on random
</code></pre>

<p>See it in action at: <a href=""https://eval.in/661183"" rel=""nofollow"">https://eval.in/661183</a></p>
"
40065404,5755166.0,2016-10-15T23:50:14Z,40065321,0,"<p>I always create a <code>requirements.txt</code> which contains all of my dependencies which can then be installed using <code>pip install -r requirements.txt</code>.
A usual <code>requirements.txt</code> looks like this (this is just an example):</p>

<pre><code>pytz==2016.4
six==1.10.0
SQLAlchemy==1.0.13
watchdog==0.8.3
Werkzeug==0.11.10
</code></pre>

<p>If you want to add a Git dependency (e.g. from Github), add a line like this to your <code>requirements.txt</code>:</p>

<pre><code>git+https://github.com/user/project.git@branch#egg=eggname
</code></pre>

<p>Replace the url with your required project, <code>branch</code> with the desired Git branch (or tag, which should also work) and <code>eggname</code> with the name of the directory where the dependency should be installed to at <code>.../python3.x/site-packages/eggname</code>.</p>

<p>My <code>setup.py</code> looks like this:</p>

<pre><code>#!/usr/bin/env python

from pip.req import parse_requirements
from setuptools import setup

requirements = parse_requirements('./requirements.txt', session=False)

setup(
    name='myproject',
    # ...
    # Stuff like version, author, packages, include_package_data,
    # entry_points, test_suite, ...
    # ...
    install_requires=[str(requirement.req) for requirement in requirements],
)
</code></pre>

<p>I am using Python >= 3.4, btw.</p>
"
40065407,5714445.0,2016-10-15T23:51:16Z,40065378,0,"<p>In the <code>plot</code> command, you could enter Hex colours. A much more simple way to beautify your plot would be to simply use matplotlib styles. For instance, before any plot function, just write
<code>plt.style.use('ggplot')</code></p>
"
40065612,2336654.0,2016-10-16T00:29:01Z,40063580,2,"<p>You are misinterpreting what you are seeing.</p>

<pre><code>     A  B
id       
101  3  x
102  5  y
</code></pre>

<p>Is not showing you a hierarchical column index.  <code>id</code> is the name of the row index.  In order to show you the name of the index, pandas is putting that space there for you.</p>

<p>The answer to your question depends on what you really want or need.</p>

<p>As the <code>df</code> is, you can dump it to a <code>csv</code> just the way you want:</p>

<pre><code>print(df.to_csv(sep='\t'))

id  A   B
101 3   x
102 5   y
</code></pre>

<hr>

<pre><code>print(df.to_csv())

id,A,B
101,3,x
102,5,y
</code></pre>

<hr>

<p>Or you can alter the <code>df</code> so that it displays the way you'd like</p>

<pre><code>print(df.rename_axis(None)) 

     A  B
101  3  x
102  5  y
</code></pre>

<hr>

<p><strong><em>please do not do this!!!!</em></strong><br>
I'm putting it to demonstrate how to manipulate</p>

<p>I could also keep the index as it is but manipulate both column and row index names to print how you would like.</p>

<pre><code>print(df.rename_axis(None).rename_axis('id', 1))

id   A  B
101  3  x
102  5  y
</code></pre>

<p>But this has named the columns' index <code>id</code> which makes no sense.</p>
"
40065645,901925.0,2016-10-16T00:34:37Z,40065479,3,"<p>So you have a different repeat array for each row?  But the total number of repeats per row is the same?</p>

<p>Just do the <code>repeat</code> on the flattened arrays, and reshape back to the correct number of rows.</p>

<pre><code>In [529]: np.repeat(arr,rep.flat)
Out[529]: array([10, 10, 10, 24, 24, 24, 24, 10, 10, 24, 24, 24, 24,  1])
In [530]: np.repeat(arr,rep.flat).reshape(2,-1)
Out[530]: 
array([[10, 10, 10, 24, 24, 24, 24],
       [10, 10, 24, 24, 24, 24,  1]])
</code></pre>

<p>If the repetitions per row vary, we have the problem of padding variable length rows.  That's come up in other SO questions.  I don't recall all the details, but I think the solution is along this line:</p>

<p>Change <code>rep</code> so the numbers differ:</p>

<pre><code>In [547]: rep
Out[547]: 
array([[3, 2, 2, 0, 0, 0, 0, 0, 0, 0],
       [2, 2, 2, 1, 0, 2, 0, 0, 0, 0]])
In [548]: lens=rep.sum(axis=1)
In [549]: lens
Out[549]: array([7, 9])
In [550]: m=np.max(lens)
In [551]: m
Out[551]: 9
</code></pre>

<p>create the target:</p>

<pre><code>In [552]: res = np.zeros((arr.shape[0],m),arr.dtype)
</code></pre>

<p>create an indexing array - details need to be worked out:</p>

<pre><code>In [553]: idx=np.r_[0:7,m:m+9]
In [554]: idx
Out[554]: array([ 0,  1,  2,  3,  4,  5,  6,  9, 10, 11, 12, 13, 14, 15, 16, 17])
</code></pre>

<p>flat indexed assignment:</p>

<pre><code>In [555]: res.flat[idx]=np.repeat(arr,rep.flat)
In [556]: res
Out[556]: 
array([[10, 10, 10, 24, 24, 24, 24,  0,  0],
       [10, 10, 24, 24, 24, 24,  1,  1,  1]])
</code></pre>
"
40066117,904393.0,2016-10-16T02:07:05Z,40058748,3,"<p>Eventlet is currently incompatible with the multiprocessing package. There is an open issue for this work: <a href=""https://github.com/eventlet/eventlet/issues/210"" rel=""nofollow"">https://github.com/eventlet/eventlet/issues/210</a>.</p>

<p>The alternative that I think will work well in your case is to use Celery to manage your queue. Celery will start a pool of worker processes that wait for tasks provided by the main process via a message queue (RabbitMQ and Redis are both supported).</p>

<p>The Celery workers do not need to use eventlet, only the main server does, so this frees them to do whatever they need to do without the limitations imposed by eventlet.</p>

<p>If you are interested in exploring this approach, I have a complete example that uses it: <a href=""https://github.com/miguelgrinberg/flack"" rel=""nofollow"">https://github.com/miguelgrinberg/flack</a>.</p>
"
40066139,5285918.0,2016-10-16T02:13:25Z,40065762,1,"<p>Piggybacking off of Psidom's comment...</p>

<pre><code>df.groupby('id').mean().plot(kind='bar')
</code></pre>

<p><a href=""https://i.stack.imgur.com/Kga1c.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/Kga1c.png"" alt=""enter image description here""></a></p>

<hr>

<pre><code>In [108]: df
Out[108]: 
   id  cost
0   1    10
1   1    20
2   1    30
3   2    40
4   2    50
</code></pre>
"
40066202,2705542.0,2016-10-16T02:24:40Z,40052328,2,"<p>Expanding on my comment, I just want to flesh out a bit just <em>how</em> hopeless it is to grasp numbers of this size.</p>

<p>I assume you already know that</p>

<pre><code>3 âââ 3 =
3 ââ (3 âââ 2) =
3 ââ (3 ââ 3) =
3 ââ (3 â 3 â 3) =
3 ââ (3 â 27) =
3 ââ 7625597484987
</code></pre>

<p>So this reduces to studying the growth of <code>3 ââ i</code>.  Let's see how that grows:</p>

<pre><code>3 ââ 0 = 1
3 ââ 1 = 3**(3ââ0) = 3**1 = 3
3 ââ 2 = 3**(3ââ1) = 3**3 = 27
3 ââ 3 = 3**(3ââ2) = 3**27 = 7625597484987
3 ââ 4 = 3**(3ââ3) = 3**7625597484987 = ...?
</code></pre>

<p>We're already beyond the limit of what's feasible to compute on most personal computers.  </p>

<pre><code>&gt;&gt;&gt; import math
&gt;&gt;&gt; math.log10(3) * 3**27
3638334640024.0996
</code></pre>

<p>That's the base 10 logarithm of <code>3 ââ 4</code>, showing that the latter has over 3 trillion (decimal) digits.  Even with a custom encoding using 4 bits per decimal digit, it would require over 1.5 terabytes of disk space to store it - and very few computers have enough RAM to compute it in a straightforward way.</p>

<p>Then <code>3 ââ 5</code> is in turn 3 raised to that multi-trillion digit power.  The words and notations we normally use are <em>already</em> inadequate to give any real idea of just how large that is, and there is no computer with enough RAM or disk space to deal with it.</p>

<p>And we're only up to <code>3 ââ 5</code>!  There are still <em>trillions</em> of layers to go to reach <code>3 ââ 7625597484987</code> = <code>3âââ3</code>.  It's beyond human comprehension.  Although, yes, infinitely many integers are even larger ;-)</p>
"
40066329,4481445.0,2016-10-16T02:51:03Z,40042223,1,"<p>Using <code>fill_bettween</code> to plot your data will automatically include the filled area in the legend.</p>

<p>To include the areas where the two datasets overlap, you can combine the legend handles from both dataset into a single legend handle.</p>

<p>As pointed out in the comments, you can also define any arbitrary legend handle with a proxy.</p>

<p>Finally, you can define exactly what handles and labels you want to appear in the legend, regardless of the data plotted in your graph.</p>

<p>See the MWE below that illustrates the points stated above:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

plt.close('all')

# Gererate some datas:
x = np.random.rand(50)
y = np.arange(len(x))

# Plot data:
fig, ax = plt.subplots(figsize=(11, 4))
fillA = ax.fill_between(y, x-0.25, 0.5, color='darkolivegreen', alpha=0.65, lw=0)
fillB = ax.fill_between(y, x, 0.5, color='indianred', alpha=0.75, lw=0)

linec, = ax.plot(y, np.zeros(len(y))+0.5, color='blue', lw=1.5)
linea, = ax.plot(y, x, color='orange', lw=1.5)
lineb, = ax.plot(y, x-0.25, color='black', lw=1.5)

# Define an arbitrary legend handle with a proxy:
rec1 = plt.Rectangle((0, 0), 1, 1, fc='blue', lw=0, alpha=0.25)

# Generate the legend:
handles = [linea, lineb, linec, fillA, fillB, (fillA, fillB),
           rec1, (fillA, fillB, rec1)]
labels = ['a', 'b', 'c', 'A', 'B', 'A+B', 'C', 'A+B+C']
ax.legend(handles, labels, loc=2, ncol=4)

ax.axis(ymin=-1, ymax=2)

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/e3hqM.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/e3hqM.png"" alt=""enter image description here""></a></p>
"
40066467,3150181.0,2016-10-16T03:17:33Z,40065479,1,"<p>Another solution similar to @hpaulj's solution:</p>

<pre><code>def repeat2dvect(arr, rep):
    lens = rep.sum(axis=-1)
    maxlen = lens.max()
    ret_val = np.zeros((arr.shape[0], maxlen))
    mask = (lens[:,None]&gt;np.arange(maxlen))
    ret_val[mask] = np.repeat(arr.ravel(), rep.ravel())
    return ret_val
</code></pre>

<p>Instead of storing indices, I'm creating a bool mask and using the mask to set the values.</p>
"
40066747,2795733.0,2016-10-16T04:12:54Z,40065971,1,"<pre><code>import pandas
from matplotlib import pyplot as plt
df = pandas.DataFrame({'fnctn':['a','a','a','b','b','b'],'x':[1,2,3,1,2,3],'y1':[2,3,4,3,2,2],'y2':[3,2,3,4,3,2]})

In [19]: df
Out[19]: 
  fnctn  x  y1  y2
0     a  1   2   3
1     a  2   3   2
2     a  3   4   3
3     b  1   3   4
4     b  2   2   3
5     b  3   2   2

for f in set(df['fnctn']): 
     df[df['fnctn']==f].plot(x='x')
</code></pre>

<p><a href=""https://i.stack.imgur.com/Rpkgo.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/Rpkgo.png"" alt=""a""></a>
<a href=""https://i.stack.imgur.com/dAqiC.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/dAqiC.png"" alt=""b""></a></p>
"
40066968,539810.0,2016-10-16T04:52:07Z,40066387,0,"<p>Okay, I'm not sure there is a good solution here, so I'm going with a different option: if a user tries to do something that messes everything up, it's their own fault. After all, Python is ""a language for consenting adults"".</p>

<p>Just because an attribute is conventionally ""public"" by PEP 8 standards, that does not imply that a user of a class can just do whatever they want with its attributes, whether that's replacing their values, altering them in place, or even deleting them. That naturally goes for properties, too, which are class attributes.</p>

<p>I'll leave this unanswered for a while, but I'm fairly certain this attitude is the right way to approach this ""problem"".</p>
"
40066992,1705598.0,2016-10-16T04:54:42Z,40063269,3,"<p>You may use <a href=""https://golang.org/pkg/runtime/#LockOSThread"" rel=""nofollow""><code>runtime.LockOSThread()</code></a> to wire the calling goroutine to its current OS thread. This will ensure that no other goroutines will be scheduled to this thread, so your goroutine will run and not get interrupted or put on hold. No other goroutines will interfere when thread is locked.</p>

<p>After this, you just need a loop until the given seconds have passed. You must call <a href=""https://golang.org/pkg/runtime/#UnlockOSThread"" rel=""nofollow""><code>runtime.UnlockOSThread()</code></a> to ""release"" the thread and make it available for other goroutines for execution, best done as a <code>defer</code> statement.</p>

<p>See this example:</p>

<pre><code>func runUntil(end time.Time) {
    runtime.LockOSThread()
    defer runtime.UnlockOSThread()
    for time.Now().Before(end) {
    }
}
</code></pre>

<p>To make it wait for 2 seconds, it could look like this:</p>

<pre><code>start := time.Now()
end := start.Add(time.Second * 2)
runUntil(end)

fmt.Println(""Verify:"", time.Now().Sub(start))
</code></pre>

<p>This prints for example:</p>

<pre><code>Verify: 2.0004556s
</code></pre>

<p>Of course you can specify less than a second too, e.g. to wait for 100 ms:</p>

<pre><code>start := time.Now()
runUntil(start.Add(time.Millisecond * 100))
fmt.Println(""Verify:"", time.Now().Sub(start))
</code></pre>

<p>Output:</p>

<pre><code>Verify: 100.1278ms
</code></pre>

<p>You may use a different version of this function if that suits you better, one that takes the amount of time to ""wait"" as a value of <a href=""https://golang.org/pkg/time/#Duration"" rel=""nofollow""><code>time.Duration</code></a>:</p>

<pre><code>func wait(d time.Duration) {
    runtime.LockOSThread()
    defer runtime.UnlockOSThread()

    for end := time.Now().Add(d); time.Now().Before(end); {
    }
}
</code></pre>

<p>Using this:</p>

<pre><code>start = time.Now()
wait(time.Millisecond * 200)
fmt.Println(""Verify:"", time.Now().Sub(start))
</code></pre>

<p>Output:</p>

<pre><code>Verify: 200.1546ms
</code></pre>

<p><strong>Note:</strong> Note that the loops in the above functions will use CPU relentlessly as there is no sleep or blocking IO in them, they will just query the current system time and compare it to the deadline.</p>

<p><strong>What if the attacker increases system load by multiple concurrent attempts?</strong></p>

<p>The Go runtime limits the system threads that can simultaneously execute goroutines. This is controlled by <a href=""https://golang.org/pkg/runtime/#GOMAXPROCS"" rel=""nofollow""><code>runtime.GOMAXPROCS()</code></a>, so this is already a limitation. It defaults to the number of available CPU cores, and you can change it anytime. This also poses a bottleneck though, as by using <code>runtime.LockOSThread()</code>, if the number of locked threads equals to <code>GOMAXPROCS</code> at any given time, that would block execution of other goroutines until a thread is unlocked.</p>

<p>See related questions:</p>

<p><a href=""http://stackoverflow.com/questions/39245660/number-of-threads-used-by-go-runtime"">Number of threads used by Go runtime</a></p>

<p><a href=""http://stackoverflow.com/questions/28186361/why-does-it-not-create-many-threads-when-many-goroutines-are-blocked-in-writing/28186656#28186656"">Why does it not create many threads when many goroutines are blocked in writing file in golang?</a></p>
"
40067260,5335649.0,2016-10-16T05:44:32Z,40066778,4,"<p>The comments made clarifications about the Big-Oh notations. So I will just start with testing the code.</p>

<p>Here is the setup I used for testing the speed of the code.</p>

<pre><code>import random

# Collapsed these because already known
def disjoint3c(A, B, C):
def set_disjoint_medium (a, b, c):
def set_disjoint_slowest (a, b, c):

a = [random.randrange(100) for i in xrange(10000)]
b = [random.randrange(100) for i in xrange(10000)]
c = [random.randrange(100) for i in xrange(10000)]

# Ran timeit.
# Results with timeit module.
1-) 0.00635750419422
2-) 0.0061145967287
3-) 0.0487953200969
</code></pre>

<p>Now to the results, as you see, the <code>O(n^3)</code> solution runs <strong>8</strong> times slower than the other solutions. But this is still fast for such an algorithm(Even faster in your test). <strong>Why this happens ?</strong></p>

<p>Because medium and slowest solutions you used, finishes the execution of the code <strong>as soon as a common element is detected</strong>. So the full complexity of the code is not realized. It breaks as soon as it finds an answer. Why the slowest solution ran almost as fast as the other ones in your test ? Probably because it finds the answer closer to the beginning of the lists.</p>

<p>To test this, you could create the lists like this. Try this yourself.</p>

<pre><code>a = range(1000)
b = range(1000, 2000)
c = range(2000, 3000)
</code></pre>

<p>Now the real difference between the times will be obvious because the slowest solution will have to run until it finishes all iterations, because there is no common element.</p>

<p>So it is a situation of <strong>Worst case</strong> and <strong>Best case</strong> performance.</p>

<p><strong>Not a part of the question edit:</strong> So, what if you want to retain the speed of finding early common occurances, but also don't want to increase complexity. I made a rough solution for that, maybe more experienced users can suggest faster code.</p>

<pre><code>def mysol(a, b, c):
    store = [set(), set(), set()]

    # zip_longest for Python3, not izip_longest.
    for i, j, k in itertools.izip_longest(a, b, c):
        if i: store[0].add(i)
        if j: store[1].add(j)
        if k: store[2].add(k)

        if (i in store[1] and i in store[2]) or (j in store[0] and i in store[2]) or (k in store[0] and i in store[1]):
            return False
    return True
</code></pre>

<p>What is basically being done in this code is, you avoid converting all the lists to sets in the beginning. Rather, iterate through all lists at the same time, add elements to sets, check for common occurances. So now, you keep the speed of finding an early solution, but it is still slow for the worst case that I shown.</p>

<p>For the speeds, this runs 3-4 times slower than your first two solutions in the worst case. But runs 4-10 times faster than those solutions in randomized lists.</p>
"
40067637,766955.0,2016-10-16T06:44:26Z,40062836,0,"<p>when you save the page into a local file <code>page.html</code>, you skip the http header that contains encoding information.
Later on, when you open this file , either with scrapy or sublime, they have no clue what was the original encoding of the document.</p>

<p><strong>Recommendation</strong>: never used documents saved to a file for parsing.</p>
"
40067923,1060350.0,2016-10-16T07:23:37Z,40066778,0,"<p>O notation ignores all the constant factors. So it will only answer for <em>infinite</em> data sets. For any finite set, it is only a rule of thumb.</p>

<p>With interpreted languages such as Python and R, constant factors can be pretty large. They need to create and collect many objects, which is all O(1) but not free. So it is fairly common to see 100x performance differences of virtually equivalent code, unfortunately.</p>

<p>Secondly, the first algorithm computes <em>all</em> common elements, while the others fail on the <em>first</em>. If you benchmark <code>algX(a,a,a)</code> (yes, all three sets be the same) then it will do much more work than the others!</p>

<p>I would not be surprised to see a sort-based O(n log n) algorithm to be very competitive (because sorting is usually incredibly well optimized). For integers, I would use numpy arrays, and by avoiding the python interpreter as much as possible you can get very fast. While numpys <code>in1d</code> and <code>intersect</code> will likely give you aan O(n^2) or O(n^3) algorithm, they may end up being faster as long as your sets are usually disjoint.</p>

<p>Also note that in your case, the sets won't necessarily be pairwise disjoint... <code>algX(set(),a,a)==True</code>.</p>
"
40068065,2336654.0,2016-10-16T07:42:07Z,40065641,2,"<p>consider the <code>list</code> <code>dfs</code> of <code>pd.DataFrame</code>s</p>

<pre><code>import pandas as pd
import numpy as np


np.random.seed([3,1415])
dfs = [pd.DataFrame(np.random.rand(10, 2),
                    columns=['Col1', 'Col2']) for _ in range(5)]
</code></pre>

<p>I'll use <code>pd.concat</code> to join</p>

<p><strong><em>raw concat</em></strong><br>
stack values without regard to where it came from</p>

<pre><code>pd.concat([d.Col1.loc[d.Col2.gt(.8)] for d in dfs], ignore_index=True)

0     0.850445
1     0.934829
2     0.879891
3     0.085823
4     0.739635
5     0.700566
6     0.542329
7     0.882029
8     0.496250
9     0.585309
10    0.883372
Name: Col1, dtype: float64
</code></pre>

<p><strong><em>join with source information</em></strong><br>
use the <code>keys</code> parameter</p>

<pre><code>pd.concat([d.Col1.loc[d.Col2.gt(.8)] for d in dfs], keys=range(len(dfs)))

0  3    0.850445
   5    0.934829
   6    0.879891
1  1    0.085823
   2    0.739635
   7    0.700566
2  4    0.542329
3  3    0.882029
   4    0.496250
   8    0.585309
4  0    0.883372
Name: Col1, dtype: float64
</code></pre>

<p><strong><em>another approach</em></strong><br>
use <code>query</code></p>

<pre><code>pd.concat([d.query('Col2 &gt; .8').Col1 for d in dfs], keys=range(len(dfs)))

0  3    0.850445
   5    0.934829
   6    0.879891
1  1    0.085823
   2    0.739635
   7    0.700566
2  4    0.542329
3  3    0.882029
   4    0.496250
   8    0.585309
4  0    0.883372
Name: Col1, dtype: float64
</code></pre>
"
40068634,5741205.0,2016-10-16T09:04:15Z,40068572,2,"<p>You can't use <code>int(Series)</code> construction (it's similar to <code>int(['1','2','3'])</code>, which also won't work), you should use <code>Series.astype(int)</code> or better <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow"">pd.to_numeric(Series)</a> instead:</p>

<pre><code>In [32]: df
Out[32]:
   id str_id
0   1    x_2
1   2    x_4
2   3    x_8
3   4    x_1
4   5  x_AAA

In [33]: df['num_id'] = pd.to_numeric(df.str_id.str.extract(r'_(\d+)', expand=False))

In [34]: df
Out[34]:
   id str_id  num_id
0   1    x_2     2.0
1   2    x_4     4.0
2   3    x_8     8.0
3   4    x_1     1.0
4   5  x_AAA     NaN
</code></pre>
"
40068730,100297.0,2016-10-16T09:17:33Z,40068695,2,"<p>Reading from a sub-process gives you a <em>bytestring</em>. You could either decode this bytestring (you'll have to find a suitable encoding), or use the <code>universal_newlines</code> option and have Python automatically decode it for you:</p>

<pre><code>netshcmd = subprocess.Popen(
    'netsh wlan stop hostednetwork', 
    shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE,
    universal_newlines=True)
</code></pre>

<p>From the <a href=""https://docs.python.org/3/library/subprocess.html#frequently-used-arguments"" rel=""nofollow""><em>Frequently Used Arguments</em> documentation section</a>:</p>

<blockquote>
  <p>If <em>universal_newlines</em> is <code>True</code>, these file objects will be opened as text streams in universal newlines mode using the encoding returned by <code>locale.getpreferredencoding(False)</code>. For <code>stdin</code>, line ending characters <code>'\n'</code> in the input will be converted to the default line separator <code>os.linesep</code>. For <code>stdout</code> and <code>stderr</code>, all line endings in the output will be converted to <code>'\n'</code>. For more information see the documentation of the <code>io.TextIOWrapper</code> class when the newline argument to its constructor is <code>None</code>.</p>
</blockquote>

<p>For a process run via the shell, <code>locale.getpreferredencoding(False)</code> should be <em>exactly</em> the right codec to use, as that gets the information on what encoding to use from the exact same location that other processes like <code>netsh</code> are supposed to consult, the <a href=""https://www.gnu.org/software/gettext/manual/html_node/Locale-Environment-Variables.html"" rel=""nofollow"">locale environment variables</a>.</p>

<p>With <code>universal_newlines=True</code>, <code>output</code> will be set to the string <code>'The hosted network stopped. \n\n'; note the newlines at the end. You may want to use</code>str.strip()` to remove the extra whitespace there:</p>

<pre><code>print(""Success"", output.strip())
</code></pre>
"
40068736,1295162.0,2016-10-16T09:18:11Z,40068695,0,"<p>That is a bytestring. Change your code to make that a str:</p>

<pre><code>netshcmd = subprocess.Popen('netsh wlan stop hostednetwork', shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
output, errors = netshcmd.communicate()
if errors:
    print(""Warrning: "", errors.decode())
else:
    print(""Success"", output.decode())
</code></pre>
"
40068737,4118756.0,2016-10-16T09:18:29Z,40068605,3,"<p>You need to specify which axis to draw on when you call <code>scatter</code>. This can be done by passing an <code>ax =</code> argument to the plotting function:</p>

<pre><code>df = pd.DataFrame(3 * np.random.rand(4, 2), columns=['a', 'b'])
plt.subplot(121)
df[""a""].plot.box()
ax = plt.subplot(122)
df.plot.scatter(x=""a"", y=""b"", ax = ax)
plt.show()
</code></pre>
"
40068900,1474847.0,2016-10-16T09:41:51Z,39997840,0,"<p>The problem turned out to be caused by Data being created in one thread and then deallocated in another one. It is so because of malloc arenas in glibc <a href=""https://siddhesh.in/posts/malloc-per-thread-arenas-in-glibc.html"" rel=""nofollow"">(for reference see this)</a>. It can be nicely demonstrated by doing:</p>

<pre><code>executor1 = concurrent.futures.ThreadPoolExecutor(1)
executor2 = concurrent.futures.ThreadPoolExecutor(1)

numbers = await loop.run_in_executor(executor1, _test.generate)
moved_numbers = await loop.run_in_executor(executor2, _test.move, numbers)
</code></pre>

<p>which would take twice the memory allocated by <code>_test.generate</code> and</p>

<pre><code>executor = concurrent.futures.ThreadPoolExecutor(1)

numbers = await loop.run_in_executor(executor, _test.generate)
moved_numbers = await loop.run_in_executor(executor, _test.move, numbers)
</code></pre>

<p>which wound't.</p>

<p>This issue can be solved either by rewriting the code so it doesn't move the elements from one container to another (my case) or by setting environment variable <code>export MALLOC_ARENA_MAX=1</code> which will limit number of malloc arenas to 1. This however might have some performance implications involved (There is a good reason for having multiple arenas).</p>
"
40069079,4401573.0,2016-10-16T10:04:33Z,40042223,0,"<p>Yes, you are absolutely right  ian_itor,  tacaswell and Jean-SÃ©bastien, user defined legend seems to be the unique solution, in addition I made different <em>linewidth</em> for those area to be distinguishable from the curves, and playing with <em>alpha</em> got the right color.</p>

<pre><code>handles, labels = ax.get_legend_handles_labels()
display = (0,1,2,3,4)

overlap_1 = plt.Line2D((0,1),(0,0), color='firebrick', linestyle='-',linewidth=15, alpha = 0.85)
overlap_2= plt.Line2D((0,1),(0,0), color='darkolivegreen',linestyle='-',linewidth=15, alpha = 0.65)
over_lo_3= plt.Line2D((0,1),(0,0), color='indianred',linestyle='-',linewidth=15, alpha = 0.75) 

ax.legend([handle for i,handle in enumerate(handles) if i in display]+[overlap_1 , overlap_2 , overlap_3 ],
      [label for i,label in enumerate(labels) if i in display]+['D','F','G'])
</code></pre>

<p><a href=""https://i.stack.imgur.com/WeOw1.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/WeOw1.png"" alt=""enter image description here""></a></p>
"
40069108,5741205.0,2016-10-16T10:08:57Z,40062965,2,"<p>It's an interesting question, indeed!</p>

<p>I can't explain the following difference (why do we have index columns indexed when using <code>data_columns=None</code> (default due to the <code>docstring</code> of the <code>HDFStore.append</code> method) and we don't have them indexed when using <code>data_columns=True</code>):</p>

<pre><code>In [114]: store.get_storer('df_dc_None').table
Out[114]:
/df_dc_None/table (Table(100000,), shuffle, blosc(9)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Int32Col(shape=(1,), dflt=0, pos=1),
  ""values_block_1"": Float64Col(shape=(1,), dflt=0.0, pos=2),
  ""date"": Int64Col(shape=(), dflt=0, pos=3),
  ""id"": Int64Col(shape=(), dflt=0, pos=4)}
  byteorder := 'little'
  chunkshape := (1820,)
  autoindex := True
  colindexes := {
    ""date"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""id"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}

In [115]: store.get_storer('df_dc_True').table
Out[115]:
/df_dc_True/table (Table(100000,), shuffle, blosc(9)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Int64Col(shape=(1,), dflt=0, pos=1),
  ""values_block_1"": Int64Col(shape=(1,), dflt=0, pos=2),
  ""id2"": Int32Col(shape=(), dflt=0, pos=3),
  ""w"": Float64Col(shape=(), dflt=0.0, pos=4)}
  byteorder := 'little'
  chunkshape := (1820,)
  autoindex := True
  colindexes := {
    ""w"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""id2"": Index(6, medium, shuffle, zlib(1)).is_csi=False}
</code></pre>

<p>NOTE: pay attention at <code>colindexes</code> in the output above.</p>

<p>But using the following simple hack we can ""fix"" this:</p>

<pre><code>In [116]: store.append('df_dc_all', df, data_columns=df.head(1).reset_index().columns)

In [118]: store.get_storer('df_dc_all').table
Out[118]:
/df_dc_all/table (Table(100000,), shuffle, blosc(9)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""id"": Int64Col(shape=(), dflt=0, pos=1),
  ""date"": Int64Col(shape=(), dflt=0, pos=2),
  ""id2"": Int32Col(shape=(), dflt=0, pos=3),
  ""w"": Float64Col(shape=(), dflt=0.0, pos=4)}
  byteorder := 'little'
  chunkshape := (1820,)
  autoindex := True
  colindexes := {
    ""w"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""date"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""id"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False,
    ""id2"": Index(6, medium, shuffle, zlib(1)).is_csi=False}
</code></pre>

<p>check:</p>

<pre><code>In [119]: pd.read_hdf(file_path,'df_dc_all', where='date&gt;start &amp; id2&gt;500')
Out[119]:
                  id2         w
id    date
10000 1981-02-02  935  0.245637
      1981-02-04  994  0.291287
...               ...       ...
10199 1981-05-11  680 -0.370745
      1981-05-12  812 -0.880742

[10121 rows x 2 columns]
</code></pre>
"
40069177,5741205.0,2016-10-16T10:18:01Z,40069151,4,"<p><strong>UPDATE:</strong> the idea is first to replace all consecutive groups of digist with single <code>1</code> and then delete everything which is not <code>1</code> and finally get the length of the changed string:</p>

<pre><code>In [159]: s.replace(['\d+', '[^1]+'], ['1', ''], regex=True).str.len()
Out[159]:
0    2
1    3
2    5
3    6
4    2
5    3
6    5
7    3
8    4
9    4
dtype: int64
</code></pre>

<p>Timing against 100K Series:</p>

<pre><code>In [160]: %timeit big.replace(['\d+', '[^1]+'], ['1', ''], regex=True).str.len()
1 loop, best of 3: 1 s per loop

In [161]: %timeit big.apply(lambda x: len(re.sub('\D+', ' ', x).strip().split()))
1 loop, best of 3: 1.18 s per loop

In [162]: %timeit big.str.replace(r'\D+', ' ').str.strip().str.split().str.len()
1 loop, best of 3: 1.25 s per loop

In [163]: big.shape
Out[163]: (100000,)
</code></pre>

<p>Timing against 1M Series:</p>

<pre><code>In [164]: big = pd.concat([s] * 10**5, ignore_index=True)

In [165]: %timeit big.replace(['\d+', '[^1]+'], ['1', ''], regex=True).str.len()
1 loop, best of 3: 9.98 s per loop

In [166]: %timeit big.apply(lambda x: len(re.sub('\D+', ' ', x).strip().split()))
1 loop, best of 3: 11.7 s per loop

In [167]: %timeit big.str.replace(r'\D+', ' ').str.strip().str.split().str.len()
1 loop, best of 3: 12.6 s per loop

In [168]: big.shape
Out[168]: (1000000,)
</code></pre>

<p>Explanation:</p>

<pre><code>In [169]: s.replace(['\d+', '[^1]+'], ['1', ''], regex=True)
Out[169]:
0        11
1       111
2     11111
3    111111
4        11
5       111
6     11111
7       111
8      1111
9      1111
dtype: object
</code></pre>

<p><strong>OLD (slow) answer:</strong></p>

<p>What about using <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.extractall.html"" rel=""nofollow"">.str.extractall()</a> in conjunction with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.groupby.html"" rel=""nofollow"">.groupby(level=0)</a>?</p>

<pre><code>In [130]: s.str.extractall('(\d+)').groupby(level=0).count()
Out[130]:
   0
0  2
1  3
2  5
3  6
4  2
5  3
6  5
7  3
8  4
9  4
</code></pre>

<p>Explanation:</p>

<pre><code>In [131]: s.str.extractall('(\d+)')
Out[131]:
             0
  match
0 0      11111
  1          1
1 0          1
  1        111
  2          1
2 0         11
  1          1
  2         11
  3         11
  4         11
3 0          1
  1         11
  2          1
  3          1
  4          1
  5          1
4 0       1111
  1       1111
5 0       1111
  1          1
  2        111
6 0          1
  1          1
  2        111
  3         11
  4          1
7 0          1
  1        111
  2          1
8 0         11
  1          1
  2          1
  3      11111
9 0          1
  1          1
  2          1
  3          1
</code></pre>
"
40069202,2336654.0,2016-10-16T10:21:00Z,40069151,2,"<p>This was my solution</p>

<pre><code>s.str.replace(r'\D+', ' ').str.strip().str.split().str.len()
</code></pre>

<p><strong><em>100,000 rows</em></strong></p>

<pre><code>np.random.seed([3,1415])
p = (.35, .35, .1, .1, .1)
s = pd.DataFrame(np.random.choice(['', 1] + list('abc'), (100000, 20), p=p)).sum(1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/7yQDE.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/7yQDE.png"" alt=""enter image description here""></a></p>
"
40069408,2137255.0,2016-10-16T10:46:29Z,40069151,4,"<p>PiRSquared and MaxU solutions are great.</p>

<p>However, I noticed <code>apply</code> is usually a bit faster than using multiple string methods.</p>

<pre><code>In [142]: %timeit s.apply(lambda x: len(re.sub('\D+', ' ', x).strip().split()))
1 loop, best of 3: 367 ms per loop

In [143]: %timeit s.str.replace(r'\D+', ' ').str.strip().str.split().str.len()
1 loop, best of 3: 403 ms per loop

In [145]: s.shape
Out[145]: (100000L,)
</code></pre>
"
40069568,2781701.0,2016-10-16T11:07:59Z,40062836,0,"<p>Only 2 things can be happening here. Either the html is malformed and scrapy can't parse it or there's some trouble with scrapy and encoding. I think the first one is more likely. <a href=""http://www.freeformatter.com/html-validator.html"" rel=""nofollow"">http://www.freeformatter.com/html-validator.html</a> kind of gives it away. </p>

<p>Since it works on Chrome what I would suggest is using selenium to make the browser fix the code and scrap the elements from there. I didn't test but maybe scrapy-splash can have the same effect.</p>
"
40069736,2285236.0,2016-10-16T11:30:08Z,40069694,2,"<p>You can use groupby.transform to get the value counts aligned with the original index, then use it as a boolean index:</p>

<pre><code>df.loc[df.groupby('label')['label'].transform('count') &lt;= threshold, 'label'] = 'zero'

df
Out: 
    date label     value
0      1     a -0.587957
1      2     a  0.341551
2      3  zero  0.516933
3      4     a  0.234042
4      3  zero -0.206185
5      7  zero  0.840724
6     12     a -0.728868
7     18  zero  0.111260
8     11  zero -0.471337
9      2  zero  0.030803
10     5  zero  1.012638
11     3  zero -1.233750
</code></pre>

<p>Here are my timings:</p>

<pre><code>df = pd.concat([df]*10**4)

%timeit df.groupby('label')['label'].transform('count') &lt;= threshold
100 loops, best of 3: 7.86 ms per loop

%%timeit 
value_count=df.label.value_counts()
df['label'].isin(value_count[value_count.values&lt;=threshold].index)
100 loops, best of 3: 9.24 ms per loop
</code></pre>
"
40069790,2137255.0,2016-10-16T11:36:25Z,40069694,1,"<p>You could do</p>

<pre><code>In [59]: df.loc[df['label'].isin(value_count[value_count.values&lt;=threshold].index),
 'label'] = 'zero'

In [60]: df
Out[60]:
    date label     value
0      1     a -0.132887
1      2     a -1.306601
2      3  zero -1.431952
3      4     a  0.928743
4      3  zero  0.278955
5      7  zero  0.128430
6     12     a  0.200825
7     18  zero -0.560548
8     11  zero -2.925706
9      2  zero -0.061373
10     5  zero -0.632036
11     3  zero -1.061894
</code></pre>

<p>Timings</p>

<pre><code>In [87]: df = pd.concat([df]*10**4, ignore_index=True)

In [88]: %timeit df['label'].isin(value_count[value_count.values&lt;=threshold].index)
100 loops, best of 3: 7.1 ms per loop

In [89]: %timeit df.groupby('label')['label'].transform('count') &lt;= threshold
100 loops, best of 3: 11.7 ms per loop

In [90]: df.shape
Out[90]: (120000, 3)
</code></pre>

<p>You may want to benchmark with larger dataset. And, this may not be aaccurate to compare, since you're precomuting <code>value_count</code></p>
"
40070781,1317944.0,2016-10-16T13:26:05Z,40070249,2,"<p><strong>First path</strong>: I think the more appropriate way for you to go is <a href=""https://docs.python.org/3.4/library/ctypes.html"" rel=""nofollow"">ctypes</a>. You can create a shared library, and then load the functions of the shared library in Python, and fill all the data containers you want in Python.</p>

<p>In Windows, you can create a DLL, and in Linux you can create a shared .so library.</p>

<p>Now this has the advantage that this will be independent of your Python version.</p>

<p><strong>Second path</strong>: I think it's less appropriate but you can get it to work, which is the <a href=""https://docs.python.org/3.4/extending/extending.html"" rel=""nofollow"">Python C Extension</a>. With this, you can call Python data containers (<code>PyObject</code>s) and fill them inside C.</p>

<p>However, the code you compile here will always need to be linked to Python libraries.</p>

<p><strong>Which one to use?</strong>: </p>

<ul>
<li>Use ctypes if you have some functions you want to call in C/C++, and then do the rest of the work in Python.</li>
<li>Use Python C Extension if you have some functions you want to call in Python, and you want to do the rest in C/C++.</li>
</ul>

<p>With both options, you can transfer huge blocks of memory between C++ and Python without necessarily involving any disk read/write operations.</p>

<p>Hope this helps.</p>
"
40070971,131187.0,2016-10-16T13:45:29Z,40070037,1,"<p>Your question itself makes it clear that you know how to make an image in a numpy array. Now make your legend using the same techniques in a smaller numpy array.</p>

<p>Finally, use the facilities in numpy to replace part of the plot array with the legend array, as discussed in <a href=""http://stackoverflow.com/questions/26506204/replace-sub-part-of-matrix-by-another-small-matrix-in-numpy"">this answer</a></p>
"
40071241,7026860.0,2016-10-16T14:10:47Z,40071214,3,"<p>Routines and subroutines are the same.
In older languages such as Fortran you had to differenciate between subroutines and functions. The latter returned something the former changed some state.</p>
"
40071258,6765112.0,2016-10-16T14:12:16Z,40071096,0,"<p>Does this produce what you're looking for?</p>

<pre><code>import matplotlib.pyplot as plt
fig,ax = plt.subplots()

for name in ['A','B','C']:
    ax.plot(df[df.name==name].year,df[df.name==name].weight,label=name)

ax.set_xlabel(""year"")
ax.set_ylabel(""weight"")
ax.legend(loc='best')
</code></pre>

<p><a href=""https://i.stack.imgur.com/vKx8d.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/vKx8d.png"" alt=""enter image description here""></a></p>
"
40071781,6169031.0,2016-10-16T15:07:03Z,40052348,0,"<p>The main problem was, when ever the button was clicked, the iterator is getting set to 0 or 1 what ever it was assigned. So, assign the variable outside of the class and call it into the class to maintain the loop structure.</p>

<pre><code>class staticVariable:
    static_count_clicked = 1

class Mainwindow(QtGui.QMainWindow, rts_class):
    def __init__(self, parent=None, *args, **kwargs):
        QtGui.QMainWindow.__init__(self, parent)
        self.setupUi(self)

        self.submit.clicked.connect(self.submit_application)

    def submit_application(self, count_clicked):

        staticVariable.static_count_clicked += 1
        print staticVariable.static_count_clicked
</code></pre>
"
40072832,565514.0,2016-10-16T16:50:43Z,40072420,2,"<p>This does it, albeit in some sections better than others.</p>

<pre><code>import matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline
import numpy as np

y = np.asarray([0,5,80,10,1,10,40,30,80,5,0])
x = np.arange(len(y))

plt.plot(x, y, 'r', ms=5)
spl = UnivariateSpline(x, y)
xs = np.linspace(0,len(y)-1, 1000)
spl.set_smoothing_factor(2)

#new code
ny = spl(xs).clip(0,max(spl(x)))
spl2 = UnivariateSpline(xs, ny)

plt.plot(xs, spl(xs) , 'g', lw=2,label=""original"")
plt.plot(xs, spl2(xs), 'b', lw=2,label=""stack mod"")

plt.legend()
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/OO5JD.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/OO5JD.png"" alt=""enter image description here""></a></p>
"
40072897,2177413.0,2016-10-16T16:57:19Z,40065378,0,"<p>The statement
<br>ax.plot(x, mpt1, color='dbz53', label='53 dBz')
<br>is wrong with quoted dbz53. Where python treated it as a string of unknown rgb value.
<br>You can simply put color='#DD3044', and it will work.
<br>Or you can try 
<br>color=dbz53.get_hex()
<br>without quote if you want to use the colour module you imported.</p>
"
40072907,1037251.0,2016-10-16T16:58:58Z,40072873,0,"<p>the GIL prevents simultaneous execution of multiple threads, but not in all situations.</p>

<p>The GIL is temporarily released during I/O operations executed by threads. That means, multiple threads can run at the same time. That's one reason you still need locks.</p>

<p>I don't know where I found this reference.... in a video or something - hard to look it up, but you can investigate further yourself</p>
"
40072999,389289.0,2016-10-16T17:07:55Z,40072873,4,"<p>GIL protects the Python interals. That means:</p>

<ol>
<li>you don't have to worry about something in the interpreter going wrong because of multithreading</li>
<li>most things do not really run in parallel, because python code is executed sequentially due to GIL</li>
</ol>

<p>But GIL does not protect your own code. For example, if you have this code:</p>

<pre><code>self.some_number += 1
</code></pre>

<p>That is going to read value of <code>self.some_number</code>, calculate <code>some_number+1</code> and then write it back to <code>self.some_number</code>.</p>

<p>If you do that in two threads, the operations (read, add, write) of one thread and the other may be mixed, so that the result is wrong.</p>

<p>This could be the order of execution:</p>

<ol>
<li>thread1 reads <code>self.some_number</code> (0)</li>
<li>thread2 reads <code>self.some_number</code> (0)</li>
<li>thread1 calculates <code>some_number+1</code> (1)</li>
<li>thread2 calculates <code>some_number+1</code> (1)</li>
<li>thread1 writes 1 to <code>self.some_number</code></li>
<li>thread2 writes 1 to <code>self.some_number</code></li>
</ol>

<p>You use locks to enforce this order of execution:</p>

<ol>
<li>thread1 reads <code>self.some_number</code> (0)</li>
<li>thread1 calculates <code>some_number+1</code> (1)</li>
<li>thread1 writes 1 to <code>self.some_number</code></li>
<li>thread2 reads <code>self.some_number</code> (1)</li>
<li>thread2 calculates <code>some_number+1</code> (2)</li>
<li>thread2 writes 2 to <code>self.some_number</code></li>
</ol>

<h2>EDIT: Let's complete this answer with some code which shows the explained behaviour:</h2>

<pre><code>import threading
import time

total = 0
lock = threading.Lock()

def increment_n_times(n):
    global total
    for i in range(n):
        total += 1

def safe_increment_n_times(n):
    global total
    for i in range(n):
        lock.acquire()
        total += 1
        lock.release()

def increment_in_x_threads(x, func, n):
    threads = [threading.Thread(target=func, args=(n,)) for i in range(x)]
    global total
    total = 0
    begin = time.time()
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()
    print('finished in {}s.\ntotal: {}\nexpected: {}\ndifference: {} ({} %)'
           .format(time.time()-begin, total, n*x, n*x-total, 100-total/n/x*100))
</code></pre>

<p>There are two functions which implement increment. One uses locks and the other does not.</p>

<p>Function <code>increment_in_x_threads</code> implements parallel execution of the incrementing function in many threads.</p>

<p>Now running this with a big enough number of threads makes it almost certain that an error will occur:</p>

<pre><code>print('unsafe:')
increment_in_x_threads(70, increment_n_times, 100000)

print('\nwith locks:')
increment_in_x_threads(70, safe_increment_n_times, 100000)
</code></pre>

<p>In my case, it printed:</p>

<pre><code>unsafe:
finished in 0.9840562343597412s.
total: 4654584
expected: 7000000
difference: 2345416 (33.505942857142855 %)

with locks:
finished in 20.564176082611084s.
total: 7000000
expected: 7000000
difference: 0 (0.0 %)
</code></pre>

<p>So without locks, there were many errors (33% of increments failed). On the other hand, with locks it was 20 time slower.</p>

<p>Of course, both numbers are blown up because I used 70 threads, but this shows the general idea.</p>
"
40073002,82294.0,2016-10-16T17:08:04Z,40072873,2,"<p>At any moment, yes, only one thread is executing Python code (other threads may be executing some IO, NumPy, whatever).  That is mostly true.  However, this is trivially true on any single-processor system, and yet people still need locks on single-processor systems.</p>

<p>Take a look at the following code:</p>

<pre><code>queue = []
def do_work():
    while queue:
        item = queue.pop(0)
        process(item)
</code></pre>

<p>With one thread, everything is fine.  With two threads, you might get an exception from <code>queue.pop()</code> because the other thread called <code>queue.pop()</code> on the last item first.  So you would need to handle that somehow.  Using a lock is a simple solution.  You can also use a proper concurrent queue (like in the <code>queue</code> module)--but if you look inside the <code>queue</code> module, you'll find that the <code>Queue</code> object has a <code>threading.Lock()</code> inside it.  So either way you are using locks.</p>

<p>It is a common newbie mistake to write multithreaded code without the necessary locks.  You look at code and think, ""this will work just fine"" and then find out many hours later that something truly bizarre has happened because threads weren't synchronized properly.</p>

<p>Or in short, there are many places in a multithreaded program where you need to prevent another thread from modifying a structure until you're done applying some changes.  This allows you to maintain the invariants on your data, and if you can't maintain invariants, then it's basically impossible to write code that is correct.</p>

<p>Or put in the shortest way possible, ""You don't need locks if you don't care if your code is correct.""</p>
"
40073112,5741205.0,2016-10-16T17:17:48Z,40072950,1,"<p>How about this?</p>

<pre><code>In [335]: cls = np.intersect1d(data1['class'], data2['class'])

In [336]: cls
Out[336]: array([4, 5], dtype=int64)

In [337]: pd.concat([data1.ix[data1['class'].isin(cls)], data2.ix[data2['class'].isin(cls)]])
Out[337]:
  first_name last_name  class
3      Alice      Aoni      4
4     Andrew   Andrews      4
5     Ayoung   Atiches      5
0      Billy    Bonder      4
1      Brian     Black      5
</code></pre>

<p>or:</p>

<pre><code>In [338]: data1.ix[data1['class'].isin(cls)].append(data2.ix[data2['class'].isin(cls)])
Out[338]:
  first_name last_name  class
3      Alice      Aoni      4
4     Andrew   Andrews      4
5     Ayoung   Atiches      5
0      Billy    Bonder      4
1      Brian     Black      5
</code></pre>
"
40073665,7015325.0,2016-10-16T18:08:40Z,40066439,2,"<p>I think this is what you want:</p>

<pre><code>from collections import Counter

# Remove elements where all nucleobases are the same.
for index in range(len(sample) - 1, -1, -1):
    if sample[index][:1] * len(sample[index]) == sample[index]:
        del sample[index]

for indexA, setA in enumerate(sample):
    for indexB, setB in enumerate(sample):
        # Don't compare samples with themselves nor compare same pair twice.
        if indexA &lt;= indexB:
            continue

        # Calculate number of unique pairs
        pair_count = Counter()
        for pair in zip(setA, setB):
            if '-' not in pair:
                pair_count[pair] += 1

        # Only analyse pairs of sets with 2 unique pairs.
        if len(pair_count) != 2:
            continue

        # Count individual bases.
        base_counter = Counter()
        for pair, count in pair_count.items():
            base_counter[pair[0]] += count
            base_counter[pair[1]] += count

        # Get the length of one of each item in the pair.
        sequence_length = sum(pair_count.values())

        # Convert counts to frequencies.
        base_freq = {}
        for base, count in base_counter.items():
            base_freq[base] = count / float(sequence_length)

        # Examine a pair from the two unique pairs to calculate float_a.
        pair = list(pair_count)[0]
        float_a = (pair_count[pair] / float(sequence_length)) - base_freq[pair[0]] * base_freq[pair[1]]

        # Step 7!
        float_b = float_a / float(base_freq.get('A', 0) * base_freq.get('T', 0) * base_freq.get('C', 0) * base_freq.get('G', 0))
</code></pre>

<p>Or, more Pythonically (with the list/dict comprehensions you don't want):</p>

<pre><code>from collections import Counter

BASES = 'ATCG'

# Remove elements where all nucleobases are the same.
sample = [item for item in sample if item[:1] * len(item) != item]

for indexA, setA in enumerate(sample):
    for indexB, setB in enumerate(sample):
        # Don't compare samples with themselves nor compare same pair twice.
        if indexA &lt;= indexB:
            continue

        # Calculate number of unique pairs
        relevant_pairs = [(elA, elB) for (elA, elB) in zip(setA, setB) if elA != '-' and elB != '-']
        pair_count = Counter(relevant_pairs)

        # Only analyse pairs of sets with 2 unique pairs.
        if len(pair_count) != 2:
            continue

        # setA and setB as tuples with pairs involving '-' removed.
        setA, setB = zip(*relevant_pairs)

        # Get the total for each base.
        seq_length = len(setA)

        # Convert counts to frequencies.
        base_freq = {base : count / float(seq_length) for (base, count) in (Counter(setA) + Counter(setB)).items()}

        # Examine a pair from the two unique pairs to calculate float_a.
        pair = list(pair_count)[0]
        float_a = (pair_count[pair] / float(seq_length)) - base_freq[pair[0]] * base_freq[pair[1]]

        # Step 7!
        denominator = 1
        for base in BASES:
            denominator *= base_freq.get(base, 0)

        float_b = float_a / denominator
</code></pre>
"
40074079,1235433.0,2016-10-16T18:44:42Z,40074054,2,"<p>You can do this with <a href=""https://pypi.python.org/pypi/PyMouse"" rel=""nofollow"">PyMouse</a>.</p>

<pre><code>&gt;&gt;&gt; import pymouse;
&gt;&gt;&gt; mouse = pymouse.PyMouse()
&gt;&gt;&gt; mouse.position()
(288.046875, 396.15625)
&gt;&gt;&gt; mouse.position()
(0.0, 0.0)
&gt;&gt;&gt; mouse.position()
(1439.99609375, 861.2890625)
</code></pre>
"
40074127,5994041.0,2016-10-16T18:49:37Z,40074004,2,"<p>It's implemented like that because it needs to be hidden most of the time, yet still active, so that <code>open()</code> could be called. Kivy doesn't seem to handle hiding of the widgets other way that actually <em>removing</em> it and keeping a reference somewhere (there's no <code>hide</code> property), so maybe even because of that. Or because it was easier to implement it this way. It's not bad implementation however and the way OO programming works you can do some fancy stuff with it too. The thing you want can be handled simply with <code>kwargs</code> in <code>__init__</code>:</p>

<p>Inherit from Popup and get a custom keyword argument:</p>

<pre><code>class StartPop(Popup):
    def __init__(self, **kwargs):
        self.caller = kwargs.get('caller')
        super(StartPop, self).__init__(**kwargs)
        print self.caller
</code></pre>

<p>Then create an instance of that custom <code>Popup</code> and set the parent:</p>

<pre><code>pop = StartPop(caller=self)
pop.open()
</code></pre>

<p>The <code>caller</code> keyword isn't limited only to Kivy widgets. Put there any object you want to do stuff with and you can then access it inside the <code>StartPop</code> object via <code>self.caller</code></p>
"
40074306,376371.0,2016-10-16T19:06:49Z,40074155,2,"<p>One common measure of similarity for use in this situation is the <a href=""https://nickgrattan.wordpress.com/2014/02/18/jaccard-similarity-index-for-measuring-document-similarity/"" rel=""nofollow"">Jaccard similarity</a>. It ranges from 0 to 1, where 0 indicates complete dissimilarity and 1 means the two documents are identical. It is defined as </p>

<pre><code>wordSet1 = set(wordSet1)
wordSet2 = set(wordSet2)
sim = len(wordSet1.intersection(wordSet2))/len(wordSet1.union(wordSet2))
</code></pre>

<p>Essentially, it is the ratio of the intersection of the sets of words to the ratio of the union of the sets of words. This helps control for emails that are of different sizes while still giving a good measure of similarity. </p>
"
40074468,143765.0,2016-10-16T19:24:25Z,40072420,5,"<p>Spline fitting is known to overshoot. You seem to be looking for one of the so-called <em>monotonic</em> interpolators. For instance,</p>

<pre><code>In [10]: from scipy.interpolate import pchip

In [11]: pch = pchip(x, y)
</code></pre>

<p>produces</p>

<pre><code>In [12]: xx = np.linspace(x[0], x[-1], 101)

In [13]: plt.plot(x, y, 'ro', label='points')
Out[13]: [&lt;matplotlib.lines.Line2D at 0x7fce0a7fe390&gt;]

In [14]: plt.plot(xx, pch(xx), 'g-', label='pchip')
Out[14]: [&lt;matplotlib.lines.Line2D at 0x7fce0a834b10&gt;]
</code></pre>

<p><a href=""https://i.stack.imgur.com/xuYMR.png""><img src=""https://i.stack.imgur.com/xuYMR.png"" alt=""enter image description here""></a></p>
"
40074559,6869965.0,2016-10-16T19:32:04Z,40074425,0,"<p>The <a href=""http://www.zipline.io/appendix.html?highlight=fetch_csv#zipline.api.fetch_csv"" rel=""nofollow"">Zipline API reference</a> says that this methods is to ""Fetch a csv from a remote url"". For local files, I would suggest <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow"">pandas</a>:</p>

<pre><code>pandas.read_csv('./test.csv')
</code></pre>
"
40074709,1322962.0,2016-10-16T19:47:10Z,40074385,0,"<p>Given the Jupyter notebook you referenced here's how you'd get the videoId from the data you've retrieved. Does this answer your question? </p>

<p>I'm not fully sure how the Youtube search API works but I might have time to explore it if this isn't a full answer.</p>

<pre><code>example = {
    'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/EK5D70JgnA5Bec8tRSnEFfhIsv0""',
    'items': [
        {
            'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/hsQmFEqp1R_glFpcQnpnOLbbxCg""',
            'id': {
                'kind': 'youtube#video', 'videoId': 'd8kCTPPwfpM'},
            'kind': 'youtube#searchResult',
            'snippet': {
                'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
                'channelTitle': 'TheEllenShow',
            'description': ""This incredible duo teamed up to perform an original song for Ellen! They may not have had a lot of rehearsal, but it's clear that this is one musical combo it ..."",
            'liveBroadcastContent': 'none',
            'publishedAt': '2012-02-21T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/d8kCTPPwfpM/mqdefault.jpg',
      'width': 320}},
    'title': 'Taylor Swift and Zac Efron Sing a Duet!'}},
  {'etag': '""I_8xdZu766_FSaexEaDXTIfEWc0/LeKypRrnWWD6mRhK1wATZB5UQGo""',
   'id': {'kind': 'youtube#video', 'videoId': '-l2KPjQ2lJA'},
   'kind': 'youtube#searchResult',
   'snippet': {'channelId': 'UCp0hYYBW6IMayGgR-WeoCvQ',
    'channelTitle': 'TheEllenShow',
    'description': ""Harry, Liam, Louis and Niall played a round of Ellen's revealing game. How well do you know the guys of One Direction?"",
    'liveBroadcastContent': 'none',
    'publishedAt': '2015-11-18T14:00:00.000Z',
    'thumbnails': {'default': {'height': 90,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/default.jpg',
      'width': 120},
     'high': {'height': 360,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/hqdefault.jpg',
      'width': 480},
     'medium': {'height': 180,
      'url': 'https://i.ytimg.com/vi/-l2KPjQ2lJA/mqdefault.jpg',
      'width': 320}},
    'title': 'Never Have I Ever with One Direction'}},]}

import pprint
for video in example[""items""]:
    pprint.pprint(video[""id""][""videoId""])

# Prints 'd8kCTPPwfpM'
# '-l2KPjQ2lJA'
</code></pre>
"
40074761,1832058.0,2016-10-16T19:54:12Z,40074637,0,"<p>If I use <code>self.old_pos</code> in <code>hideMe</code> and <code>showMe</code> then it works for me (Linux).</p>

<pre><code>def hideMe(self):
    self.old_pos = self.pos()
    self.hide()
    QTimer.singleShot(300, self.showMe)

def showMe(self):
    self.show()
    self.move(self.old_pos)
</code></pre>
"
40074796,2074981.0,2016-10-16T19:57:53Z,40074739,3,"<p>Iterating through the rows doesn't take advantage of Pandas' strengths.  If you want to do something with a column based on values of another column, you can use <a href=""http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.loc.html"" rel=""nofollow""><code>.loc[]</code></a>:</p>

<pre><code>dataFrame.loc[dataFrame['Dates'] == 'Oct-16', 'Score 1']
</code></pre>

<p>The first part of <code>.loc[]</code> selects the rows you want, using your specified criteria (<code>dataFrame['Dates'] == 'Oct-16'</code>).  The second part specifies the column you want (<code>Score 1</code>).  Then if you want to get the mean, you can just put <code>.mean()</code> on the end:</p>

<pre><code>dataFrame.loc[dataFrame['Dates'] == 'Oct-16', 'Score 1'].mean()
</code></pre>
"
40074798,5093840.0,2016-10-16T19:58:09Z,40074739,0,"<pre><code>import pandas as pd
import numpy as np
import os

dataFrame = pd.read_csv(""test.csv"")

dates = dataFrame[""Dates""]
score1s = dataFrame[""Score 1""]
result = []

for i in range(0,len(dates)):
    if dates[i] == ""Oct-16"":
        result.append(score1s[i])

print(result.mean())
</code></pre>
"
40075040,2336654.0,2016-10-16T20:20:08Z,40074739,1,"<p>How about the mean for all dates</p>

<pre><code>dataframe.groupby('Dates').['Score 1'].mean()
</code></pre>
"
40075212,5741205.0,2016-10-16T20:41:06Z,40075106,1,"<p>You can do it using <code>regex=True</code> parameter:</p>

<pre><code>In [37]: s.replace(d, regex=True)
Out[37]:
0    aBc
1    aBe
2    aBg
dtype: object
</code></pre>

<p>As you have already <a href=""http://stackoverflow.com/questions/40075106/replace-values-in-pandas-series-with-dictionary/40075212#comment67423617_40075106"">found out yourself</a> - it's a RegEx replacement and it won't work as you expected:</p>

<pre><code>In [36]: s.replace(d)
Out[36]:
0    abc
1    abe
2    abg
dtype: object
</code></pre>

<p>this is working as expected:</p>

<pre><code>In [38]: s.replace({'abc':'ABC'})
Out[38]:
0    ABC
1    abe
2    abg
dtype: object
</code></pre>
"
40075274,3293881.0,2016-10-16T20:47:01Z,40075164,1,"<p>Here's a NumPy based approach -</p>

<pre><code># Extract the relevant cond column as a 1D NumPy array and pad with False at
# either ends, as later on we would try to find the start (rising edge) 
# and stop (falling edge) for each interval of True values
arr = np.concatenate(([False],df.cond.values,[False]))

# Determine the rising and falling edges as start and stop 
start = np.nonzero(arr[1:] &gt; arr[:-1])[0]
stop = np.nonzero(arr[1:] &lt; arr[:-1])[0]

# Get the interval lengths and determine the largest interval ID
maxID = (stop - start).argmax()

# With maxID get max interval range and thus get mean on the second col
out = df.data.iloc[start[maxID]:stop[maxID]].mean()
</code></pre>

<p><strong>Runtime test</strong></p>

<p>Approaches as functions -</p>

<pre><code>def pandas_based(df): # @ayhan's soln
    res = df['data'].groupby((df['cond'] != df['cond'].shift()).\
                                cumsum()).agg(['count', 'mean'])
    return res[res['count'] == res['count'].max()]

def numpy_based(df):
    arr = np.concatenate(([False],df.cond.values,[False]))
    start = np.nonzero(arr[1:] &gt; arr[:-1])[0]
    stop = np.nonzero(arr[1:] &lt; arr[:-1])[0]
    maxID = (stop - start).argmax()
    return df.data.iloc[start[maxID]:stop[maxID]].mean()
</code></pre>

<p>Timings -</p>

<pre><code>In [208]: # Setup dataframe
     ...: N = 1000  # Datasize
     ...: df = pd.DataFrame(np.random.rand(N),columns=['data'])
     ...: df['cond'] = np.random.rand(N)&gt;0.3 # To have 70% True values
     ...: 

In [209]: %timeit pandas_based(df)
100 loops, best of 3: 2.61 ms per loop

In [210]: %timeit numpy_based(df)
1000 loops, best of 3: 215 Âµs per loop

In [211]: # Setup dataframe
     ...: N = 10000  # Datasize
     ...: df = pd.DataFrame(np.random.rand(N),columns=['data'])
     ...: df['cond'] = np.random.rand(N)&gt;0.3 # To have 70% True values
     ...: 

In [212]: %timeit pandas_based(df)
100 loops, best of 3: 4.12 ms per loop

In [213]: %timeit numpy_based(df)
1000 loops, best of 3: 331 Âµs per loop
</code></pre>
"
40075307,2285236.0,2016-10-16T20:50:23Z,40075164,2,"<p>Using the approach from <a href=""http://stackoverflow.com/questions/29142487/calculating-the-number-of-specific-consecutive-equal-values-in-a-vectorized-way"">Calculating the number of specific consecutive equal values in a vectorized way in pandas</a>:</p>

<pre><code>df['data'].groupby((df['cond'] != df['cond'].shift()).cumsum()).agg(['count', 'mean'])[lambda x: x['count']==x['count'].max()]
Out: 
      count      mean
cond                 
3         3  1.466667
</code></pre>

<p>Indexing by a callable requires 0.18.0, for earlier versions, you can do:</p>

<pre><code>res = df['data'].groupby((df['cond'] != df['cond'].shift()).cumsum()).agg(['count', 'mean'])

res[res['count'] == res['count'].max()]
Out: 
      count      mean
cond                 
3         3  1.466667
</code></pre>

<p>How it works:</p>

<p>The first part, <code>df['cond'] != df['cond'].shift()</code> returns a boolean array:</p>

<pre><code>df['cond'] != df['cond'].shift()
Out: 
0     True
1     True
2     True
3    False
4    False
5     True
6     True
Name: cond, dtype: bool
</code></pre>

<p>So the value is False whenever the row is the same as the above. That means that if you take the cumulative sum, these rows (consecutive ones) will have the same number:</p>

<pre><code>(df['cond'] != df['cond'].shift()).cumsum()
Out: 
0    1
1    2
2    3
3    3
4    3
5    4
6    5
Name: cond, dtype: int32
</code></pre>

<p>Since groupby accepts any Series to group on (it is not necessary to pass a column, you can pass an arbitrary list), this can be used to group the results. <code>.agg(['count', 'mean']</code> part just gives the respective counts and means for each group and at the end it selects the one with the highest count.</p>

<p>Note that this would group consecutive False's together, too. If you want to only consider consecutive True's, you can change the grouping Series to:</p>

<pre><code>((df['cond'] != df['cond'].shift()) | (df['cond'] != True)).cumsum()
</code></pre>

<p>Since we want False's when the condition is True, the condition became 'not equal to the row below <em>OR</em> not True'. So the original line would change to:</p>

<pre><code>df['data'].groupby(((df['cond'] != df['cond'].shift()) | (df['cond'] != True)).cumsum()).agg(['count', 'mean'])[lambda x: x['count']==x['count'].max()]
</code></pre>
"
40075652,2429640.0,2016-10-16T21:24:22Z,40074155,1,"<p>You didn't mention the type of <code>wordset1</code> and <code>wordset2</code>. I'll assume they are both <code>strings</code>.</p>

<p>You defined your distance as the word counting and got a bad score. It's obvious text length is not a good dissimilarity measure: two emails with different sizes can talk about the same thing, while two emails of same size be talking about completely different things.</p>

<p>So, as suggested above, you could try and check for SIMILAR WORDS instead:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def distance(wordset1, wordset2):
    wordset1 = set(wordset1.split())
    wordset2 = set(wordset2.split())

    common_words = wordset1 &amp; wordset2
    if common_words:
        return 1 / len(common_words) 
    else:
        # They don't share any word. They are infinitely different.
        return np.inf
</code></pre>

<p>The problem with that is that two big emails are more likely to share words than two small ones, and this metric would favor those, making them ""more similar to each other"" in comparison to the small ones. How do we solve this? Well, we normalize the metric somehow:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

def distance(wordset1, wordset2):
    wordset1 = set(wordset1.split())
    wordset2 = set(wordset2.split())

    common_words = wordset1 &amp; wordset2
    if common_words:
        # The distance, normalized by the total 
        # number of different words in the emails.
        return 1 / len(common_words) / (len(wordset1 | wordset2))
    else:
        # They don't share any word. They are infinitely different.
        return np.inf
</code></pre>

<p>This seems cool, but completely ignores the FREQUENCY of the words. To account for this, we can use the <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow"">Bag-of-words</a> model. That is, create a list of all possible words and histogram their appearance in each document. Let's use <a href=""http://scikit-learn.org/stable/modules/feature_extraction.html"" rel=""nofollow"">CountVectorizer</a> implementation from scikit-learn to make our job eaiser:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import CountVectorizer

def distance(wordset1, wordset2):
    model = CountVectorizer()
    X = model.fit_transform([wordset1, wordset2]).toarray()

    # uses Euclidean distance between bags.
    return np.linalg.norm(X[0] - X[1])
</code></pre>

<p>But now consider two pairs of emails. The emails in the first pair are composed by perfectly written English, full of ""small"" words (e.g. <code>a</code>, <code>an</code>, <code>is</code>, <code>and</code>, <code>that</code>) necessary for it to be grammatically correct. The emails in the second pair are different: only containing the keywords, it's extremely dry. You see, chances are the first pair will be more similar than the second one. That happens because we are currently accounting for all words the same, while we should be prioritizing the MEANINGFUL words in each text. To do that, let's use <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow"">term frequencyâinverse document frequency</a>. Luckly, there's a very similar implementation in scikit-learn:</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.feature_extraction.text import TfidfVectorizer

def distance(wordset1, wordset2):
    model = TfidfVectorizer()
    X = model.fit_transform([wordset1, wordset2]).toarray()

    similarity_matrix = X.dot(X.T)
    # The dissimilarity between samples wordset1 and wordset2.
    return 1-similarity_matrix[0, 1]
</code></pre>

<p>Read more about this in this <a href=""http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents"">question</a>. Also, duplicate?</p>

<p>You should now have a fairly good accuracy. Try it out. If it's still not as good as you want, then we have to go deeper... (get it? Because... Deep-learning). The first thing is that we need either a dataset to train over or an already trained model. That's required because networks have many parameters that MUST be adjusted in order to provide useful transformations.</p>

<p>What's been missing so far is UNDERSTANDING. We histogrammed the words, striping them from any context or meaning. Instead, let's keep them where they are and try to recognize blocks of patterns. How can this be done?</p>

<ol>
<li>Embed the words into numbers, which will deal with the different sizes of words.</li>
<li>Pad every number (word embed) sequence to a single length.</li>
<li>Use convolutional networks to extract meaninful features from sequences.</li>
<li>Use fully-connected networks to project the features extracted to a space that minimizes the distance between similar emails and maximizes the distance between non-similar ones.</li>
</ol>

<p>Let's use <a href=""https://keras.io/"" rel=""nofollow"">Keras</a> to simply our lives. It should look something like this:</p>

<pre class=""lang-py prettyprint-override""><code># ... imports and params definitions

model = Sequential([
    Embedding(max_features,
              embedding_dims,
              input_length=maxlen,
              dropout=0.2),
    Convolution1D(nb_filter=nb_filter,
                  filter_length=filter_length,
                  border_mode='valid',
                  activation='relu',
                  subsample_length=1),
    MaxPooling1D(pool_length=model.output_shape[1]),
    Flatten(),
    Dense(256, activation='relu'),
])

# ... train or load model weights.

def distance(wordset1, wordset2):
    global model
    # X = ... # Embed both emails.
    X = sequence.pad_sequences(X, maxlen=maxlen)
    y = model.predict(X)
    # Euclidean distance between emails.
    return np.linalg.norm(y[0]-y[1])
</code></pre>

<p>There's a practical example on sentence processing which you can check <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"" rel=""nofollow"">Keras github repo</a>. Also, someone solves this exact same problem using a siamese recurrent network in this <a href=""http://stackoverflow.com/questions/39289050/sentence-similarity-using-keras"">stackoverflow question</a>.</p>

<p>Well, I hope this gives you some direction. :-)</p>
"
40075769,984421.0,2016-10-16T21:37:15Z,40074637,0,"<p>It seems that in Qt5 the geometry won't be re-set if it is exactly the same - but I don't know why this behaviour has changed, or whether it is a bug. And note that it is not just the position that is affected - resizing is also ignored.</p>

<p>Here is a hack to work around the problem:</p>

<pre><code>from PyQt5.QtCore import QMargins

class demo(QMainWindow):
    ...

    def hideMe(self):
        print('hide:', self.geometry())
        self.hide()
        QTimer.singleShot(300, self.showMe)

    def showMe(self):
        print('show1:', self.geometry())
        hack = QMargins(0, 0, 0, 1)
        self.setGeometry(self.geometry() + hack)
        self.show()
        self.setGeometry(self.geometry() - hack)
        print('show2:', self.geometry())
</code></pre>
"
40076095,3125566.0,2016-10-16T22:19:44Z,40075829,2,"<p>You almost got it right. Only that the last digit (or first from behind) should be considered as odd for your 16 digit card. So you should set:</p>

<pre><code>digit = len(cardNumber) - 1
</code></pre>

<p>And then your <em>while</em> condition should stop at <code>&gt;= 0</code> (zeroth item inclusive); note that the <code>len( cardNumber ) == 16</code> is redundant as the length of the card is constant:</p>

<pre><code>while digit &gt;= 0:
</code></pre>

<p>And finally your indexing of the creditcard number will no longer need a minus 1:</p>

<pre><code>value = int(cardNumber[digit]) * 2
...
...
total = total + int(cardNumber[digit])
</code></pre>
"
40076307,704848.0,2016-10-16T22:45:12Z,40076176,3,"<p>You're being confused by the fact that the row labels have been preserved so the last row label is still <code>99</code>.</p>

<p>Example:</p>

<pre><code>In [2]:
df = pd.DataFrame({'a':[0,1,np.NaN, np.NaN, 4]})
df

Out[2]:
    a
0   0
1   1
2 NaN
3 NaN
4   4
</code></pre>

<p>After calling <code>dropna</code> the index row labels are preserved:</p>

<pre><code>In [3]:
df = df.dropna()
df

Out[3]:
   a
0  0
1  1
4  4
</code></pre>

<p>If you want to reset so that they are contiguous then call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html"" rel=""nofollow""><code>reset_index(drop=True)</code></a> to assign a new index:</p>

<pre><code>In [4]:
df = df.reset_index(drop=True)
df

Out[4]:
   a
0  0
1  1
2  4
</code></pre>
"
40076387,5847976.0,2016-10-16T22:57:11Z,40075829,0,"<p>So your code is mostly correct, the only issue is that you haven't properly defined what should be considered an ""odd"" and an ""even"" number. As you read the number from the end, ""odd and even"" are also relative from the end, so  :</p>

<ul>
<li>odd numbers start from the <em>last one</em>, and then every other one </li>
<li>even numbers start from the <em>last but one</em>, and then every other one </li>
</ul>

<p>Example : 1234 is EOEO, 12345 is OEOEO (O means odd , E means even)</p>

<p>Here the fixed code (I only modified three lines, see comments):</p>

<pre><code>digit = len(cardNumber)
value = 0
total = 0
while  digit &gt; 0: # I removed the length condition
    # HANDLE even digit positions
    if ( (len(cardNumber)+1-digit) % 2 == 0 ): # &lt;- modification here
        value = ( int( cardNumber[digit - 1]) * 2 )
        if( value &gt; 9 ):
            double = str( value )
            value = int( double[:1] ) + int( double[-1] )
            total = total + value
            digit = digit - 1
        else:
            total = total + value
            digit = digit - 1

    # HANDLE odd digit positions
    elif ( (len(cardNumber)+1-digit) % 2 != 0): # &lt;- modification here
        value=int( cardNumber[digit - 1] )
        total = total + int( cardNumber[digit - 1] )
        digit = digit - 1

return total
</code></pre>

<p>Some tests :</p>

<pre><code>In : '0378282246310005' -&gt;  Out : 60
In : '00378282246310005' -&gt;  Out : 60
In : '0004222222222222' -&gt;  Out : 40
</code></pre>
"
40076499,5994041.0,2016-10-16T23:12:14Z,40076274,0,"<p>You are already in the widget, go directly for it, not through <code>ids</code>. <code>Ids</code> are for property <code>id</code> set in the children of a widget in kv language e.g. if your TriangleButton had a child <code>Image</code> with an <code>id: myimage</code>, you'd get it with this:</p>

<pre><code>self.ids.myimage
</code></pre>

<p>Therefore removing the unnecessary stuff is enough:</p>

<pre><code>self.triangle_down_color = (1,0,1,1)
</code></pre>

<p>It's also nice to print what you are actually looking for - if it prints some object, or if that thing doesn't even exist. And binding is nicer than putting something manually into <code>on_press</code> :)</p>

<pre><code>t = TriangleButton()
t.bind(on_press=function)
</code></pre>
"
40076558,5741205.0,2016-10-16T23:19:54Z,40076534,4,"<p>Try this:</p>

<pre><code>In [75]: df
Out[75]:
   age   name
0   21   Bill
1   28  Steve
2   22   John
3   30   John
4   29   John

In [76]: df.sort_values('age').drop_duplicates('name', keep='last')
Out[76]:
   age   name
0   21   Bill
1   28  Steve
3   30   John
</code></pre>

<p>or this depending on your goals:</p>

<pre><code>In [77]: df.drop_duplicates('name', keep='last')
Out[77]:
   age   name
0   21   Bill
1   28  Steve
4   29   John
</code></pre>
"
40076716,609782.0,2016-10-16T23:44:42Z,39959206,0,"<p>Well,<br>
in python, this is how things work - <code>$limit</code> needs to be wrapped in <code>""""</code>,<br>
and you need to create a pipeline to execute it as a command.</p>

<p>In my code -</p>

<pre><code>    pipeline = [{ '$limit': 15000 },{'$out': ""destination_collection""}]
    db.command('aggregate', ""source_collection"", pipeline=pipeline)
</code></pre>

<p>You need to wrap everything in double quotes, including your source and destination collection.
And in <code>db.command</code> db is the object of your database (ie dbclient.database_name)</p>

<p>As per this answer -</p>

<blockquote>
  <p>It works about 100 times faster than forEach at least in my case. This is because the entire aggregation pipeline runs in the mongod process, whereas a solution based on find() and insert() has to send all of the documents from the server to the client and then back. This has a performance penalty, even if the server and client are on the same machine.</p>
</blockquote>

<p>The one that really helped me figure this answer out - <a href=""http://blog.pythonisito.com/2012/06/using-mongodbs-new-aggregation.html"" rel=""nofollow"">Reference 1</a>
<br>
And <a href=""https://docs.mongodb.com/v3.2/aggregation/"" rel=""nofollow"">official documentation</a></p>
"
40076927,2682863.0,2016-10-17T00:20:43Z,40076887,1,"<p>a simple dictionary would work</p>

<p>eg</p>

<pre><code>month_dict = {""jan"" : ""January"", ""feb"" : ""February"" .... }
</code></pre>

<blockquote>
  <blockquote>
    <blockquote>
      <p>month_dict[""jan""]</p>
      
      <p>'January'</p>
    </blockquote>
  </blockquote>
</blockquote>
"
40076928,5067311.0,2016-10-17T00:20:50Z,40076887,3,"<p>If you insist on using <code>datetime</code> as per your tags, you can convert the short version of the month to a datetime object, then reformat it with the full name:</p>

<pre><code>import datetime
datetime.datetime.strptime('apr','%b').strftime('%B')
</code></pre>
"
40076933,309746.0,2016-10-17T00:21:45Z,40076887,1,"<p>One quick and dirty way:</p>

<pre><code>conversions = {""Apr"": ""April"", ""May"": ""May"", ""Dec"": ""December""}
date = ""Apr""

if date in conversions:
    converted_date = conversions[date]
</code></pre>
"
40076945,190597.0,2016-10-17T00:24:10Z,40076806,3,"<p>Rename the <code>orderid</code> columns so that <code>df</code> has a column named <code>orderid_left</code>,
and <code>df2</code> has a column named <code>orderid_right</code>:</p>

<pre><code>import pandas as pd    
df = pd.DataFrame([[1,'a'], [2, 'b'], [3, 'c']], columns=['orderid', 'ordervalue'])
df['orderid'] = df['orderid'].astype(str)
df2 = pd.DataFrame([[1,200], [2, 300], [3, 400], [4,500]], columns=['orderid', 'ordervalue'])
df2['orderid'] = df2['orderid'].astype(str)

df = df.rename(columns={'orderid':'orderid_left'})
df2 = df2.rename(columns={'orderid':'orderid_right'})
result = pd.merge(df, df2, left_on='orderid_left', right_on='orderid_right', 
                  how='outer', suffixes=('_left', '_right'))
print(result)
</code></pre>

<p>yields</p>

<pre><code>  orderid_left ordervalue_left orderid_right  ordervalue_right
0            1               a             1               200
1            2               b             2               300
2            3               c             3               400
3          NaN             NaN             4               500
</code></pre>
"
40076983,2336654.0,2016-10-17T00:30:21Z,40076861,3,"<p>There is nothing that ties these dataframes together other than the positional index.  You can accomplish your desired example output with <code>pd.concat</code></p>

<pre><code>pd.concat([distancesDF, datesDF.dates], axis=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/5YLVN.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/5YLVN.png"" alt=""enter image description here""></a></p>

<hr>

<p>To address the edit and @kartik's comment</p>

<p>if we create the dfs to match what's displayed.</p>

<pre><code>distances = {'names': ['A', 'B','C'] ,'distances':[100, 200, 300]}
dates = {'flights': ['A', 'B', 'C'] ,'dates':['1/1/16', '1/2/16', '1/3/16']}

distancesDF = pd.DataFrame(distances)
datesDF = pd.DataFrame(dates)
</code></pre>

<p>then the following two options produce the same and probably desired result.</p>

<p><strong><em>merge</em></strong></p>

<pre><code> distancesDF.merge(datesDF, left_on='names', right_on='flights')[['distances', 'names', 'dates']]
</code></pre>

<p><strong><em>join</em></strong></p>

<pre><code>distancesDF.join(datesDF.set_index('flights'), on='names')
</code></pre>

<p>both produce</p>

<p><a href=""https://i.stack.imgur.com/5YLVN.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/5YLVN.png"" alt=""enter image description here""></a></p>
"
40077241,2639729.0,2016-10-17T01:17:32Z,40077180,-1,"<p>you didnt load your data into <code>dict_member</code> before displaying the roster </p>

<p>when you load your data in the loadData function you redefine <code>dict_member</code>
so it will ""shadow"" the outer <code>dict_member</code> so when the <code>display</code> function is called <code>dict_member</code> will always be empty</p>

<p>so you probably want to remove this line </p>

<pre><code>def loadData(self):
    #  **remove this line ---&gt;** dict_member = {}
    roster = input(""Filename to load: "")
    file = open(roster, ""r"")
    while True:
        inLine = file.readline()
        if not inLine:
            break
        inLine = inLine[:-1]
        name, number, jersey = inLine.split("","")
        dict_member[name] = (name, number, jersey)
    print(""Data Loaded Successfully."")
    file.close()
    return dict_member
</code></pre>
"
40077242,5129275.0,2016-10-17T01:17:36Z,40077180,-1,"<p>Try this:</p>

<pre><code>def loadData(self):
        file = open(input(""Filename to load: ""), ""r"")
        text = file.read()
        file.close()
        for line in text:
            name, number, jersey = (line.rstrip()).split(',')
            dict_member[name] = (name, number, jersey)
        print(""Data Loaded Successfully."")
        return dict_member

def saveData(self, dict_member):
        file = open(input(""Filename to save: ""), ""a"")
        for number, player in dict_member.items():
            rstr.write(player.name + ', ' + player.number + ', ' + player.jersey)
        print(""Data saved."")
        file.close()
</code></pre>

<p>What I think was the error was you using <code>""break""</code> in string from instead of the command, <code>break</code> (no quotes required). I optimized the code a little so maybe it will work now? If not, what exactly happens? Try debugging as well as checking your file.</p>
"
40077285,2639729.0,2016-10-17T01:26:23Z,40077230,1,"<p>well, i can see 2 problems:</p>

<p>1)when you do:</p>

<pre><code>for i in creditCard[-1]
</code></pre>

<p>you dont iterate on the creditCard you simply take the last digit.
you probably meant to do </p>

<pre><code>for i in creditCard[::-1]
</code></pre>

<p>this will iterate the digits from the last one to the first one</p>

<p>2)
the pseudocode said to double the number if its POSITION is even, not if the digit itself is even</p>

<p>so you can do this:</p>

<pre><code>digit_count = len(creditCard)
for i in range(digit_count -1, -1, -1):
    digit = creditCard[i]
</code></pre>

<p>or have a look at the <code>enumerate</code>  built-in function</p>

<p>edit:</p>

<p>complete sample:</p>

<pre><code>creditCard = input(""What is your creditcard?"")
total = 0
digit_count = len(creditCard)
for i in range(0, digit_count, -1):
    digit = creditCard[i]

    if i % 2 == 0:
        digit = digit  * 2
        if digit  &gt; 9:
            digit = digit / 10 + digit % 10 # also noticed you didnt sum the digits of the number  

    total = total + digit



if total % 10 == 0:
    print(""Valid"")

else:
    print(""Invalid"")
</code></pre>
"
40077286,6777350.0,2016-10-17T01:26:50Z,40076887,2,"<p>Here is a method to use <em>calendar</em> library.</p>

<pre><code>&gt;&gt;&gt; import calendar
&gt;&gt;&gt; calendar.month_name [list(calendar.month_abbr).index('Apr')]
'April'
&gt;&gt;&gt;
</code></pre>
"
40077293,4983450.0,2016-10-17T01:27:35Z,40077188,4,"<p>You can try something as follows: Create a <code>row_id</code> within each group by the <code>Game_ID</code> and then unstack by the <code>row_id</code> which will transform your data to wide format:</p>

<pre><code>import pandas as pd
df['row_id'] = df.groupby('Game_ID').Game_ID.transform(lambda g: pd.Series(range(g.size)))
df.set_index(['row_id', 'Game_ID']).unstack(level=0).sortlevel(level = 1, axis = 1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Lsl0a.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/Lsl0a.png"" alt=""enter image description here""></a></p>

<p><em>Update</em>:</p>

<p>If the <code>row_id</code> is preferred to be dropped, you can drop the level from the columns:</p>

<pre><code>df1 = df.set_index(['row_id', 'Game_ID']).unstack(level=0).sortlevel(level = 1, axis = 1)   
df1.columns = df1.columns.droplevel(level = 1)
df1
</code></pre>

<p><a href=""https://i.stack.imgur.com/aF5Gc.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/aF5Gc.png"" alt=""enter image description here""></a></p>
"
40077302,901925.0,2016-10-17T01:29:20Z,40076280,0,"<p>My instinct is to do:</p>

<pre><code>def ...(arg, pad):
    out_shape = &lt;arg.shape + padding&gt;  # math on tuples/lists
    idx = [slice(x1, x2) for ...]   # again math on shape and padding
    res = np.zeros(out_shape, dtype=arg.dtype)
    res[idx] = arg     # may need tuple(idx)
    return res
</code></pre>

<p>In other words, make the target array, and copy the input with the appropriate  indexing tuple.  It will require some math and maybe iteration to construct the required shape and slicing, but that should be straight forward if tedious.</p>

<p>However it appears that <code>np.pad</code> iterates on the axes (if I've identified the correct alternative:</p>

<pre><code>   newmat = narray.copy()
   for axis, ((pad_before, pad_after), (before_val, after_val)) \
            in enumerate(zip(pad_width, kwargs['constant_values'])):
        newmat = _prepend_const(newmat, pad_before, before_val, axis)
        newmat = _append_const(newmat, pad_after, after_val, axis)
</code></pre>

<p>where <code>_prepend_const</code> is:</p>

<pre><code>np.concatenate((np.zeros(padshape, dtype=arr.dtype), arr), axis=axis)
</code></pre>

<p>(and <code>append</code> would be similar).  So it is adding each pre and post piece separately for each dimension.  Conceptually that is simple even if it might not be the fastest.</p>

<pre><code>In [601]: np.lib.arraypad._prepend_const(np.ones((3,5)),3,0,0)
Out[601]: 
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.]])

In [604]: arg=np.ones((3,5),int)
In [605]: for i in range(2):
     ...:     arg=np.lib.arraypad._prepend_const(arg,1,0,i)
     ...:     arg=np.lib.arraypad._append_const(arg,2,2,i)
     ...:     
In [606]: arg
Out[606]: 
array([[0, 0, 0, 0, 0, 0, 2, 2],
       [0, 1, 1, 1, 1, 1, 2, 2],
       [0, 1, 1, 1, 1, 1, 2, 2],
       [0, 1, 1, 1, 1, 1, 2, 2],
       [0, 2, 2, 2, 2, 2, 2, 2],
       [0, 2, 2, 2, 2, 2, 2, 2]])
</code></pre>
"
40077348,6897807.0,2016-10-17T01:35:46Z,40076618,0,"<p>Your problem is in the first line of your <code>while</code> loop, where you write</p>

<pre><code>new_solution= current_best 
</code></pre>

<p>What this does is puts a reference to the <code>current_best</code> list into <code>new_solution</code>. This means that when you change <code>new_solution</code>, you're actually changing <code>current_best</code> as well, which was not your intention. </p>

<p>The problem could be solved by replacing the problematic line with one that copies the list into a new list, like so:</p>

<pre><code>new_solution = list(current_best)
</code></pre>
"
40077679,2320035.0,2016-10-17T02:31:27Z,40077432,3,"<h3>General remarks about SVM-learning</h3>

<p>SVM-training (with nonlinear-kernels; default in sklearn's SVC) is complexity-wise approximately (depends on the data and parameters) <strong>using the widely used SMO-algorithm (don't compare it with SGD-based approaches)</strong>: <code>O(n_samples^2 * n_features)</code> <a href=""https://www.quora.com/What-is-the-computational-complexity-of-an-SVM"" rel=""nofollow"">link to some question with this approximation given by one of sklearn's devs</a>.</p>

<p>So we can do some math to approximate the time-difference between 1k and 100k samples:</p>

<pre><code>1k = 1000^2 = 1.000.000 steps = Time X
100k = 100.000^2 = 10.000.000.000 steps = Time X * 10000 !!!
</code></pre>

<p>This is only an approximation and can be even worse or less worse (e.g. setting cache-size; trading-off memory for speed-gains)!</p>

<h3>Scikit-learn specific remarks</h3>

<p>The situation could also be much more complex because of all that nice stuff scikit-learn is doing for us behind the bars. The above is valid for the classic 2-class SVM. If you are by any chance trying to learn some multi-class data; scikit-learn will automatically use OneVsRest or OneVsAll approaches to do this (as the core SVM-algorithm does not support this). Read up scikit-learns docs to understand this part.</p>

<p>The same warning applies to generating probabilities: SVM's do not naturally produce probabilities for final-predictions. So to use these (activated by parameter) scikit-learn uses a heavy cross-validation procedure called <strong>Platt-scaling</strong> which will take a lot of time too!</p>

<h3>Scikit-learn documentation</h3>

<p>Because sklearn has one of the best docs, there is often a good part within these docs to explain something like that (<a href=""http://scikit-learn.org/stable/modules/svm.html#complexity"" rel=""nofollow"">link</a>):</p>

<p><a href=""https://i.stack.imgur.com/gQlv9.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/gQlv9.png"" alt=""enter image description here""></a></p>
"
40077752,5647260.0,2016-10-17T02:42:09Z,40077591,0,"<p>I'm not exactly sure how the <code>for</code> loop is relevant here, but you can use the formula that follows (based off Java's random number in range equation, mentioned <a href=""http://stackoverflow.com/questions/363681/generating-random-integers-in-a-specific-range"">here</a>) and in particular <a href=""http://stackoverflow.com/a/363732/5647260"">this</a> answer, which is as follows:</p>

<pre><code>minimum + int(random.random() * (maximum - minimum))
</code></pre>

<p>The above will generate a number in the interval <code>[minimum, maximum)</code>. This is because multiplying the random number in <code>[0, 1)</code> by <code>maximum - minimum</code> will give you a range of values, from <code>maximum</code> (inclusive) to <code>maximum</code> exclusive. Then you just add the <code>minimum</code> to that range to achieve the range of random values.</p>

<p>We can nerf it to fit your needs of <code>[30, 35]</code> by setting minimum to 31 (inclusive) and 35 (exclusive):</p>

<pre><code>31 + int(random.random() * (35 - 31))
</code></pre>

<p>The above will generate <code>(30, 35)</code>, or a number between 30 and 35 exclusive:</p>

<pre><code>&gt;&gt;&gt; print(31 + int(random.random() * (35 - 31)))
34
&gt;&gt;&gt; print(31 + int(random.random() * (35 - 31)))
32
&gt;&gt;&gt; print(31 + int(random.random() * (35 - 31)))
32
...
</code></pre>

<p>This would most optimally be applied into a function, like so:</p>

<pre><code>def random_in_range(minimum, maximum): #exclusive to exclusive
    return (minimum + 1) + int(random.random() * (maximum - (minimum + 1)))
</code></pre>

<p>And called like:</p>

<pre><code>&gt;&gt;&gt; print(random_in_range(30, 35))
33 
</code></pre>
"
40077830,309746.0,2016-10-17T02:55:54Z,40077591,0,"<p>Here is another way to do it:</p>

<pre><code>import random
#initial random seed based on current system time
#https://docs.python.org/2/library/random.html#random.seed

random.seed(9) #We set this so random is repeatable in testing
random_range = [30, 31, 32, 33, 34, 35]

while True:
     num = int(round(random.random(),1)*100)
     if num in random_range:
        print num
        break
</code></pre>

<p>The seed is set to 9, so if you run this over and over again you will get the same random value...thus setting a seed.  Note that multiple iterations are run because it was mentioned to not use a choice like random.choice  </p>
"
40077889,624829.0,2016-10-17T03:03:30Z,39913847,0,"<p>To freeze your python executable and ship it along your code, embed it in an empty shell app. Follow the instructions how to embed python in an application from the <a href=""https://docs.python.org/3/extending/embedding.html#embedding-python-in-another-application"" rel=""nofollow"">official documentation</a>. You can start building a sample app directly from the C sample code they give on the web page. </p>

<p>Make that program execute your python application through the embedded python. Ship the program, the embedded python you used and your python program. Execute that program.</p>
"
40077933,2336654.0,2016-10-17T03:09:13Z,40077188,1,"<p>Knowing that games always involve exactly 2 teams, we can manipulate the underlying numpy array.</p>

<pre><code>pd.DataFrame(df.values[:, 1:].reshape(-1, 4),
             pd.Index(df.values[::2, 0], name='Game_ID'),
             ['Team', 'Score'] * 2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/YZPPL.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/YZPPL.png"" alt=""enter image description here""></a></p>
"
40077981,2548934.0,2016-10-17T03:15:35Z,40077880,1,"<p>Always better to do that logic before it gets to the template. What if you set the ordering on ingredients so then you won't have to order them in the template? Does that work and improve the performance?</p>

<pre><code>class Ingredient(models.Model):
  ...

  class Meta:
    ordering = ['ingredient_name']


&lt;div class=""panel-group"" id=""accordion""&gt;
    {% for recipe in recipes %}
    &lt;div class=""panel panel-default""&gt;
        &lt;div class=""panel-heading""&gt;
            &lt;h4 class=""panel-title""&gt;
                &lt;a data-toggle=""collapse"" data-parent=""#accordion"" href=""#collapse{{ forloop.counter }}""&gt;
                    {{ recipe }}
                &lt;/a&gt;
            &lt;/h4&gt;
        &lt;/div&gt;
        &lt;div id=""collapse{{ forloop.counter }}"" class=""panel-collapse collapse""&gt;
            &lt;div class=""panel-body""&gt;
                &lt;table class=""table table-hover""&gt;
                    {% for ingredient in recipe.ingredient_set.all %}
                        &lt;tr&gt;
                            &lt;td&gt;
                                {{ ingredient.ingredient_name }}
                            &lt;/td&gt;
                            &lt;td&gt;
                                {{ ingredient.ingredient_quantity }}
                            &lt;/td&gt;
                        &lt;/tr&gt;
                    {% endfor %}
                    &lt;p&gt;{{ recipe.details }}&lt;/p&gt;
                &lt;/table&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    {% endfor %}
&lt;/div&gt;
</code></pre>
"
40078202,2348832.0,2016-10-17T03:46:11Z,39935335,0,"<p>I'm using <code>OS X 10.12</code> and <code>python 2.7.10</code></p>

<pre><code>sudo easy_install BeautifulSoup4
sudo easy_install pyopenssl
</code></pre>

<p>They all worked fine.</p>
"
40078263,5755166.0,2016-10-17T03:55:14Z,40078015,1,"<p>Unfortunately for you, it is handled this way ""by design"" of the Flask-RESTful APIs <code>errors</code> functionality.
The exceptions which are thrown are logged and the corresponding response defined in the <code>errors</code> dict is returned.</p>

<p>However, you can change the level of log output by modifying the log level of Flask's logger like this:</p>

<pre><code>app = Flask(__name__)
app.logger.setLevel(logging.CRITICAL)
</code></pre>

<p>I think you would actually have to set it to <code>CRITICAL</code> because these errors are still getting logged even on log level <code>ERROR</code> as far as I know.</p>

<p>Furthermore, both Flask and Flask-RESTful are open-source. That being said, after looking at the code I found <a href=""https://github.com/pallets/flask/blob/0.11.1/flask/app.py#L1576"" rel=""nofollow"">the function of a Flask app that is responsible for adding the exception traceback to the log</a> (Flask version 0.11.1).
Of course you could just create your own <code>App</code> class (extending the original class of Flask) which overrides this method (or a caller of it) and does something else instead. However, you should be careful when updating your Flask version if you make use of undocumented stuff like this.</p>
"
40078374,4661269.0,2016-10-17T04:10:18Z,40078265,1,"<p>in the control statements under the comment  <code>#Figure out what credit card the user has</code>, the variable <code>cardType</code> is defined in every branch except <code>else</code>. Since the name was never defined outside the scope of the control statement, the interpreter gives you a NameError when you try to access the variable when the code followed the else branch of the if statement.</p>

<p>to fix this you can do a couple of different things. you can create a a special value for <code>cardType</code> when <code>CardNumber</code> is invalid and check for it in the next control statement:</p>

<pre><code>if ...:
    ...
else:
    cardType = ""some special value""

if cardType == ""some special value"":
    ...
</code></pre>

<p>or you could use a try/except statement:</p>

<pre><code>try:
    print(cardType)
except NameError:
    print(""invalid card number"")
</code></pre>

<p><strong>EDIT</strong>: Also you should note that currently the <code>total</code> variable will always be <code>0</code> as the for loop doesn't actually run. If you want to decrement a range, the first argument should be greater than the second, or the range function will just create an empty list.</p>
"
40078640,4580942.0,2016-10-17T04:45:49Z,40078536,1,"<p>This will work for you : </p>

<pre><code>&gt;&gt;&gt; import requests
&gt;&gt;&gt; from lxml import html
&gt;&gt;&gt; r = requests.get(""https://www.google.co.uk/search?q=how+to+do+web+scraping&amp;num=10"")
&gt;&gt;&gt; source = html.fromstring((r.text).encode('utf-8'))
&gt;&gt;&gt; links = source.xpath('//h3[@class=""r""]//a//@href')
&gt;&gt;&gt; for link in links:
        print link.replace(""/url?q="","""").split(""&amp;sa="")[0]
</code></pre>

<p>Output :</p>

<pre><code>http://newcoder.io/scrape/intro/
https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/
http://docs.python-guide.org/en/latest/scenarios/scrape/
http://webscraper.io/
https://blog.hartleybrody.com/web-scraping/
https://first-web-scraper.readthedocs.io/
https://www.youtube.com/watch%3Fv%3DE7wB__M9fdw
http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/
http://analystcave.com/web-scraping-tutorial/
https://en.wikipedia.org/wiki/Web_scraping
</code></pre>

<p><strong>Note</strong>: <em>I am using Python 2.7.X , for Python 3.X you just have to surround the print output like this</em> <strong>print (link.replace(""/url?q="","""").split(""&amp;sa="")[0])</strong></p>
"
40078682,2279150.0,2016-10-17T04:51:15Z,40078532,3,"<p>Maybe you need to change varibale name of a1,b1 in your code to some other names.</p>

<pre><code>....
a1 = 0
b1 = 0
...
</code></pre>

<p>They will remove input a1/b1 as the same name, I don't see why that needed :)</p>

<pre><code>a0,a1,a2 = [int(a0),int(a1),int(a2)]
b0,b1,b2 = [int(b0),int(b1),int(b2)]
</code></pre>
"
40078722,7029089.0,2016-10-17T04:56:31Z,40078532,4,"<p>I find 2 issues in this. 
1. variable names are same. Notice a1 in list and and a1 as a separate Variable.
2. Instead of print you can use '{0} {1}'.format(a1,b1)
Also I would suggest using raw_input() instead of input(), that will help your input treated as a string.</p>
"
40079139,5266998.0,2016-10-17T05:36:54Z,40057253,0,"<p>Do it in gmail python client(provide by google).under main function </p>

<pre><code>request = {
        'labelIds': ['INBOX'],
        'topicName': 'projects/myprojects/topics/getTopic'
    }
    print(service.users().watch(userId='me', body=request).execute())
</code></pre>
"
40079399,6915675.0,2016-10-17T05:59:38Z,40069185,1,"<p>I do not have an answer for you but would love to know myself</p>
"
40079772,104349.0,2016-10-17T06:27:14Z,40079728,6,"<p>Use the double-underscore syntax.</p>

<pre><code>User.objects.order_by('-pet__age')[:10]
</code></pre>

<p><strong>Edit</strong></p>

<p>To get the ten friends of Tom, you can get the instance and filter:</p>

<pre><code>User.objects.get(name='Tom').friends.order_by('-pet__age')[:10]
</code></pre>

<p>or if you already have Tom:</p>

<pre><code>tom.friends.order_by('-pet__age')[:10]
</code></pre>
"
40079852,320726.0,2016-10-17T06:33:19Z,40069185,1,"<p>The key formulas are (python):</p>

<pre><code># (x0, y0) and (x1, y1) are two points on the mirroring line
# dx, dy, L is the vector and lenght
dx, dy = x1 - x0, y1 - y0
L = (dx**2 + dy**2) ** 0.5

# Tangent (tx, ty) and normal (nx, ny) basis unit vectors
tx, ty = dx / L, dy / L
nx, ny = -dy / L, dx / L

# For each pixel
for y in range(h):
    for x in range(w):
        # Map to tangent/normal space
        n = (x+0.5 - x0)*nx + (y+0.5 - y0)*ny
        t = (x+0.5 - x0)*tx + (y+0.5 - y0)*ty

        # If we're in the positive half-space
        if n &gt;= 0:
            # Compute mirrored point in XY space
            # (negate the normal component)
            xx = int(x0 + t*tx - n*nx + 0.5)
            yy = int(y0 + t*ty - n*ny + 0.5)

            # If valid copy to destination
            if 0 &lt;= xx &lt; w and 0 &lt;= yy &lt; h:
                img[y][x] = img[yy][xx]
</code></pre>

<p>Here you can see an example of the results
<a href=""https://i.stack.imgur.com/4lzrZ.jpg"" rel=""nofollow""><img src=""https://i.stack.imgur.com/4lzrZ.jpg"" alt=""example of mirroring""></a></p>

<p>The top-left red corner are pixels that would be mirroring pixels outside of the original image and they're left untouched by the above code.</p>
"
40080190,6448351.0,2016-10-17T06:56:41Z,40079728,0,"<p>Try this :
First define <strong>unicode</strong> in model User like this:
By this,User model objects will always return name field of the user records.</p>

<pre><code> class User(models.Model):
    name = models.CharField(max_length=50, null=False, blank=False)
    friends = models.ManyToManyField(self, ...)

    def __unicode__(self):
       return self.name
</code></pre>

<p>Then use this query:</p>

<pre><code>   User.objects.filter(friends='Tom').order_by('-pet__age')[:10]
</code></pre>
"
40080720,3846612.0,2016-10-17T07:30:33Z,40077966,0,"<p>If you are working with <code>numpy</code>, you can use it inside the brackets</p>

<pre><code>m = x
M = x + OrbitalPeriod
croppedList = List[m &lt;= List]
croppedList = croppedList[croppedList &lt; M]
</code></pre>
"
40080877,6748546.0,2016-10-17T07:39:42Z,40080783,1,"<pre><code>class USPatent(Patent):
    """"""""Class for holding information of uspto patents in Specific format""""""
    def __init__(self, CC, PN, KC=""""):
        Patent.__init__(self, CC, PN, KC="""")
</code></pre>

<p>Here you pass <code>KC</code>as <code>""""</code> by coding <code>KC=""""</code>, instead of <code>KC=KC</code></p>

<p>To pass the inputted <code>KC</code>: </p>

<pre><code>class USPatent(Patent):
    """"""""Class for holding information of uspto patents in Specific format""""""
    def __init__(self, CC, PN, KC=""""):
        Patent.__init__(self, CC, PN, KC)
</code></pre>
"
40080883,100297.0,2016-10-17T07:39:52Z,40080783,4,"<p>You are passing in an empty string:</p>

<pre><code>Patent.__init__(self, CC, PN, KC="""")
</code></pre>

<p>That calls the <code>Patent.__init__()</code> method setting <code>KC</code> to <code>""""</code>, always.</p>

<p>Pass in whatever value of <code>KC</code> you received instead:</p>

<pre><code>class USPatent(Patent):
    """"""""Class for holding information of uspto patents in Specific format""""""
    def __init__(self, CC, PN, KC=""""):
        Patent.__init__(self, CC, PN, KC=KC)
</code></pre>

<p>Within <code>USPatent.__init__()</code>, <code>KC</code> is just another variable, just like <code>self</code>, <code>CC</code> and <code>PN</code>. It is either set to <code>""""</code> already, or to whatever was passed in when you call <code>USPatent(...)</code> with arguments. You simply want to call the <code>Patent.__init__()</code> method passing on all the values you have.</p>

<p>You can drop the keyword argument syntax from the call too:</p>

<pre><code>Patent.__init__(self, CC, PN, KC)
</code></pre>
"
40080893,5276520.0,2016-10-17T07:40:31Z,40080783,1,"<p>The line</p>

<pre><code>Patent.__init__(self, CC, PN, KC="""")
</code></pre>

<p>Should be</p>

<pre><code>Patent.__init__(self, CC, PN, KC)
</code></pre>

<p>The former sets the argument with the name ""KC"" to the value <code>""""</code> (the empty string) using the keyword-style argument syntax. What you want is pass the value of the variable <code>KC</code> instead.</p>
"
40081252,633961.0,2016-10-17T08:01:54Z,39704298,0,"<p>Since I love condition-less code, I use this solution. It's like in the django docs, but the ulgy <code>if __name__ == '__pain__'</code> gets avoided.</p>

<p>File with code:</p>

<pre><code># utils/do_good_stuff.py
# This file contains the code
# No `django.setup()` needed
# This code can be used by web apps and console scripts.

from otherlib import othermethod

def say_thank_you():
    ...
</code></pre>

<p>File for main:</p>

<pre><code># do_good_stuff_main.py
import django
django.setup()

def say_thank_you_main():
    from myapp import do_good_stuff
    return do_good_stuff()
</code></pre>

<p>setup.py:</p>

<pre><code>    'console_scripts': [
        'say_thank_you=myapp.do_good_stuff_main:say_thank_you_main',
    ...
</code></pre>

<p>This is my current solution. Ffeeback welcome. Is there something to improve?</p>
"
40081519,4272651.0,2016-10-17T08:17:42Z,40081237,1,"<p>Because the <code>yield</code> command returns a generator. The conversion of a generator to a set is triggering the unhashable type error.</p>

<p>You can make your code work by a simple alteration.</p>

<pre><code>shingles1 = get_shingle(2,list1[0])
lst = [x for x in shingles1]
</code></pre>

<p>This will give you all the bigrams from <code>list1[0]</code> and put it into <code>lst</code></p>
"
40081589,2526441.0,2016-10-17T08:22:41Z,40081237,1,"<p>The issue lies in the fact that your get_shingle() function yields <code>lists</code>.
Lists are not hashable, which is needed to build a set. You can easily solve this by yielding a tuple (which is hashable), instead of a list.</p>

<p>Transform the following line in your code:</p>

<pre><code>yield tuple(f[i:i+2])
</code></pre>

<p>This will result in the following:</p>

<pre><code>list1 = [['hello','there','you','too'],['hello','there','you','too','there'],['there','you','hello']]

def get_shingle(size,f):
    #shingles = set()
    print(f)
    for i in range (0,len(f)-2+1):
        yield tuple(f[i:i+2])

shingles1 = { i for i in get_shingle(2,list1[0])}
print(shingles1)
</code></pre>

<p>and outputs:</p>

<pre><code>['hello', 'there', 'you', 'too']
{('you', 'too'), ('hello', 'there'), ('there', 'you')}
</code></pre>
"
40081707,6697265.0,2016-10-17T08:29:15Z,39704298,1,"<p>I have worked in two production CLI python packages with explicitly calling <code>django.setup()</code> in <code>console_scripts</code>. </p>

<p>The most important thing you should note is <code>DJANGO_SETTINGS_MODULE</code> in <code>env</code> path.</p>

<p>You can set this value in shell script or even load default settings in your python script.</p>

<p>Here is example:</p>

<pre><code># setup.py
entry_points={
    'my-cli = mypackage.cli:main'
}
</code></pre>

<p>.</p>

<pre><code># cli.py
import logging
from os import environ as env


if not 'DJANGO_SETTINGS_MODULE' in env:
    from mypackage import settings
    env.setdefault('DJANGO_SETTINGS_MODULE', settings.__name__)


import django
django.setup()

# this line must be after django.setup() for logging configure
logger = logging.getLogger('mypackage')

def main():
    # to get configured settings
    from django.conf import settings

    # do stuffs


if __name__ == '__main__':
    main()
</code></pre>
"
40081924,4190304.0,2016-10-17T08:42:43Z,40081237,1,"<p>Yield command generates a generator and set(iterator) expects an iterator which is immutable </p>

<p>So something like this will work</p>

<pre><code>shingles1 = set(get_shingle(2,list1[0]))
set(tuple(x) for x in shingles1)
</code></pre>
"
40082910,4575071.0,2016-10-17T09:33:38Z,40079728,0,"<p>Another solution (alternative to <code>order_by</code>) is using <code>nlargest</code> function of <code>heapq</code> module, this might be better if you already have <code>friends</code> list (tom's friends in this case) with a large number of items (I mean from performance perspective).</p>

<pre><code>import heapq

heapq.nlargest(
    10,
    User.objects.get(name='Tom').friends.all(),
    key=lambda f: f.pet.age
)
</code></pre>

<p><strong>Note:</strong> You have also <code>nsmallest</code> function that you can use to get the youngest pets.</p>
"
40083051,4153426.0,2016-10-17T09:41:54Z,40083007,2,"<p>instead of appending you sould wrap <code>x</code> and call recursively the method till call number is lesser than <code>n</code></p>

<pre><code>def nest(x, n):
    if n &lt;= 0:
        return x
    else:
        return [nest(x, n-1)]
</code></pre>
"
40083055,3890632.0,2016-10-17T09:42:23Z,40083007,5,"<p>Every turn through the loop you are adding to the list. You want to be further nesting the list, not adding more stuff onto it. You could do it something like this:</p>

<pre><code>def nest(x, n):
    for _ in range(n):
        x = [x]
    return x
</code></pre>

<p>Each turn through the loop, <code>x</code> has another list wrapped around it.</p>
"
40083670,2867928.0,2016-10-17T10:11:54Z,40083007,1,"<p>Here is a pythonic recursion approach:</p>

<pre><code>In [8]: def nest(x, n):
   ...:     return nest([x], n-1) if n else x 
</code></pre>

<p>DEMO:</p>

<pre><code>In [9]: nest(3, 4)
Out[9]: [[[[3]]]]

In [11]: nest(""Stackoverflow"", 7)
Out[11]: [[[[[[['Stackoverflow']]]]]]]
</code></pre>
"
40083720,5043793.0,2016-10-17T10:14:25Z,40080263,1,"<p>You can use recursive function that takes the first number in pattern and generates all the combinations of that length from remaining items. Then recurse with remaining pattern &amp; items and generated prefix. Once you have consumed all the numbers in pattern just <code>yield</code> the prefix all the way to caller:</p>

<pre><code>from itertools import combinations

pattern = [2, 1, 1]
chars = ['A', 'B', 'C', 'D']

def patterns(shape, items, prefix=None):
    if not shape:
        yield prefix
        return

    prefix = prefix or []
    for comb in combinations(items, shape[0]):
        child_items = items[:]
        for char in comb:
            child_items.remove(char)
        yield from patterns(shape[1:], child_items, prefix + [comb])

for pat in patterns(pattern, chars):
    print(pat)
</code></pre>

<p>Output:</p>

<pre><code>[('A', 'B'), ('C',), ('D',)]
[('A', 'B'), ('D',), ('C',)]
[('A', 'C'), ('B',), ('D',)]
[('A', 'C'), ('D',), ('B',)]
[('A', 'D'), ('B',), ('C',)]
[('A', 'D'), ('C',), ('B',)]
[('B', 'C'), ('A',), ('D',)]
[('B', 'C'), ('D',), ('A',)]
[('B', 'D'), ('A',), ('C',)]
[('B', 'D'), ('C',), ('A',)]
[('C', 'D'), ('A',), ('B',)]
[('C', 'D'), ('B',), ('A',)]
</code></pre>

<p>Note that above works only with Python 3 since it's using <code>yield from</code>.</p>
"
40084268,891354.0,2016-10-17T10:40:33Z,40078164,1,"<p>I assume it's because of Django's (or middleware's) transaction management , I'm not completely sure, it's better to test it on your code, but it looks for me like: when you try to acquire a lock, Django might start a new transaction, so when you're actually getting lock at <code>cursor.execute(LOCK_SQL, [self._meta.db_table, self.id])</code> you are already isolated. </p>

<p>While you waiting for lock, another process (with acquired lock) does insert to the database and commits its transaction, but the first process won't see this change when it actually acquires the lock, because transaction has started before.</p>

<p>You could check your application settings for <code>ATOMIC_REQUESTS</code> or any middleware that could enable transactions per request.</p>
"
40085052,3293881.0,2016-10-17T11:20:00Z,40084931,5,"<p><strong>Approach #1 :</strong>  Using <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasting</code></a> -</p>

<pre><code>nrows = ((a.size-L)//S)+1
out = a[S*np.arange(nrows)[:,None] + np.arange(L)]
</code></pre>

<p><strong>Approach #2 :</strong> Using more efficient <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html"" rel=""nofollow""><code>NumPy strides</code></a> -</p>

<pre><code>n = a.strides[0]
out = np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))
</code></pre>

<p>Sample run -</p>

<pre><code>In [183]: a
Out[183]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])

In [184]: L = 5 # length

In [185]: S = 3 # stride

In [186]: nrows = ((a.size-L)//S)+1

In [187]: a[S*np.arange(nrows)[:,None] + np.arange(L)]
Out[187]: 
array([[ 1,  2,  3,  4,  5],
       [ 4,  5,  6,  7,  8],
       [ 7,  8,  9, 10, 11]])

In [188]: n = a.strides[0]

In [189]: np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))
Out[189]: 
array([[ 1,  2,  3,  4,  5],
       [ 4,  5,  6,  7,  8],
       [ 7,  8,  9, 10, 11]])
</code></pre>
"
40085620,5714445.0,2016-10-17T11:48:22Z,40084895,0,"<p>I don't know if this was intended, but the output of your signal simply oscillates between 0 and 1. I tried</p>

<pre><code>import numpy as np
a = np.genfromtxt('test.csv', delimiter=',')    #Using numpy to directly read csv file into numpy array. Also, I renamed the csv file to test.csv
a=a[1:]    #To remove the headers

print(np.nonzero(a[::2, 1]))
print(np.nonzero(a[1::2, 1]-1))
</code></pre>

<p>both yield empty lists, indicating all even positions have the value 1 and all odd indices have 0. That is why you see the mushed up plot.</p>

<p>So taking the mean of 1/4th the x-span(for four stages) and rendering it would be meaningless. So the only option you would be left with is to plot only a very small sub-section of your csv file</p>

<p>That can be easily done with just (assuming you want to plot from the 10th to the 20th value)</p>

<pre><code>plt.plot( a[10:20,0], a[10:20,1])
</code></pre>

<p><a href=""https://i.stack.imgur.com/e546v.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/e546v.png"" alt=""enter image description here""></a></p>

<p>An observation: although y-value jumps between 0 and 1 alternatively, x-value does not change in any pattern</p>
"
40086324,4985733.0,2016-10-17T12:24:25Z,40084895,2,"<p>As a rudimentary analysis, you could do the following using a <code>deque</code> to process a sliding window of values:</p>

<pre><code>from collections import deque
import csv
import matplotlib
import matplotlib.pyplot as plt


maxlen = 20
window = deque(maxlen=maxlen)

with open('12a6-data_extracted_2.csv') as f_input:
    csv_input = csv.reader(f_input, skipinitialspace=True)
    header = next(csv_input)

    freq = []
    x = []

    for v1, v2 in csv_input:
        v1 = float(v1)
        window.append(v1)

        if len(window) == maxlen:
            x.append(v1)
            freq.append(maxlen / ((window[-1] - window[0])))

    plt.plot(x, freq)
    plt.show()
</code></pre>

<p>This would give you an output looking like:</p>

<p><a href=""https://i.stack.imgur.com/SmIBH.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/SmIBH.png"" alt=""Frequency""></a></p>
"
40086329,4814919.0,2016-10-17T12:24:35Z,39704298,-1,"<p>I have some scripts that are placed inside the django folder, i just placed this at the start of the script.</p>

<pre><code>import sys, os, django
os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""eternaltool.settings"")
django.setup()
</code></pre>
"
40086403,3125566.0,2016-10-17T12:28:34Z,40086365,6,"<p>The year directive should be in caps:</p>

<pre><code>date_time = datetime.strptime(date + "" "" + time, '%Y/%m/%d %H:%M:%S.%f')
#                                                  ^
</code></pre>

<blockquote>
  <p><code>%y</code>  Year without century as a zero-padded decimal number </p>
  
  <p><code>%Y</code>  Year with century as a decimal number.</p>
</blockquote>

<hr>

<p>Reference:</p>

<p><a href=""http://strftime.org/"">http://strftime.org/</a></p>
"
40086417,5276520.0,2016-10-17T12:29:20Z,40086365,2,"<p><code>%y</code> is the specifier for ""Year without century as a zero-padded decimal number."", which means a two-digit number. Use <code>%Y</code> instead.</p>
"
40086534,2063361.0,2016-10-17T12:35:03Z,40086365,1,"<p>If you are not very familiar with the <a href=""https://docs.python.org/2/library/time.html#time.strftime"" rel=""nofollow"">datetime directives</a>, I will suggest you to use <a href=""http://dateutil.readthedocs.io/en/stable/parser.html"" rel=""nofollow""><code>dateutil.parser</code></a> instead. It is much simpler. For example:</p>

<pre><code>&gt;&gt;&gt; import dateutil.parser
&gt;&gt;&gt; date = ""2016/07/20""
&gt;&gt;&gt; time = ""11:44:20.920""
&gt;&gt;&gt; my_date = dateutil.parser.parse('{date} {time}'.format(date=date, time=time))
&gt;&gt;&gt; my_date
datetime.datetime(2016, 7, 20, 11, 44, 20, 920000)
</code></pre>
"
40086840,1841194.0,2016-10-17T12:49:47Z,40086500,4,"<p>No. I think this approach somewhat breaks software engineering principles (e.g. <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"" rel=""nofollow"">single responsibility</a>). In single responsibility principle, each module is only in charge of its assigned task and nothing else. If we consider UI a separate layer, so your processing modules shouldn't have anything to do with the UI layer.</p>

<p>In this case, your modules should have a method called <code>publish_progress(callback)</code> which <code>callback</code> is a function to be called for each progress step (<a href=""https://en.wikipedia.org/wiki/Callback_(computer_programming)"" rel=""nofollow"">more info</a>). Then, in your UI layer define a function which is given an integer (between 0 to 100) and updates the progress bar. Once you've defined it, you should register it with <code>publish_progress</code> method of your modules.</p>

<pre><code>def progress_callback(prg):
    self.dlg.progressBar.setValue(prg)
</code></pre>

<p>Registering it:</p>

<pre><code>my_module.publish_progress(progress_callback)
</code></pre>

<p>Calling the callback in your module:</p>

<pre><code>progress_callback(0)
...
# do something
...
progress_callback(20)
...
# do something
...
progress_callback(100)
</code></pre>
"
40087743,1025222.0,2016-10-17T13:29:54Z,40087681,3,"<p>Simply open the /dev/null device and overwrite the <code>sys.stdout</code> variable to that value when you need it to be quiet.</p>

<pre><code>import os
import sys

old_stdout = sys.stdout
sys.stdout = open(os.devnull, ""w"")

from py1 import test

sys.stdout = old_stdout
print test
</code></pre>
"
40087752,4099813.0,2016-10-17T13:30:23Z,40087681,5,"<p>py1.py use an <code>if __name__==""__main__"":</code></p>

<p>So like your py1.py would look like: </p>

<pre><code>def main():
    test=(""hi"", ""hello"")

    print test[0]

if __name__==""__main__"":
    main()  
</code></pre>

<p>This will allow you to still use py1.py normally, but when you import it, it won't run the <code>main()</code> function unless you call it.</p>

<p><a href=""https://docs.python.org/3/library/__main__.html"" rel=""nofollow"">This explains what's going on</a></p>
"
40087778,5199146.0,2016-10-17T13:31:31Z,40087681,0,"<p>You might want to consider changing the other script to still print when its run 'in the other place' - if you're running py1 as a shell command, try to make sure all ""executable statements"" in a file are inside a block.</p>

<pre><code>if __name__ == ""__main__"":
    print test
</code></pre>

<p>(see <a href=""http://stackoverflow.com/questions/419163/what-does-if-name-main-do"">What does `if __name__ == &quot;__main__&quot;:` do?</a>)</p>

<p>This would fix the underlying issue without having you do weird things (which would be redirecting the standard out, and then putting it back etc), or opening the file and executing line by line on an if block.</p>
"
40087816,1949771.0,2016-10-17T13:33:13Z,40087681,0,"<p>You could implement this functionality with methods:</p>

<p><code>py1.py</code></p>

<pre><code>test=(""hi"", ""hello"")

def print_test():
    print(test)

def print_first_index():
    print(test[0])
</code></pre>

<p><code>py2.py</code></p>

<pre><code>import py1
py1.print_test()
</code></pre>

<p>As MooingRawr pointed out, this would require you to change whichever classes use <code>py1.py</code> to import it and call the <code>py1.print_first_index()</code> function which may not be to your liking.</p>
"
40088327,438117.0,2016-10-17T13:55:56Z,39939780,2,"<p>The best examples are located in the unit-tests shipped with the code. <a href=""https://github.com/dahlia/wand/blob/master/tests/sequence_test.py"" rel=""nofollow""><code>wand/tests/sequence_test.py</code></a> for example.</p>

<p>For creating an animated gif with wand, remember to load the image into the sequence, and then set the additional delay/optimize handling after all frames are loaded.</p>

<pre class=""lang-py prettyprint-override""><code>from wand.image import Image

with Image() as wand:
    # Add new frames into sequance
    with Image(filename='1.png') as one:
        wand.sequence.append(one)
    with Image(filename='2.png') as two:
        wand.sequence.append(two)
    with Image(filename='3.png') as three:
        wand.sequence.append(three)
    # Create progressive delay for each frame
    for cursor in range(3):
        with wand.sequence[cursor] as frame:
            frame.delay = 10 * (cursor + 1)
    # Set layer type
    wand.type = 'optimize'
    wand.save(filename='animated.gif')
</code></pre>

<p><a href=""https://i.stack.imgur.com/do522.gif"" rel=""nofollow""><img src=""https://i.stack.imgur.com/do522.gif"" alt=""output animated.gif""></a></p>
"
40088759,1331399.0,2016-10-17T14:14:39Z,40084895,2,"<h2>Zoomable plot with Gnuplot</h2>

<p>Your problem is that you are plotting this with linear interpolation, with Gnuplot you plot digital data with the <code>with steps</code> style. If you use the <code>wxt</code> terminal (some other terminals also work) you get a zoomable plot, e.g.:</p>

<pre class=""lang-gnuplot prettyprint-override""><code>set term wxt
set key above
plot 'foo.csv' with steps title columnhead
</code></pre>

<p>Results in:</p>

<p><img src=""http://i.imgur.com/WqjEsi7.png"" alt=""Properly &quot;stepped&quot; pwm signal""></p>

<p>Or to plot a subsection of the data:</p>

<pre class=""lang-gnuplot prettyprint-override""><code>set term wxt
set key above
set datafile separator comma
plot [1.7:1.8] [-.1:1.1] 'foo.csv' with steps title columnhead""
</code></pre>

<p>Output:</p>

<p><img src=""http://i.imgur.com/nXBF0Lb.png"" alt=""Zoom in at 1.7 to 1.8""></p>

<h2>Determine the PWM frequency with awk</h2>

<p>As each line in your dataset represents a state switch, the switch frequency can be calculated by counting the switches and dividing by the difference in timestamps. This can be expressed in awk like this:</p>

<ol>
<li>Split data into <code>winsz</code> chunks</li>
<li>For each chunk output <code>winsz / delta_t</code></li>
</ol>

<p>Note I ignore the first two lines of the csv file.</p>

<pre><code>winsz=10

# Ignore heading and the first data point
tail -n+3 foo.csv | 

# Chunk data into winsz blocks
awk -F, 'NR % winsz == 0 { printf ""\n"" } 1' winsz=$winsz | 

# Output winsz
awk -F, 'NF &gt; 2 { print winsz / ($(NF-1) - $1)}' RS= winsz=$winsz &gt; foo-freq.txt
</code></pre>

<p>Here is a sample of <code>foo-freq.txt</code>:</p>

<pre><code>6.294413875000000 1237.47
6.303694208333334 1194.89
6.313335750000000 1150.17
6.323380625000000 1103.85
6.333885375000000 1055.28
6.344918833333334 1004.47
6.356571500000000 950.826
6.368958500000001 894.181
6.382239458333333 833.608
6.396642625000000 768.32
</code></pre>

<p>You can plot this with the following Gnuplot code:</p>

<pre class=""lang-gnuplot prettyprint-override""><code>set xlabel 'Time (s)'
set ylabel 'Frequency (Hz)'
plot 'foo-freq.txt' with lines
</code></pre>

<p>Result:</p>

<p><img src=""http://i.imgur.com/3kW1v3W.png"" alt=""PWM frequency plot""></p>

<h2>Determine the duty cycle with awk</h2>

<p>I know you did not ask for it, but here is how you can determine the duty cycle of the PWM with <code>awk</code>. You probably need to use GNU awk with multi-precision support, as you have 14 decimals in the sample timestamps.</p>

<p><em>duty-cycle.awk</em></p>

<pre class=""lang-awk prettyprint-override""><code>NR == 1 { 
  start_time = time_stamp = $1
  next
} 

# Count the length of time the signal is 0 and 1 respectiviely
$2 == 0 { len0 += $1-time_stamp }
$2 == 1 { len1 += $1-time_stamp }

# Remember previous timestamp
{ time_stamp = $1 }

# How frequently to calculate and output the duty cycle
NR % winsz == 0 { 
  delta_t = time_stamp - start_time
  print len0 / delta_t, len1 / delta_t
  start_time = time_stamp
  len0 = len1 = 0
}
</code></pre>

<p>Run it like this on your data:</p>

<pre><code>tail -n+3 foo.csv | awk -M -F, -f duty-cycle.awk winsz=50 &gt; duty-cycle.txt
</code></pre>

<p>Result:</p>

<p><img src=""http://i.imgur.com/GYyjHrp.png"" alt=""Plot of the calculated duty cycle""></p>

<h2>Datafile for future reference</h2>

<p>I have uploaded the datafile to a <a href=""http://toggle.be/stackexchange/foo.csv"" rel=""nofollow"">separate location</a> for future reference.</p>
"
40088783,6748546.0,2016-10-17T14:15:37Z,40088559,4,"<p>Two word solution:</p>

<pre><code>for string in array:
    if 'car' in string and 'motorbike' in string.split():
        print(""Car and motorbike are in string"")
</code></pre>

<p>n-word solution to check if <strong>all</strong> words in <code>test_words</code> are in <code>string</code>:</p>

<pre><code>test_words = ['car', 'motorbike']
contains_all = True

for string in array:
    for test_word in test_words:
        if test_word not in string.split()::
            contains_all = False
            break
    if not contains_all:
        break

if contains_all:
    print(""All words in each string"")
else:
    print(""Not all words in each string"")
</code></pre>
"
40088805,7030141.0,2016-10-17T14:16:25Z,40088559,1,"<p>Use an auxiliar boolean.</p>

<pre><code>car=False
 motorbike=False
 for elem in array:

        if ""car"" in elem:
            car=True
        if ""motorbike"" in elem:
            motorbike=True
        if car and motorbike:
            break
</code></pre>

<p>EDIT: I just read ""in each element"". Just use AND.</p>
"
40089159,2429640.0,2016-10-17T14:33:19Z,40088559,0,"<p>I think a simple solution is this:</p>

<pre class=""lang-py prettyprint-override""><code>all(map(lambda w: w in text, ('car', 'motorbike')))
</code></pre>

<p>But there might be a problem with this, depending on how picky you need the comparison to be:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; text = 'Can we buy motorbikes in carshops?'
&gt;&gt;&gt; all(map(lambda w: w in text, ('car', 'motorbike')))
True
</code></pre>

<p>The words 'car' and 'motorbike' are NOT in the <code>text</code>, and this still says <code>True</code>. You might need a full match in words. I would do this:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; words = ('car', 'motorbike')
&gt;&gt;&gt; text = 'Can we buy motorbikes in carshops?'
&gt;&gt;&gt; set(words).issubset(text.split())
False
&gt;&gt;&gt; text = 'a car and a motorbike'
&gt;&gt;&gt; set(words).issubset(text.split())
True
</code></pre>

<p>And now it works!</p>
"
40089268,7031637.0,2016-10-17T14:38:32Z,40088930,1,"<p>Try this... </p>

<pre><code># Start with an empty list
B = []
# Take A in chunks of m
for i in range( int(len(A)/m) ):
    # Take an m-sized chunk of A
    chunk = A[m*i:m*(i+1)]
    # Shift it to the right by k (python style!)
    shift = chunk[-k:] + chunk[:-k]
    # Add it to B
    B += shift
print (B)
</code></pre>
"
40089289,7030141.0,2016-10-17T14:39:43Z,40089134,2,"<p>Yes, it is. </p>

<p>First method:</p>

<pre><code>@app.route('/api/users/&lt;int:id&gt;', methods=['GET']
def get_user(id):
    pass  # handle user here with given id
</code></pre>

<p>Defines a route and a method. This function is only triggered with this method over this path.</p>

<p>Second method:</p>

<pre><code>@app.route('/api/users')
def get_user():
    id = request.args.get('id')
    # handle user here with given id
</code></pre>

<p>It just defines a route. You can execute the function with all methods. </p>

<p>The route in the first method is: <code>webexample.com/api/users/1</code> for user 1</p>

<p>The route in the second one is: <code>webexample.com/api/users?id=1</code> for user 1</p>
"
40089666,131187.0,2016-10-17T14:56:58Z,40088930,0,"<p>Alternative:</p>

<pre><code>m=4
n=3
k=1

A=list(range(1,1+m*n))
print (A)

t_1=[A[_:_+4] for _ in range(0,len(A), 4)]
print (t_1)

t_2=[]
for sublist in t_1:
    t_2.append(sublist[-k:]+sublist[:-k])
print (t_2)

B=[]
for sublist in t_2:
    B.extend(sublist)

print (B)
</code></pre>

<p>If you want greater speed then you could use a <strong>deque</strong> from the <strong>collections</strong> module to build <strong>t_2</strong>.</p>

<p>Here's the result.</p>

<pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]
[[4, 1, 2, 3], [8, 5, 6, 7], [12, 9, 10, 11]]
[4, 1, 2, 3, 8, 5, 6, 7, 12, 9, 10, 11]
</code></pre>
"
40089749,4265407.0,2016-10-17T15:00:53Z,40050149,1,"<p>You can use <a href=""https://docs.python.org/3.6/library/datetime.html"" rel=""nofollow"">datetime</a> module:</p>

<pre><code>#First argument - string like 2015365, second argument - format
dt = datetime.datetime.strptime(year_day,'%Y%j')
#Time shift
dt = dt + datetime.timedelta(days=-1)
#Year with shift
nyear = dt.year
#Day in year with shift
nday = dt.timetuple().tm_yday
</code></pre>
"
40089844,923871.0,2016-10-17T15:06:30Z,40089134,1,"<p>The main difference is that the URL triggering your function will be different. </p>

<p>If you use the flask function <a href=""http://flask.pocoo.org/docs/0.11/api/#flask.url_for"" rel=""nofollow"">url_for</a>(that i REALLY recommend), the URL structure returned by the function will be different because all the variables that you use and are not part of the endpoint will be treated like query parameters. </p>

<p>So in this case you can change your route without impacting your existing codebase. </p>

<p>In other words in your case you would have: </p>

<p>Using method variables: </p>

<pre><code>url_for('get_user', id=1) =&gt; '/api/users/1'
</code></pre>

<p>Without method variables:</p>

<pre><code>url_for('get_user', id=1) =&gt; '/api/users?id=1'
</code></pre>

<p>Which approach is better depends from the context you are working on. 
If you want to realize a REST based API you should define the identifiers argument as path arguments and the metadata as query arguments(you can read more about that <a href=""https://tools.ietf.org/html/rfc3986"" rel=""nofollow"">here</a>). </p>
"
40090310,5741205.0,2016-10-17T15:29:24Z,40090235,3,"<p>try this:</p>

<pre><code>In [29]: df1.groupby('Group').ffill().groupby(['Group','Age','City']).Name.apply(' '.join)
Out[29]:
Group  Age   City
1      24.0  Seattle     Alice Marie Smith
2      26.0  Portland      Mallory Bob Doe
Name: Name, dtype: object
</code></pre>
"
40090424,2074981.0,2016-10-17T15:35:29Z,40082844,2,"<p>Your problem is that <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.window.Rolling.corr.html"" rel=""nofollow""><code>.corr()</code></a> is being called without specifying the <code>other</code> argument.  Even though your dataframe only has two columns, Pandas doesn't know which correlation you actually want, so it calculates all possible correlations (<code>col1</code> x <code>col1</code>, <code>col1</code> x <code>col2</code>, <code>col2</code> x <code>col1</code>, <code>col2</code> x <code>col2</code>) and gives the results to you in a 2x2 datastructure.  If you want to get the results from one correlation, you need to specify the correlation you want by setting the base column and the <code>other</code> column.  If you weren't using <code>groupby</code> you'd just do it this way:</p>

<pre><code>df['col1'].rolling(min_periods=1, window=3).corr(other=g['col2'])
</code></pre>

<p>Since you're using <code>groupby</code>, you need to nest it in an <code>apply</code> clause with a lambda function (or you could move it into a separate function if you preferred):</p>

<pre><code>df.groupby(level=0).apply(lambda g: g['col1'].rolling(min_periods=1, window=3).corr(other=g['col2']))
</code></pre>
"
40090538,2336654.0,2016-10-17T15:41:07Z,40090235,3,"<p>using <code>dropna</code> and <code>assign</code> with <code>groupby</code></p>

<p><a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.assign.html"" rel=""nofollow"">docs to assign</a></p>

<pre><code>df1.dropna(subset=['Age', 'City']) \
   .assign(Name=df1.groupby('Group').Name.apply(' '.join).values)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Vh8Go.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/Vh8Go.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>timing</em></strong><br>
per request</p>

<p><a href=""https://i.stack.imgur.com/iYTBw.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/iYTBw.png"" alt=""enter image description here""></a></p>

<hr>

<p><strong><em>update</em></strong><br>
use <code>groupby</code> and <code>agg</code><br>
I thought of this and it feels far more satisfying</p>

<pre><code>df1.groupby('Group').agg(dict(Age='first', City='first', Name=' '.join))
</code></pre>

<p>to get the exact output</p>

<pre><code>df1.groupby('Group').agg(dict(Age='first', City='first', Name=' '.join)) \
   .reset_index().reindex_axis(df1.columns, 1)
</code></pre>
"
40090625,4952130.0,2016-10-17T15:46:21Z,40090600,3,"<p>No checking is performed by Python itself. This is specified in the <a href=""https://www.python.org/dev/peps/pep-0484/#non-goals"" rel=""nofollow"">""Non- Goals"" section</a> of PEP 484. When executed (i.e during run-time), Python completely ignores the annotations you provided and evaluates your statements as it usually does, dynamically.</p>

<p>If you need type checking, you should perform it yourself. This can currently be performed by static type checking tools like <a href=""http://mypy.readthedocs.io/en/latest/"" rel=""nofollow""><code>mypy</code></a>.</p>
"
40090688,2336654.0,2016-10-17T15:49:49Z,40090522,4,"<p>use <code>loc</code> and <code>where</code></p>

<pre><code>cols = ['a', 'b', 'c']
df.loc[:, cols] = df[cols].where(df[cols].whwere.ge(0), np.nan)
</code></pre>

<p><strong><em>demonstration</em></strong></p>

<pre><code>df = pd.DataFrame(np.random.randn(10, 5), columns=list('abcde'))
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/LT639.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/LT639.png"" alt=""enter image description here""></a></p>

<pre><code>cols = list('abc')
df.loc[:, cols] = df[cols].where(df[cols].ge(0), np.nan)
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/62dsI.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/62dsI.png"" alt=""enter image description here""></a></p>

<hr>

<p>You could speed it up with numpy</p>

<pre><code>df[cols] = np.where(df[cols] &lt; 0, np.nan, df[cols])
</code></pre>

<p>to do the same thing.</p>

<hr>

<p><strong><em>timing</em></strong>  </p>

<pre><code>def gen_df(n):
    return pd.DataFrame(np.random.randn(n, 5), columns=list('abcde'))
</code></pre>

<p>since assignment is an important part of this, I create the <code>df</code> from scratch each loop.  I also added the timing for <code>df</code> creation.</p>

<p>for <code>n = 10000</code></p>

<p><a href=""https://i.stack.imgur.com/3jaVi.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/3jaVi.png"" alt=""enter image description here""></a></p>

<p>for <code>n = 100000</code></p>

<p><a href=""https://i.stack.imgur.com/nGnNQ.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/nGnNQ.png"" alt=""enter image description here""></a></p>
"
40090694,2285236.0,2016-10-17T15:50:03Z,40090522,5,"<p>Here's a way:</p>

<pre><code>df[df.columns.isin(['a', 'b', 'c']) &amp; (df &lt; 0)] = np.nan
</code></pre>
"
40090735,704848.0,2016-10-17T15:51:51Z,40090522,4,"<p>You can use <code>np.where</code> to achieve this:</p>

<pre><code>In [47]:
df = pd.DataFrame(np.random.randn(5,5), columns=list('abcde'))
df

Out[47]:
          a         b         c         d         e
0  0.616829 -0.933365 -0.735308  0.665297 -1.333547
1  0.069158  2.266290 -0.068686 -0.787980 -0.082090
2  1.203311  1.661110 -1.227530 -1.625526  0.045932
3 -0.247134 -1.134400  0.355436  0.787232 -0.474243
4  0.131774  0.349103 -0.632660 -1.549563  1.196455

In [48]:    
df[['a','b','c']] = np.where(df[['a','b','c']] &lt; 0, np.NaN, df[['a','b','c']])
df

Out[48]:
          a         b         c         d         e
0  0.616829       NaN       NaN  0.665297 -1.333547
1  0.069158  2.266290       NaN -0.787980 -0.082090
2  1.203311  1.661110       NaN -1.625526  0.045932
3       NaN       NaN  0.355436  0.787232 -0.474243
4  0.131774  0.349103       NaN -1.549563  1.196455
</code></pre>
"
40090749,2391370.0,2016-10-17T15:53:08Z,40090522,1,"<p>If it has to be a one-liner:</p>

<pre><code>df[['a', 'b', 'c']] = df[['a', 'b', 'c']].apply(lambda c: [x&gt;0 and x or np.nan for x in c])
</code></pre>
"
40090781,5015569.0,2016-10-17T15:55:19Z,40090522,9,"<p>I don't think you'll get much simpler than this:</p>

<pre><code>&gt;&gt;&gt; df = pd.DataFrame({'a': np.arange(-5, 2), 'b': np.arange(-5, 2), 'c': np.arange(-5, 2), 'd': np.arange(-5, 2), 'e': np.arange(-5, 2)})
&gt;&gt;&gt; df
   a  b  c  d  e
0 -5 -5 -5 -5 -5
1 -4 -4 -4 -4 -4
2 -3 -3 -3 -3 -3
3 -2 -2 -2 -2 -2
4 -1 -1 -1 -1 -1
5  0  0  0  0  0
6  1  1  1  1  1
&gt;&gt;&gt; df[df[cols] &lt; 0] = np.nan
&gt;&gt;&gt; df
     a    b    c  d  e
0  NaN  NaN  NaN -5 -5
1  NaN  NaN  NaN -4 -4
2  NaN  NaN  NaN -3 -3
3  NaN  NaN  NaN -2 -2
4  NaN  NaN  NaN -1 -1
5  0.0  0.0  0.0  0  0
6  1.0  1.0  1.0  1  1
</code></pre>
"
40090782,56541.0,2016-10-17T15:55:24Z,40090522,3,"<p>Sure, just pick your desired columns out of the mask:</p>

<pre><code>(df &lt; 0)[['a', 'b', 'c']]
</code></pre>

<p>You can use this mask in <code>df[(df &lt; 0)[['a', 'b', 'c']]] = np.nan</code>.</p>
"
40091177,2236185.0,2016-10-17T16:17:19Z,40090619,1,"<p>Not very beautiful, but i think it works.</p>

<pre><code>data['specialID'] = None
foolist = list(data2['myID'])
for i in data.index:
    if data.myID[i] in foolist:
        if data.Timestamp[i]&gt; list(data2[data2['myID'] == data.myID[i]].time)[0]:
            data['specialID'][i] = list(data2[data2['myID'] == data.myID[i]].specialID)[0]
            foolist.remove(list(data2[data2['myID'] == data.myID[i]].myID)[0])

In [95]: data
Out[95]:
             Timestamp myID specialID
0  2016-01-01 00:00:00  001      None
1  2016-01-01 01:00:00  001      None
2  2016-01-01 02:00:00  001      None
3  2016-01-01 00:00:00  002      None
4  2016-01-01 01:00:00  002     foo_0
5  2016-01-01 02:00:00  002      None
6  2016-01-01 00:00:00  003      None
7  2016-01-01 01:00:00  003      None
8  2016-01-01 02:00:00  003     foo_1
9  2016-01-02 00:00:00  004      None
10 2016-01-02 01:00:00  004     foo_2
11 2016-01-02 02:00:00  004      None
</code></pre>
"
40091556,2588654.0,2016-10-17T16:41:00Z,40077209,1,"<p>Usually when there is a tricky problem like this with multiple formulas and intermediate steps, I like to modularize it by splitting the work into several functions. Here is the resulting commented code which handles the cases in the original question and in the comments:</p>

<pre><code>from collections import Counter

def get_base_freq(seq):
    """"""
    Returns the normalized frequency of each base in a given sequence as a dictionary.
    A dictionary comprehension converts the Counter object into a ""normalized"" dictionary.
    """"""
    seq_len = len(seq)
    base_counts = Counter(seq)
    base_freqs = {base: float(count)/seq_len for base, count in base_counts.items()}
    return base_freqs

def get_pair_freq(seq1, seq2):
    """"""
    Uses zip to merge two sequence strings together.
    Then performs same counting and normalization as in get_base_freq.
    """"""
    seq_len = len(seq1)
    pair_counts = Counter(zip(seq1, seq2))
    pair_freqs = {pair: float(count)/seq_len for pair, count in pair_counts.items()}
    return pair_freqs

def calc_a(d1, d2):
    """"""
    Arbitrarily takes the first pair in d2 and calculates the a-value from it.
    """"""
    first_pair, pair_freq = d2.items()[0]
    base1, base2 = first_pair
    a = pair_freq - (d1[0][base1]*d1[1][base2])
    return a

def calc_b(a, d1):
    """"""
    For this calculation, we need to use all of the values from d1 and multiply them together.
    This is done by merging the two sequence half-results together and multiplying in a for loop.
    """"""
    denom_ACGT = d1[0].values() + d1[1].values()
    denom = 1
    for val in denom_ACGT:
        denom *= val
    b = a*a/float(denom)
    return b

if __name__ == ""__main__"":
    sample = [['CGG','ATT'], ['GCGC','TAAA'], ['ACAA','CAAC']]
    b_result = []
    for seq_pair in sample:
        d1 = [get_base_freq(seq) for seq in seq_pair]
        d2 = get_pair_freq(*seq_pair)
        a = calc_a(d1, d2)
        b = calc_b(a, d1)
        b_result.append(b)
    print b_result
</code></pre>

<p>Let me know if anything needs to be cleared up or if it fails for a case which I have not considered!</p>
"
40091640,3339965.0,2016-10-17T16:46:00Z,40090619,5,"<p>You can use <a href=""https://pandas-docs.github.io/pandas-docs-travis/generated/pandas.merge_asof.html"" rel=""nofollow""><code>merge_asof</code></a>, which is new in Pandas 0.19, to do most of the work.  Then, combine <code>loc</code> and <code>duplicated</code> to remove secondary matches:</p>

<pre><code># Data needs to be sorted for merge_asof.
data = data.sort_values(by='Timestamp')

# Perform the merge_asof.
df = pd.merge_asof(data, data2, left_on='Timestamp', right_on='time', by='myID').drop('time', axis=1)

# Make the additional matches null.
df.loc[df['specialID'].duplicated(), 'specialID'] = np.nan

# Get the original ordering.
df = df.set_index(data.index).sort_index()
</code></pre>

<p>The resulting output:</p>

<pre><code>             Timestamp myID specialID
0  2016-01-01 00:00:00  001       NaN
1  2016-01-01 01:00:00  001       NaN
2  2016-01-01 02:00:00  001       NaN
3  2016-01-01 00:00:00  002       NaN
4  2016-01-01 01:00:00  002     foo_0
5  2016-01-01 02:00:00  002       NaN
6  2016-01-01 00:00:00  003       NaN
7  2016-01-01 01:00:00  003       NaN
8  2016-01-01 02:00:00  003     foo_1
9  2016-01-02 00:00:00  004       NaN
10 2016-01-02 01:00:00  004     foo_2
11 2016-01-02 02:00:00  004       NaN
</code></pre>
"
40091652,2184364.0,2016-10-17T16:46:29Z,40091617,4,"<p>Here is another solution:</p>

<pre><code>def is_consecutive(l):
    setl = set(l)
    return len(l) == len(setl) and setl == set(range(min(l), max(l)+1))
</code></pre>

<p>However, your solution is probably better as you don't store the whole range in memory.</p>

<p>Note that you can always simplify</p>

<pre><code>if boolean_expression:
    return True
else:
    return False
</code></pre>

<p>by</p>

<pre><code>return boolean_expression
</code></pre>
"
40091765,5907474.0,2016-10-17T16:53:48Z,40090892,2,"<p>This happens when you run out of memory in the GPU. Are you sure you stopped the first script properly? Check the running processes on your system (<code>ps -A</code> in ubuntu) and see if the python script is still running. Kill it if it is. You should also check the memory being used in your GPU (<code>nvidia-smi</code>).</p>
"
40091913,704848.0,2016-10-17T17:01:42Z,40091796,2,"<p>You can do it like this, first filter the cols of interest and take a slice, then call <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmin.html"" rel=""nofollow""><code>idxmin</code></a> on the cols of interest to return the columns where the boolean condition is met:</p>

<pre><code>In [11]:
df_slice = df.ix[:,df.columns.str.startswith('time')]
df_slice[df_slice!=0].idxmin(axis=1)

Out[11]:
0    timeto1000m
1     timeto800m
2     timeto400m
3     timeto600m
dtype: object

In [15]:
df['first_valid'] = df_slice[df_slice!=0].idxmin(axis=1)
df[['id','first_valid']]

Out[15]:
   id  first_valid
0   1  timeto1000m
1   2   timeto800m
2   3   timeto400m
3   4   timeto600m
</code></pre>
"
40091914,2336654.0,2016-10-17T17:01:56Z,40091796,1,"<p>use <code>idxmax(1)</code></p>

<pre><code>df.set_index(['id', 'distance']).ne(0).idxmax(1)

id  distance
1   1400m       timeto1000m
2   1200m        timeto800m
3   1800m        timeto400m
4   1000m        timeto600m
dtype: object
</code></pre>
"
40092012,2141635.0,2016-10-17T17:08:19Z,40091617,1,"<p>A better approach in terms of how many times you look at the elements would be to incorporate finding the <em>min</em>, <em>max</em> and <em>short circuiting</em> on any dupe all in one pass, although would probably be beaten by the speed of the builtin functions depending on the inputs:</p>

<pre><code>def mn_mx(l):
    mn, mx = float(""inf""), float(""-inf"")
    seen = set()
    for ele in l:
        # if we already saw the ele, end the function
        if ele in seen:
            return False, False
        if ele &lt; mn:
            mn = ele
        if ele &gt; mx:
            mx = ele
        seen.add(ele)
    return mn, mx

def isconsecutive(lst):
    """"""
    Returns True if all numbers in lst can be ordered consecutively, and False otherwise
    """"""
    mn, mx = mn_mx(lst)
    # could check either, if mn is False we found a dupe
    if mn is False:
        return False
    # if we get here there are no dupes
    return mx - mn == len(lst) - 1
</code></pre>
"
40092170,2336654.0,2016-10-17T17:17:28Z,40091996,4,"<p>use <code>groupby</code> and <code>apply</code></p>

<pre><code>f = lambda x: x.sort_values(ascending=True).reset_index(drop=True)
dftest.groupby(['Name', 'Year']).Amt.apply(f).unstack()
</code></pre>

<p><a href=""https://i.stack.imgur.com/IsAFv.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/IsAFv.png"" alt=""enter image description here""></a></p>
"
40092428,2336654.0,2016-10-17T17:35:30Z,40092294,2,"<p><code>pd.Series.hist</code> uses <code>np.histogram</code> underneath.</p>

<p>Let's explore that</p>

<pre><code>np.random.seed([3,1415])
s = pd.Series(np.random.randn(100))
d = np.histogram(s, normed=True)
print('\nthese are the normalized counts\n')
print(d[0])
print('\nthese are the bin values, or average of the bin edges\n')
print(d[1])

these are the normalized counts

[ 0.11552497  0.18483996  0.06931498  0.32346993  0.39278491  0.36967992
  0.32346993  0.25415494  0.25415494  0.02310499]

these are the bin edges

[-2.25905503 -1.82624818 -1.39344133 -0.96063448 -0.52782764 -0.09502079
  0.33778606  0.77059291  1.20339976  1.6362066   2.06901345]
</code></pre>

<p>We can plot these while calculating the mean bin edges</p>

<pre><code>pd.Series(d[0], pd.Series(d[1]).rolling(2).mean().dropna().round(2).values).plot.bar()
</code></pre>

<p><a href=""https://i.stack.imgur.com/fZxSi.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/fZxSi.png"" alt=""enter image description here""></a></p>

<p><strong><em>ACTUAL ANSWER</em></strong><br>
OR</p>

<p>We could have simply passed <code>normed=True</code> to the <code>pd.Series.hist</code> method.  Which passes it along to <code>np.histogram</code></p>

<pre><code>s.hist(normed=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/aypHc.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/aypHc.png"" alt=""enter image description here""></a></p>
"
40092512,3142347.0,2016-10-17T17:40:18Z,40086386,0,"<p>Ok, it's pretty embarrassing what happened. The code above was right since the beginning. The problem is that I'm merging some rows and concatenating in that column the merged values in a different function and I forgot. I thought the error was only in my validation function when it was never there.</p>

<p>What did I do to find it out?</p>

<p>I change the value of the column to a big value (100) so the code above would work, then I ran the following SQL:</p>

<pre><code>SELECT length(foo) AS ln_foo FROM inventory WHERE length(foo) &gt; 11
</code></pre>
"
40092858,3579977.0,2016-10-17T18:02:44Z,40090892,3,"<p>Your GPU memory is not getting freed. This happens when the previous process is stopped but not terminated. See my answer <a href=""http://stackoverflow.com/a/35748621/3579977"">here</a>.</p>
"
40094628,2357112.0,2016-10-17T19:57:18Z,40094470,5,"<p><code>yield</code> expressions must be parenthesized in any context except as an entire statement or as the right-hand side of an assignment:</p>

<pre><code># If your code doesn't look like this, you need parentheses:
yield x
y = yield x
</code></pre>

<p>This is stated in the <a href=""https://www.python.org/dev/peps/pep-0342/"" rel=""nofollow"">PEP that introduced <code>yield</code> expressions</a> (as opposed to <code>yield</code> statements), and it's implied by the contexts in which <code>yield_expr</code> appears in the <a href=""https://docs.python.org/2/reference/grammar.html"" rel=""nofollow"">grammar</a>, although no one is expecting you to read the grammar:</p>

<blockquote>
  <p>A yield-expression must always be parenthesized except when it
      occurs at the top-level expression on the right-hand side of an
      assignment.</p>
</blockquote>
"
40094676,4099813.0,2016-10-17T19:59:57Z,40094588,2,"<pre><code>import re

x = '123456789ABCDE'
pattern = r'[\dA-C]'
print(re.findall(pattern,x))    
#prints ['1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C']
</code></pre>

<p>Is this what you are looking for? </p>

<p>If you don't have <code>x</code> and just want to match ascii characters you can use :</p>

<pre><code>import re
import string

x = string.ascii_uppercase + string.digits
pattern = r'[\dA-C]'
print(re.findall(pattern,x))    
</code></pre>

<p>If you want to take inputs for the pattern you can simply just do:</p>

<pre><code> pattern = input() #with either one from above
</code></pre>
"
40094792,3832970.0,2016-10-17T20:06:53Z,40091894,1,"<p><em>DISCLAIMER</em>: The answer is not aiming at a generic interrogative sentence splitting solution, rather show how the strings supplied by OP can be matched with regular expressions. The best solution is to tokenize the text into sentences with <a href=""http://www.nltk.org/"" rel=""nofollow""><code>nltk</code></a> and parse sentences (see <a href=""http://stackoverflow.com/questions/17879551/nltk-find-if-a-sentence-is-in-a-questioning-form"">this thread</a>).</p>

<p>The regex you might want to use for strings like the one you posted is based on matching all chars that are not final punctuation and then matching the subtring you want to appear inside the sentence, and then matching those chars other than final punctuation again. To negated a single character, use negated character classes.</p>

<pre><code>\s*([^!.?]*?NSF[^!.?]*?[?])
</code></pre>

<p>See the <a href=""https://regex101.com/r/LXr6x8/3"" rel=""nofollow"">regex demo</a>.</p>

<p><strong>Details</strong>:</p>

<ul>
<li><code>\s*</code> - 0+ whitespaces</li>
<li><code>([^!.?]*?NSF[^.?]*?[?])</code>  - Group 1 capturing

<ul>
<li><code>[^!.?]*?</code> - 0+ chars other than <code>.</code>, <code>!</code> and <code>?</code>, as few as possible</li>
<li><code>NSF</code> - the value you need to be present, a sequence of chars <code>NSF</code></li>
<li><code>[^.?]*?</code> -  ibid.</li>
<li><code>[?]</code> -  a literal <code>?</code> (can be replaced with <code>\?</code>)</li>
</ul></li>
</ul>
"
40094825,2063361.0,2016-10-17T20:08:55Z,40094588,7,"<p>I think what you are looking for is <a href=""https://docs.python.org/2/library/string.html#string.printable"" rel=""nofollow""><code>string.printable</code></a> which returns all the printable characters in Python. For example:</p>

<pre><code>&gt;&gt;&gt; import string
&gt;&gt;&gt; string.printable
'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!""#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~ \t\n\r\x0b\x0c'
</code></pre>

<p>Now to check content satisfied by your regex, you may do:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; x = string.printable
&gt;&gt;&gt; pattern = r'[\dA-C]'
&gt;&gt;&gt; print(re.findall(pattern, x))
['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C']
</code></pre>

<p><code>string.printable</code> is a combination of <em>digits, letters, punctuation,</em> and <em>whitespace</em>. Also check <a href=""https://docs.python.org/2/library/string.html#string-constants"" rel=""nofollow"">String Constants</a> for complete list of constants available with <a href=""https://docs.python.org/2/library/string.html"" rel=""nofollow"">string</a> module.</p>

<hr>

<p><em>In case you need the list of all <code>unicode</code> characters</em>, you may do:</p>

<pre><code>import sys
unicode_list = [chr(i) for i in range(sys.maxunicode)]
</code></pre>

<p><strong>Note:</strong> It will be a huge list, and console might get stuck for a while to give the result as value of <code>sys.maxunicode</code> is:</p>

<pre><code>&gt;&gt;&gt; sys.maxunicode
1114111
</code></pre>

<p>In case you are dealing with some specific unicode formats, refer <a href=""http://billposer.org/Linguistics/Computation/UnicodeRanges.html"" rel=""nofollow"">Unicode Character Ranges</a> for limiting the ranges you are interested in.</p>
"
40094986,20712.0,2016-10-17T20:20:19Z,40094823,0,"<p>You have the default, and then you have per view.  You can set the default to <code>IsAuthenticated</code>, and then you override your view's particular <code>permission_classes</code>. e.g.</p>

<pre><code>class ObtainJSONWebLogin(APIView):
    permission_classes = ()
</code></pre>

<p>or </p>

<pre><code>class Foo(viewsets.ModelViewSet):
    permission_classes = ()
</code></pre>
"
40095184,5890240.0,2016-10-17T20:33:55Z,40050149,0,"<p>Based on feedback from the community I was able to get the logic needed to fix the files downloaded from the org! The logic was the biggest hurdle. It turns out that the <code>datetime</code> module can be used, I need to read up more on that. </p>

<p>I combined the logic with the batch renaming using the <code>os</code> module, I put the code below to help future users who may have a similar question!</p>

<pre><code># open all files
all_data = glob.glob('/some_dir/org*.asc')

# loop through
for f in all_data:
    # get first part of string, renders org2015365
    f_split = f.split('_')[1]
    # get only year day - renders 2015365
    year_day = f_split.replace(f_split[:10], '')
    # first argument - string 2015365, second argument - format the string to datetime
    dt = datetime.datetime.strptime(year_day, '%Y%j')
    # create a threshold where version changes its naming convention
    # only rename files greater than threshold
    threshold = '2014336'
    th = datetime.datetime.strptime(threshold, '%Y%j')
    if dt &gt; th:
        # Time shift - go back one day
        dt = dt + datetime.timedelta(days=-1)
        # Year with shift
        nyear = dt.year
        # Day in year with shift
        nday = dt.timetuple().tm_yday
        # rename files correctly
        f_output = 'org' + str(nyear) + str(nday).zfill(3) + '_res_version.asc'
        os.rename(f, '/some_dir/' + f_output)
    else:
        pass
</code></pre>
"
40095393,3579910.0,2016-10-17T20:47:34Z,39935335,0,"<p>See <a href=""http://stackoverflow.com/a/31576259/3579910"">http://stackoverflow.com/a/31576259/3579910</a>:</p>

<p>Try:</p>

<pre><code>sudo apt-get purge python-openssl
sudo apt-get install libffi-dev
sudo pip install pyopenssl
</code></pre>

<p>Apparently you can't vote duplicate if there is an open bounty.</p>

<p>Background:</p>

<blockquote>
  <p>That happend because Ubuntu 12.04 (that is my server's OS) has old
  pyOpenSSL library which not accept attribute 'set_tlsext_host_name'.
  For fix that, you need to add dependence pyOpenSSL >= 0.13. On Ubuntu
  for update pyOpenSSL use pip, you also need to install libffi-dev and
  remove python-openssl by apt.</p>
</blockquote>

<p><a href=""https://github.com/passslot/passslot-python-sdk/issues/1"" rel=""nofollow"">Source</a></p>

<hr>

<p>On Mac, you can get homebrew to replace the apt-get calls: follow the instructions for installing <a href=""http://stackoverflow.com/a/19688479/3579910"">homebrew</a>.</p>
"
40095793,102441.0,2016-10-17T21:16:38Z,40095686,3,"<p>It looks like you want the following:</p>

<pre><code>res = np.einsum('pi,qi-&gt;pq', w, a)
</code></pre>

<p>Which is shorthand for the following in index notation:</p>

<pre><code>res[p,q] = w[p,i]*a[q,i]
</code></pre>

<p>In this notation, the convention is to sum over all indices which do not appear in the output</p>

<hr>

<p>However, note that <code>ij,jk-&gt;ik</code> is just the standard matrix product, and <code>ij-&gt;ji</code> is just the matrix transpose. So we can simplify this as follows</p>

<pre><code>np.einsum('pi,qi-&gt;pq', w, a)   # as before
np.einsum('pi,iq-&gt;pq', w, a.T) # transpose and swapping indices cancel out
np.einsum('ij,jk-&gt;ik', w, a.T) # index names don't matter
w @ a.T                        # wait a sec, this is just matrix multiplication (python 3.5+)
</code></pre>
"
40095866,699305.0,2016-10-17T21:22:36Z,40094588,3,"<p>You probably hoped to just extract them from the regexp itself, but it's not that easy: Consider specifications like <code>\S</code>, which doesn't match a contiguous range of characters, negated specifications like <code>[^abc\d]</code>, and of course goodies like <code>(?![aeiou])\w</code> (which matches any single letter <em>except</em> the five vowels given). So it's far simpler to just try out each candidate character against your regexp.</p>

<p>But checking all Unicode codepoints is not very practical, both because of the large number of tests and because the result could be a very large list: A character class regexp might contain specifications like <code>\w</code>,
which can match an enormous number of characters from all over the Unicode table. Or it could contain a negated specification like <code>[^abc\d]</code>,
which matches even more.  So let's assume that you can restrict your interest to a particular
subset of the
Unicode range. After consulting a <a href=""https://en.wikipedia.org/wiki/Unicode_block#collapsibleTable0"" rel=""nofollow"">table of Unicode ranges</a>,
you might decide, for the sake of example, that you are interested in the ranges [0000-024F]
(Basic and Extended Latin) and [0590-074F] (Hebrew and Arabic).</p>

<p>You can then churn through each of these unicode codepoints,
checking which ones are matched by your regexp:</p>

<pre><code>import re

myregexp = r""[\dA-C]""
interest = [ (0x0000, 0x024F),
             (0x0590, 0x06FF) ]


pattern = re.compile(myregexp)
matched = []    
for low, high in interest:
    matched.extend(chr(p) for p in range(low, high+1) if pattern.match(chr(p)))

&gt;&gt;&gt; print("""".join(matched))
0123456789ABCÙ Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹
</code></pre>
"
40095999,5741205.0,2016-10-17T21:31:19Z,40095712,2,"<p>If you already have numeric dtypes (<code>int8|16|32|64</code>,<code>float64</code>,<code>boolean</code>) you can convert it to another ""numeric"" dtype using <strong>Pandas</strong> <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html"" rel=""nofollow"">.astype()</a> method.</p>

<p>Demo:</p>

<pre><code>In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64)

In [91]: df
Out[91]:
         a        b        c
0  9059440  9590567  2076918
1  5861102  4566089  1947323
2  6636568   162770  2487991
3  6794572  5236903  5628779
4   470121  4044395  4546794

In [92]: df.dtypes
Out[92]:
a    int64
b    int64
c    int64
dtype: object

In [93]: df['a'] = df['a'].astype(float)

In [94]: df.dtypes
Out[94]:
a    float64
b      int64
c      int64
dtype: object
</code></pre>

<p>It won't work for <code>object</code> (string) dtypes, that <strong>can't</strong> be converted to numbers:</p>

<pre><code>In [95]: df.ix[1, 'b'] = 'XXXXXX'

In [96]: df
Out[96]:
           a        b        c
0  9059440.0  9590567  2076918
1  5861102.0   XXXXXX  1947323
2  6636568.0   162770  2487991
3  6794572.0  5236903  5628779
4   470121.0  4044395  4546794

In [97]: df.dtypes
Out[97]:
a    float64
b     object
c      int64
dtype: object

In [98]: df['b'].astype(float)
...
skipped
...
ValueError: could not convert string to float: 'XXXXXX'
</code></pre>

<p>So here we want to use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html"" rel=""nofollow"">pd.to_numeric()</a> method:</p>

<pre><code>In [99]: df.b = pd.to_numeric(df['b'], errors='coerse')

In [100]: df
Out[100]:
           a          b        c
0  9059440.0  9590567.0  2076918
1  5861102.0        NaN  1947323
2  6636568.0   162770.0  2487991
3  6794572.0  5236903.0  5628779
4   470121.0  4044395.0  4546794

In [101]: df.dtypes
Out[101]:
a    float64
b    float64
c      int64
dtype: object
</code></pre>
"
40096392,1464343.0,2016-10-17T22:02:04Z,40095133,1,"<p>Putting quotes around something makes it a String. You want to actually reference the variable which contains your number, i.e. <code>A</code> instead of <code>""A""</code></p>

<pre><code>A = 100
with open('my_file') as f:
    for line in f:
        # str() converts an integer into a string for searching.
        if str(A) in line:
            print line
</code></pre>
"
40096493,5741205.0,2016-10-17T22:11:30Z,40096278,1,"<p>You can do it this way:</p>

<pre><code>bins = [0, 5, 10,50,100,500,1000,5000,10000,50000,100000]
df.groupby(pd.cut(df.a, bins=bins, labels=bins[1:])).size().plot.bar(rot=0)
</code></pre>

<p>Demo:</p>

<pre><code>df = pd.DataFrame(np.random.randint(0,10**5,(10**4,2)),columns=list('ab'))
bins = [0, 5, 10,50,100,500,1000,5000,10000,50000,100000]
df.groupby(pd.cut(df.a, bins=bins, labels=bins[1:])).size().plot.bar(rot=0)
</code></pre>

<p><a href=""https://i.stack.imgur.com/63Awq.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/63Awq.png"" alt=""enter image description here""></a></p>

<p>filtering results:</p>

<pre><code>threshold = 100
(df.groupby(pd.cut(df.a,
                   bins=bins, 
                   labels=bins[1:]))
   .size()
   .to_frame('count')
   .query('count &gt; @threshold')
)

Out[84]:
        count
a
5000      396
10000     492
50000    4044
100000   4961
</code></pre>

<p>plotting filtered:</p>

<pre><code>(df.groupby(pd.cut(df.a,
                   bins=bins, 
                   labels=bins[1:]))
   .size()
   .to_frame('count')
   .query('count &gt; @threshold')
   .plot.bar(rot=0, width=1.0)
)
</code></pre>

<p><a href=""https://i.stack.imgur.com/dL0yM.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/dL0yM.png"" alt=""enter image description here""></a></p>
"
40096640,674039.0,2016-10-17T22:25:05Z,40096601,2,"<p>The restriction is gone in Python 3, when print was changed from a statement to a function.  Indeed, you can get the new behaviour in Python 2 with a future import:</p>

<pre><code>&gt;&gt;&gt; from __future__ import print_function
&gt;&gt;&gt; import sys
&gt;&gt;&gt; class A(object):
...     def print(self):
...         sys.stdout.write(""I'm A\n"")
...     
&gt;&gt;&gt; a = A()
&gt;&gt;&gt; a.print()
I'm A
</code></pre>

<p>As a style note, it's unusual for a python class to define a <code>print</code> method.  More pythonic is <em>return</em> a value from the <code>__str__</code> method, which customises how an instance will display when it is printed.  </p>

<pre><code>&gt;&gt;&gt; class A(object):
...     def __str__(self):
...         return ""I'm A""
...     
&gt;&gt;&gt; a = A()
&gt;&gt;&gt; print(a)
I'm A
</code></pre>
"
40096661,384091.0,2016-10-17T22:26:46Z,40096601,0,"<p><strong>print</strong> is a reserved word in Python 2.x, so you can't use it as an identifier. Here is a list of reserved words in Python: <a href=""https://docs.python.org/2.5/ref/keywords.html"" rel=""nofollow"">https://docs.python.org/2.5/ref/keywords.html</a>.</p>
"
40096669,1405065.0,2016-10-17T22:27:20Z,40096323,2,"<p>The issue is that you're creating your <code>lambda</code> functions in a loop, and they refer to the variables <code>i</code> and <code>j</code> that may change as the loop goes on.</p>

<p>The lambda doesn't copy the values of <code>i</code> or <code>j</code> when it is created, it just keeps a reference to the namespace that they are defined in. When it uses the variables when it is called later, it looks them up in that namespace. Since your lambdas get called after the loop (and indeed, the whole function) has ended, they all see the final values the variables were given, which is not what you intended. This explains why the two versions of your code give the same output on the last iteration. The final value of <code>i</code> and <code>j</code> is the expected one for the last <code>lambda</code> function.</p>

<p>You can work around this issue by making the <code>lambda</code> keep a copy of the current value of <code>i</code> and <code>j</code> when it is defined. The easiest way to do this is with a default argument:</p>

<pre><code>for i,reac in enumerate(self.S.T):
  if all(z&gt;=0 for z in reac):
    prop.append(lambda z, rate, i=i: rate[i]) # add i=i here and further down

  if any(z==-1 for z in reac):
    j=np.where(reac==-1)[0]
    prop.append(lambda z, rate, i=i, j=j: rate[i]*np.prod(z[j]))

  if any(z==-2 for z in reac):
    j=np.where(reac==-2)[0][0]
    prop.append(lambda z, rate, i=i, j=j: (0.5*rate[i]*z[j]*(z[j]-1))[0])
</code></pre>

<p>The <code>i=i</code> (and <code>j=j</code> where necessary) in the lambda definitions makes the variables arguments of the lambda function with a default value that is the current value of <code>i</code> (and <code>j</code>) in the outer namespace. Since you only pass two arguments when you call the lambda function, the saved default values will be used.</p>
"
40096738,3579910.0,2016-10-17T22:36:53Z,40096621,0,"<p>I think you're asking an <a href=""http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem"">XY Problem</a>: You want to know how to find the DHCP IP address on windows, via python?</p>

<p>There is a solution on SuperUser for <a href=""http://superuser.com/questions/314145/how-to-find-my-dhcp-server-ip-address-via-a-command-prompt-in-windows/314147"">obtaining DHCP server ip from the command line</a>. You can wrap <code>ipconfig /all</code> with <code>subprocess</code> and then parse the output:</p>

<pre><code>import subprocess  # Runs a command on the cmd line

res = subprocess.check_output(""ipconfig /all"")
</code></pre>
"
40096817,5247911.0,2016-10-17T22:44:35Z,40096308,2,"<p><strong>Update:</strong></p>

<p>Installing 64 bit Python solves the issue.</p>

<p>OP was using 32 bit Python that's why getting into memory limitation.</p>

<hr>

<p>Reading whole comments I think this can help you.</p>

<ul>
<li>You can't read file in chunk (as 1024) since you want to process data.</li>
<li>Instead, read file in chunk of lines i.e N lines at a time.</li>
<li>You can use <a href=""http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do"">yield</a> keyword and <a href=""https://pymotw.com/2/itertools/"" rel=""nofollow"">itertools</a> in Python to achieve above. </li>
</ul>

<p><strong><em>Summary</strong> : Get N lines at time, process it and then write it.</em></p>

<p><strong>Sample Code :</strong></p>

<pre><code>from itertools import islice
#You can change num_of_lines
def get_lines(file_handle,num_of_lines = 10):
    while True:
        next_n_lines = list(islice(file_handle, num_of_lines))
        if not next_n_lines:
            break
        yield next_n_lines


o = open('mproducts.txt','w')

with open('reviewsNew.txt','r') as f1:
    for data_lines in get_lines(f1):
        for line in data_lines:
            line = line.strip()
            line2 = line.split('\t')
            o.write(str(line))
            o.write(""\n"")
o.close()
</code></pre>
"
40096878,4983450.0,2016-10-17T22:51:11Z,40096846,1,"<p>You can <code>zip</code> the first element of the list with the remaining elements of the list after splitting the string in each sublist:</p>

<pre><code># to split string in the sublists
lst = [i[0].split(',') for i in lst]

[dict(zip(lst[0], v)) for v in lst[1:]]

#[{'A': '1', 'B': '2', 'C': '3', 'D': '4'},
# {'A': '5', 'B': '6', 'C': '7', 'D': '8'}]
</code></pre>
"
40096881,674039.0,2016-10-17T22:51:30Z,40096846,1,"<p>Just use a <a href=""https://docs.python.org/2/library/csv.html#csv.DictReader"" rel=""nofollow"">DictReader</a> instance.  People usually use these with a file object, but actually it doesn't really care what you pass it as long as it can iterate the thing.</p>

<pre><code>&gt;&gt;&gt; L = [['A,B,C,D'],
...  ['1,2,3,4'],
...  ['5,6,7,8']]
&gt;&gt;&gt; import csv
&gt;&gt;&gt; reader = csv.DictReader((line for [line] in L))
&gt;&gt;&gt; d1, d2 = reader
&gt;&gt;&gt; d1
{'A': '1', 'B': '2', 'C': '3', 'D': '4'}
&gt;&gt;&gt; d2
{'A': '5', 'B': '6', 'C': '7', 'D': '8'}
</code></pre>
"
40096906,3030305.0,2016-10-17T22:54:28Z,40096846,1,"<p>Let's start with your data as presented in the question:</p>

<pre><code>&gt;&gt;&gt; a = [['A,B,C,D'], ['1,2,3,4'], ['5,6,7,8']]
</code></pre>

<p>Now, let's convert that to the desired list of dictionaries:</p>

<pre><code>&gt;&gt;&gt; [dict(zip(a[0][0].split(','), c[0].split(','))) for c in a[1:]]
[{'A': '1', 'C': '3', 'B': '2', 'D': '4'}, {'A': '5', 'C': '7', 'B': '6', 'D': '8'}
</code></pre>
"
40096939,3125566.0,2016-10-17T22:58:00Z,40096826,2,"<p>Use a <a href=""https://docs.python.org/2/library/collections.html#collections.Counter"" rel=""nofollow""><code>collections.Counter</code></a> object and exploit its <a href=""https://docs.python.org/2/library/collections.html#collections.Counter.most_common"" rel=""nofollow""><code>most_common</code></a> method to return the keys with the highest frequency up to the required percentile.</p>

<p>For the 25th percentile, divide the length of the dictionary by 4 and pass that value to <code>most_common</code>:</p>

<pre><code>&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; dct = {'oranges': 4 , 'apple': 3 , 'banana': 3 , 'pear' :1, 'strawberry' : 1}
&gt;&gt;&gt; c = Counter(dct)
&gt;&gt;&gt; [tup[0] for tup in c.most_common(len(dct)//4)]
['oranges']
</code></pre>

<p>Note that potential elements in that percentile with equal frequencies will be selected <em>arbitrarily</em>.</p>
"
40097001,6868170.0,2016-10-17T23:05:22Z,40096621,0,"<p>Okay, I'm going to make the assumption that your default gateway is configured to point at your DHCP server. I found the following package and was able to get my default gateway:</p>

<pre><code>#!/usr/bin/env python
import netifaces

gateway_info = netifaces.gateways()
print(gateway_info)
</code></pre>

<p>I of course first had to install the <code>netifaces</code> module via pip:</p>

<p>$> pip install --user netifaces</p>

<p>Code returns the following:</p>

<p>$> ./test3.py </p>

<p>{'default': {2: ('192.168.0.1', 'en0')}, 2: [('192.168.0.1', 'en0', True)]}</p>

<p>I hope this helps.</p>

<p>Best regards,</p>

<p>Aaron C.</p>
"
40097165,2141635.0,2016-10-17T23:22:49Z,40097088,2,"<p>You just need to pull the <em>attributes</em> once you find the <em>graph node</em>:</p>

<pre><code>import requests
from bs4 import BeautifulSoup

soup = BeautifulSoup(requests.get(""http://charts.realclearpolitics.com/charts/1044.xml"").content,""xml"")
g = soup.find(""graph"", gid=""1"")
data = {""title"":g[""title""], ""color"": g[""color""]}
</code></pre>

<p>Which will give you:</p>

<pre><code>{'color': '#000000', 'title': 'Approve'}
</code></pre>
"
40097515,1048539.0,2016-10-18T00:06:09Z,40097144,0,"<p>Note that when you run:</p>

<pre><code>docker exec -it
</code></pre>

<p>your <code>-it</code> flags are doing something contrary to the point of what you are trying. You are trying to open an interactive session. This isn't actually <em>executing</em> the command.</p>

<p>You can remove both of these from your command and just run <code>docker exec</code>:</p>

<pre><code>dockerscript = ""docker exec docker_1 ./node_modules/.bin/babel-node --debug --presets es2015 app/exportToCouch %s %s"" % (x,x)
</code></pre>

<p>For what it's worth I would probably look into using <a href=""https://github.com/docker/docker-py"" rel=""nofollow"">docker-py</a>, too. It's a lot more flexible for running docker commands if you end up with more than just a few command line docker commands. It also lets you avoid <code>shell=true</code>.</p>
"
40097584,3254859.0,2016-10-18T00:14:34Z,40097086,2,"<p>Your code is failing because it thinks that <code>'bo'</code> is the <code>yerr</code> argument since the third argument in <code>plt.errorbar</code> is <code>yerr</code>.  If you want to pass the format specifier, then you should use the <code>fmt</code> keyword.  </p>

<pre><code>plt.errorbar(x, avg_data, fmt='bo', yerr=err)
</code></pre>
"
40097707,6779307.0,2016-10-18T00:31:09Z,40097366,2,"<p>So you want to check to see that every letter in <code>dict2</code> has at mapping in <code>dict1</code> least as large as that letters mapping in <code>dict2</code>?  That's accomplished fairly easily.</p>

<pre><code>def can_spell(dict1, dict2):
    try:
        return all(dict1[k] &gt;= v for k, v in dict2.items())
    except KeyError:
        return False
</code></pre>

<p>This gets every (key, value) pair in <code>dict2</code> and then compares <code>v</code> with the mapping of that key in <code>dict1</code>.  <code>all</code> returns <code>True</code> iff every expression in that generator comprehension is true.</p>
"
40097861,1048539.0,2016-10-18T00:54:35Z,40097711,3,"<blockquote>
  <p>I can't write a function definition for every single possible label, they would number in the hundreds or thousands. I'm sure there's a very pythonic way to do it, but I have no idea what.</p>
</blockquote>

<p>Check out <a href=""http://www.secnetix.de/olli/Python/lambda_functions.hawk"" rel=""nofollow"">lambda functions</a> which are nearly identical to what you want.</p>

<p>In your case, something like:</p>

<pre><code>def update_bottom_scroll_bar(text):
    # whatever you want to do to update the text at the bottom

for treatment in treament_list:  # For each treatment in the list
    label = ttk.Label(frames[treatment[1] - 1], text=treatment[0])  # Build the label for treatment

    label.bind(""&lt;Enter&gt;"", lambda event, t=treatment: update_bottom_scroll_bar(text=t))
    label.bind(""&lt;Leave&gt;"", lambda event: update_bottom_scroll_bar(text='Default label text'))

    labels.append(label)  # Add the treatment to the list
    label.pack()
</code></pre>

<p>Also please spell your variables right, I corrected <code>treament</code> to <code>treatment</code>...</p>
"
40097901,1048539.0,2016-10-18T01:00:29Z,40097863,1,"<p>You can do this by converting the dictionary first to a string and then replacing all the brackets with empty strings:</p>

<pre><code>d = {'Chin PTE LTD': {'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89}, 'COQSEAFOOD': {'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1}}

print(str(d).replace(""{"","""").replace(""}"", """"))
</code></pre>

<p>which will print what you are looking for:</p>

<pre><code>'Chin PTE LTD': 'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89, 'COQSEAFOOD': 'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1
</code></pre>
"
40097968,1540468.0,2016-10-18T01:12:07Z,40097863,0,"<p>Build up the string like so</p>

<pre><code>d = {'Chin PTE LTD': {'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89}, 'COQ SEAFOOD': {'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1}}

st = ', '.join('%r: %s' % (k, ', '.join('%r: %r' % (sk, sv) for sk, sv in v.items())) for k, v in d.items())
print(st)
</code></pre>

<p>This code builds the string by first iterating over the outer dict. It appends the key to the string (plus a ':' in keeping with your formatting requirements). Then it iterates over the inner dict and appends the key and value the same way. It uses the <code>%r</code> format specifier which means that the elements being printed are converted using their <code>repr</code> function. This gives the strings their quotes, without having to manually add them.</p>

<p>You can't count on the order being fixed though. So for different runs you'll get slightly different orders. </p>

<p>Output looks like</p>

<blockquote>
  <p>'Chin PTE LTD': 'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89, 'COQ SEAFOOD': 'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1 </p>
</blockquote>

<p>Someone could probably wrap it up into a giant <code>join</code>/comprehension to make it more functional if they really wanted to.</p>
"
40097973,3453737.0,2016-10-18T01:12:51Z,40097194,0,"<p>It looks like you need to use the <code>applymap</code> method. See the docs <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html?highlight=applymap"" rel=""nofollow"">here</a> for more info. </p>

<pre><code>df.applymap(lambda x: x.match('a'))
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/J5twk.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/J5twk.png"" alt=""enter image description here""></a></p>
"
40098210,7033869.0,2016-10-18T01:40:57Z,40097863,0,"<pre><code>d = {'Chin PTE LTD': {'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89}, 'COQ SEAFOOD': {'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1}}

', '.join(['{}: {}'.format(merchant, ', '.join(['{}: {}'.format(product, quantity) for product, quantity in products.items()])) for merchant, products in d.items()])
</code></pre>

<p>if you are using python2 instead of python3, replace items with iteritems</p>
"
40098340,7010618.0,2016-10-18T01:57:39Z,40096612,-1,"<p>You simply need to use .readlines() on fh</p>

<p>like this:</p>

<pre><code>#!/Python34/python
from math import *

fh = open('temperature.txt')

num_list = []

read_lines = fh.readlines()
for line in read_lines:
    num_list.append(int(line))

fh.close()
</code></pre>
"
40098342,3111778.0,2016-10-18T01:57:56Z,40045159,0,"<p>Basically, your network is not deep enough. That is why both your training and validation accuracy are low. You can try to deepen your network from two aspects.</p>

<ol>
<li><p>Use larger number of filters for each convolutional layer. (30, 5, 5) or (15, 3, 3) are just not enough. Change the first convolutional layer to (64, 3, 3). After max pooling, which reduces your 2D size, the network should provide ""deeper"" features. Thus, the second should not be 15, but something like (64, 3,3) or even (128,3,3).</p></li>
<li><p>Add more convolutional layers. 5 or 6 layers for this problem may be good.</p></li>
</ol>

<p>Overall, your question is beyond programming. It is more about CNN network architecture. You may read more research papers on this topic to get a better understanding. For this specific problem, Keras has a very good tutorial on how to improve the performance with very small set of cats and dogs images:
<a href=""https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"" rel=""nofollow"">Building powerful image classification models using very little data</a></p>
"
40098348,3874768.0,2016-10-18T01:58:40Z,40097863,0,"<pre><code>import re
result={'Chin PTE LTD': {'Carrot Cake': 22, 'Chocolate Cake': 12, 'Beer': 89}, 'COQ SEAFOOD': {'GRILLED AUSTRALIA ANGU': 1, 'CRISPY CHICKEN WINGS': 1}}

expected_output=re.sub(""}|{"","""",str(result))
</code></pre>
"
40098512,5249307.0,2016-10-18T02:21:13Z,40098500,5,"<p>The <a href=""https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects"">Python 3 Documentation</a> notes that the <code>dict.keys</code> method is set-like and implements <a href=""https://docs.python.org/3/library/collections.abc.html#collections.abc.Set""><code>collections.abc.Set</code></a>.</p>

<p>Note that <code>dict.values</code> is <strong>not set-like</strong> even though it might appear to be so in your examples:</p>

<pre><code>aonce = a.keys() | a.values()
bonce = b.keys() | b.values()
</code></pre>

<p>However these are leveraging off the fact that the keys view implements <code>__or__</code> (and <code>__ror__</code>) over arbitrary iterables.</p>

<p>For example, the following will not work:</p>

<pre><code>&gt;&gt;&gt; a.values() | b.values()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: unsupported operand type(s) for |: 'dict_values' and 'dict_values'
</code></pre>
"
40099230,1813277.0,2016-10-18T03:55:21Z,40078718,0,"<p>When searching through documentation on a separate issue, I came across a reference to the <code>gtksink</code> widget. This seems to be the correct way to put video in a gtk window, but unfortunately none of the tutorials on this use it.</p>

<p>Using the <code>gtksink</code> widget fixes all the problems and greatly reduces code complexity. </p>

<p>The revised code:</p>

<pre><code>from pprint import pprint

import gi
gi.require_version('Gtk', '3.0')
gi.require_version('Gst', '1.0')
gi.require_version('GstVideo', '1.0')

from gi.repository import Gtk, Gst
Gst.init(None)
Gst.init_check(None)


class GstWidget(Gtk.Box):
    def __init__(self, pipeline):
        super().__init__()
        self.connect('realize', self._on_realize)
        self._bin = Gst.parse_bin_from_description('videotestsrc', True)

    def _on_realize(self, widget):
        pipeline = Gst.Pipeline()
        factory = pipeline.get_factory()
        gtksink = factory.make('gtksink')
        pipeline.add(gtksink)
        pipeline.add(self._bin)
        self._bin.link(gtksink)
        self.pack_start(gtksink.props.widget, True, True, 0)
        gtksink.props.widget.show()
        pipeline.set_state(Gst.State.PLAYING)


window = Gtk.ApplicationWindow()

header_bar = Gtk.HeaderBar()
header_bar.set_show_close_button(True)
window.set_titlebar(header_bar)  # Place 2

widget = GstWidget('videotestsrc')
widget.set_size_request(200, 200)

window.add(widget)

window.show_all()

def on_destroy(win):
    try:
        Gtk.main_quit()
    except KeyboardInterrupt:
        pass

window.connect('destroy', on_destroy)

Gtk.main()
</code></pre>
"
40099489,1821750.0,2016-10-18T04:24:57Z,40099427,2,"<p>Not sure what you're asking, as clearly thats what happens...</p>

<pre><code>&gt;&gt;&gt; x
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
&gt;&gt;&gt; x = 10
&gt;&gt;&gt; 'x' in vars()
True
&gt;&gt;&gt; vars()['x']
10
&gt;&gt;&gt; del x
&gt;&gt;&gt; 'x' in vars()
False
&gt;&gt;&gt; x
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
</code></pre>

<p>As you can see, Python stores valid identifiers in <code>vars()</code> and <code>locals()</code>... So del essentially removes it from memory, removing it from these data structures as well. So the identifier is no longer valid. It's not like in C where you might free some memory and set the value to <code>NULL</code>.</p>

<pre><code>&gt;&gt;&gt; x = 10
&gt;&gt;&gt; def func():
...   global x
...   del x
...
&gt;&gt;&gt; x
10
&gt;&gt;&gt; func()
&gt;&gt;&gt; x
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
&gt;&gt;&gt;
</code></pre>

<h2>Update:</h2>

<p>Raymond made a good point when he mentioned that the object itself (read: the actual data in memory that the identifier points too, for lack of a more detailed explanation) is only freed when it's reference count goes to 0. IMO he could have done a better job detailing this in the python interpreter, so I'll have a go at it.</p>

<p>We'll use the ID function to prove this:</p>

<pre><code>Help on built-in function id in module __builtin__:

id(...)
    id(object) -&gt; integer

    Return the identity of an object.  This is guaranteed to be unique among
    simultaneously existing objects.  (Hint: it's the object's memory address.)
</code></pre>

<p>Pay attention to whats going on here, you'll see that for immutable types, different values reference the same memory (strings are immutable in python and are interned, which means only one copy of each unique string is stored -- in ruby, symbols are interned strings).</p>

<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; x = 10     # 10 is a common value, probably exists in memory already
&gt;&gt;&gt; sys.getrefcount(x)
26
&gt;&gt;&gt; id(x)      # memory location of x
140266396760096
&gt;&gt;&gt; y = x
&gt;&gt;&gt; id(y) == id(x)
True
&gt;&gt;&gt; z = 10
&gt;&gt;&gt; id(z) == id(y) == id(x)
True
&gt;&gt;&gt; sys.getrefcount(y)
28
&gt;&gt;&gt; sys.getrefcount(z)
28
&gt;&gt;&gt; del y, z
&gt;&gt;&gt; sys.getrefcount(x)
26
&gt;&gt;&gt; del x
&gt;&gt;&gt; x = 'charlie'
&gt;&gt;&gt; id(x)
4442795056
&gt;&gt;&gt; y = 'charlie'
&gt;&gt;&gt; z = x
&gt;&gt;&gt; id(x) == id(y) == id(z)
True
&gt;&gt;&gt; sys.getrefcount(x)
4
&gt;&gt;&gt; sys.getrefcount(y)
4
&gt;&gt;&gt; sys.getrefcount(z)
4
&gt;&gt;&gt; del y
&gt;&gt;&gt; del x
&gt;&gt;&gt; sys.getrefcount(z)     # will be two because this line is an additional reference
2
&gt;&gt;&gt; id(z)                  # pay attention to this memory location because this 
4442795056                 # is the last remaining reference to 'charlie', and
&gt;&gt;&gt; del z                  # when it goes out of scope 'charlie' is removed from
&gt;&gt;&gt;                        # memory.
&gt;&gt;&gt; id('charlie')          # This has a different memory location because 'charlie'
4442795104                 # had to be re-created.
</code></pre>

<p>First we set the identifier 'x' == 10, a common integer value. Since 10 is such a common value, it's almost guaranteed that something in the processes memory already has that value. Since integers are immutable in python, we only ever need one copy of each unique value stored in memory. In this case, there are 24 other references to 10 in memory. Setting <code>x = 10</code> creates the 25th reference, and calling <code>sys.getrefcount(x)</code> is the 26th reference (though it quickly goes out of scope). When we set <code>y = 10</code> and <code>z = x</code>, we know that they all point to the same data because they all have the same memory location. Calling <code>del</code> alters the reference count, but even when all 3 are deleted the integer 10 still exists in memory.</p>

<p>Next we create set <code>x = 'charlie'</code>, followed by <code>y = 'charlie'</code>, and finally <code>z = x</code>. You can see all of these variables have the same memory address. Once we delete all of these variables there are no more references to <code>'charlie'</code>. We can verify this by calling <code>id('charlie')</code> which will produce a different memory address meaning the string did not exist in memory when we called that function. </p>

<p>One more thing to note is the location of <code>'charlie'</code> and <code>10</code> in memory. <code>10</code> has a significantly higher memory address than Charlie does. This is because they exist in different locations of memory. <code>'charlie'</code> exists on the heap whereas <code>10</code> exists on the stack.</p>

<pre><code>&gt;&gt;&gt; hex(id(10))         # high address, this is on the stack
'0x7f9250c0b820'
&gt;&gt;&gt; hex(id('charlie'))  # lower address, this is on the heap
'0x108cfac60
</code></pre>
"
40099819,790387.0,2016-10-18T04:56:02Z,40099427,1,"<blockquote>
  <p>Shouldn't it be removed from the list of known variable and shouldn't
  the python interpreter spit out ""unresolved reference"" error?</p>
</blockquote>

<p>This is exactly what happens. Once you <code>del</code> something, the next reference to it will raise <code>NameError</code>.</p>

<blockquote>
  <p>Or is it simply just deleting the object and leaving the name (var)
  not pointing anywhere? Why would that kind of behavior be useful? In
  what cases?</p>
</blockquote>

<p>In Python, things are a bit different. There are no ""variables"" as you might be used to in other languages.</p>

<p>There is the object space, where data lives - and the namespace, where <em>names</em> live.</p>

<p><em>names</em> are what we normally refer to in other languages as <em>variables</em> but in Python they are simply labels to an object in the data space.</p>

<p>Once the label (name) is removed with <code>del</code>, its simply taking away the label that points to the object. The object (which has the value) remains; unless its not referred to by any other name, in which case Python will then garbage collect it. Names are just lightweight labels that point to objects in the object space, and are the only way we can access those objects.</p>

<p>Here is an example to illustrate this:</p>

<pre><code>&gt;&gt;&gt; x = 5
&gt;&gt;&gt; y = x
&gt;&gt;&gt; del x
&gt;&gt;&gt; y
5
</code></pre>

<p>Now I deleted <code>x</code>, but since <code>y</code> is still pointing to the same object as <code>x</code> (the integer <code>5</code>), it remains and I can access it again; but if I try to access <code>x</code>:</p>

<pre><code>&gt;&gt;&gt; x
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
NameError: name 'x' is not defined
</code></pre>
"
40099844,1001643.0,2016-10-18T04:58:17Z,40099427,1,"<p>The ""del"" removes the name from the current namespace. If the underlying object has its reference count drop to zero, then the object itself is freed.</p>

<pre><code>&gt;&gt;&gt; import sys
&gt;&gt;&gt; x = 123456
&gt;&gt;&gt; y = x               # create a second reference to the number
&gt;&gt;&gt; dir()               # known variables include ""x"" and ""y""
['__builtins__', '__doc__', '__name__', '__package__', 'sys', 'x', 'y']
&gt;&gt;&gt; sys.getrefcount(x)
3
&gt;&gt;&gt; del x               # remove ""x"" as a known variable
&gt;&gt;&gt; dir()               # known variables includes only ""y""
['__builtins__', '__doc__', '__name__', '__package__', 'sys', 'y']
&gt;&gt;&gt; sys.getrefcount(y)  # reference count is now lower by 1
2
&gt;&gt;&gt; del y               # remove ""y"" as a known variable
&gt;&gt;&gt; dir()               # known variables no longer include ""x"" and ""y""
['__builtins__', '__doc__', '__name__', '__package__', 'sys']
&gt;&gt;&gt; x                   # unresolved variable raises a ""NameError""

Traceback (most recent call last):
  File ""&lt;pyshell#12&gt;"", line 1, in &lt;module&gt;
    x                  # unresolved variable raises a ""NameError""
NameError: name 'x' is not defined
</code></pre>
"
40099944,2061991.0,2016-10-18T05:06:17Z,39913847,0,"<p>You might wish to investigate <a href=""http://nuitka.net/"" rel=""nofollow"">Nuitka</a>. It takes python source code and converts it in to C++ API calls. Then it compiles into an executable binary (ELF on Linux). It has been around for a few years now and supports a wide range of Python versions.</p>

<p>You will probably also get a performance improvement if you use it. Recommended.</p>
"
40099956,2901002.0,2016-10-18T05:07:44Z,40099924,2,"<p>I think you need <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow""><code>boolean indexing</code></a> for filtering, instead <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html"" rel=""nofollow""><code>dropna</code></a> you can add new (third) condition with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.notnull.html"" rel=""nofollow""><code>notnull</code></a> - get all not <code>NaN</code> values in column <code>t</code>. <code>NaN</code> values in first column are filtered by first and second condition:</p>

<pre><code>df['t'] = pd.to_datetime(df['t'], format = '%d/%m/%Y %H:%M', errors='coerce')
df.iloc[:,1] = pd.to_numeric(df.iloc[:,1], errors='coerce')  
df = df[(df.iloc[:,1] &lt; 60) &amp; (df.iloc[:,1] &gt; 0) &amp; (df['t'].notnull())]

print (df)
                     t  baaa
0  2014-11-13 23:43:00  17.6
1  2014-11-13 23:44:00  17.7
3  2014-11-13 23:46:00  17.7
4  2014-11-14 00:34:00  16.0
5  2014-11-14 00:35:00  15.9
7  2014-11-14 01:25:00  14.9
8  2014-11-14 01:26:00  14.9
10 2014-11-14 02:16:00  14.3
11 2014-11-14 02:17:00  14.3
13 2014-11-14 03:09:00  13.0
15 2014-11-14 02:19:00  14.3
16 2014-11-14 03:59:00  12.6
17 2014-11-14 04:00:00  12.6
18 2014-11-14 05:41:00  11.7
19 2014-11-14 05:42:00  11.7
21 2014-11-14 04:53:00  12.2
</code></pre>
"
40100036,2061991.0,2016-10-18T05:14:31Z,39935335,1,"<p>""That happend because your OS has old pyOpenSSL library which is does not an accept attribute 'set_tlsext_host_name'.
To fix this, you need to add dependence pyOpenSSL >= 0.13.</p>

<pre><code>$ brew purge python-openssl
$ brew install libffi-dev
$ brew install pyOpenSSL
</code></pre>

<p>Let me know if this is unclear or if it doesn't work for you.</p>
"
40101012,5895553.0,2016-10-18T06:27:29Z,40100733,3,"<p>The reason this is not working is because bool <code>QPolygon.contains( QPoint )</code> returns true if the point is one of the vertices of the polygon, or if the point falls on the edges of the polygon. Some examples of points that would return true with your setup would be <code>QPoint(0,0)</code>, <code>QPoint(0,200)</code>, or anything that does match either of those criteria.</p>

<p>What you are looking for, presumably, is a function that returns true if the point resides within the polygon, rather than on it. The function you are looking for is <code>QPolygon.containsPoint ( QPoint , Qt.FillRule )</code>. The point is as you think it is, and the second value is either a 1 or a 0 (represented as either Qt.OddEvenFill or Qt.WindingFill respectively), which determine what method of finding whether or not a point is inside a polygon. Information about Qt.FillRule can be found here: <a href=""https://doc.qt.io/archives/qtjambi-4.5.2_01/com/trolltech/qt/core/Qt.FillRule.html"" rel=""nofollow"">https://doc.qt.io/archives/qtjambi-4.5.2_01/com/trolltech/qt/core/Qt.FillRule.html</a></p>

<p>corrected code:</p>

<pre><code>print(poly)
print(poly.containsPoint(QPoint(1,1), Qt.OddEvenFill))
</code></pre>
"
40101292,5967252.0,2016-10-18T06:45:07Z,40092294,0,"<p>First of all it would be better if you create a function that splits your data in age groups</p>

<pre><code># This function splits our data frame in predifined age groups
def cutDF(df):
    return pd.cut(
        df,[0, 10, 20, 30, 40, 50, 60, 70, 80], 
        labels=['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80'])


data['AgeGroup'] = data[['Age']].apply(cutDF)
</code></pre>

<p>Then you can plot your graph as follows:</p>

<pre><code>survival_per_age_group = data.groupby('AgeGroup')['Survived'].mean()

# Creating the plot that will show survival % per age group and gender
ax = survival_per_age_group.plot(kind='bar', color='green')
ax.set_title(""Survivors by Age Group"", fontsize=14, fontweight='bold')
ax.set_xlabel(""Age Groups"")
ax.set_ylabel(""Percentage"")
ax.tick_params(axis='x', top='off')
ax.tick_params(axis='y', right='off')
plt.xticks(rotation='horizontal')             

# Importing the relevant fuction to format the y axis 
from matplotlib.ticker import FuncFormatter

ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
plt.show()
</code></pre>
"
40101592,2867928.0,2016-10-18T07:00:57Z,40101371,3,"<p>Use <code>np.repeat()</code>:</p>

<pre><code>In [9]: A = np.array([[1, 2, 3, 4]])
In [10]: np.repeat(np.repeat(A, 2).reshape(2, 4), 2, 0)
Out[10]: 
array([[1, 1, 2, 2],
       [1, 1, 2, 2],
       [3, 3, 4, 4],
       [3, 3, 4, 4]])
</code></pre>

<p><strong><em>Explanation:</em></strong> </p>

<p>First off you can repeat the arrya items:</p>

<pre><code>  In [30]: np.repeat(A, 3)
  Out[30]: array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4])
</code></pre>

<p>then all you need is reshaping the result (based on your expected result this can be different):</p>

<pre><code>  In [32]: np.repeat(A, 3).reshape(2, 3*2)
  array([[1, 1, 1, 2, 2, 2],
         [3, 3, 3, 4, 4, 4]])
</code></pre>

<p>And now you should repeat the result along the the first axis:</p>

<pre><code>  In [34]: np.repeat(np.repeat(A, 3).reshape(2, 3*2), 3, 0)
  Out[34]: 
  array([[1, 1, 1, 2, 2, 2],
         [1, 1, 1, 2, 2, 2],
         [1, 1, 1, 2, 2, 2],
         [3, 3, 3, 4, 4, 4],
         [3, 3, 3, 4, 4, 4],
         [3, 3, 3, 4, 4, 4]])
</code></pre>
"
40101614,1427416.0,2016-10-18T07:02:08Z,40101130,6,"<p>There is no simple way to do that, because the argument that is passed to the rolling-applied function is a plain numpy array, not a pandas Series, so it doesn't know about the index.  Moreover, the rolling functions must return a float result, so they can't directly return the index values if they're not floats.</p>

<p>Here is one approach:</p>

<pre><code>&gt;&gt;&gt; s.index[s.rolling(3).apply(np.argmax)[2:].astype(int)+np.arange(len(s)-2)]
Index([u'c', u'c', u'e', u'e', u'e', u'f', u'i', u'i'], dtype='object')
</code></pre>

<p>The idea is to take the argmax values and align them with the series by adding a value indicating how far along in the series we are.  (That is, for the first argmax value we add zero, because it is giving us the index into a subsequence starting at index 0 in the original series; for the second argmax value we add one, because it is giving us the index into a subsequence starting at index 1 in the original series; etc.)</p>

<p>This gives the correct results, but doesn't include the two ""None"" values at the beginning; you'd have to add those back manually if you wanted them.</p>

<p>There is <a href=""https://github.com/pandas-dev/pandas/issues/9481"">an open pandas issue</a> to add rolling idxmax.</p>
"
40102380,3293881.0,2016-10-18T07:42:20Z,40101371,1,"<p>Another approach could be with <a href=""https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.kron.html"" rel=""nofollow""><code>np.kron</code></a> -</p>

<pre><code>np.kron(a.reshape(-1,2),np.ones((2,2),dtype=int))
</code></pre>

<p>Basically, we reshape input array into a <code>2D</code> array keeping the second axis of <code>length=2</code>. Then <code>np.kron</code> essentially replicates the elements along both rows and columns for a length of <code>2</code> each with that array : <code>np.ones((2,2),dtype=int)</code>.</p>

<p>Sample run -</p>

<pre><code>In [45]: a
Out[45]: array([7, 5, 4, 2, 8, 6])

In [46]: np.kron(a.reshape(-1,2),np.ones((2,2),dtype=int))
Out[46]: 
array([[7, 7, 5, 5],
       [7, 7, 5, 5],
       [4, 4, 2, 2],
       [4, 4, 2, 2],
       [8, 8, 6, 6],
       [8, 8, 6, 6]])
</code></pre>

<p>If you would like to have <code>4</code> rows, use <code>a.reshape(2,-1)</code> instead.</p>
"
40102473,2225682.0,2016-10-18T07:47:40Z,40102274,4,"<p>You need to specify <code>restype</code>, <code>argtypes</code> of the function:</p>

<pre><code>zelib = ctypes.CDLL('...')
zelib.multiplier.restype = ctypes.c_float   # return type
zelib.multiplier.argtypes = [ctypes.c_float, ctypes.c_float]  # argument types
</code></pre>

<p>According to <a href=""https://docs.python.org/3/library/ctypes.html#specifying-the-required-argument-types-function-prototypes"" rel=""nofollow"">Specifying the required argument types (function prototypes)</a>:</p>

<blockquote>
  <p>It is possible to specify the required argument types of functions exported from DLLs by setting the <code>argtypes</code> attribute.</p>
</blockquote>

<p>and <a href=""https://docs.python.org/3/library/ctypes.html#return-types"" rel=""nofollow"">Return types</a> in <a href=""https://docs.python.org/3/library/ctypes.html"" rel=""nofollow""><code>ctypes</code> module documentation</a>:</p>

<blockquote>
  <p>By default functions are assumed to return the C int type. Other return types can be specified by setting the <code>restype</code> attribute of the function object.</p>
</blockquote>

<hr>

<pre><code># without specifying types
&gt;&gt;&gt; import ctypes
&gt;&gt;&gt; zelib = ctypes.CDLL('testlib.so')
&gt;&gt;&gt; zelib.multiplier(2, 3)
0

# specifying types
&gt;&gt;&gt; zelib.multiplier.restype = ctypes.c_float
&gt;&gt;&gt; zelib.multiplier.argtypes = [ctypes.c_float, ctypes.c_float]
&gt;&gt;&gt; zelib.multiplier(2, 3)
6.0
</code></pre>
"
40102656,3293881.0,2016-10-18T07:58:03Z,40101130,2,"<p>Here's an approach using <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasting</code></a> -</p>

<pre><code>maxidx = (s.values[np.arange(s.size-3+1)[:,None] + np.arange(3)]).argmax(1)
out = s.index[maxidx+np.arange(maxidx.size)]
</code></pre>

<p>This generates all the indices corresponding to the rolling windows, indexes into the extracted array version with those and thus gets the max indices for each window. For a more efficient indexing, we can use <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html"" rel=""nofollow""><code>NumPy strides</code></a>, like so -</p>

<pre><code>arr = s.values
n = arr.strides[0]
maxidx = np.lib.stride_tricks.as_strided(arr, \
                   shape=(s.size-3+1,3), strides=(n,n)).argmax(1)
</code></pre>
"
40103020,2336654.0,2016-10-18T08:16:52Z,40101130,3,"<p>I used a generator</p>

<pre><code>def idxmax(s, w):
    i = 0
    while i + w &lt;= len(s):
        yield(s.iloc[i:i+w].idxmax())
        i += 1

pd.Series(idxmax(s, 3), s.index[2:])

c    c
d    c
e    e
f    e
g    e
h    f
i    i
j    i
dtype: object
</code></pre>
"
40103074,3426606.0,2016-10-18T08:19:12Z,40096612,0,"<p>The pythonic way to do this is </p>

<pre><code>#!/Python34/python
from math import *

num_list = []

with open('temperature.text', 'r') as fh:
    for line in fh:
        num_list.append(int(line))
</code></pre>

<p>You don't need to use close here because the 'with' statement handles that automatically.</p>

<p>If you are comfortable with List comprehensions - this is another method : </p>

<pre><code>#!/Python34/python
from math import *

with open('temperature.text', 'r') as fh:
    num_list = [int(line) for line in fh]
</code></pre>

<p>In both cases 'temperature.text' must be in your current directory, and I have left the math module import, although neither piece of code needs it</p>
"
40103282,15277.0,2016-10-18T08:30:52Z,40102274,1,"<p>While @falsetru's answer is the better way of doing it an alternative is to simply write your C function to use doubles.</p>

<p>Floats are automatically promoted to double when  calling a function without a parameter list.</p>
"
40105858,6786975.0,2016-10-18T10:31:51Z,40101873,0,"<p>The first column contains the tuples <code>(ind_document, ind_word)</code> where <code>ind_document</code> is the index of your document (in your case a <code>string</code>) contained in your data set, and <code>ind_word</code> the index of the word in the dictionary of words generated by the <code>TfidfVectorizer</code> object.</p>

<p>The second column contains the TF-IDF value of your given <code>word</code> (the word corresponding to <code>(ind_document, ind_word)</code>.</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>If you look closer to the implementation of <code>TfidfVectorizer</code><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"" rel=""nofollow"">here</a>, you can see that there is a parameter called <code>norm</code>. By <strong>default</strong> this parameter is set to <code>l2</code> which is the L2-norm used to normalize the data obtained.</p>

<p>If you don't want to normalize your data and compare it to the results obtained manually <strong>change this parameter</strong> to <code>norm = None</code></p>
"
40106599,6748546.0,2016-10-18T11:08:16Z,40106468,2,"<p>It is due to pythons triple quoted strings:</p>

<pre><code>''' '''
</code></pre>

<p>It interprets everything in between as a character. So in your string:</p>

<pre><code>'''''abc\'abcddd'''''
</code></pre>

<p>The first three quotes 'open' the string. Than it encounters 2 quotes, which it interprets as characters. Next it encounters an escaped quote, which would be printed as a quote anyway, but it still uses the escaped quote. It then encounters the first 3 of the last 5 quotes, ending the triple quoted string. It then encounters 2 more quotes forming an empty string <code>''</code>. </p>

<p>A space at the places python considers 1 'thing':</p>

<pre><code>''' ''abc\'abcddd ''' ''
</code></pre>
"
40107590,3868428.0,2016-10-18T11:55:06Z,40101130,1,"<p>You can also simulate the rolling window by creating a <code>DataFrame</code> and use <code>idxmax</code> as follows: </p>

<pre><code>window_values = pd.DataFrame({0: s, 1: s.shift(), 2: s.shift(2)})
s.index[np.arange(len(s)) - window_values.idxmax(1)]

Index(['a', 'b', 'c', 'c', 'e', 'e', 'e', 'f', 'i', 'i'], dtype='object', name=0)
</code></pre>

<p>As you can see, the first two terms are the <code>idxmax</code> as applied to the initial windows of lengths 1 and 2 rather than null values. 
It's not as efficient as the accepted answer and probably not a good idea for large windows but just another perspective. </p>
"
40108198,2445864.0,2016-10-18T12:24:27Z,40108070,3,"<p>According to <a href=""http://unix.stackexchange.com/questions/55212/how-can-i-monitor-disk-io"">http://unix.stackexchange.com/questions/55212/how-can-i-monitor-disk-io</a> the most usable solution includes the tool sysstat or iostat (same package).</p>

<p>But seriously, since you have sudo permissions on the host, you can check yourself whether any IO intensive tasks are going on using any of the popular system monitoring tools. You cannot kill all IO effectively without your measurements also going nuts. Over a longer time the measurements should give you reasonable results nonetheless, since the deviations converge towards stable background noise.</p>

<p>Aside from that what would you need artificial measurements for? If you simply want to test the hardware capabilities without any RL context, <strong>do not mount the disk</strong> and test it in binary mode. A measurement while real traffic is going on usually gives you results that are closer to what you can actually expect at load times.</p>
"
40110569,4624988.0,2016-10-18T14:11:44Z,40088745,2,"<p>I did not find a solution using python, but given the information I have now it should be possible. I used wget with a .netrc file and cookie file shown as follows (<a href=""http://disc.sci.gsfc.nasa.gov/recipes/?q=recipes/How-to-Download-Data-Files-from-HTTP-Service-with-wget"" rel=""nofollow"">http://disc.sci.gsfc.nasa.gov/recipes/?q=recipes/How-to-Download-Data-Files-from-HTTP-Service-with-wget</a>):</p>

<pre><code>#!/bin/bash 

cd # path to output files 
touch .netrc
echo ""machine urs.earthdata.nasa.gov login &lt;username&gt; password &lt;password&gt;"" &gt;&gt; .netrc
chmod 0600 .netrc
touch .urs_cookies
wget --content-disposition --trust-server-names --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --auth-no-challenge=on --keep-session-cookies 
-i &lt;path to text file of url list&gt;
</code></pre>

<p>Hope it helps anyone else working with NASA data from this server. </p>
"
40110683,3046069.0,2016-10-18T14:16:44Z,40092789,-1,"<p>How about using Jquery? (Since the fieldsets are using Jquery anyway)</p>

<p>For example to move Tags under Summary....</p>

<pre><code>$('body.template-edit.portaltype-document #formfield-form-widgets-IDublinCore-subjects').insertAfter('#formfield-form-widgets-IDublinCore-description')
</code></pre>

<p>Note: This is a copy of my answer <a href=""http://stackoverflow.com/a/31205979/3046069"">here</a></p>
"
40110795,3190054.0,2016-10-18T14:21:19Z,40008998,0,"<p>As you loop over your files you need to keep a record of which patterns are not eligible for saving. You could use a <code>set</code> for this purpose. To group your entries in each file you could use <code>itertools.groupby</code>. Using your example:</p>

<pre class=""lang-python prettyprint-override""><code>import itertools

f = [i.split(""   "") for i in """"""1       2           time    4
13.45   9/29/2016   6:00   98765
12.56   9/29/2016   6:05   76548
13.45   9/29/2016   6:07   98764
13.45   9/29/2016   6:21   98766
13.45   9/29/2016   6:20   96765
12.56   9/29/2016   6:06   76553"""""".split(""\n"")[1:]]


seen_patterns = set([('9/29/2016', '96765')])   # You need to add entries to this set which you want to exclude

# Sort and group your entries by the first and second columns
col1 = itertools.groupby(sorted(f, key=lambda x: (x[0], x[1])), key=lambda x: (x[0], x[1]))
for k, v in col1:
    v = list(v)
    # Filter out patterns which are not allowed
    to_save = ["" "".join(i) for i in v if (i[1], i[3]) not in seen_patterns]
    for i in to_save:
        print i  # Save this to an appropriate file
    print

&gt;&gt;&gt;
12.56 9/29/2016 6:05 76548
12.56 9/29/2016 6:06 76553

13.45 9/29/2016 6:00 98765
13.45 9/29/2016 6:07 98764
13.45 9/29/2016 6:21 98766
</code></pre>

<p>As a further suggestion, have a look at the <code>glob</code> module for collecting file paths from directories, it is really useful. </p>
"
40110943,364696.0,2016-10-18T14:27:44Z,40110468,0,"<p>If you need to handle nested <code>str</code> defining <code>dict</code>, <a href=""https://docs.python.org/3/library/json.html#json.loads"" rel=""nofollow""><code>json.loads</code> with an <code>object_hook</code></a> might work for you:</p>

<pre><code>import json

def convert_subdicts(d):
    for k, v in d.items():
        try:
            # Try to decode a dict
            newv = json.loads(v, object_hook=convert_subdicts)
        except Exception:
            continue
        else:
            if isinstance(newv, dict):
                d[k] = newv  # Replace with decoded dict
    return d

origdict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': 'another string'}""}
newdict = convert_subdicts(origdict.copy())  # Omit .copy() if mutating origdict okay
</code></pre>

<p>That should recursively handle the case where the contained <code>dict</code>s might contain <code>str</code>s values that define subdicts. If you don't need to handle that case, you can omit the use of the <code>object_hook</code>, or replace <code>json.loads</code> entirely with <code>ast.literal_eval</code>.</p>
"
40110967,1036843.0,2016-10-18T14:28:40Z,40110468,1,"<p>The general idea referenced in my above comment is to run thru the dictionary and try and evaluate. Store that in a local variable, and then check if that evaluated expression is a dictionary. If so, then reassign it to the passed input. If not, leave it alone. </p>

<pre><code>my_dict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': 'another string'}""}

def convert_to_dict(d):
    for key, val in d.items():
        try:
            check = ast.literal_eval(val)
        except:
            continue 
        if isinstance(check, dict):
            d[key] = check 
    return d

convert_to_dict(my_dict)
</code></pre>
"
40111122,2141635.0,2016-10-18T14:35:06Z,40110468,1,"<p>You can check if you have a dict after using <em>literal_eval</em> and reassign:</p>

<pre><code>from ast import literal_eval

def reassign(d):
    for k, v in d.items():
        try:
            evald = literal_eval(v)
            if isinstance(evald, dict):
                d[k] = evald
        except ValueError:
            pass
</code></pre>

<p>Just pass in the dict:</p>

<pre><code>In [2]: my_dict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': 'another stri
   ...: ng'}""}

In [3]: reassign(my_dict)

In [4]: my_dict
Out[4]: {'a': 42, 'b': 'my_string', 'c': {'d': 33, 'e': 'another string'}}

In [5]: my_dict = {'a': '42', 'b': ""my_string"", '5': ""{'d': 33, 'e': 'another st
...: ring', 'other_dict':{'foo':'bar'}}""}
In [6]: reassign(my_dict)  
In [7]: my_dict
Out[7]: 
{'5': {'d': 33, 'e': 'another string', 'other_dict': {'foo': 'bar'}},
 'a': '42',
 'b': 'my_string'}
</code></pre>

<p>You should also be aware that if you had certain other objects in the dict like <em>datetime</em> objects etc.. then literal_eval would fail so it really depends on what your dict can contain as to whether it will work or not.</p>

<p>If you need a recursive approach, all you need is to call reassign on the new dict.</p>

<pre><code>def reassign(d):
    for k, v in d.items():
        try:
            evald = literal_eval(v)
            if isinstance(evald, dict):
                d[k] = evald
                reassign(evald)
        except ValueError:
            pass
</code></pre>

<p>And again just pass the dict:</p>

<pre><code>In [10]: my_dict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': \""{'f' : 64}
    ...: \""}""}

In [11]: reassign(my_dict)

In [12]: my_dict
Out[12]: {'a': 42, 'b': 'my_string', 'c': {'d': 33, 'e': {'f': 64}}}
</code></pre>

<p>And if you want a new dict:</p>

<pre><code>from ast import literal_eval
from copy import deepcopy

def reassign(d):
    for k, v in d.items():
        try:
            evald = literal_eval(v)
            if isinstance(evald, dict):
                yield k, dict(reassign(evald))
        except ValueError:
            yield k, deepcopy(v)
</code></pre>

<p>Which will give you a new dict:</p>

<pre><code>In [17]: my_dict = {'a': [1, 2, [3]], 'b': ""my_string"", 'c': ""{'d': 33, 'e': \""{
    ...: 'f' : 64}\""}""}

In [18]: new =  dict(reassign(my_dict))

In [19]: my_dict[""a""][-1].append(4)

In [20]: new
Out[20]: {'a': [1, 2, [3]], 'b': 'my_string', 'c': {'d': 33, 'e': {'f': 64}}}

In [21]: my_dict
Out[21]: 
{'a': [1, 2, [3, 4]],
 'b': 'my_string',
 'c': '{\'d\': 33, \'e\': ""{\'f\' : 64}""}'}
</code></pre>

<p>You need to make sure to <em>deepcopy</em> objects or you won't get a true independent copy of the dict when you have nested object like  the list of lists above.</p>
"
40111199,5847976.0,2016-10-18T14:38:44Z,40110468,1,"<p>Here is a proposition that handles recursion. As it was suggested in the comments, it tries to eval everything then check if the result is a dict, if it is we recurse, else we skip the value . I sligthly altered the initial dict to show that it hanldes recusion fine :</p>

<pre><code>import ast
my_dict = {'a': 42, 'b': ""my_string"", 'c': ""{'d': 33, 'e': \""{'f' : 64}\""}""}

def recursive_dict_eval(old_dict):
    new_dict = old_dict.copy()
    for key,value in old_dict.items():
        try:
            evaled_value=ast.literal_eval(value)
            assert isinstance(evaled_value,dict)
            new_dict[key]=recursive_dict_eval(evaled_value)

        except (SyntaxError, ValueError, AssertionError):
            #SyntaxError, ValueError are for the literal_eval exceptions
            pass
    return new_dict

print(my_dict)
print(recursive_dict_eval(my_dict))
</code></pre>

<p>Output:</p>

<pre><code>{'a': 42, 'b': 'my_string', 'c': '{\'d\': 33, \'e\': ""{\'f\' : 64}""}'}
{'a': 42, 'b': 'my_string', 'c': {'e': {'f': 64}, 'd': 33}}
</code></pre>
"
40111415,3190054.0,2016-10-18T14:47:47Z,40101371,0,"<p>The better solution is to use numpy but you could use iteration also:</p>

<pre class=""lang-python prettyprint-override""><code>a = [[1, 2, 3, 4]]

v = iter(a[0])

b = []
for i in v:
    n = next(v)
    [b.append([i for k in range(2)] + [n for k in range(2)]) for j in range(2)]

print b

&gt;&gt;&gt; [[1, 1, 2, 2], [1, 1, 2, 2], [3, 3, 4, 4], [3, 3, 4, 4]]
</code></pre>
"
40111537,4985733.0,2016-10-18T14:52:46Z,40008998,0,"<p>The following should do what you need. It reads a csv file in and generates a matching <code>datetime</code> for each of the entries to allow them to be correctly sorted. It creates output csv files based on the pattern number with the entries sorted by date. Column 4 entries already seen are omitted:</p>

<pre><code>from itertools import groupby
from datetime import datetime
import csv
import os

filename = 'my_data.csv'
data = []

with open(filename, 'rb') as f_input:
    csv_input = csv.reader(f_input, delimiter='\t')
    header = next(csv_input)

    for row in csv_input:
        dt = datetime.strptime('{} {}'.format(row[2], row[1]), '%H:%M %m/%d/%Y')
        data.append([dt] + row)

for index, (k, g) in enumerate(groupby(sorted(data, key=lambda x: x[1]), key=lambda x: x[1]), start=1):
    line = 1
    seen = set()

    with open('{}_pattern_{}.csv'.format(os.path.splitext(filename)[0], index), 'wb') as f_output:
        csv_output = csv.writer(f_output)

        for item in sorted(g, key=lambda x: x[0]):
            if item[4] not in seen:
                seen.add(item[4])
                csv_output.writerow([line] + item[1:])
                line += 1
</code></pre>
"
40112030,6779307.0,2016-10-18T15:17:10Z,40111730,2,"<p>You could build a boolean vector that checks those attributes.  Probably a better way though: </p>

<pre><code>df[risk == 'no' and smoking == 'yes' and sex == 'female' for (age, risk, sex, smoking) in df.itertuples()]
</code></pre>
"
40112316,4983450.0,2016-10-18T15:31:04Z,40111730,3,"<p>You can create a look up data frame from the dictionary and then do an inner join with the <code>data</code> which will have the same effect as <code>query</code>:</p>

<pre><code>from pandas import merge, DataFrame
merge(DataFrame(tmp, index =[0]), data)
</code></pre>

<p><a href=""https://i.stack.imgur.com/xW4kf.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/xW4kf.png"" alt=""enter image description here""></a></p>
"
40112387,5741205.0,2016-10-18T15:33:25Z,40111730,3,"<p>I would use <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#the-query-method-experimental"" rel=""nofollow"">.query()</a> method for this task:</p>

<pre><code>In [103]: qry = ' and '.join([""{} == '{}'"".format(k,v) for k,v in tmp.items()])

In [104]: qry
Out[104]: ""sex == 'female' and risk == 'no' and smoking == 'yes'""

In [105]: data.query(qry)
Out[105]:
   age risk     sex smoking
7   24   no  female     yes
22  43   no  female     yes
23  42   no  female     yes
25  24   no  female     yes
32  29   no  female     yes
40  34   no  female     yes
43  35   no  female     yes
</code></pre>
"
40113775,3142347.0,2016-10-18T16:45:02Z,40113552,3,"<p>This should do the trick</p>

<pre><code>df['new_column'] = df['old_column'].apply(lambda x: ""#""+x.replace(' ', ''))
</code></pre>

<p>Example</p>

<pre><code>&gt;&gt;&gt; names = ['Hello World', 'US Election', 'Movie Night']
&gt;&gt;&gt; df = pd.DataFrame(data = names, columns=['Names'])
&gt;&gt;&gt; df
     Names
0    Hello World
1    US Election
2    Movie Night

&gt;&gt;&gt; df['Names2'] = df['Names'].apply(lambda x: ""#""+x.replace(' ', ''))
&gt;&gt;&gt; df
     Names         Names2
0    Hello World   #HelloWorld
1    US Election   #USElection
2    Movie Night   #MovieNight
</code></pre>
"
40113784,6005062.0,2016-10-18T16:45:25Z,40113552,2,"<p>Try a list comprehesion:</p>

<pre><code>df = pandas.DataFrame({'columnOne': ['Hello World', 'US Election', 'Movie Night']})

df['column2'] = ['#' + item.replace(' ', '') for item in df.columnOne]

In [2]: df
</code></pre>

<p><a href=""https://i.stack.imgur.com/iiwuK.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/iiwuK.png"" alt=""enter image description here""></a></p>
"
40114137,674301.0,2016-10-18T17:04:34Z,40113552,3,"<p>Your general approach is totally fine, you just have a few problems. When you use apply on an entire dataframe, it will pass either a row or a column to the function it is applying. In your case, you don't want a row or a column - you want the string that is within each cell in the first column. So, instead of running <code>df.apply</code>, you want <code>df['columnOne'].apply</code>.</p>

<p>Here's what I would do: </p>

<pre><code>import pandas as pd

df = pd.DataFrame(['First test here', 'Second test'], columns=['A'])

# Note that this function expects a string, and returns a string
def new_string(s):
    # Get rid of the spaces
    s = s.replace(' ','')
    # Add the hash
    s = '#' + s
    return s

# The, apply it to the first column, and save it in the second, new column
df['B'] = df['A'].apply(new_string)
</code></pre>

<p>Or, if you really want it in a one-liner:</p>

<pre><code>df['B'] = df['A'].apply(lambda x: '#' + x.replace(' ',''))
</code></pre>
"
40114245,1828879.0,2016-10-18T17:11:36Z,40028755,0,"<p>I found the following regex-based solution to be simplest, albeit &hellip; <em>regex-based</em>.</p>

<pre><code>import json
import re
data = {
    'x': [1, {'$special': 'a'}, 2],
    'y': {'$special': 'b'},
    'z': {'p': True, 'q': False}
}
text = json.dumps(data, indent=2)
pattern = re.compile(r""""""
{
\s*
""\$special""
\s*
:
\s*
""
((?:[^""]|\\""))*  # Captures zero or more NotQuote or EscapedQuote
""
\s*
}
"""""", re.VERBOSE)
print(pattern.sub(r'{""$special"": ""\1""}', text))
</code></pre>

<p>The output follows.</p>

<pre><code>{
  ""x"": [
    1,
    {""$special"": ""a""},
    2
  ],
  ""y"": {""$special"": ""b""},
  ""z"": {
    ""q"": false,
    ""p"": true
  }
}
</code></pre>
"
40114562,2901002.0,2016-10-18T17:32:39Z,40113552,3,"<p>You can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html"" rel=""nofollow""><code>str.replace</code></a> as commented <a href=""http://stackoverflow.com/questions/40113552/pandas-create-another-column-while-splitting-each-row-from-the-first-column/40114562#comment67498560_40113552"">MaxU</a> or <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html"" rel=""nofollow""><code>Series.replace</code></a> with parameter <code>regex=True</code> for replacing all whitespaces by empty strings:</p>

<pre><code>df['column2'] = '#' + df.column1.str.replace('\s+','')
df['column3'] = '#' + df.column1.replace('\s+','', regex=True)

print (df)
       column1      column2      column3
0  Hello World  #HelloWorld  #HelloWorld
1  US Election  #USElection  #USElection
</code></pre>
"
40116249,2901002.0,2016-10-18T19:13:55Z,40116219,4,"<p>I think you can use double <code>sum</code> - first <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html"" rel=""nofollow""><code>DataFrame.sum</code></a> create <code>Series</code> of sums and second <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sum.html"" rel=""nofollow""><code>Series.sum</code></a> get sum of <code>Series</code>:</p>

<pre><code>print (df[['a','b']].sum())
a     6
b    12
dtype: int64

print (df[['a','b']].sum().sum())
18
</code></pre>

<p>You can also use:</p>

<pre><code>print (df[['a','b']].sum(axis=1))
0    3
1    6
2    9
dtype: int64

print (df[['a','b']].sum(axis=1).sum())
18
</code></pre>

<p>Thank you <a href=""http://stackoverflow.com/questions/40116219/sum-of-several-columns-from-a-pandas-dataframe/40116249?noredirect=1#comment67506082_40116249"">pirSquared</a> for another solution - convert <code>df</code> to <code>numpy array</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.values.html"" rel=""nofollow""><code>values</code></a> and then <code>sum</code>:</p>

<pre><code>print (df[['a','b']].values.sum())
18
</code></pre>

<hr>

<pre><code>print (df.sum().sum())
21
</code></pre>
"
40117903,2901002.0,2016-10-18T20:54:15Z,40117685,1,"<p>I think you need:</p>

<pre><code>df = pd.DataFrame({'company1': {0: 'South', 1: 'South', 2:'South'}, 
                   'company2': {0: 'Southern', 1: 'Route South', 2: 'South Route'}})

print (df)
  company1     company2
0    South     Southern
1    South  Route South
2    South  South Route

df1=df[df['company2'].str.contains(""|"".join('^' + df['company1'] + ' '))]
print (df1)
  company1     company2
2    South  South Route
</code></pre>
"
40119137,2141635.0,2016-10-18T22:29:46Z,40118133,1,"<p>It does seem to come down to different between dicts in python2 vs python3  in relation to <a href=""https://docs.python.org/3/whatsnew/3.3.html#porting-python-code"" rel=""nofollow"">Hash randomization is enabled by default</a> since python3.3 and the server needing at least the <em>cgi</em> field to come first, the following can reproduce:</p>

<pre><code>good = requests.post('http://evds.tcmb.gov.tr/cgi-bin/famecgi', data=([
    ('cgi', '$ozetweb'),
    ('ARAVERIGRUP', 'bie_yymkpyuk.db'),
    ('DIL', 'UK'),
    ('ONDALIK', '5'),
    ('wfmultiple_selection', 'ZAMANSERILERI'),
    ('f_begdt', '07-01-2005'),
    ('f_enddt', '07-10-2016'),
    ('ZAMANSERILERI',
     ['TP.PYUK1', 'TP.PYUK2', 'TP.PYUK21', 'TP.PYUK22', 'TP.PYUK3', 'TP.PYUK4', 'TP.PYUK5', 'TP.PYUK6']),
    ('YON', '3'),
    ('SUBMITDEG', 'Report'),
    ('GRTYPE', '1'),
    ('EPOSTA', 'xxx'),
    ('RESIMPOSTA', '***')]))


bad = requests.post('http://evds.tcmb.gov.tr/cgi-bin/famecgi', data=([
    ('ARAVERIGRUP', 'bie_yymkpyuk.db'),
    ('cgi', '$ozetweb'),
    ('DIL', 'UK'),
    ('wfmultiple_selection', 'ZAMANSERILERI'),
    ('ONDALIK', '5'),
    ('f_begdt', '07-01-2005'),
    ('f_enddt', '07-10-2016'),
    ('ZAMANSERILERI',
     ['TP.PYUK1', 'TP.PYUK2', 'TP.PYUK21', 'TP.PYUK22', 'TP.PYUK3', 'TP.PYUK4', 'TP.PYUK5', 'TP.PYUK6']),
    ('YON', '3'),
    ('SUBMITDEG', 'Report'),
    ('GRTYPE', '1'),
    ('EPOSTA', 'xxx'),
    ('RESIMPOSTA', '***')]))
</code></pre>

<p>Running the code above using python2:</p>

<pre><code>In [6]: print(good.request.body)
   ...: print(bad.request.body)
   ...: 
   ...: print(len(good.text), len(bad.text))
   ...: 
cgi=%24ozetweb&amp;ARAVERIGRUP=bie_yymkpyuk.db&amp;DIL=UK&amp;ONDALIK=5&amp;wfmultiple_selection=ZAMANSERILERI&amp;f_begdt=07-01-2005&amp;f_enddt=07-10-2016&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;YON=3&amp;SUBMITDEG=Report&amp;GRTYPE=1&amp;EPOSTA=xxx&amp;RESIMPOSTA=%2A%2A%2A
ARAVERIGRUP=bie_yymkpyuk.db&amp;cgi=%24ozetweb&amp;DIL=UK&amp;wfmultiple_selection=ZAMANSERILERI&amp;ONDALIK=5&amp;f_begdt=07-01-2005&amp;f_enddt=07-10-2016&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;YON=3&amp;SUBMITDEG=Report&amp;GRTYPE=1&amp;EPOSTA=xxx&amp;RESIMPOSTA=%2A%2A%2A
(71299, 134)
</code></pre>

<p>And python3:</p>

<pre><code>In [4]: print(good.request.body)
   ...: print(bad.request.body)
   ...: 
   ...: print(len(good.text), len(bad.text))
   ...: 
cgi=%24ozetweb&amp;ARAVERIGRUP=bie_yymkpyuk.db&amp;DIL=UK&amp;ONDALIK=5&amp;wfmultiple_selection=ZAMANSERILERI&amp;f_begdt=07-01-2005&amp;f_enddt=07-10-2016&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;YON=3&amp;SUBMITDEG=Report&amp;GRTYPE=1&amp;EPOSTA=xxx&amp;RESIMPOSTA=%2A%2A%2A
ARAVERIGRUP=bie_yymkpyuk.db&amp;cgi=%24ozetweb&amp;DIL=UK&amp;wfmultiple_selection=ZAMANSERILERI&amp;ONDALIK=5&amp;f_begdt=07-01-2005&amp;f_enddt=07-10-2016&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;YON=3&amp;SUBMITDEG=Report&amp;GRTYPE=1&amp;EPOSTA=xxx&amp;RESIMPOSTA=%2A%2A%2A
71299 134
</code></pre>

<p>Passing your dict as posted in python2:</p>

<pre><code>In [4]: response.request.body
Out[4]: 'cgi=%24ozetweb&amp;DIL=UK&amp;f_enddt=07-10-2016&amp;YON=3&amp;RESIMPOSTA=%2A%2A%2A&amp;wfmultiple_selection=ZAMANSERILERI&amp;ARAVERIGRUP=bie_yymkpyuk.db&amp;GRTYPE=1&amp;SUBMITDEG=Report&amp;f_begdt=07-01-2005&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;ONDALIK=5&amp;EPOSTA=xxx'

In [5]: len(response.text)
Out[5]: 71299
</code></pre>

<p>And the same dict in python3:</p>

<pre><code>In [3]: response.request.body
Out[3]: 'EPOSTA=xxx&amp;ARAVERIGRUP=bie_yymkpyuk.db&amp;DIL=UK&amp;SUBMITDEG=Report&amp;cgi=%24ozetweb&amp;GRTYPE=1&amp;f_enddt=07-10-2016&amp;wfmultiple_selection=ZAMANSERILERI&amp;ONDALIK=5&amp;f_begdt=07-01-2005&amp;RESIMPOSTA=%2A%2A%2A&amp;YON=3&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6'

In [4]: len(response.text)
Out[4]: 134
</code></pre>

<p>And running <code>~$ export PYTHONHASHSEED=1234</code> before starting another ipython2 shell:</p>

<pre><code>In [4]: response.request.body
Out[4]: 'DIL=UK&amp;GRTYPE=1&amp;ARAVERIGRUP=bie_yymkpyuk.db&amp;f_begdt=07-01-2005&amp;RESIMPOSTA=%2A%2A%2A&amp;ONDALIK=5&amp;EPOSTA=xxx&amp;YON=3&amp;SUBMITDEG=Report&amp;wfmultiple_selection=ZAMANSERILERI&amp;cgi=%24ozetweb&amp;ZAMANSERILERI=TP.PYUK1&amp;ZAMANSERILERI=TP.PYUK2&amp;ZAMANSERILERI=TP.PYUK21&amp;ZAMANSERILERI=TP.PYUK22&amp;ZAMANSERILERI=TP.PYUK3&amp;ZAMANSERILERI=TP.PYUK4&amp;ZAMANSERILERI=TP.PYUK5&amp;ZAMANSERILERI=TP.PYUK6&amp;f_enddt=07-10-2016'

In [5]: os.environ[""PYTHONHASHSEED""]
Out[5]: '1234'
In [6]: len(response.text)
Out[6]: 134
</code></pre>

<p>You can run the code numerous times to the same end but definitely  <code>('cgi', '$ozetweb')</code> coming first is essential for the code to work, it happened to work using python3 intermittently as the order of the keys sometimes put <em>cgi</em> first. There is a bit more on the <a href=""http://stackoverflow.com/a/27522708/2141635"">hashing topic</a></p>
"
40119652,364696.0,2016-10-18T23:26:06Z,40119616,5,"<p>It's because you took two slices and one indexing operation and tried to concatenate. slices return sub-lists, indexing returns a single element.</p>

<p>Make the middle component a slice too, e.g. <code>listOrString[index:index+1]</code>, (even though it's only a one element slice) so it keeps the type of whatever is being sliced (becoming a one element sequence of that type:</p>

<pre><code>return listOrString[index + 1:] + listOrString[index:index+1] + listOrString[:index]
</code></pre>
"
40120457,4983450.0,2016-10-19T01:06:28Z,40120299,5,"<p>You may try something like this, convert the <code>index</code> to a series that have the same <code>NaN</code> values as column <code>B</code> and then use <code>ffill()</code> which carries the last non missing index forward for all subsequent <code>NaN</code>s:</p>

<pre><code>import pandas as pd
import numpy as np
df['Last_index_notnull'] = df.index.to_series().where(df.B.notnull(), np.nan).ffill()
df['Last_value_notnull'] = df.B.ffill()
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/b7p1B.png""><img src=""https://i.stack.imgur.com/b7p1B.png"" alt=""enter image description here""></a></p>

<p>Now at index <code>4</code>, you know the last non missing value is <code>4.6</code> and index is <code>1</code>.</p>
"
40120714,901925.0,2016-10-19T01:41:38Z,40099817,2,"<p>You could define a <code>type</code> function that adds the required extension, e.g.</p>

<pre><code>def txtname(astr):
    if not astr.endswith('.txt'):
        astr += '.txt'
    return astr

In [724]: parser=argparse.ArgumentParser()
In [725]: parser.add_argument('-i',type=txtname);
In [726]: parser.add_argument('-o',type=txtname);
In [728]: parser.parse_args(['-i','inname','-o','oname.txt'])
Out[728]: Namespace(i='inname.txt', o='oname.txt')
</code></pre>

<p>That function could also raise a ValueError if you don't like certain extensions or forms of filename.</p>
"
40120790,1126841.0,2016-10-19T01:53:29Z,40120770,1,"<p><code>self.assertRaises</code> takes a callable (and optionally one or more arguments for that callable) as its argument; you are providing the value that results from calling the callable with its arguments. The correct test would be <strike><code>self.assertRaises(AttributeError, eval, 'myA.myattribute = 9')</code></strike></p>

<pre><code># Thanks to @mgilson for something that actually works while
# resembling the original attempt.
self.assertRaises(AttributeError, eval, 'myA.myattribute = 9', locals())
</code></pre>

<p>However, you should use <code>assertRaises</code> as a context manager, which allows you to write the much more natural</p>

<pre><code>with self.assertRaises(AttributeError):
    myA.myattribute = 9
</code></pre>
"
40120791,1048539.0,2016-10-19T01:53:41Z,40120770,4,"<p>You can also use <code>assertRaises</code> as a context manager:</p>

<pre><code>with self.assertRaises(AttributeError):
    myA.myattribute = 9
</code></pre>

<p>The <a href=""https://docs.python.org/3/library/unittest.html#basic-example"" rel=""nofollow"">documentation shows more examples for this if you are interested</a>. The documentation for <a href=""https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertRaises"" rel=""nofollow"">assertRaises</a> has a lot more detail on this subject as well.</p>

<p>From that documentation:</p>

<blockquote>
  <p>If only the exception and possibly the msg arguments are given, return a context manager so that the code under test can be written
  inline rather than as a function:</p>

<pre><code>with self.assertRaises(SomeException):
     do_something()
</code></pre>
</blockquote>

<p>which is exactly what you are trying to do.</p>
"
40121467,2336654.0,2016-10-19T03:09:42Z,40120299,4,"<p>some useful methods to know</p>

<p><strong><em><code>last_valid_index</code></em></strong><br>
<strong><em><code>first_valid_index</code></em></strong><br>
for columns <code>B</code> as of index <code>4</code></p>

<pre><code>df.B.ix[:4].last_valid_index()

1
</code></pre>

<p>you can use this for all columns in this way</p>

<pre><code>pd.concat([df.ix[:i].apply(pd.Series.last_valid_index) for i in df.index],
          axis=1).T
</code></pre>

<p><a href=""https://i.stack.imgur.com/CNBgf.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/CNBgf.png"" alt=""enter image description here""></a></p>
"
40121556,2336654.0,2016-10-19T03:21:49Z,40121350,2,"<pre><code>df2 = pd.to_datetime(df1['registerdate'].str[0:10])
#     \____________/
#    returns a series
</code></pre>

<hr>

<pre><code>df2['registerdate'].str.replace('-', '').str.strip()
#\_______________/
# is only something
# if 'registration
# is in the index
# this is probably the source of your error
</code></pre>

<hr>

<p>At this point <code>df2</code> is a <code>pd.Series</code> of <code>Timestamps</code>.  the format <code>yyyy-mm-dd</code> is just the way that <code>Timestamp</code> is being displayed.  To display it as <code>yyyymmdd</code>, do this</p>

<pre><code>df2.dt.strftime('%Y%m%d')

0    20160331
1    20160401
2    20160402
3    20160403
4    20160404
Name: registerdate, dtype: object
</code></pre>
"
40121644,1422451.0,2016-10-19T03:30:51Z,40119050,2,"<p>Consider reading in raw file, cleaning it line by line while writing to a new file using <code>csv</code> module. Regex is used to identify column headers using the <em>i</em> as match criteria. Below assumes more than one space separates columns:</p>

<pre><code>import os
import csv, re
import pandas as pd

rawfile = ""path/To/RawText.txt""
tempfile = ""path/To/TempText.txt""

with open(tempfile, 'w', newline='') as output_file:
    writer = csv.writer(output_file)    

    with open(rawfile, 'r') as data_file:
        for line in data_file:            
            if re.match('^.*i', line):                     # KEEP COLUMN HEADER ROW
                line = line.replace('\n', '')                
                row = line.split(""  "")                
                writer.writerow(row)

            elif line.startswith('#') == False:            # REMOVE HASHTAG LINES
                line = line.replace('\n', '')
                row = line.split(""  "")            
                writer.writerow(row)

df = pd.read_csv(tempfile)                                 # IMPORT TEMP  FILE
df.columns = [c.replace('# ', '') for c in df.columns]     # REMOVE '#' IN COL NAMES     

os.remove(tempfile)                                        # DELETE TEMP FILE
</code></pre>
"
40121723,5059823.0,2016-10-19T03:42:02Z,40121350,0,"<p>It seems df2 has no column 'registerdate', It is a timestamp list.
I think <code>df2.map(lambda x: x.strftime('%Y%m%d')</code> can convert timestamp to the format you need.</p>
"
40121920,1192111.0,2016-10-19T04:04:18Z,40121871,2,"<p>For the first part of the question:</p>

<pre><code>def ordered_subset(s1, s2):
    s2 = iter(s2)
    try:
        for c in s1:
            while next(s2) != c:
                pass
        else:
            return True
    except StopIteration:
        return False
</code></pre>

<p>For the second part of the question:</p>

<pre><code>pd.concat([s1, s2], axis=1).apply(lambda x: ordered_subset(*x), axis=1)

0     True
1    False
dtype: bool
</code></pre>
"
40122055,2336654.0,2016-10-19T04:18:56Z,40119486,2,"<p>first of all, you say ""where is not NaN"" but you <code>replace</code> with <code>''</code>.<br>
I'll replace <code>''</code> with <code>np.nan</code> then <code>dropna</code></p>

<pre><code>df.iloc[0].replace('', np.nan).dropna().index

Int64Index([0, 1, 3], dtype='int64')
</code></pre>

<hr>

<pre><code>df[df.iloc[0].replace('', np.nan).dropna().index]
</code></pre>

<p><a href=""https://i.stack.imgur.com/1arU5.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/1arU5.png"" alt=""enter image description here""></a></p>
"
40122385,2336654.0,2016-10-19T04:51:35Z,40121871,2,"<p>use <code>'.*'.join</code> to create a regex pattern to match against sequence.</p>

<pre><code>import re
import pandas as pd

s1 = pd.Series(['abc', 'egd'])
s2 = pd.Series(['axbyc', 'edg'])

match = lambda x: bool(re.match(*x))
pd.concat([s1.str.join('.*'), s2], axis=1).T.apply(match)

0     True
1    False
dtype: bool
</code></pre>

<hr>

<p>Notice that</p>

<pre><code>s1.str.join('.*')

0    a.*b.*c
1    e.*g.*d
Name: x, dtype: object
</code></pre>
"
40122792,5399734.0,2016-10-19T05:22:44Z,40122713,5,"<p>Simply run a test:</p>

<pre><code>&gt;&gt;&gt; import timeit
&gt;&gt;&gt; min(timeit.repeat('a = b = c = d = 1', number=10000000))
0.4885740280151367
&gt;&gt;&gt; min(timeit.repeat('a = 1; b = 1; c = 1; d = 1', number=10000000))
0.6283371448516846
</code></pre>

<p>Also note:</p>

<pre><code>&gt;&gt;&gt; min(timeit.repeat('a, b, c, d = 1, 1, 1, 1', number=10000000))
0.4040501117706299
</code></pre>
"
40125462,1716549.0,2016-10-19T08:00:35Z,40092789,3,"<p>Since you got your own Dexterity Type you can handle with <code>form directives</code> aka <code>setting taggedValues</code> on the interface.</p>

<pre><code>from plone.autoform import directives


class IYourSchema(model.Schema):

    directives.order_before(collage='IDublinCore.title')
    collage = schema.TextLine(
        title=u'Collage',
    )
</code></pre>

<p>You find excellent documentation about this feature in the plone documentation  <a href=""http://docs.plone.org/external/plone.app.dexterity/docs/reference/form-schema-hints.html#appearance-related-directives"" rel=""nofollow"">http://docs.plone.org/external/plone.app.dexterity/docs/reference/form-schema-hints.html#appearance-related-directives</a></p>
"
40125748,2901002.0,2016-10-19T08:15:30Z,40111730,2,"<p>You can use list comprehension with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow""><code>concat</code></a> and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.all.html"" rel=""nofollow""><code>all</code></a>:</p>

<pre><code>import numpy as np
import pandas as pd

np.random.seed(123)
x = pd.Series(np.random.randint(0,2,10), dtype='category')
x.cat.categories = ['no', 'yes']
y = pd.Series(np.random.randint(0,2,10), dtype='category')
y.cat.categories = ['no', 'yes']
z = pd.Series(np.random.randint(0,2,10), dtype='category')
z.cat.categories = ['male', 'female']

a = pd.Series(np.random.randint(20,60,10), dtype='category')

data = pd.DataFrame({'risk':x, 'smoking':y, 'sex':z, 'age':a})
print (data)
  age risk     sex smoking
0  24   no    male     yes
1  23  yes    male     yes
2  22   no  female      no
3  40   no  female     yes
4  59   no  female      no
5  22   no    male     yes
6  40   no  female      no
7  27  yes    male     yes
8  55  yes    male     yes
9  48   no    male      no
</code></pre>



<pre><code>tmp = {'risk':'no', 'smoking':'yes', 'sex':'female'}
mask = pd.concat([data[x[0]].eq(x[1]) for x in tmp.items()], axis=1).all(axis=1)
print (mask)
0    False
1    False
2    False
3     True
4    False
5    False
6    False
7    False
8    False
9    False
dtype: bool

df1 = data[mask]
print (df1)
 age risk     sex smoking
3  40   no  female     yes
</code></pre>



<pre><code>L = [(x[0], x[1]) for x in tmp.items()]
print (L)
[('smoking', 'yes'), ('sex', 'female'), ('risk', 'no')]

L = pd.concat([data[x[0]].eq(x[1]) for x in tmp.items()], axis=1)
print (L)
  smoking    sex   risk
0    True  False   True
1    True  False  False
2   False   True   True
3    True   True   True
4   False   True   True
5    True  False   True
6   False   True   True
7    True  False  False
8    True  False  False
9   False  False   True
</code></pre>

<p><strong>Timings</strong>: </p>

<p><code>len(data)=1M</code>.  </p>

<pre><code>N = 1000000
np.random.seed(123)
x = pd.Series(np.random.randint(0,2,N), dtype='category')
x.cat.categories = ['no', 'yes']
y = pd.Series(np.random.randint(0,2,N), dtype='category')
y.cat.categories = ['no', 'yes']
z = pd.Series(np.random.randint(0,2,N), dtype='category')
z.cat.categories = ['male', 'female']

a = pd.Series(np.random.randint(20,60,N), dtype='category')

data = pd.DataFrame({'risk':x, 'smoking':y, 'sex':z, 'age':a})

#[1000000 rows x 4 columns]
print (data)


tmp = {'risk':'no', 'smoking':'yes', 'sex':'female'}


In [133]: %timeit (data[pd.concat([data[x[0]].eq(x[1]) for x in tmp.items()], axis=1).all(axis=1)])
10 loops, best of 3: 89.1 ms per loop

In [134]: %timeit (data.query(' and '.join([""{} == '{}'"".format(k,v) for k,v in tmp.items()])))
1 loop, best of 3: 237 ms per loop

In [135]: %timeit (pd.merge(pd.DataFrame(tmp, index =[0]), data.reset_index()).set_index('index'))
1 loop, best of 3: 256 ms per loop
</code></pre>
"
40125786,2756793.0,2016-10-19T08:17:38Z,40123132,3,"<p>Replace the <code>for i, row in df.loc[df.fold == 1, :].iterrows():</code>-loop with this:</p>

<pre><code>df0 = pd.merge(df[df.fold == 1],aux2,on='group').set_index('id')
df = df.set_index('id')
df.loc[(df.fold == 1),'group_average'] = df0.loc[:,'group_average_y']
df = df.reset_index()
</code></pre>

<p>This gives me the same result as your code and is almost 7 times faster.</p>
"
40126366,3190054.0,2016-10-19T08:45:41Z,40111730,0,"<p>I think you can could use the <code>to_dict</code> method on your dataframe, and then filter using a list comprehension:</p>

<pre class=""lang-python prettyprint-override""><code>df = pd.DataFrame(data={'age':[28, 29], 'sex':[""M"", ""F""], 'smoking':['y', 'n']})
print df
tmp = {'age': 28, 'smoking': 'y', 'sex': 'M'}

print pd.DataFrame([i for i in df.to_dict('records') if i == tmp])


&gt;&gt;&gt;    age sex smoking
0   28   M       y
1   29   F       n

   age sex smoking
0   28   M       y
</code></pre>

<p>You could also convert tmp to a series:</p>

<pre><code>ts = pd.Series(tmp)

print pd.DataFrame([i[1] for i in df.iterrows() if i[1].equals(ts)])
</code></pre>
"
40126452,4952130.0,2016-10-19T08:49:35Z,40126403,3,"<p>You can't have a-priory knowledge about the operation for a given function. You need to either look at the source and deduce this information, or, examine the docstring for it and hope the developer documents this behavior.</p>

<p>For example, in <code>list.sort</code>:</p>

<pre><code>help(list.sort)
Help on method_descriptor:

sort(...)
    L.sort(key=None, reverse=False) -&gt; None -- stable sort *IN PLACE*
</code></pre>

<p>For functions operating on certain types, their mutability generally lets you extract some knowledge about the operation. You can be certain, for example, that all functions operating on strings will eventually return a new one, meaning, they can't perform in-place operations.</p>
"
40126548,816349.0,2016-10-19T08:54:13Z,40126403,2,"<p>I don't think there is special variable that defines some function as in-place but a standard function should have a doc string that says that it is in-place and does not return any value. For example:</p>

<p><code>&gt;&gt;&gt; print(shuffle.__doc__)</code></p>

<p><code>Shuffle list x in place, and return None.</code></p>

<pre><code>    `Optional argument random is a 0-argument function returning a
    random float in [0.0, 1.0); if it is the default None, the
    standard random.random will be used.`
</code></pre>
"
40127067,930909.0,2016-10-19T09:15:22Z,40110540,1,"<p>I don't think there is an out-of-the-box way to do that not using a <code>try..except</code> statement in your cells. AFAIK <a href=""https://github.com/ipython/ipython/issues/1977"" rel=""nofollow"">a 4 years old issue</a> mentions this, but is still in open status.</p>

<p>However, the <a href=""https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tree/master/src/jupyter_contrib_nbextensions/nbextensions/runtools"" rel=""nofollow"">runtools extension</a> may do the trick.</p>
"
40127616,3545273.0,2016-10-19T09:38:31Z,40126683,1,"<p>Shuffle is the way to go as suggested by @jonrsharpe:</p>

<pre><code>import random

def get_number(size):
    l = [ str(i) for i in list(range(10))]
    while l[0] == '0':
        random.shuffle(l)
    return int("""".join(l[:size]))
</code></pre>

<p>Limits:</p>

<ul>
<li>is you ask for a number of more than 10 digits, you will only get 10 digits</li>
<li>it can take some steps if first digit is initially a 0</li>
</ul>
"
40128704,5341338.0,2016-10-19T10:26:33Z,40110540,2,"<p>A such magic command does not exist, but you can write it.</p>

<pre><code>from IPython.core.magic import register_cell_magic

@register_cell_magic
def handle(line, cell):
    try:
        exec(cell)
    except Exception as e:
        send_mail_to_myself(e)
</code></pre>

<p>It is not possible to load automatically the magic command for the whole notebook, you have to add it at each cell where you need this feature. </p>

<pre><code>%%handle

some_code()
raise ValueError('this exception will be caught by the magic command')
</code></pre>
"
40128849,3190054.0,2016-10-19T10:32:09Z,40126683,0,"<p>Just use shuffle:</p>

<pre><code>import string

x = list(string.digits)
random.shuffle(x)

print int(str(random.choice(range(1, 10))) + """".join(x[:3]))
</code></pre>
"
40129109,3293881.0,2016-10-19T10:42:16Z,40128895,2,"<p>Here's an approach using <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasting</code></a> to get those sliding windowed elements and then just some stacking to get <code>A</code> -</p>

<pre><code>col2 = matrix[:,2]
nrows = col2.size-nr+1
out = np.zeros((nr-1+nrows,nr))
col2_2D = np.take(col2,np.arange(nrows)[:,None] + np.arange(nr))
out[nr-1:] = col2_2D
</code></pre>

<p>Here's an efficient alternative using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.strides.html"" rel=""nofollow""><code>NumPy strides</code></a> to get <code>col2_2D</code> -</p>

<pre><code>n = col2.strides[0]
col2_2D = np.lib.stride_tricks.as_strided(col2, shape=(nrows,nr), strides=(n,n))
</code></pre>

<p>It would be even better to initialize an output array of zeros of the size as <code>total</code> and then assign values into it with <code>col2_2D</code> and finally with input array <code>matrix</code>. </p>

<p><strong>Runtime test</strong></p>

<p>Approaches as functions -</p>

<pre><code>def org_app1(matrix,nr):    
    A = np.zeros((nr-1,nr))
    for x in range( matrix.shape[0]-nr+1):
        newrow =  (np.transpose( matrix[x:x+nr,2:3] ))
        A = np.vstack([A , newrow])
    return A

def vect_app1(matrix,nr):    
    col2 = matrix[:,2]
    nrows = col2.size-nr+1
    out = np.zeros((nr-1+nrows,nr))
    col2_2D = np.take(col2,np.arange(nrows)[:,None] + np.arange(nr))
    out[nr-1:] = col2_2D
    return out

def vect_app2(matrix,nr):    
    col2 = matrix[:,2]
    nrows = col2.size-nr+1
    out = np.zeros((nr-1+nrows,nr))
    n = col2.strides[0]
    col2_2D = np.lib.stride_tricks.as_strided(col2, \
                        shape=(nrows,nr), strides=(n,n))
    out[nr-1:] = col2_2D
    return out
</code></pre>

<p>Timings and verification -</p>

<pre><code>In [18]: # Setup input array and params
    ...: matrix = np.arange(1800).reshape((60, 30))
    ...: nr=3
    ...: 

In [19]: np.allclose(org_app1(matrix,nr),vect_app1(matrix,nr))
Out[19]: True

In [20]: np.allclose(org_app1(matrix,nr),vect_app2(matrix,nr))
Out[20]: True

In [21]: %timeit org_app1(matrix,nr)
1000 loops, best of 3: 646 Âµs per loop

In [22]: %timeit vect_app1(matrix,nr)
10000 loops, best of 3: 20.6 Âµs per loop

In [23]: %timeit vect_app2(matrix,nr)
10000 loops, best of 3: 21.5 Âµs per loop

In [28]: # Setup input array and params
    ...: matrix = np.arange(7200).reshape((120, 60))
    ...: nr=30
    ...: 

In [29]: %timeit org_app1(matrix,nr)
1000 loops, best of 3: 1.19 ms per loop

In [30]: %timeit vect_app1(matrix,nr)
10000 loops, best of 3: 45 Âµs per loop

In [31]: %timeit vect_app2(matrix,nr)
10000 loops, best of 3: 27.2 Âµs per loop
</code></pre>
"
40132462,4369952.0,2016-10-19T13:14:34Z,40132352,0,"<p>code: </p>

<pre><code>import numpy as np

my_list = [[20,0,5,1],
    [20,0,5,1],
    [20,0,5,0],
    [20,1,5,0],
    [20,1,5,0],
    [20,2,5,1],
    [20,3,5,0],
    [20,3,5,0],
    [20,3,5,1],
    [20,4,5,0],
    [20,4,5,0],
    [20,4,5,0]]

all_ids = np.array(my_list)[:,1]
unique_ids = np.unique(all_ids)
indices = [np.where(all_ids==ui)[0][0] for ui in unique_ids ]

final = []
for id in unique_ids:
    try:
        tmp_group = my_list[indices[id]:indices[id+1]]
    except:
        tmp_group = my_list[indices[id]:]
    if 1 in np.array(tmp_group)[:,3]:
        final.extend(tmp_group)

print np.array(final)
</code></pre>

<p>result: </p>

<pre><code>[[20  0  5  1]
 [20  0  5  1]
 [20  0  5  0]
 [20  2  5  1]
 [20  3  5  0]
 [20  3  5  0]
 [20  3  5  1]]
</code></pre>
"
40132496,6779307.0,2016-10-19T13:16:29Z,40132352,0,"<p>This gets rid of all rows with 1 in the second position:</p>

<pre><code>[sublist for sublist in list_ if sublist[1] != 1]
</code></pre>

<p>This get's rid of all rows with 1 in the second position unless the fourth position is also 1:</p>

<pre><code>[sublist for sublist in list_ if not (sublist[1] == 1 and sublist[3] != 1) ]
</code></pre>
"
40132759,18771.0,2016-10-19T13:26:23Z,40132067,1,"<p>Don't build SQL strings from user input. Ever. </p>

<p><em>Always</em> use parameterized queries.</p>

<pre><code># Adds an item to queue
def recordScan(isbn, shop_id):
    insert = ""INSERT INTO scans ( isbn, shop_id ) VALUES ( ?, ? )""
    conn = connect()
    conn.cursor().execute(insert, [isbn, shop_id])
    conn.commit()
    conn.close()
</code></pre>

<p>Please read <a href=""https://docs.python.org/2/library/sqlite3.html"" rel=""nofollow"">https://docs.python.org/2/library/sqlite3.html</a>, at the very least the upper part of the page, where they explain this approach.</p>
"
40133488,2901002.0,2016-10-19T13:57:53Z,40133016,5,"<p>I think you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.merge_ordered.html"" rel=""nofollow""><code>merge_ordered</code></a>:</p>

<pre><code>#first convert columns to datetime
df1.Quarter_End = pd.to_datetime(df1.Quarter_End)
df2.Date = pd.to_datetime(df2.Date)


df = pd.merge_ordered(df1, 
                      df2, 
                      left_on=['Company','Quarter_End'], 
                      right_on=['Company','Date'], 
                      how='outer')
print (df)
   Company  Period Quarter_End       Date  Price
0        M  2016Q1  2015-05-02        NaT    NaN
1        M     NaN         NaT 2015-06-20   1.05
2        M     NaN         NaT 2015-06-22   4.05
3        M     NaN         NaT 2015-07-10   3.45
4        M     NaN         NaT 2015-07-29   1.86
5        M  2016Q2  2015-08-01        NaT    NaN
6        M     NaN         NaT 2015-08-24   1.58
7        M     NaN         NaT 2015-09-02   8.64
8        M     NaN         NaT 2015-09-22   2.56
9        M     NaN         NaT 2015-10-20   5.42
10       M  2016Q3  2015-10-31        NaT    NaN
11       M     NaN         NaT 2015-11-02   1.58
12       M     NaN         NaT 2015-11-24   4.58
13       M     NaN         NaT 2015-12-03   6.48
14       M     NaN         NaT 2015-12-05   4.56
15       M     NaN         NaT 2016-01-03   7.14
16       M  2016Q4  2016-01-30 2016-01-30   6.34
17     WFM  2015Q2  2015-04-12        NaT    NaN
18     WFM     NaN         NaT 2015-06-20   1.05
19     WFM     NaN         NaT 2015-06-22   4.05
20     WFM  2015Q3  2015-07-05        NaT    NaN
21     WFM     NaN         NaT 2015-07-10   3.45
22     WFM     NaN         NaT 2015-07-29   1.86
23     WFM     NaN         NaT 2015-08-24   1.58
24     WFM     NaN         NaT 2015-09-02   8.64
25     WFM     NaN         NaT 2015-09-22   2.56
26     WFM  2015Q4  2015-09-27        NaT    NaN
27     WFM     NaN         NaT 2015-10-20   5.42
28     WFM     NaN         NaT 2015-11-02   1.58
29     WFM     NaN         NaT 2015-11-24   4.58
30     WFM     NaN         NaT 2015-12-03   6.48
31     WFM     NaN         NaT 2015-12-05   4.56
32     WFM     NaN         NaT 2016-01-03   7.14
33     WFM  2016Q1  2016-01-17 2016-01-17   6.34
</code></pre>

<p>Then backfill <code>NaN</code> in columns <code>Period</code> and <code>Quarter_End</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.bfill.html"" rel=""nofollow""><code>bfill</code></a> and aggregate <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.sum.html"" rel=""nofollow""><code>sum</code></a>. If need remove all NaN values, add <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dropna.html"" rel=""nofollow""><code>Series.dropna</code></a> and last <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.reset_index.html"" rel=""nofollow""><code>reset_index</code></a>:</p>

<pre><code>df.Period = df.Period.bfill()
df.Quarter_End = df.Quarter_End.bfill()

print (df.groupby(['Company','Period','Quarter_End'])['Price'].sum().dropna().reset_index())

  Company  Period Quarter_End  Price
0       M  2016Q2  2015-08-01  10.41
1       M  2016Q3  2015-10-31  18.20
2       M  2016Q4  2016-01-30  30.68
3     WFM  2015Q3  2015-07-05   5.10
4     WFM  2015Q4  2015-09-27  18.09
5     WFM  2016Q1  2016-01-17  36.10
</code></pre>
"
40133589,2336654.0,2016-10-19T14:02:16Z,40133016,3,"<ul>
<li><code>set_index</code></li>
<li><code>pd.concat</code> to align indices</li>
<li><code>groupby</code> with <code>agg</code></li>
</ul>

<hr>

<pre><code>prd_df = period_df.set_index(['Company', 'Quarter_End'])

prc_df = price_df.set_index(['Company', 'Date'], drop=False)

df = pd.concat([prd_df, prc_df], axis=1)

df.groupby([df.index.get_level_values(0), df.Period.bfill()])  \
  .agg(dict(Date='last', Price='sum')).dropna()
</code></pre>

<p><a href=""https://i.stack.imgur.com/EuJ86.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/EuJ86.png"" alt=""enter image description here""></a></p>
"
40133930,7008416.0,2016-10-19T14:14:53Z,40133826,3,"<p>You need a file object, not just a file name. Try this for saving:</p>

<pre><code>pickle.dump(mylist, open(""save.txt"", ""wb""))
</code></pre>

<p>or better, to guarantee the file is closed properly:</p>

<pre><code>with open(""save.txt"", ""wb"") as f:
    pickle.dump(mylist, f)
</code></pre>

<p>and then this for loading:</p>

<pre><code>with open(""save.txt"", ""rb"") as f:
    mylist = pickle.load(f)
</code></pre>

<p>Also, I suggest a different extension from <code>.txt</code>, like maybe <code>.dat</code>, because the contents is not plain text.</p>
"
40133939,2281436.0,2016-10-19T14:15:16Z,40133826,1,"<pre><code>with open(""save.txt"", ""w"") as f:
    pickle.dump(f, mylist)
</code></pre>

<p>Refer to python pickle documentation for usage.</p>
"
40133995,7031759.0,2016-10-19T14:18:00Z,40133826,1,"<p>pickle.dump accept file object as argument instead of filename string</p>

<pre><code>pickle.dump(mylist, open(""save.txt"", ""wb""))
</code></pre>
"
40134174,4637585.0,2016-10-19T14:23:57Z,40132067,1,"<p>You appear to be opening and closing the database each and every time. That will clearly add a huge overhead, especially as you are ""hammering"" away at it.<br>
Connect to the database once at the beginning and close it upon exit.<br>
In between, simply perform your <code>insert</code>, <code>update</code> and <code>delete</code> statements. </p>

<p>Edit:<br>
For the purposes of this I renamed <code>db.py</code> to be called <code>barcode1.py</code> so edit appropriately.
Alter <code>listen.py</code> to be as follows:    </p>

<pre><code>#!/usr/bin/env python

import logging
import barcode1
DB_FILE_NAME = ""scan-queue.db""
my_db = barcode1.sqlite3.connect(DB_FILE_NAME)
my_cursor = my_db.cursor()

def InsertScan(isbn, shop_id):
    insert = ""INSERT INTO scans ( isbn, shop_id ) VALUES ( ?, ? )""
    my_cursor.execute(insert, [isbn, shop_id])
    my_db.commit()

while True:
    barcode = raw_input(""Scan ISBN: "")
    if ( len(barcode) &gt; 1 ):
        logging.info(""Recording scanned ISBN: "" + barcode)
        print ""Recording scanned ISBN: "" + barcode
        InsertScan(barcode, 1)
my_db.close()
</code></pre>

<p>For your purposes replace references to ""barcode1"" with ""db""<br>
As you can see all that happens here is that a separate function has been added to do the writing and only the writing.<br>
Clearly this is a quick mock up and could be improved immeasurably, in fact I'd rewrite it as a single script. This is one of those classic examples where in an attempt to write object oriented code, you end up shooting yourself in the foot.<br>
In fact you could do without the function and just include the <code>insert</code> code within the <code>while</code> statement.</p>

<p>Locking:
from the sqlite3 documents:</p>

<pre><code> sqlite3.connect(database[, timeout, detect_types, isolation_level, check_same_thread, factory, cached_statements, uri])
</code></pre>

<p>Opens a connection to the SQLite database file database. You can use "":memory:"" to open a database connection to a database that resides in RAM instead of on disk.</p>

<p>When a database is accessed by multiple connections, and one of the processes modifies the database, the SQLite database is locked until that transaction is committed. The timeout parameter specifies how long the connection should wait for the lock to go away until raising an exception. The default for the timeout parameter is 5.0 (five seconds).</p>
"
40134398,3293881.0,2016-10-19T14:32:19Z,40132352,3,"<p><strong>Generic approach :</strong> Here's an approach using <a href=""https://numeric.scipy.org/doc/numpy/reference/generated/numpy.unique.html"" rel=""nofollow""><code>np.unique</code></a> and <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.html"" rel=""nofollow""><code>np.bincount</code></a> to solve for a generic case -</p>

<pre><code>unq,tags = np.unique(data[:,1],return_inverse=1)
goodIDs = np.flatnonzero(np.bincount(tags,data[:,3]==1)&gt;=1)
out = data[np.in1d(tags,goodIDs)]
</code></pre>

<p>Sample run -</p>

<pre><code>In [15]: data
Out[15]: 
array([[20, 10,  5,  1],
       [20, 73,  5,  0],
       [20, 73,  5,  1],
       [20, 31,  5,  0],
       [20, 10,  5,  1],
       [20, 10,  5,  0],
       [20, 42,  5,  1],
       [20, 54,  5,  0],
       [20, 73,  5,  0],
       [20, 54,  5,  0],
       [20, 54,  5,  0],
       [20, 31,  5,  0]])

In [16]: out
Out[16]: 
array([[20, 10,  5,  1],
       [20, 73,  5,  0],
       [20, 73,  5,  1],
       [20, 10,  5,  1],
       [20, 10,  5,  0],
       [20, 42,  5,  1],
       [20, 73,  5,  0]])
</code></pre>

<p><strong>Specific case approach :</strong>  If the second column data is always sorted and have sequential numbers starting from <code>0</code>, we can use a simplified version, like so -</p>

<pre><code>goodIDs = np.flatnonzero(np.bincount(data[:,1],data[:,3]==1)&gt;=1)
out = data[np.in1d(data[:,1],goodIDs)]
</code></pre>

<p>Sample run -</p>

<pre><code>In [44]: data
Out[44]: 
array([[20,  0,  5,  1],
       [20,  0,  5,  1],
       [20,  0,  5,  0],
       [20,  1,  5,  0],
       [20,  1,  5,  0],
       [20,  2,  5,  1],
       [20,  3,  5,  0],
       [20,  3,  5,  0],
       [20,  3,  5,  1],
       [20,  4,  5,  0],
       [20,  4,  5,  0],
       [20,  4,  5,  0]])

In [45]: out
Out[45]: 
array([[20,  0,  5,  1],
       [20,  0,  5,  1],
       [20,  0,  5,  0],
       [20,  2,  5,  1],
       [20,  3,  5,  0],
       [20,  3,  5,  0],
       [20,  3,  5,  1]])
</code></pre>

<p>Also, if <code>data[:,3]</code> always have ones and zeros, we can just use <code>data[:,3]</code> in place of <code>data[:,3]==1</code> in the above listed codes.</p>

<hr>

<p><strong>Benchmarking</strong> </p>

<p>Let's benchmark the vectorized approaches on the specific case for a larger array -</p>

<pre><code>In [69]: def logical_or_based(data): #@ Eric's soln
    ...:     b_vals = data[:,1]
    ...:     d_vals = data[:,3]
    ...:     is_ok = np.zeros(np.max(b_vals) + 1, dtype=np.bool_)
    ...:     np.logical_or.at(is_ok, b_vals, d_vals)
    ...:     return is_ok[b_vals]
    ...: 
    ...: def in1d_based(data):
    ...:     goodIDs = np.flatnonzero(np.bincount(data[:,1],data[:,3])!=0)
    ...:     out = np.in1d(data[:,1],goodIDs)
    ...:     return out
    ...: 

In [70]: # Setup input
    ...: data = np.random.randint(0,100,(10000,4))
    ...: data[:,1] = np.sort(np.random.randint(0,100,(10000)))
    ...: data[:,3] = np.random.randint(0,2,(10000))
    ...: 

In [71]: %timeit logical_or_based(data) #@ Eric's soln
1000 loops, best of 3: 1.44 ms per loop

In [72]: %timeit in1d_based(data)
1000 loops, best of 3: 528 Âµs per loop
</code></pre>
"
40134524,102441.0,2016-10-19T14:37:34Z,40132352,0,"<p>Let's assume the following:</p>

<ul>
<li><code>b &gt;= 0</code></li>
<li><code>b</code> is an integer</li>
<li><code>b</code> is fairly dense, ie <code>max(b) ~= len(unique(b))</code></li>
</ul>

<p>Here's a solution using <a href=""https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.ufunc.at.html"" rel=""nofollow""><code>np.ufunc.at</code></a>:</p>

<pre><code># unpack for clarity - this costs nothing in numpy
b_vals = data[:,1]
d_vals = data[:,3]

# build an array indexed by b values
is_ok = np.zeros(np.max(b_vals) + 1, dtype=np.bool_)
np.logical_or.at(is_ok, b_vals, d_vals)
# is_ok == array([ True, False,  True,  True, False], dtype=bool)

# take the rows which have a b value that was deemed OK
result = data[is_ok[b_vals]]
</code></pre>

<hr>

<p><code>np.logical_or.at(is_ok, b_vals, d_vals)</code> is a more efficient version of:</p>

<pre><code>for idx, val in zip(b_vals, d_vals):
    is_ok[idx] = np.logical_or(is_ok[idx], val)
</code></pre>
"
40134973,5626112.0,2016-10-19T14:56:04Z,40134811,2,"<p>insert the <code>columm.append</code> into the <code>try:</code></p>

<pre><code>for col in list_of_columns:
    column = []
    for row in list(df[col]):
        try:
            column.append(remove_html(row))
        except ValueError:
            pass

    del df[col]

    df[col] = column

return df
</code></pre>
"
40135113,4330567.0,2016-10-19T15:01:53Z,39913847,0,"<p>If you are on a Mac you can use py2app to create a .app bundle, which starts your Django app when you double-click on it.</p>

<p>I described how to bundle Django and CherryPy into such a bundle at <a href=""https://moosystems.com/articles/14-distribute-django-app-as-native-desktop-app-01.html"" rel=""nofollow"">https://moosystems.com/articles/14-distribute-django-app-as-native-desktop-app-01.html</a></p>

<p>In the article I use pywebview to display your Django site in a local application window.</p>
"
40135527,2336654.0,2016-10-19T15:18:32Z,40134811,0,"<p>consider the <code>pd.DataFrame</code> <code>df</code></p>

<pre><code>df = pd.DataFrame(dict(A=[1, '2', '_', '4']))
</code></pre>

<p><a href=""https://i.stack.imgur.com/m05NY.png"" rel=""nofollow""><img src=""https://i.stack.imgur.com/m05NY.png"" alt=""enter image description here""></a></p>

<p>You want to use the function <code>pd.to_numeric</code>...<br>
<strong><em>Note</em></strong><br>
<code>pd.to_numeric</code> operates on scalars and <code>pd.Series</code>.  It doesn't operate on a <code>pd.DataFrame</code><br>
<strong><em>Also</em></strong><br>
Use the parameter <code>errors='coerce'</code> to get numbers where you can and <code>NaN</code> elsewhere.</p>

<pre><code>pd.to_numeric(df['A'], 'coerce')

0    1.0
1    2.0
2    NaN
3    4.0
Name: A, dtype: float6
</code></pre>

<p>Or, to get numbers where you can, and what you already had elsewhere</p>

<pre><code>pd.to_numeric(df['A'], 'coerce').combine_first(df['A'])

0    1
1    2
2    _
3    4
Name: A, dtype: object
</code></pre>

<p>you can then assign it back to your <code>df</code></p>

<pre><code>df['A'] = pd.to_numeric(df['A'], 'coerce').combine_first(df['A'])
</code></pre>
"
40135692,4983450.0,2016-10-19T15:26:28Z,40135459,6,"<p>It seems like your data is fixed width columns, you can try <code>pandas.read_fwf()</code>:</p>

<pre><code>from io import StringIO
import pandas as pd

df = pd.read_fwf(StringIO(""""""0    0CF00400 X       8  66  7D  91  6E  22  03  0F  7D       0.021650 R
0    18EA0080 X       3  E9  FE  00                           0.022550 R
0    00000003 X       8  D5  64  22  E1  FF  FF  FF  F0       0.023120 R""""""), 
                 header = None, widths = [1,12,2,8,4,4,4,4,4,4,4,4,16,2])
</code></pre>

<p><a href=""https://i.stack.imgur.com/G3bLR.png""><img src=""https://i.stack.imgur.com/G3bLR.png"" alt=""enter image description here""></a></p>
"
40135720,5798899.0,2016-10-19T15:27:24Z,40134811,0,"<p>Works like this:</p>

<pre><code>def clean_df(df):
df = df.astype(str)
list_of_columns = list(df.columns)
for col in list_of_columns:
    column = []
    for row in list(df[col]):
        try:
            column.append(int(remove_html(row)))
        except ValueError:
            column.append(remove_html(row))

    del df[col]

    df[col] = column

return df
</code></pre>
"
40135726,2503352.0,2016-10-19T15:27:30Z,40132771,2,"<p>By default, click will intelligently map intra-option commandline hyphens to underscores so your code should work as-is. This is used in the click documentation, e.g., in the <a href=""http://click.pocoo.org/5/options/#choice-options"" rel=""nofollow"">Choice example</a>. If --delete-thing is intended to be a boolean option, you may also want to make it a <a href=""http://stackoverflow.com/questions/40132694/gradientboostingclassifier-analog-in-r"">boolean argument</a>.</p>
"
40135960,4110625.0,2016-10-19T15:38:44Z,40110540,0,"<p>@show0k gave the correct answer to my question (in regards to magic methods). Thanks a lot! :)</p>

<p>That answer inspired me to dig a little deeper and I came across an IPython method that lets you define a <strong>custom exception handler for the whole notebook</strong>.</p>

<p>I got it to work like this:</p>

<pre><code>from IPython.core.ultratb import AutoFormattedTB

# initialize the formatter for making the tracebacks into strings
itb = AutoFormattedTB(mode = 'Plain', tb_offset = 1)

# this function will be called on exceptions in any cell
def custom_exc(shell, etype, evalue, tb, tb_offset=None):

    # still show the error within the notebook, don't just swallow it
    shell.showtraceback((etype, evalue, tb), tb_offset=tb_offset)

    # grab the traceback and make it into a list of strings
    stb = itb.structured_traceback(etype, evalue, tb)
    sstb = itb.stb2text(stb)

    print (sstb) # &lt;--- this is the variable with the traceback string
    print (""sending mail"")
    send_mail_to_myself(sstb)

# this registers a custom exception handler for the whole current notebook
get_ipython().set_custom_exc((Exception,), custom_exc)
</code></pre>

<p>So this can be put into a single cell at the top of any notebook and as a result it will do the mailing in case something goes wrong.</p>

<p>Note to self / TODO: make this snippet into a little python module that can be imported into a notebook and activated via line magic.</p>

<p>Be careful though. The documentation contains a warning for this <code>set_custom_exc</code> method: ""WARNING: by putting in your own exception handler into IPythonâs main execution loop, you run a very good chance of nasty crashes. This facility should only be used if you really know what you are doing.""</p>
"
40135964,2823755.0,2016-10-19T15:38:50Z,40134811,0,"<p>Use the try/except in a function and use that function with <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html"" rel=""nofollow""><code>DataFrame.applymap()</code></a></p>

<pre><code>df = pd.DataFrame([['a','b','1'],
                   ['2','c','d'],
                   ['e','3','f']])
def foo(thing):
    try:
        return int(thing)
    except ValueError as e:
        return thing

&gt;&gt;&gt; df[0][2]
'e'
&gt;&gt;&gt; df[0][1]
'2'
&gt;&gt;&gt; df = df.applymap(foo)
&gt;&gt;&gt; df[0][2]
'e'
&gt;&gt;&gt; df[0][1]
2
&gt;&gt;&gt;
</code></pre>
"
40136108,2659342.0,2016-10-19T15:45:53Z,40134664,0,"<p>Use <code>names</code> instead of <code>usecols</code> while specifying parameter.</p>
"
40136593,613246.0,2016-10-19T16:09:35Z,40132352,0,"<p>Untested since in a hurry, but this should work:</p>

<pre><code>import numpy_indexed as npi
g = npi.group_by(data[:, 1])
ids, valid = g.any(data[:, 3])
result = data[valid[g.inverse]]
</code></pre>
"
40137410,735926.0,2016-10-19T16:57:13Z,40046952,0,"<blockquote>
  <p>it is possible to brew install python 3.5.1?</p>
</blockquote>

<p>Yes it is. See <a href=""http://stackoverflow.com/a/4158763/735926"">this StackOverflow answer</a>.</p>

<blockquote>
  <p>If not, what will it mean to install 3.5.1 via .pkg file?</p>
</blockquote>

<p>The most noticeable change will be that you wonât be able to upgrade your Python installation without downloading the new version and installing it by hand (compared to <code>brew upgrade python3</code>). Itâll also be <a href=""http://stackoverflow.com/a/3819829/735926"">slightly more complicated to remove</a> compared to <code>brew rm python3</code>.</p>

<p>Other than these minor differences you should have the same experience with both installations. Be sure that the <code>python</code> installed from <code>python-3.5.1-macosx10.6.pkg</code> is before Homebrewâs in your <code>PATH</code> or use its full path.</p>
"
40137700,2063361.0,2016-10-19T17:14:42Z,40137072,2,"<p>There are two changes in <code>hash()</code> function between Python 2.7 and Python 3.4</p>

<ol>
<li>Adoptions of <em>SipHash</em></li>
<li>Default enabling of <em>Hash randomization</em></li>
</ol>

<hr>

<p><em>References:</em></p>

<ul>
<li>Since from Python 3.4, it uses <a href=""https://131002.net/siphash/"" rel=""nofollow"">SipHash</a> for it's hashing function. Read: <a href=""https://lwn.net/Articles/574761/"" rel=""nofollow"">Python adopts SipHash</a></li>
<li>Since Python 3.3 <em>Hash randomization is enabled by default.</em> Reference: <a href=""https://docs.python.org/3/reference/datamodel.html#object.__hash__"" rel=""nofollow""><code>object.__hash__</code></a> (last line of this section). Specifying <a href=""https://docs.python.org/3/using/cmdline.html#envvar-PYTHONHASHSEED"" rel=""nofollow""><code>PYTHONHASHSEED</code></a> the value 0 will disable hash randomization.</li>
</ul>
"
40137811,3160869.0,2016-10-19T17:21:32Z,39981210,2,"<p>I finally found a solution, with the help of (<em>SO user</em>) sytech.</p>

<p>The answer to my original question is that using the original <strong><a href=""https://github.com/OneDrive/onedrive-sdk-python"" rel=""nofollow"">Python OneDrive SDK</a></strong>, it's <strong>not possible</strong> to upload a file to the <code>Shared Documents</code> folder of a <code>SharePoint Online</code> site (at the moment of writing this): when the SDK queries the <a href=""https://dev.onedrive.com/auth/aad_oauth.htm#step-3-discover-the-onedrive-for-business-resource-uri"" rel=""nofollow""><strong>resource discovery service</strong></a>, it drops all services whose <code>service_api_version</code> is not <code>v2.0</code>. However, I get the SharePoint service with <code>v1.0</code>, so it's dropped, although it could be accessed using API v2.0 too.</p>

<p><strong>However</strong>, by extending the <code>ResourceDiscoveryRequest</code> class (in the OneDrive SDK), we can create a workaround for this. I managed to <strong>upload a file</strong> this way:</p>

<pre><code>import json
import re
import onedrivesdk
import requests
from onedrivesdk.helpers.resource_discovery import ResourceDiscoveryRequest, \
    ServiceInfo

# our domain (not the original)
redirect_uri = 'https://example.ourdomain.net/' 
# our client id (not the original)
client_id = ""a1234567-1ab2-1234-a123-ab1234abc123""  
# our client secret (not the original)
client_secret = 'ABCaDEFGbHcd0e1I2fghJijkL3mn4M5NO67P8Qopq+r=' 
resource = 'https://api.office.com/discovery/'
auth_server_url = 'https://login.microsoftonline.com/common/oauth2/authorize'
auth_token_url = 'https://login.microsoftonline.com/common/oauth2/token'

# our sharepoint URL (not the original)
sharepoint_base_url = 'https://{tenant}.sharepoint.com/'
# our site URL (not the original)
sharepoint_site_url = sharepoint_base_url + 'sites/{site}'

file_to_upload = 'C:/test.xlsx'
target_filename = 'test.xlsx'


class AnyVersionResourceDiscoveryRequest(ResourceDiscoveryRequest):

    def get_all_service_info(self, access_token, sharepoint_base_url):
        headers = {'Authorization': 'Bearer ' + access_token}
        response = json.loads(requests.get(self._discovery_service_url,
                                           headers=headers).text)
        service_info_list = [ServiceInfo(x) for x in response['value']]
        # Get all services, not just the ones with service_api_version 'v2.0'
        # Filter only on service_resource_id
        sharepoint_services = \
            [si for si in service_info_list
             if si.service_resource_id == sharepoint_base_url]
        return sharepoint_services


http = onedrivesdk.HttpProvider()
auth = onedrivesdk.AuthProvider(http_provider=http, client_id=client_id,
                                auth_server_url=auth_server_url,
                                auth_token_url=auth_token_url)

should_authenticate_via_browser = False
try:
    # Look for a saved session. If not found, we'll have to
    # authenticate by opening the browser.
    auth.load_session()
    auth.refresh_token()
except FileNotFoundError as e:
    should_authenticate_via_browser = True
    pass

if should_authenticate_via_browser:
    auth_url = auth.get_auth_url(redirect_uri)
    code = ''
    while not re.match(r'[a-zA-Z0-9_-]+', code):
        # Ask for the code
        print('Paste this URL into your browser, approve the app\'s access.')
        print('Copy the resulting URL and paste it below.')
        print(auth_url)
        code = input('Paste code here: ')
        # Parse code from URL if necessary
        if re.match(r'.*?code=([a-zA-Z0-9_-]+).*', code):
            code = re.sub(r'.*?code=([a-zA-Z0-9_-]*).*', r'\1', code)

    auth.authenticate(code, redirect_uri, client_secret, resource=resource)
    service_info = AnyVersionResourceDiscoveryRequest().\
        get_all_service_info(auth.access_token, sharepoint_base_url)[0]
    auth.redeem_refresh_token(service_info.service_resource_id)
    auth.save_session()

client = onedrivesdk.OneDriveClient(sharepoint_site_url + '/_api/v2.0/',
                                    auth, http)
# Get the drive ID of the Documents folder.
documents_drive_id = [x['id']
                      for x
                      in client.drives.get()._prop_list
                      if x['name'] == 'Documents'][0]
items = client.item(drive=documents_drive_id, id='root')
# Upload file
uploaded_file_info = items.children[target_filename].upload(file_to_upload)
</code></pre>

<p>Authenticating for a different service gives you a different token.</p>
"
40137973,624829.0,2016-10-19T17:31:53Z,40119050,1,"<p>This is the way I'm mentioning in the comment: it uses a file object to skip the custom dirty data you need to skip at the beginning. You land the file offset at the appropriate location in the file where <code>read_fwf</code> simply does the job:</p>

<pre><code>with open(rawfile, 'r') as data_file:
    while(data_file.read(1)=='#'):
        last_pound_pos = data_file.tell()
        data_file.readline()
    data_file.seek(last_pound_pos)
    df = pd.read_fwf(data_file)

df
Out[88]: 
   i      mult  stat (+/-)  syst (+/-)        Q2         x       x.1       Php
0  0  0.322541    0.018731    0.026681  1.250269  0.037525  0.148981  0.104192
1  1  0.667686    0.023593    0.033163  1.250269  0.037525  0.150414  0.211203
2  2  0.766044    0.022712    0.037836  1.250269  0.037525  0.149641  0.316589
3  3  0.668402    0.024219    0.031938  1.250269  0.037525  0.148027  0.415451
4  4  0.423496    0.020548    0.018001  1.250269  0.037525  0.154227  0.557743
5  5  0.237175    0.023561    0.007481  1.250269  0.037525  0.159904  0.750544
</code></pre>
"
40138071,2054138.0,2016-10-19T17:37:19Z,40132067,1,"<p>After much experimenting based on helpful advice from users @tomalak, @rolf-of-saxony and @hevlastka my conclusion is that <strong>yes, this <em>is</em> an inevitability that I just have to live with.</strong> </p>

<p>Even if you strip the example down to the basics by removing the database write process and making it a simple <em>parrot</em> script that just repeats back inputs (See <a href=""http://stackoverflow.com/questions/40156905/python-on-raspberry-pi-user-input-inside-infinite-loop-misses-inputs-when-hit-wi"">Python on Raspberry Pi user input inside infinite loop misses inputs when hit with many</a>), it is still possible to scan items so fast that inputs get missed/skipped/ignored. The Raspberry Pi simply cannot keep up. </p>

<p>So my approach will now be to add an audio feedback feature such as a beep sound to indicate to the user when the device is ready to receive the next input. A route I didn't want to go down but it seems my code is the most efficient it can be and we're still able to hit the limits. Responsibility is with the user to not go at breakneck speed and the best we can do a responsible product builders is give them good feedback. </p>
"
40138275,2029132.0,2016-10-19T17:47:52Z,40134664,1,"<p>In your message, you said that you're a running:</p>

<pre><code>df = pd.read_csv('SHL1_TAQ_600000_201201.txt',usecols=fields)
</code></pre>

<p>Which did not throw an error for me and @Anil_M. But from your traceback, it is possible to see that the command used is another one:</p>

<pre><code>df = pd.read_csv('SHL1_TAQ_600000_201201.txt',usecols=fields, header=1)
</code></pre>

<p>which includes a <code>header=1</code> and it throws the error mentioned.</p>

<p>So, I would guess that the error comes from some confusion on your code.</p>
"
40141039,2776376.0,2016-10-19T20:34:39Z,40140933,2,"<p>From the <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow"">documentation</a></p>

<blockquote>
  <p>The operators are: | for or, &amp; for and, and ~ for not. These must be
  grouped by using parentheses.</p>
</blockquote>

<p><a href=""https://docs.python.org/3/reference/simple_stmts.html#augmented-assignment-statements"" rel=""nofollow"">Augmented assignment statements</a></p>

<blockquote>
  <p>An augmented assignment evaluates the target (which, unlike normal
  assignment statements, cannot be an unpacking) and the expression
  list, performs the binary operation specific to the type of assignment
  on the two operands, and assigns the result to the original target.
  The target is only evaluated once.</p>
</blockquote>

<p>just like <code>a += 1</code> increments <code>a</code>, <code>a &amp;= b</code> compares <code>a</code> and <code>b</code> and assigns the result to <code>a</code>.</p>

<pre><code>a = 1
b = 0
print(a &amp; b)
&gt;&gt;&gt; 0
a &amp;= b
print(a)
&gt;&gt;&gt; 0
</code></pre>

<p>And a <code>pandas</code> example</p>

<p>Let's generate a dataframe of zeros and ones.</p>

<pre><code>import numpy as np
import pandas as pd
a = pd.DataFrame(np.random.randint(0, 2, size=(6,4)), columns=list('ABCD'))
b = pd.DataFrame(np.random.randint(0, 2, size=(6,4)), columns=list('ABCD'))
</code></pre>

<p>Our initial dataframe</p>

<pre><code>print(a)
</code></pre>

<blockquote>
<pre><code>   A  B  C  D
0  0  1  1  0
1  0  0  1  0
2  1  0  0  1
3  1  1  0  0
4  0  0  0  1
5  0  0  0  0
</code></pre>
</blockquote>

<pre><code>print(b)
</code></pre>

<blockquote>
<pre><code>   A  B  C  D
0  0  0  0  0
1  1  1  1  0
2  0  1  1  1
3  0  1  1  1
4  1  1  1  0
5  1  1  1  1
</code></pre>
</blockquote>

<p>The 4th row of <code>a</code> and <code>b</code></p>

<pre><code>print(a.loc[3])
</code></pre>

<blockquote>
<pre><code>A    1
B    1
C    0
D    0
Name: 1, dtype: int32
</code></pre>
</blockquote>

<pre><code>print(b.loc[3])
</code></pre>

<blockquote>
<pre><code>A    0
B    1
C    1
D    1
Name: 1, dtype: int32
</code></pre>
</blockquote>

<p>Now evaluate and assign row 4</p>

<pre><code>a.loc[3] &amp;= b.loc[3]
</code></pre>

<p>Row 4 of <code>a</code> has changed. Only where both rows have 1 at the same position a 1 is written back to <code>a</code>.</p>

<pre><code>print(a.loc[3])
</code></pre>

<blockquote>
<pre><code>A    0
B    1
C    0
D    0
Name: 3, dtype: int32
</code></pre>
</blockquote>
"
40141272,3293881.0,2016-10-19T20:48:48Z,40140942,1,"<p>One approach using a combination of <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow""><code>broadcasting</code></a> and <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html"" rel=""nofollow""><code>np.einsum</code></a> -</p>

<pre><code>np.einsum('ij,jk-&gt;ik',nodes,x**np.array([2,1,0])[:,None])
</code></pre>

<p>Another one using matrix-multiplication with <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html"" rel=""nofollow""><code>np.dot</code></a> -</p>

<pre><code>nodes.dot(x**np.array([2,1,0])[:,None])
</code></pre>
"
40142095,5741205.0,2016-10-19T21:50:48Z,40141881,3,"<p>It looks like a bug to me:</p>

<pre><code>In [19]: df.groupby('A').transform(lambda x: x.sum())
Out[19]:
   B
0  1
1  1

In [20]: df.groupby('A').transform(lambda x: len(x))
Out[20]:
   B
0  2
1  2

In [21]: df.groupby('A').transform(lambda x: x.sum()/len(x))
Out[21]:
   B
0  0
1  0
</code></pre>

<p>PS Pandas version: 0.19.0</p>
"
40143349,6934347.0,2016-10-19T23:54:02Z,40077010,0,"<p>I solved my own problem defining the following function. I adjusted the code provided in tf.image.central_crop(image, central_fraction).
The function RandomCrop will crop an image taking a central_fraction drawn from a uniform distribution. You can just specify the min and max fraction you want. 
You can replace random_uniform distribution to a different one obviously.</p>

<pre><code>def RandomCrop(image,fMin, fMax):
  from tensorflow.python.ops import math_ops
  from tensorflow.python.ops import array_ops
  from tensorflow.python.framework import ops
  image = ops.convert_to_tensor(image, name='image')

  if fMin &lt;= 0.0 or fMin &gt; 1.0:
    raise ValueError('fMin must be within (0, 1]')
  if fMax &lt;= 0.0 or fMax &gt; 1.0:
    raise ValueError('fMin must be within (0, 1]')

  img_shape = array_ops.shape(image)
  depth = image.get_shape()[2]
  my_frac2 = tf.random_uniform([1], minval=fMin, maxval=fMax, dtype=tf.float32, seed=42, name=""uniform_dist"") 
  fraction_offset = tf.cast(math_ops.div(1.0 , math_ops.div(math_ops.sub(1.0,my_frac2[0]), 2.0)),tf.int32)
  bbox_h_start = math_ops.div(img_shape[0], fraction_offset)
  bbox_w_start = math_ops.div(img_shape[1], fraction_offset)
  bbox_h_size = img_shape[0] - bbox_h_start * 2
  bbox_w_size = img_shape[1] - bbox_w_start * 2

  bbox_begin = array_ops.pack([bbox_h_start, bbox_w_start, 0])
  bbox_size = array_ops.pack([bbox_h_size, bbox_w_size, -1])
  image = array_ops.slice(image, bbox_begin, bbox_size)

  # The first two dimensions are dynamic and unknown.
  image.set_shape([None, None, depth])
  return(image)
</code></pre>
"
